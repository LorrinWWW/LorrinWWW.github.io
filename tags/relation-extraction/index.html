<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Tag: relation-extraction - Jue&#039;s Blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Jue Wang (王珏)"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Jue Wang (王珏)"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="Effective and Efficient NLP"><meta property="og:type" content="blog"><meta property="og:title" content="Jue&#039;s Blog"><meta property="og:url" content="https://juewang.me/"><meta property="og:site_name" content="Jue&#039;s Blog"><meta property="og:description" content="Effective and Efficient NLP"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://juewang.me/img/og_image.png"><meta property="article:author" content="Jue Wang"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://juewang.me"},"headline":"Jue's Blog","image":["https://juewang.me/img/og_image.png"],"author":{"@type":"Person","name":"Jue Wang"},"publisher":{"@type":"Organization","name":"Jue's Blog","logo":{"@type":"ImageObject","url":"https://juewang.me/img/favicon.svg"}},"description":"Effective and Efficient NLP"}</script><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=UA-115582186-1" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'UA-115582186-1');</script><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="Jue's Blog" type="application/atom+xml">
</head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/favicon.svg" alt="Jue&#039;s Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/tags">Tags</a></li><li class="is-active"><a href="#" aria-current="page">relation-extraction</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2018-04-14T06:00:00.000Z" title="4/14/2018, 8:00:00 AM">2018-04-14</time></span><span class="level-item">Updated&nbsp;<time dateTime="2020-11-03T03:26:07.006Z" title="11/3/2020, 4:26:07 AM">2020-11-03</time></span><span class="level-item"><a class="link-muted" href="/categories/research/">research</a></span><span class="level-item">13 minutes read (About 1927 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/posts/%5B2018.4.14%5DReinforcement-Learning-for-Relation-Classification-from-Noisy-Data/">Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记</a></h1><div class="content"><h2 id="Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data-阅读笔记"><a href="#Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data-阅读笔记" class="headerlink" title="Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记"></a>Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记</h2><p><strong>摘要</strong>：现有关系分类方法依赖远程监督(distant supervision)，它假定提到实体对的句子都描述了这个实体对的关系。这样的方法一般在句子集合进行分类，不能识别关系和句子之间的映射，并且很大程度上受到标签噪音问题的影响。在这篇论文[^1]中，作者提出了一个从有噪声多数据的句子层次的关系分类模型。该模型有两个模块：一个实例选择器和一个关系分类器。实例选择器通过增强学习选择高质量的句子，并将选定的句子输入到关系分类器中，关系分类器进行句子级预测，并向实例选择器提供奖励。这两个模块共同训练以优化实例选择和关系分类过程。实验结果表明，我们的模型可以有效地处理数据中的噪音，并在句子级别获得更好的关系分类性能。</p>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>关系抽取是nlp领域中一个非常重要的任务，尤其在知识图谱构建等任务上。相关的工作可以参考我之前写的笔记，主要还是分为两种：传统的手工特征方法，和深度神经网络。</p>
<p>为了获得更大量的训练数据集，半监督、远程监督，甚至无监督模型被提出。半监督模型对一开始的少量数据要求较高，容易产生较大的偏差；无监督学习目前还没有比较成熟的解决方案。</p>
<p>这里主要提一下远程监督模型。远程监督模型有一个很强的假设：如果两个实体在给定的知识库中有一种关系，则包含这两个实体的所有句子都会提及该关系，实际上当然会有很大问题，会带来很多标注错误的信息。有一些解决方法就是转化为bag-level的关系标注。一个bag包含提及相同实体对的句子，但有可能描述不同的关系，如下图。</p>
<p><img src="https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-04-14-WX20180414-152317%402x.png" alt="X20180414-152317@2"></p>
<p>不过实际上还是会有问题：1. 不能处理句子级别的关系分类；2. 如果一个bag里的句子都不含知识库中的关系，即都是噪声，这样对性能会有很大影响。</p>
<p>为解决上述的两个缺陷，作者提出了实例选取器，并将其定义为一个强化学习任务。它有两个特征：1. 句子选择是一个反复试错的过程，需要从分类器中得到选取句子质量的反馈；2. 反馈在挑选结束后得到，因此是滞后的。这两点非常满足强化学习的特点。</p>
<h2 id="2-Methodology"><a href="#2-Methodology" class="headerlink" title="2. Methodology"></a>2. Methodology</h2></div><a class="article-more button is-small is-size-7" href="/posts/%5B2018.4.14%5DReinforcement-Learning-for-Relation-Classification-from-Noisy-Data/#more">Read more</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2018-01-14T07:00:00.000Z" title="1/14/2018, 8:00:00 AM">2018-01-14</time></span><span class="level-item">Updated&nbsp;<time dateTime="2020-11-03T03:26:13.796Z" title="11/3/2020, 4:26:13 AM">2020-11-03</time></span><span class="level-item"><a class="link-muted" href="/categories/research/">research</a></span><span class="level-item">13 minutes read (About 1902 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/posts/%5B2018.1.14%5DModels-for-relation-extraction/">几个 relation extraction 远程监督模型</a></h1><div class="content"><h2 id="几个-relation-extraction-远程监督模型"><a href="#几个-relation-extraction-远程监督模型" class="headerlink" title="几个 relation extraction 远程监督模型"></a>几个 relation extraction 远程监督模型</h2><p><strong>摘要：</strong>远程监督（Distant supervision）显著地减少了建立用于分类任务的训练集所需要的人工。但是这一项技术也会带来很大的噪音，并可能因此而大大地影响了模型的性能表现。这里，我们以 relation extraction 这项任务为例，深入讨论分析该噪声的分布。文献[1]提出了 dynamic-transition matrix，并证明了它能很好地代表了由 distant supervision 所带来的噪声。通过该矩阵，我们能够大大提高 relation extraction 的效果。文献[2]则是一种经典的方法，通过定义规则，定义否定模式（negative pattern）过滤掉一些噪音数据，可以很大程度提高性能。缺点是规则依赖人工定义，但是方法本身简单有效。文献[3]将 relation extraction 定义为一个 Multi-instance Multi-label 学习问题，一定程度上解决了错误标签的问题。</p>
<h3 id="1-Problem-of-distant-supervision"><a href="#1-Problem-of-distant-supervision" class="headerlink" title="1. Problem of distant supervision"></a>1. Problem of distant supervision</h3><p>Distant supervision 是一种生成关系抽取训练集的常用方法。它把现有知识库中的三元组 &lt;e1, r, e2&gt; （或写成&lt;subj, r, obj&gt;）作为种子，匹配同时含有 e1 和 e2 的文本，得到的文本用作关系 r 的标注数据。这样可以省去大量人工标记的工作。</p>
<p>但是，相比于人工标注方法，这种匹配方式会产生很多噪音：比如三元组&lt;DonaldTrump, born-in, New York&gt;，可能对齐到“Donald Trump was born in New York”，也可能对齐到“DonaldTrump worked in New York”。其中前一句是我们想要的标注数据，后一句则是噪音数据，它并不表示born-in关系。如何去除这些噪音数据，是一个重要的研究课题。</p>
<h3 id="2-Approaches-to-this-problems"><a href="#2-Approaches-to-this-problems" class="headerlink" title="2. Approaches to this problems"></a>2. Approaches to this problems</h3><ul>
<li> 拟合噪音</li>
<li>dynamic-transition matrix [1]</li>
<li> 去除噪音</li>
<li>通过定义规则过滤掉一些噪音数据[2]，缺点是依赖人工定义，并且被关系种类所限制。</li>
<li>Multi-instance learning[3], 把训练语句分包学习，包内取平均值，或者用 attention 加权，可以中和掉包内的噪音数据。缺点是受限于 at-least-one-assumption：每个包内至少有一个正确的数据。</li>
</ul>
<p>下面我们简单介绍这几个模型。</p>
<h4 id="2-1-Learning-with-dynamic-transition-matrix-1"><a href="#2-1-Learning-with-dynamic-transition-matrix-1" class="headerlink" title="2.1 Learning with dynamic-transition matrix [1]"></a>2.1 Learning with dynamic-transition matrix [1]</h4><p>文献[1] 提出了 dynamic-transition matrix，用于表达 Distant supervision 所产生的噪声。dynamic-transition matrix 可以通过基于 curriculum learning 的方法训练得到。通过该矩阵，我们能够大大提高 relation extraction 的效果，能够达到目前该领域的 state-of-the-art。</p></div><a class="article-more button is-small is-size-7" href="/posts/%5B2018.1.14%5DModels-for-relation-extraction/#more">Read more</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2018-01-04T07:00:00.000Z" title="1/4/2018, 8:00:00 AM">2018-01-04</time></span><span class="level-item">Updated&nbsp;<time dateTime="2020-11-03T03:26:12.569Z" title="11/3/2020, 4:26:12 AM">2020-11-03</time></span><span class="level-item"><a class="link-muted" href="/categories/research/">research</a></span><span class="level-item">14 minutes read (About 2154 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/posts/%5B2018.1.4%5DOvercoming-Limited-Supervision-in-Relation-Extraction/">Overcoming Limited Supervision in Relation Extraction 笔记</a></h1><div class="content"><p>这次主要阅读的论文是《Overcoming Limited Supervision in Relation Extraction: A Pattern-enhanced Distributional Representation Approach》[1]。该文主要针对了现有模型对标注数据的依赖，提出一种比较有意思的思路。基于分布的方法（distributional approach）利用两个实体共同出现的统计频率来预测他们的关系，需要大量标注数据，而基于模式的方法（pattern-based approach）一般使用神经网络建模，但这种方法需要更多的标注数据。本文同时建立两个模型，互相为对方提供监督。以分布模型作为判别模型，模式模型作为生成模型。训练过程中不断迭代，从而提升两个模型的性能。</p>
<p><img src="https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-201441.jpg" alt="illustration"></p>
<h3 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h3><h4 id="1-1-Weakly-Supervised-Learning"><a href="#1-1-Weakly-Supervised-Learning" class="headerlink" title="1.1 Weakly Supervised Learning"></a>1.1 Weakly Supervised Learning</h4><p>弱监督学习介于监督学习和无监督学习之间，它提供的标注数据带有较大的噪音，或标注的相对粗糙，标注结果可能出错。对于关系抽取而言，就是将一些关系实例作为seed，用它们从大型语料库中去除冗余信息并提取更多的实例。</p>
<p>弱监督学习的基本思路：</p>
<ol>
<li>用容易获得的标注替代较难获得的标注</li>
<li>选择最需要做精细标注的样例</li>
<li>模型训练和自动标注交替进行</li>
</ol>
<h4 id="1-2-Co-training-strategy"><a href="#1-2-Co-training-strategy" class="headerlink" title="1.2 Co-training strategy"></a>1.2 Co-training strategy</h4><p>以往的工作主要是单个模型，该文采用了co-training策略[2]，将两个模型互相协作，取得了比较好的效果。</p>
<p>co-training策略是一种半监督方法，核心就是利用少量已标记样本，通过两个（或多个）模型去学习，对未标记样本进行标记，挑选置信度最高的样本加入已标记样本阵营。</p></div><a class="article-more button is-small is-size-7" href="/posts/%5B2018.1.4%5DOvercoming-Limited-Supervision-in-Relation-Extraction/#more">Read more</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2017-12-17T07:00:00.000Z" title="12/17/2017, 8:00:00 AM">2017-12-17</time></span><span class="level-item">Updated&nbsp;<time dateTime="2020-11-03T03:26:14.244Z" title="11/3/2020, 4:26:14 AM">2020-11-03</time></span><span class="level-item"><a class="link-muted" href="/categories/research/">research</a></span><span class="level-item">13 minutes read (About 1884 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/posts/%5B2017.12.17%5DRelation-Classification-via-Attention-Model/">Relation Classification via Attention Model 笔记</a></h1><div class="content"><h2 id="Relation-Classification-via-Attention-Model"><a href="#Relation-Classification-via-Attention-Model" class="headerlink" title="Relation Classification via Attention Model"></a>Relation Classification via Attention Model</h2><p>这个笔记主要是阅读论文[1]，它的工作重点是在神经网络构成的端到端学习的关系抽取任务中加入Attention机制。作者主要通过自动学习关系句中注意力较高的部分，而引入attention机制，对反映实体关系更加重要的词语给予更大的attention，较好地提高了关系抽取的效果。</p>
<img src="https://github.com/lawlietAi/relation-classification-via-attention-model/raw/master/acnn_structure.png" width="50%">

<h3 id="1-Attention"><a href="#1-Attention" class="headerlink" title="1. Attention"></a>1. Attention</h3><h4 id="1-1-概述"><a href="#1-1-概述" class="headerlink" title="1.1 概述"></a>1.1 概述</h4><p>Attention机制最早是在视觉图像领域被提出来的。在NLP任务上，Bahdanau[2]等人使用类似attention的机制在机器翻译任务上将翻译和对齐同时进行。接着类似的基于attention机制的深度学习模型开始广泛应用到各种NLP任务中。</p>
<h4 id="1-2-Recurrent-Models-of-Visual-Attention"><a href="#1-2-Recurrent-Models-of-Visual-Attention" class="headerlink" title="1.2 Recurrent Models of Visual Attention"></a>1.2 Recurrent Models of Visual Attention</h4><p>人们在进行观察图像的时候，其实并不是一次就把整幅图像的每个位置像素都看过，大多是根据需求将注意力集中到图像的特定部分。由此，在传统的RNN上加入了attention机制，每次当前状态，都会根据前一个状态学习得到的要关注的位置和当前输入的图像，去处理注意力部分像素。可以看到应用Attention机制后，任务的复杂度被降低了很多。</p>
<h4 id="1-3-Attention-based-RNN-in-NLP"><a href="#1-3-Attention-based-RNN-in-NLP" class="headerlink" title="1.3 Attention-based RNN in NLP"></a>1.3 Attention-based RNN in NLP</h4><p>[1]的成果是在机器翻译任务，一般机器翻译工作由一个Encoder和一个Decoder构成，一个典型的Seq2seq任务。Encoder将源句子进行编码，再利用Decoder将编码后的向量解码成目标语言。</p></div><a class="article-more button is-small is-size-7" href="/posts/%5B2017.12.17%5DRelation-Classification-via-Attention-Model/#more">Read more</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2017-12-10T07:00:00.000Z" title="12/10/2017, 8:00:00 AM">2017-12-10</time></span><span class="level-item">Updated&nbsp;<time dateTime="2018-01-27T11:07:30.140Z" title="1/27/2018, 12:07:30 PM">2018-01-27</time></span><span class="level-item"><a class="link-muted" href="/categories/research/">research</a></span><span class="level-item">13 minutes read (About 1948 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/posts/%5B2017.12.10%5DEntity-resolution/">实体解析 Entity resolution</a></h1><div class="content"><h2 id="1-Entity-resolution"><a href="#1-Entity-resolution" class="headerlink" title="1. Entity resolution"></a>1. Entity resolution</h2><p>###1.1 Sequence labeling</p>
<p>我们通常在ML中把Named Entity Recognition任务认为是一个Sequence labeling任务，事实上很多nlp任务都可以被转化为sequence labeling。暑假实习的时候也在这方面看了一些文献。目前业内比较主流的解决方案是RNN-CRF模型，一般来说分为：</p>
<ul>
<li>Embedding layer</li>
<li>Bi-directional RNN (usually LSTM) layer</li>
<li>Tanh hidden layer</li>
<li>CRF layer</li>
</ul>
<p>从结果上来看，该模型对大多数sequence labeling任务有较好的效果，如named entity recognition等。但是对于一些更灵活的标注任务（如暑假实习时，我曾试图将event recognition转化为seq labeling任务），尤其是在训练集不足的情况下，往往效果还是不能令人满意。</p>
<h4 id="1-1-1-应用Attention"><a href="#1-1-1-应用Attention" class="headerlink" title="1.1.1 应用Attention"></a>1.1.1 应用Attention</h4><blockquote>
<p>[1]在 RNN-CRF 模型结构基础上，重点改进了词向量与字符向量的拼接。使用 attention 机制将原始的字符向量和词向量拼接改进为了权重求和，使用两层传统神经网络隐层来学习 attention 的权值，这样就使得模型可以动态地利用词向量和字符向量信息。实验结果表明比原始的拼接方法效果更好。</p>
<p>[2]在原始 BiLSTM-CRF 模型上，加入了音韵特征，并在字符向量上使用 attention 机制来学习关注更有效的字符。</p>
<p>​                      — from paperweekly</p>
</blockquote>
<h4 id="1-1-2-使用少量标注数据"><a href="#1-1-2-使用少量标注数据" class="headerlink" title="1.1.2 使用少量标注数据"></a>1.1.2 使用少量标注数据</h4><p>深度学习方法一般需要大量标注数据，但是在一些领域很难有海量的标注数据。所以在基于神经网络结构方法中如何使用少量标注数据也是一个重点。</p>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://openreview.net/forum?id=ry018WZAZ">Deep Active Learning for Named Entity Recognition</a>[7]</p>
<p>ICLR 2018看到的paper。这片文章把active learning应用到了CNN-CNN-LSTM模型，用于处理NER问题，也就是seq labeling问题。它能够仅使用25%的数据，达到state-of-the-art的水平。</p>
<p>这篇paper总结了很多做seq labeling的方法，本身的思路也深入简出。decoder使用了LSTM而不是常用的CRF，发现LSTM比CRF有一些的优势。同时该文也证明了active learning能提高seq labeling的表现。</p>
</li>
<li><p>Semi-supervised sequence tagging with bidirectional language models[4]</p>
<p>该论文使用海量无标注语料库训练了一个双向神经网络语言模型，然后使用这个训练好的语言模型来获取当前要标注词的语言模型向量（LM embedding），然后将该向量作为特征加入到原始的双向 RNN-CRF 模型中。</p>
<p>实验结果表明，在少量标注数据上，加入这个语言模型向量能够大幅度提高 NER 效果，即使在大量的标注训练数据上，加入这个语言模型向量仍能提供原始 RNN-CRF 模型的效果。</p>
</li>
</ul></div><a class="article-more button is-small is-size-7" href="/posts/%5B2017.12.10%5DEntity-resolution/#more">Read more</a></article></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/avatar.png" alt="Jue Wang"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Jue Wang</p><p class="is-size-6 is-block">Ph.D Student</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Hangzhou, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">16</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Category</p><a href="/categories"><p class="title">1</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">26</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/lorrinWWW" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/lorrinWWW/"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com/JueWANG26088228"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/atom.xml"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-04-21T14:00:00.000Z">2022-04-21</time></p><p class="title"><a href="/posts/about/">Jue Wang</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2018-08-22T11:58:00.000Z">2018-08-22</time></p><p class="title"><a href="/posts/%5B2018.8.22%5DRegEx-with-NN/">RegEx with NN</a></p><p class="categories"><a href="/categories/research/">research</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2018-07-01T18:59:58.000Z">2018-07-01</time></p><p class="title"><a href="/posts/intro-about-KG/">intro-about-KG</a></p><p class="categories"><a href="/categories/research/">research</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2018-05-10T06:00:00.000Z">2018-05-10</time></p><p class="title"><a href="/posts/%5B2018.5.10%5DKnowledge-Graph-Augmented-Neural-Networks-for-NLP/">Learning beyond datasets - Knowledge Graph Augmented Neural Networks for Natural language Processing 阅读笔记</a></p><p class="categories"><a href="/categories/research/">research</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2018-04-14T06:00:00.000Z">2018-04-14</time></p><p class="title"><a href="/posts/%5B2018.4.14%5DReinforcement-Learning-for-Relation-Classification-from-Noisy-Data/">Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记</a></p><p class="categories"><a href="/categories/research/">research</a></p></div></article></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/research/"><span class="level-start"><span class="level-item">research</span></span><span class="level-end"><span class="level-item tag">15</span></span></a></li></ul></div></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2022/04/"><span class="level-start"><span class="level-item">April 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/08/"><span class="level-start"><span class="level-item">August 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/07/"><span class="level-start"><span class="level-item">July 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/05/"><span class="level-start"><span class="level-item">May 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/04/"><span class="level-start"><span class="level-item">April 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/03/"><span class="level-start"><span class="level-item">March 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/02/"><span class="level-start"><span class="level-item">February 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/01/"><span class="level-start"><span class="level-item">January 2018</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2017/12/"><span class="level-start"><span class="level-item">December 2017</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/BiLSTM/"><span class="tag">BiLSTM</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CNN/"><span class="tag">CNN</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/KGC/"><span class="tag">KGC</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/LSTM/"><span class="tag">LSTM</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NLP/"><span class="tag">NLP</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/RNN/"><span class="tag">RNN</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/attention/"><span class="tag">attention</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/co-reference/"><span class="tag">co-reference</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/convolution/"><span class="tag">convolution</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/deep-learning/"><span class="tag">deep-learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/distant-supervision/"><span class="tag">distant-supervision</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/entity-resolution/"><span class="tag">entity-resolution</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/event-detection/"><span class="tag">event-detection</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/event-extraction/"><span class="tag">event-extraction</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/knowledge-graph/"><span class="tag">knowledge-graph</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/knowledge-reasoning/"><span class="tag">knowledge-reasoning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/limited-supervision/"><span class="tag">limited-supervision</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/machine-learning/"><span class="tag">machine-learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/neural-network/"><span class="tag">neural-network</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/regular-expression/"><span class="tag">regular-expression</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/reinforcement-learning/"><span class="tag">reinforcement-learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/relation-classification/"><span class="tag">relation-classification</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/relation-extraction/"><span class="tag">relation-extraction</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/sequence-labeling/"><span class="tag">sequence-labeling</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/summarization/"><span class="tag">summarization</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/weak-supervision/"><span class="tag">weak-supervision</span><span class="tag">1</span></a></div></div></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/favicon.svg" alt="Jue&#039;s Blog" height="28"></a><p class="is-size-7"><span>&copy; 2022 Jue Wang</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div><div id="divmap" style="visibility:hidden; position: absolute; top: 0px"><script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&amp;w=2&amp;t=n&amp;d=I_238wU69nrKH0DIm55b1z-y84aVsLuSzQSglFoJ1ww&amp;co=ffffff&amp;ct=ffffff&amp;cmo=ffffff&amp;cmn=ffffff"></script></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>