<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Relation Classification via Attention Model 笔记 - Jue&#039;s Blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Jue Wang (王珏)"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Jue Wang (王珏)"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="Relation Classification via Attention Model这个笔记主要是阅读论文[1]，它的工作重点是在神经网络构成的端到端学习的关系抽取任务中加入Attention机制。作者主要通过自动学习关系句中注意力较高的部分，而引入attention机制，对反映实体关系更加重要的词语给予更大的attention，较好地提高了关系抽取的效果。   1. Attention1.1"><meta property="og:type" content="blog"><meta property="og:title" content="Relation Classification via Attention Model 笔记"><meta property="og:url" content="https://juewang.me/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"><meta property="og:site_name" content="Jue&#039;s Blog"><meta property="og:description" content="Relation Classification via Attention Model这个笔记主要是阅读论文[1]，它的工作重点是在神经网络构成的端到端学习的关系抽取任务中加入Attention机制。作者主要通过自动学习关系句中注意力较高的部分，而引入attention机制，对反映实体关系更加重要的词语给予更大的attention，较好地提高了关系抽取的效果。   1. Attention1.1"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://github.com/lawlietAi/relation-classification-via-attention-model/raw/master/acnn_structure.png"><meta property="og:image" content="https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-201155.jpg"><meta property="og:image" content="https://github.com/lawlietAi/relation-classification-via-attention-model/raw/master/acnn_structure.png"><meta property="og:image" content="https://pic3.zhimg.com/50/v2-2399a406ad0960c422702728b6418fa3_hd.jpg"><meta property="article:published_time" content="2017-12-17T07:00:00.000Z"><meta property="article:modified_time" content="2020-11-03T03:26:14.244Z"><meta property="article:author" content="Jue Wang"><meta property="article:tag" content="relation-extraction"><meta property="article:tag" content="relation-classification"><meta property="article:tag" content="attention"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://github.com/lawlietAi/relation-classification-via-attention-model/raw/master/acnn_structure.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://juewang.me/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},"headline":"Relation Classification via Attention Model 笔记","image":["https://github.com/lawlietAi/relation-classification-via-attention-model/raw/master/acnn_structure.png","https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-201155.jpg","https://github.com/lawlietAi/relation-classification-via-attention-model/raw/master/acnn_structure.png","https://pic3.zhimg.com/50/v2-2399a406ad0960c422702728b6418fa3_hd.jpg"],"datePublished":"2017-12-17T07:00:00.000Z","dateModified":"2020-11-03T03:26:14.244Z","author":{"@type":"Person","name":"Jue Wang"},"publisher":{"@type":"Organization","name":"Jue's Blog","logo":{"@type":"ImageObject","url":"https://juewang.me/img/favicon.svg"}},"description":"Relation Classification via Attention Model这个笔记主要是阅读论文[1]，它的工作重点是在神经网络构成的端到端学习的关系抽取任务中加入Attention机制。作者主要通过自动学习关系句中注意力较高的部分，而引入attention机制，对反映实体关系更加重要的词语给予更大的attention，较好地提高了关系抽取的效果。   1. Attention1.1"}</script><link rel="canonical" href="https://juewang.me/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=UA-115582186-1" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'UA-115582186-1');</script><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="Jue's Blog" type="application/atom+xml">
</head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/favicon.svg" alt="Jue&#039;s Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2017-12-17T07:00:00.000Z" title="12/17/2017, 8:00:00 AM">2017-12-17</time></span><span class="level-item">Updated&nbsp;<time dateTime="2020-11-03T03:26:14.244Z" title="11/3/2020, 4:26:14 AM">2020-11-03</time></span><span class="level-item"><a class="link-muted" href="/categories/research/">research</a></span><span class="level-item">13 minutes read (About 1884 words)</span></div></div><h1 class="title is-3 is-size-4-mobile">Relation Classification via Attention Model 笔记</h1><div class="content"><h2 id="Relation-Classification-via-Attention-Model"><a href="#Relation-Classification-via-Attention-Model" class="headerlink" title="Relation Classification via Attention Model"></a>Relation Classification via Attention Model</h2><p>这个笔记主要是阅读论文[1]，它的工作重点是在神经网络构成的端到端学习的关系抽取任务中加入Attention机制。作者主要通过自动学习关系句中注意力较高的部分，而引入attention机制，对反映实体关系更加重要的词语给予更大的attention，较好地提高了关系抽取的效果。</p>
<img src="https://github.com/lawlietAi/relation-classification-via-attention-model/raw/master/acnn_structure.png" width="50%">

<h3 id="1-Attention"><a href="#1-Attention" class="headerlink" title="1. Attention"></a>1. Attention</h3><h4 id="1-1-概述"><a href="#1-1-概述" class="headerlink" title="1.1 概述"></a>1.1 概述</h4><p>Attention机制最早是在视觉图像领域被提出来的。在NLP任务上，Bahdanau[2]等人使用类似attention的机制在机器翻译任务上将翻译和对齐同时进行。接着类似的基于attention机制的深度学习模型开始广泛应用到各种NLP任务中。</p>
<h4 id="1-2-Recurrent-Models-of-Visual-Attention"><a href="#1-2-Recurrent-Models-of-Visual-Attention" class="headerlink" title="1.2 Recurrent Models of Visual Attention"></a>1.2 Recurrent Models of Visual Attention</h4><p>人们在进行观察图像的时候，其实并不是一次就把整幅图像的每个位置像素都看过，大多是根据需求将注意力集中到图像的特定部分。由此，在传统的RNN上加入了attention机制，每次当前状态，都会根据前一个状态学习得到的要关注的位置和当前输入的图像，去处理注意力部分像素。可以看到应用Attention机制后，任务的复杂度被降低了很多。</p>
<h4 id="1-3-Attention-based-RNN-in-NLP"><a href="#1-3-Attention-based-RNN-in-NLP" class="headerlink" title="1.3 Attention-based RNN in NLP"></a>1.3 Attention-based RNN in NLP</h4><p>[1]的成果是在机器翻译任务，一般机器翻译工作由一个Encoder和一个Decoder构成，一个典型的Seq2seq任务。Encoder将源句子进行编码，再利用Decoder将编码后的向量解码成目标语言。</p>
<p>我们在求注意力分配概率分布的时候，对于输入句子中任意一个单词都给出个概率，从而得到一个概率分布，再对输入句子所有单词的概率进行加权求和，得到Decoder的注意力分配。如下图。</p>
<img src="https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-201155.jpg" width="30%">

<p>另一个扩展性更好的论文是[3]，他们的工作告诉了大家attention在RNN中可以如何进行扩展。</p>
<h4 id="1-4-Attention-based-CNN-in-NLP"><a href="#1-4-Attention-based-CNN-in-NLP" class="headerlink" title="1.4 Attention-based CNN in NLP"></a>1.4 Attention-based CNN in NLP</h4><p>[4]这篇论文研究的是两个CNN网络，分别处理两个句子，最后输入到分类器中处理。但是这样的模型在输入分类器前句对间是没有相互联系的，作者就想通过设计attention机制将不同cnn通道的句对联系起来。于是提出了3中在CNN中使用attention的方法。</p>
<ul>
<li>ABCNN-1: 在卷积前进行attention，通过attention矩阵计算出相应句对的attention feature map，然后连同原来的feature map一起输入到卷积层。</li>
<li>ABCNN-2: 在池化时进行attention，通过attention对卷积后的表达重新加权，然后再进行池化.</li>
<li>ABCNN-3: ABCNN-1 + ABCNN-2</li>
</ul>
<h3 id="2-Relation-Classification"><a href="#2-Relation-Classification" class="headerlink" title="2. Relation Classification"></a>2. Relation Classification</h3><img src="https://github.com/lawlietAi/relation-classification-via-attention-model/raw/master/acnn_structure.png" width="50%">

<h4 id="2-1-Classification-Objective"><a href="#2-1-Classification-Objective" class="headerlink" title="2.1 Classification Objective"></a>2.1 Classification Objective</h4><p>作者提出了一种距离函数，即正则化向量差的L2范数：<br>$$<br>\delta_{\theta}(S,y) = ||\frac{w^O}{|w^O|} - W_y^L||<em>{L^2} \<br>S:\text{Sentence}, y:\text{Output relation}, w^O: \text{Network output}, W^L:\text{Relation embedding}<br>$$<br>基于此，作者定义了目标函数：<br>$$<br>\mathcal{L} = [\delta_\theta(S,y) + (1-\delta_\theta(S, \hat{y}^-))] + \beta||\theta||^2 \<br>\hat{y}^- : \text{A selected incorrect relation label chosen as the one with the highest score among all i.e.} \<br>\hat{y}^- = argmax</em>{y’\in \mathcal{Y},y’\ne y}(\delta(S, y’))<br>$$<br>目标中的两个距离分别为网络输出向量与正例和与某负例的距离，该负例是所有错误类别中与该输出最接近的。最后加上一个正则项，通过使该目标函数最小化来训练网络中的各参数，$\beta$用于控制其比重。</p>
<h4 id="2-2-Input-Representation"><a href="#2-2-Input-Representation" class="headerlink" title="2.2 Input Representation"></a>2.2 Input Representation</h4><p>现有句子，以及两个已知的实体e1,e2：<br>$$<br>S = (w_1,w_2,…,w_n) \<br>e_1 := w_p, e_2 := w_t . p,t\in [1,n], p\ne t<br>$$<br>为了得到它们的关系，我们把所有词转为词向量；并且根据每个词与实体的相对位置，也转为word position embeddings，每个词与两个实体有两个相对位置，所以得到第i个词的Embedding：<br>$$<br>w_i^M = [(w_i^d)^T, (w_{i,2}^p)^T,(w_{i,2}^p)^T]^T<br>$$<br>为了充分得到上下文的信息，再考虑大小为k的滑窗，得到最终的input representation<br>$$<br>z_i = [(w_{i - (k-1)/2}^M)^T,…,(w_{i + (k-1)/2}^M)^T]^T<br>$$</p>
<h4 id="2-3-Input-Attention-Mechanism"><a href="#2-3-Input-Attention-Mechanism" class="headerlink" title="2.3 Input Attention Mechanism"></a>2.3 Input Attention Mechanism</h4><img src="https://pic3.zhimg.com/50/v2-2399a406ad0960c422702728b6418fa3_hd.jpg" width="70%">

<p>输入级的attention机制是设计两个关于实体对上下文相关的对角矩阵，该矩阵中各元素反映该词语与给定实体间联系的强弱，如$A_{i,i}^j=f(e_j,w_i)$反映了wi和ej之间的联系强弱，这里作者给的 f 就是内积。我们定义：<br>$$<br>\alpha_i^j = \frac{exp(A_{i,i}^j)}{\sum_{i’=1}^{n}{exp(A_{i’,i}^j)}}<br>$$<br>对于j=1,2 两个相关因子，作者提出了三种处理方式:</p>
<ul>
<li><p>平均<br>$$<br>r_i = z_i \frac{\alpha_i^1 + \alpha_i^2}{2}<br>$$</p>
</li>
<li><p>串联<br>$$<br>r_i = [(z_i \alpha_i^1)^T, (z_i \alpha_i^2)^T]^T<br>$$</p>
</li>
<li><p>距离<br>$$<br>r_i = z_i \frac{\alpha_i^1 - \alpha_i^2}{2}<br>$$</p>
</li>
</ul>
<p>最终得到$R = [r_1, r_2,…,r_n]$</p>
<h4 id="2-4-Convolutional-Max-Pooling-with-Secondary-Attention"><a href="#2-4-Convolutional-Max-Pooling-with-Secondary-Attention" class="headerlink" title="2.4 Convolutional Max-Pooling with Secondary Attention"></a>2.4 Convolutional Max-Pooling with Secondary Attention</h4><p>将前面得到的矩阵R送入卷积核大小为dc的卷积层，卷积操作可形式化表示为:<br>$$<br>R^\star = tanh(W_fR+B_f), \text{where the siaze of Wf is } d^c \times k(d^w+2d^p)<br>$$<br>然后构建一个相关性矩阵来捕获卷积层输出R*与实体关系WL之间的联系<br>$$<br>G = R^{\star T}UW^L, \U :\text{weighting matrix learnt by the network}<br>$$</p>
<p>再用softmax函数来处理相关性矩阵G，获得attention pooling matrix Ap:<br>$$<br>A_{i,j}^p = \frac{exp(G_{i,j})}{\sum_{i’=1}^n{exp(G_{i’,j})}}<br>$$<br>最后用Ap与卷积层输出R*相乘，也就是加入混合中的attention，然后取出每一维度的最大值，得到网络的输出<br>$$<br>w_i^O = max_j(R^\star A^p)_{i,j}<br>$$</p>
<h3 id="3-总结"><a href="#3-总结" class="headerlink" title="3. 总结"></a>3. 总结</h3><p>从[1]中提到的结果上看，attention的表现确实是在重要的词上有更好的权重，在Sem-Eval-2010 Task 8数据集上取得了显著的效果提升。对于关系抽取来说无疑是非常大的一个进步。</p>
<p>但是还是有一些不足：</p>
<ul>
<li>它要求实体已知，因此需要其他工作来完成实体的识别，使得一些信息的丢失以及错误累加。此时并行模型或端到端模型，同时完成实体识别可能效果会更好；</li>
<li>关系是事先定义的集合，因此更多的是对关系的分类，若能启发式地抽取关系可能会有更广的应用空间；</li>
<li>对于一些上下文没有明显帮助的隐式关系或是使用了比喻之类的修辞，较为容易出错。</li>
</ul>
<p>这次选择读这篇文章也是想更具体地了解Attention机制，同时了解一些关系抽取的方案，它也有一个pytorch版本的<a target="_blank" rel="noopener" href="https://github.com/lawlietAi/relation-classification-via-attention-model">实现</a>，可以辅以参考。</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>笔记部分参考<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/22867750">https://zhuanlan.zhihu.com/p/22867750</a></p>
<p>[1] Wang, L., Cao, Z., Melo, G. D., &amp; Liu, Z. (2016). Relation Classification via Multi-Level Attention CNNs. <em>Meeting of the Association for Computational Linguistics</em> (pp.1298-1307).</p>
<p>[2] Bahdanau, D., Cho, K., &amp; Bengio, Y. (2014). Neural machine translation by jointly learning to align and translate. <em>Computer Science</em>.</p>
<p>[3] Luong, M. T., Pham, H., &amp; Manning, C. D. (2015). Effective approaches to attention-based neural machine translation. <em>Computer Science</em>.</p>
<p>[4] Yin, W., Schütze, H., Xiang, B., &amp; Zhou, B. (2015). Abcnn: attention-based convolutional neural network for modeling sentence pairs. <em>Computer Science</em>.</p>
</div><div class="article-licensing box"><div class="licensing-title"><p>Relation Classification via Attention Model 笔记</p><p><a href="https://juewang.me/posts/[2017.12.17]Relation-Classification-via-Attention-Model/">https://juewang.me/posts/[2017.12.17]Relation-Classification-via-Attention-Model/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Jue Wang</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2017-12-17</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2020-11-03</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/relation-extraction/">relation-extraction</a><a class="link-muted mr-2" rel="tag" href="/tags/relation-classification/">relation-classification</a><a class="link-muted mr-2" rel="tag" href="/tags/attention/">attention</a></div><div class="sharethis-inline-share-buttons"></div><script src="https://platform-api.sharethis.com/js/sharethis.js#property=610cdfe06a9baf001281bbf4&amp;product=inline-share-buttons" defer></script></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/posts/%5B2018.1.4%5DOvercoming-Limited-Supervision-in-Relation-Extraction/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">Overcoming Limited Supervision in Relation Extraction 笔记</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/posts/%5B2017.12.10%5DEntity-resolution/"><span class="level-item">实体解析 Entity resolution</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><div id="comment-container"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.min.js"></script><script>var gitalk = new Gitalk({
            id: "1715114a711b42bed3ea686fe18d1089",
            repo: "LorrinWWW.github.io",
            owner: "lorrinWWW",
            clientID: "bc305d76488227d8c5cf",
            clientSecret: "f7a2801a15ead49e3258304a0cf20f249f3962f3",
            admin: ["lorrinWWW"],
            createIssueManually: false,
            distractionFreeMode: false,
            perPage: 10,
            pagerDirection: "last",
            
            
            enableHotKey: true,
            
        })
        gitalk.render('comment-container')</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/avatar.png" alt="Jue Wang"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Jue Wang</p><p class="is-size-6 is-block">Ph.D Student</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Hangzhou, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">16</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Category</p><a href="/categories"><p class="title">1</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">26</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/lorrinWWW" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/lorrinWWW/"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com/JueWANG26088228"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/atom.xml"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-04-21T14:00:00.000Z">2022-04-21</time></p><p class="title"><a href="/posts/about/">Jue Wang</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2018-08-22T11:58:00.000Z">2018-08-22</time></p><p class="title"><a href="/posts/%5B2018.8.22%5DRegEx-with-NN/">RegEx with NN</a></p><p class="categories"><a href="/categories/research/">research</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2018-07-01T18:59:58.000Z">2018-07-01</time></p><p class="title"><a href="/posts/intro-about-KG/">intro-about-KG</a></p><p class="categories"><a href="/categories/research/">research</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2018-05-10T06:00:00.000Z">2018-05-10</time></p><p class="title"><a href="/posts/%5B2018.5.10%5DKnowledge-Graph-Augmented-Neural-Networks-for-NLP/">Learning beyond datasets - Knowledge Graph Augmented Neural Networks for Natural language Processing 阅读笔记</a></p><p class="categories"><a href="/categories/research/">research</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2018-04-14T06:00:00.000Z">2018-04-14</time></p><p class="title"><a href="/posts/%5B2018.4.14%5DReinforcement-Learning-for-Relation-Classification-from-Noisy-Data/">Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记</a></p><p class="categories"><a href="/categories/research/">research</a></p></div></article></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/research/"><span class="level-start"><span class="level-item">research</span></span><span class="level-end"><span class="level-item tag">15</span></span></a></li></ul></div></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2022/04/"><span class="level-start"><span class="level-item">April 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/08/"><span class="level-start"><span class="level-item">August 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/07/"><span class="level-start"><span class="level-item">July 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/05/"><span class="level-start"><span class="level-item">May 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/04/"><span class="level-start"><span class="level-item">April 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/03/"><span class="level-start"><span class="level-item">March 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/02/"><span class="level-start"><span class="level-item">February 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/01/"><span class="level-start"><span class="level-item">January 2018</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2017/12/"><span class="level-start"><span class="level-item">December 2017</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/BiLSTM/"><span class="tag">BiLSTM</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CNN/"><span class="tag">CNN</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/KGC/"><span class="tag">KGC</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/LSTM/"><span class="tag">LSTM</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NLP/"><span class="tag">NLP</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/RNN/"><span class="tag">RNN</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/attention/"><span class="tag">attention</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/co-reference/"><span class="tag">co-reference</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/convolution/"><span class="tag">convolution</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/deep-learning/"><span class="tag">deep-learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/distant-supervision/"><span class="tag">distant-supervision</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/entity-resolution/"><span class="tag">entity-resolution</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/event-detection/"><span class="tag">event-detection</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/event-extraction/"><span class="tag">event-extraction</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/knowledge-graph/"><span class="tag">knowledge-graph</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/knowledge-reasoning/"><span class="tag">knowledge-reasoning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/limited-supervision/"><span class="tag">limited-supervision</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/machine-learning/"><span class="tag">machine-learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/neural-network/"><span class="tag">neural-network</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/regular-expression/"><span class="tag">regular-expression</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/reinforcement-learning/"><span class="tag">reinforcement-learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/relation-classification/"><span class="tag">relation-classification</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/relation-extraction/"><span class="tag">relation-extraction</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/sequence-labeling/"><span class="tag">sequence-labeling</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/summarization/"><span class="tag">summarization</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/weak-supervision/"><span class="tag">weak-supervision</span><span class="tag">1</span></a></div></div></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/favicon.svg" alt="Jue&#039;s Blog" height="28"></a><p class="is-size-7"><span>&copy; 2022 Jue Wang</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div><div id="divmap" style="visibility:hidden; position: absolute; top: 0px"><script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&amp;w=2&amp;t=n&amp;d=I_238wU69nrKH0DIm55b1z-y84aVsLuSzQSglFoJ1ww&amp;co=ffffff&amp;ct=ffffff&amp;cmo=ffffff&amp;cmn=ffffff"></script></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>