<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>nlp short reviews - week 1 - Jue Wang</title><meta name="robots" content="noindex"><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Jue Wang (王珏)"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Jue Wang (王珏)"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="9.21Robust Spoken Language Understanding via Paraphrasing. (arXiv:1809.06444v1 [cs.CL])Abstract: Learning intents and slot labels from user utterances is a fundamental step in all spoken language unde"><meta property="og:type" content="blog"><meta property="og:title" content="nlp short reviews - week 1"><meta property="og:url" content="https://lorrinwww.github.io/posts/[2018.9]nlp-short-reviews-week-1/"><meta property="og:site_name" content="Jue Wang"><meta property="og:description" content="9.21Robust Spoken Language Understanding via Paraphrasing. (arXiv:1809.06444v1 [cs.CL])Abstract: Learning intents and slot labels from user utterances is a fundamental step in all spoken language unde"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://lorrinwww.github.io/img/og_image.png"><meta property="article:published_time" content="2018-09-18T18:24:29.000Z"><meta property="article:modified_time" content="2022-09-27T13:09:07.036Z"><meta property="article:author" content="Jue Wang"><meta property="article:tag" content="nlp"><meta property="article:tag" content="review"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://lorrinwww.github.io/posts/[2018.9]nlp-short-reviews-week-1/"},"headline":"nlp short reviews - week 1","image":["https://lorrinwww.github.io/img/og_image.png"],"datePublished":"2018-09-18T18:24:29.000Z","dateModified":"2022-09-27T13:09:07.036Z","author":{"@type":"Person","name":"Jue Wang"},"publisher":{"@type":"Organization","name":"Jue Wang","logo":{"@type":"ImageObject","url":"https://lorrinwww.github.io/img/favicon.svg"}},"description":"9.21Robust Spoken Language Understanding via Paraphrasing. (arXiv:1809.06444v1 [cs.CL])Abstract: Learning intents and slot labels from user utterances is a fundamental step in all spoken language unde"}</script><link rel="canonical" href="https://lorrinwww.github.io/posts/[2018.9]nlp-short-reviews-week-1/"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons/css/academicons.min.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=UA-115582186-1" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'UA-115582186-1');</script><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="Jue Wang" type="application/atom+xml">
</head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/favicon.svg" alt="Jue Wang" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/posts/about/">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2018-09-18T18:24:29.000Z" title="9/18/2018, 11:24:29 AM">2018-09-18</time></span><span class="level-item">Updated&nbsp;<time dateTime="2022-09-27T13:09:07.036Z" title="9/27/2022, 6:09:07 AM">2022-09-27</time></span><span class="level-item"><a class="link-muted" href="/categories/research/">research</a></span><span class="level-item">12 minutes read (About 1776 words)</span></div></div><h1 class="title is-3 is-size-4-mobile">nlp short reviews - week 1</h1><div class="content"><h2 id="9-21"><a href="#9-21" class="headerlink" title="9.21"></a>9.21</h2><h3 id="Robust-Spoken-Language-Understanding-via-Paraphrasing-arXiv-1809-06444v1-cs-CL"><a href="#Robust-Spoken-Language-Understanding-via-Paraphrasing-arXiv-1809-06444v1-cs-CL" class="headerlink" title="Robust Spoken Language Understanding via Paraphrasing. (arXiv:1809.06444v1 [cs.CL])"></a>Robust Spoken Language Understanding via Paraphrasing. (arXiv:1809.06444v1 [cs.CL])</h3><p><em><strong>Abstract:</strong></em> Learning intents and slot labels from user utterances is a fundamental step in all spoken language understanding (SLU) and dialog systems. State-of-the-art neural network based methods, after deployment, often suffer from performance degradation on encountering paraphrased utterances, and out-of-vocabulary words, rarely observed in their training set. We address this challenging problem by introducing a novel paraphrasing based SLU model which can be integrated with any existing SLU model in order to improve their overall performance. We propose two new paraphrase generators using RNN and sequence-to-sequence based neural networks, which are suitable for our application. Our experiments on existing benchmark and in house datasets demonstrate the robustness of our models to rare and complex paraphrased utterances, even under adversarial test distributions.</p>
<p><em><strong>Comment:</strong></em>  一篇利用句子改写优化SLU的文章。目前基于神经网络的state-of-the-art需要大量的语料，并且对语料库中少见的说法支持较差，因此我们寄希望于句子改写。在state-of-the-art的基础上，如果产生的结果置信度低于阈值，则尝试使用句子改写，从而让SLU模型更好地工作。具体来讲，这里我们把第一次置信度较低的“O”label替换成《?》，再将《?》填充成常见的语句。但是最后的试验未能表现出较大的进步，推测句子改写仅仅只是改变几个word，对整句的判断影响并不大；另一方面它不能对句子结构进行改写，因此提升有限。</p>
<h2 id="9-20"><a href="#9-20" class="headerlink" title="9.20"></a>9.20</h2><h3 id="User-Information-Augmented-Semantic-Frame-Parsing-using-Coarse-to-Fine-Neural-Networks-arXiv-1809-06559v1-cs-CL"><a href="#User-Information-Augmented-Semantic-Frame-Parsing-using-Coarse-to-Fine-Neural-Networks-arXiv-1809-06559v1-cs-CL" class="headerlink" title="User Information Augmented Semantic Frame Parsing using Coarse-to-Fine Neural Networks. (arXiv:1809.06559v1 [cs.CL])"></a>User Information Augmented Semantic Frame Parsing using Coarse-to-Fine Neural Networks. (arXiv:1809.06559v1 [cs.CL])</h3><p><em><strong>Abstract:</strong></em> Semantic frame parsing is a crucial component in spoken language understanding (SLU) to build spoken dialog systems. It has two main tasks: intent detection and slot filling. Although state-of-the-art approaches showed good results, they require large annotated training data and long training time. In this paper, we aim to alleviate these drawbacks for semantic frame parsing by utilizing the ubiquitous user information. We design a novel coarse-to-fine deep neural network model to incorporate prior knowledge of user information intermediately to better and quickly train a semantic frame parser. Due to the lack of benchmark dataset with real user information, we synthesize the simplest type of user information (location and time) on ATIS benchmark data. The results show that our approach leverages such simple user information to outperform state-of-the-art approaches by 0.25% for intent detection and 0.31% for slot filling using standard training data. When using smaller training data, the performance improvement on intent detection and slot filling reaches up to 1.35% and 1.20% respectively. We also show that our approach can achieve similar performance as state-of-the-art approaches by using less than 80% annotated training data. Moreover, the training time to achieve the similar performance is also reduced by over 60%.</p>
<p><em><strong>Comment:</strong></em>  工作相关。目前intent classification和slot filling的state-of-the-art是attention-based-biLSTM，这篇文章的创新点在于利用了用户的信息，并将其融合进上下文，使模型对数据的依赖减少并让结果更加准确。但是似乎提升比较有限，不过思考的方向可以参考。</p>
<h3 id="Learning-Universal-Sentence-Representations-with-Mean-Max-Attention-Autoencoder-arXiv-1809-06590v1-cs-CL"><a href="#Learning-Universal-Sentence-Representations-with-Mean-Max-Attention-Autoencoder-arXiv-1809-06590v1-cs-CL" class="headerlink" title="Learning Universal Sentence Representations with Mean-Max Attention Autoencoder. (arXiv:1809.06590v1 [cs.CL])"></a>Learning Universal Sentence Representations with Mean-Max Attention Autoencoder. (arXiv:1809.06590v1 [cs.CL])</h3><p><em><strong>Abstract:</strong></em> In order to learn universal sentence representations, previous methods focus on complex recurrent neural networks or supervised learning. In this paper, we propose a mean-max attention autoencoder (mean-max AAE) within the encoder-decoder framework. Our autoencoder rely entirely on the MultiHead self-attention mechanism to reconstruct the input sequence. In the encoding we propose a mean-max strategy that applies both mean and max pooling operations over the hidden vectors to capture diverse information of the input. To enable the information to steer the reconstruction process dynamically, the decoder performs attention over the mean-max representation. By training our model on a large collection of unlabelled data, we obtain high-quality representations of sentences. Experimental results on a broad range of 10 transfer tasks demonstrate that our model outperforms the state-of-the-art unsupervised single methods, including the classical skip-thoughts and the advanced skip-thoughts+LN model. Furthermore, compared with the traditional recurrent neural network, our mean-max AAE greatly reduce the training time.</p>
<p><em><strong>Comment:</strong></em>  提出了一种mean-max AAE，用self-attention和mean-max pooling组成encoder和decoder，最后的结果在无监督单模型中达到了state-of-the-art，并且能够有效利用并行计算，使训练过程非常快。</p>
<h3 id="Transfer-and-Multi-Task-Learning-for-Noun-Noun-Compound-Interpretation-arXiv-1809-06748v1-cs-CL"><a href="#Transfer-and-Multi-Task-Learning-for-Noun-Noun-Compound-Interpretation-arXiv-1809-06748v1-cs-CL" class="headerlink" title="Transfer and Multi-Task Learning for Noun-Noun Compound Interpretation. (arXiv:1809.06748v1 [cs.CL])"></a>Transfer and Multi-Task Learning for Noun-Noun Compound Interpretation. (arXiv:1809.06748v1 [cs.CL])</h3><p><em><strong>Abstract:</strong></em> In this paper, we empirically evaluate the utility of transfer and multi-task learning on a challenging semantic classification task: semantic interpretation of noun–noun compounds. Through a comprehensive series of experiments and in-depth error analysis, we show that transfer learning via parameter initialization and multi-task learning via parameter sharing can help a neural classification model generalize over a highly skewed distribution of relations. Further, we demonstrate how dual annotation with two distinct sets of relations over the same set of compounds can be exploited to improve the overall accuracy of a neural classifier and its F1 scores on the less frequent, but more difficult relations.</p>
<p><em><strong>Comment:</strong></em> 这是一篇interpretation，主要分析transfer learning和multi-task learning对于noun-noun compound。一般来说像relation extraction等任务主要研究的也是名词与名词的关系，文中提到的方法对于很多信息抽取任务都可以参考一下。</p>
<h2 id="9-18"><a href="#9-18" class="headerlink" title="9.18"></a>9.18</h2><h3 id="Learning-to-Accept-New-Classes-without-Training-arXiv-1809-06004v1-cs-CL"><a href="#Learning-to-Accept-New-Classes-without-Training-arXiv-1809-06004v1-cs-CL" class="headerlink" title="Learning to Accept New Classes without Training. (arXiv:1809.06004v1 [cs.CL])]"></a>Learning to Accept New Classes without Training. (arXiv:1809.06004v1 [cs.CL])]</h3><p><em><strong>Abstract:</strong></em> Classic supervised learning makes the closed-world assumption, meaning that classes seen in testing must have been seen in training. However, in the dynamic world, new or unseen class examples may appear constantly. A model working in such an environment must be able to reject unseen classes (not seen or used in training). If enough data is collected for the unseen classes, the system should incrementally learn to accept/classify them. This learning paradigm is called open-world learning (OWL). Existing OWL methods all need some form of re-training to accept or include the new classes in the overall model. In this paper, we propose a meta-learning approach to the problem. Its key novelty is that it only needs to train a meta-classifier, which can then continually accept new classes when they have enough labeled data for the meta-classifier to use, and also detect/reject future unseen classes. No re-training of the meta-classifier or a new overall classifier covering all old and new classes is needed. In testing, the method only uses the examples of the seen classes (including the newly added classes) on-the-fly for classification and rejection. Experimental results demonstrate the effectiveness of the new approach.</p>
<p><em><strong>Comment:</strong></em> 想法还是值得借鉴的，用一个meta-classifier来避免新类需要重新训练、缺少数据集等问题。但是最后的实现有点像knn，这样纯粹的非监督学习总觉得效果可能还不能达到现有监督学习。可以考虑怎么把meta-classifier加到常见的有监督学习模型中，不降低准确率的情况下，提高泛化效果。</p>
<h3 id="Events-Beyond-ACE-Curated-Training-for-Events-arXiv-1809-05576v1-cs-CL"><a href="#Events-Beyond-ACE-Curated-Training-for-Events-arXiv-1809-05576v1-cs-CL" class="headerlink" title="Events Beyond ACE: Curated Training for Events. (arXiv:1809.05576v1 [cs.CL])"></a>Events Beyond ACE: Curated Training for Events. (arXiv:1809.05576v1 [cs.CL])</h3><p><em><strong>Abstract:</strong></em> We explore a human-driven approach to annotation, curated training (CT), in which annotation is framed as teaching the system by using interactive search to identify informative snippets of text to annotate, unlike traditional approaches which either annotate preselected text or use active learning. A trained annotator performed 80 hours of CT for the thirty event types of the NIST TAC KBP Event Argument Extraction evaluation. Combining this annotation with ACE results in a 6% reduction in error and the learning curve of CT plateaus more slowly than for full-document annotation. 3 NLP researchers performed CT for one event type and showed much sharper learning curves with all three exceeding ACE performance in less than ninety minutes, suggesting that CT can provide further benefits when the annotator deeply understands the system.</p>
<p><em><strong>Comment:</strong></em> 模型驱动标注，使得标注效率更高，mark。</p>
<h3 id="Extending-Neural-Generative-Conversational-Model-using-External-Knowledge-Sources-arXiv-1809-05524v1-cs-CL"><a href="#Extending-Neural-Generative-Conversational-Model-using-External-Knowledge-Sources-arXiv-1809-05524v1-cs-CL" class="headerlink" title="Extending Neural Generative Conversational Model using External Knowledge Sources. (arXiv:1809.05524v1 [cs.CL])"></a>Extending Neural Generative Conversational Model using External Knowledge Sources. (arXiv:1809.05524v1 [cs.CL])</h3><p><em><strong>Abstract:</strong></em> The use of connectionist approaches in conversational agents has been progressing rapidly due to the availability of large corpora. However current generative dialogue models often lack coherence and are content poor. This work proposes an architecture to incorporate unstructured knowledge sources to enhance the next utterance prediction in chit-chat type of generative dialogue models. We focus on Sequence-to-Sequence (Seq2Seq) conversational agents trained with the Reddit News dataset, and consider incorporating external knowledge from Wikipedia summaries as well as from the NELL knowledge base. Our experiments show faster training time and improved perplexity when leveraging external knowledge.</p>
<p><em><strong>Comment:</strong></em> 近期这一类利用外部知识库的paper挺多的，到时候需要用到知识图谱的时候都可以参考一下。</p>
<!-- flag of hidden posts --></div><div class="article-licensing box"><div class="licensing-title"><p>nlp short reviews - week 1</p><p><a href="https://lorrinwww.github.io/posts/[2018.9]nlp-short-reviews-week-1/">https://lorrinwww.github.io/posts/[2018.9]nlp-short-reviews-week-1/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Jue Wang</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2018-09-18</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2022-09-27</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/nlp/">nlp</a><a class="link-muted mr-2" rel="tag" href="/tags/review/">review</a></div><div class="sharethis-inline-share-buttons"></div><script src="https://platform-api.sharethis.com/js/sharethis.js#property=610cdfe06a9baf001281bbf4&amp;product=inline-share-buttons" defer></script></article></div><!--!--><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><div id="comment-container"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.min.js"></script><script>var gitalk = new Gitalk({
            id: "a4c25304f5f0919f5d66b4a0fc82a5ce",
            repo: "LorrinWWW.github.io",
            owner: "lorrinWWW",
            clientID: "bc305d76488227d8c5cf",
            clientSecret: "f7a2801a15ead49e3258304a0cf20f249f3962f3",
            admin: ["lorrinWWW"],
            createIssueManually: false,
            distractionFreeMode: false,
            perPage: 10,
            pagerDirection: "last",
            
            
            enableHotKey: true,
            
        })
        gitalk.render('comment-container')</script></div></div></div><div class="column column-left is-3-tablet is-3-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/avatar.jpeg" alt="Jue Wang"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Jue Wang</p><p class="is-size-6 is-block">Ph.D Student</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Hangzhou, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Post</p><a href="/archives"><p class="title">1</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">0</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">0</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/lorrinWWW" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Scholar" href="https://scholar.google.com/citations?user=PykI8xcAAAAJ&amp;hl=en"><i class="ai ai-google-scholar"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/lorrinWWW/"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com/JueWANG26088228"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/atom.xml"><i class="fas fa-rss"></i></a></div></div></div><!--!--></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/favicon.svg" alt="Jue Wang" height="28"></a><p class="is-size-7"><span>&copy; 2024 Jue Wang</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div><div id="divmap" style="visibility:hidden; position: absolute; top: 0px"><script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&amp;w=2&amp;t=n&amp;d=I_238wU69nrKH0DIm55b1z-y84aVsLuSzQSglFoJ1ww&amp;co=ffffff&amp;ct=ffffff&amp;cmo=ffffff&amp;cmn=ffffff"></script></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>