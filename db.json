{"meta":{"version":1,"warehouse":"4.0.0"},"models":{"Asset":[{"_id":"source/CNAME","path":"CNAME","modified":0,"renderable":0},{"_id":"source/baidu_verify_TcRbK06C8G.html","path":"baidu_verify_TcRbK06C8G.html","modified":0,"renderable":0},{"_id":"source/baidu_verify_eUOah4Iuy2.html","path":"baidu_verify_eUOah4Iuy2.html","modified":0,"renderable":0},{"_id":"source/about/resume-Jue.Wang.pdf","path":"about/resume-Jue.Wang.pdf","modified":0,"renderable":0},{"_id":"themes/theme-icarus/source/css/cyberpunk.styl","path":"css/cyberpunk.styl","modified":0,"renderable":1},{"_id":"themes/theme-icarus/source/css/default.styl","path":"css/default.styl","modified":0,"renderable":1},{"_id":"themes/theme-icarus/source/css/style.styl","path":"css/style.styl","modified":0,"renderable":1},{"_id":"themes/theme-icarus/source/img/avatar.png","path":"img/avatar.png","modified":0,"renderable":1},{"_id":"themes/theme-icarus/source/img/favicon.svg","path":"img/favicon.svg","modified":0,"renderable":1},{"_id":"themes/theme-icarus/source/img/logo.svg","path":"img/logo.svg","modified":0,"renderable":1},{"_id":"themes/theme-icarus/source/img/og_image.png","path":"img/og_image.png","modified":0,"renderable":1},{"_id":"themes/theme-icarus/source/img/razor-bottom-black.svg","path":"img/razor-bottom-black.svg","modified":0,"renderable":1},{"_id":"themes/theme-icarus/source/img/razor-top-black.svg","path":"img/razor-top-black.svg","modified":0,"renderable":1},{"_id":"themes/theme-icarus/source/js/animation.js","path":"js/animation.js","modified":0,"renderable":1},{"_id":"themes/theme-icarus/source/js/back_to_top.js","path":"js/back_to_top.js","modified":0,"renderable":1},{"_id":"themes/theme-icarus/source/js/column.js","path":"js/column.js","modified":0,"renderable":1},{"_id":"themes/theme-icarus/source/js/main.js","path":"js/main.js","modified":0,"renderable":1}],"Cache":[{"_id":"source/CNAME","hash":"584ac3adaeb479e7552e94ce2d7b42f3814d180e","modified":1581306453090},{"_id":"source/baidu_verify_TcRbK06C8G.html","hash":"e68267f32b8f78af851edccc9c01f6597fcc50b8","modified":1520891053690},{"_id":"source/_data/recommended_posts.json","hash":"a1c68e96cf5df3fddef415bc4214f82522242c71","modified":1523870065240},{"_id":"source/baidu_verify_eUOah4Iuy2.html","hash":"fe1eac761615be2ba4f62006849696dffa0e9b9f","modified":1520868387919},{"_id":"source/about-zh/index.md","hash":"2264b074699446b897a2e1345fd195b600f54a87","modified":1623203363109},{"_id":"source/about/.DS_Store","hash":"70254de4df8040a49427f48db94e96921c7a30f3","modified":1612420918618},{"_id":"source/about/index.md","hash":"668b0423565241c0555723ac8cf47ef95616435c","modified":1637226627259},{"_id":"source/about/resume-Jue.Wang.pdf","hash":"2b91dcdc01832142e6a24bb6f8e38723c789d510","modified":1628407300267},{"_id":"source/tags/index.md","hash":"fa774b869f2d5acee7c55cc2e2a33ea69e3c6953","modified":1520718447796},{"_id":"source/_posts/Bayes-estimation.md","hash":"3e5aad795853e179d954d3205db4e7b2289908a9","modified":1483807698000},{"_id":"source/_posts/EDP-basic-matrix-review.md","hash":"c1fbeae751298cce65cc1985c9690442e015336b","modified":1486464009000},{"_id":"source/_posts/EDP-basic-models.md","hash":"80c26fd753d9e2648551ee0afc29dca700bcd40d","modified":1488798686000},{"_id":"source/_posts/EDP-finite-element-method.md","hash":"b94164cd9990c46a88bcfaf364c99235f9e3630c","modified":1490993312000},{"_id":"source/_posts/Hands-on-Scrapy.md","hash":"5f8b0cbc7126f9e21a2fedb93944cf5e9f647b59","modified":1498140911000},{"_id":"source/_posts/Hilbert-space.md","hash":"3c6db443cefd12c241cbb609a6b75e7765101f9a","modified":1486817852000},{"_id":"source/_posts/Generative-Adversarial-Network.md","hash":"d4529ae15957921f1d52ba3ee9ceeb72ec6fcf8e","modified":1604373963479},{"_id":"source/.DS_Store","hash":"c2d3f70bf8c66bcbe7b5c921b343de266b2216a5","modified":1637227148211},{"_id":"source/_posts/ML-CNN.md","hash":"ef69323536f07aab053cd6d2895c486ad8f21ac9","modified":1581257192526},{"_id":"source/_posts/Method-of-programming-facing-to-exams.md","hash":"1a8fd1060250aede23ca2874e6128a18d349f844","modified":1480502724000},{"_id":"source/_posts/MongoDB-Docker-and-Python.md","hash":"c18c88e9b8c0e3c851ebc5fdb03c7208151154ce","modified":1498115279000},{"_id":"source/_posts/Note-of-NLP.md","hash":"bf0a5c3d8ad8149978bdef469deef2d33d26bc35","modified":1498641238000},{"_id":"source/_posts/Note-of-datamining.md","hash":"33cfd2f704bd9ae07d4ead3c17696e0a1a52400e","modified":1481058293000},{"_id":"source/_posts/Note-of-knowledge-graph.md","hash":"1807aafc17a5b04a03842cb403395c4f9f0ad49e","modified":1498369219000},{"_id":"source/_posts/Note-of-learning-Algo.md","hash":"b7b7e260aaeb65905941d8b4bd4571e1574b7c55","modified":1480503398000},{"_id":"source/_posts/Note-of-probability.md","hash":"ad5d49330cc0839512071921029120eda20f715c","modified":1480947409000},{"_id":"source/_posts/Note-of-statistic.md","hash":"4b9ce4b578af1309675aaa85a8d39c9634a632db","modified":1485626526000},{"_id":"source/_posts/.DS_Store","hash":"ce4ed11b971875e5bd9a780269d26d959cc000c0","modified":1637227153588},{"_id":"source/_posts/OS-notes.md","hash":"bc3910e673839fe47e8d3263d8a120b20a78597f","modified":1492368056000},{"_id":"source/_posts/QuadTree.md","hash":"0710c38532531abb236221ee2b0c06c00efbbf65","modified":1604373962624},{"_id":"source/_posts/Sobolev-space.md","hash":"ccf8b23ac2f15a49576f204f3b309f6c94493017","modified":1486831727000},{"_id":"source/_posts/[2017.12.10]Entity-resolution.md","hash":"16f28c7814d71616705dba24cceff08615abc5d4","modified":1517051250140},{"_id":"source/_posts/[2018.1.14]Models-for-relation-extraction.md","hash":"9fae07ef6801998f2c8667f72ac52b590e49862a","modified":1604373973796},{"_id":"source/_posts/[2017.12.17]Relation-Classification-via-Attention-Model.md","hash":"b38160535bd679994cee6af3c6d3395ed03585d2","modified":1604373974244},{"_id":"source/_posts/[2018.1.21]Event-detection-and-co-referentce.md","hash":"be995f237ed57a26bfef64a555b8deef9620d85c","modified":1604373973403},{"_id":"source/_posts/[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction.md","hash":"bb7d4473bd0ae982781e2fa17f7df2954febbdb6","modified":1604373972997},{"_id":"source/_posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction.md","hash":"4c408a3fd5298309d963ccef2641e53454569f7b","modified":1604373972569},{"_id":"source/_posts/[2018.2.26]Open-World-Knowledge-Graph-Completion.md","hash":"978c1e51fa66641d7a8264ae7091c673f5bc1077","modified":1604373972121},{"_id":"source/_posts/[2018.2.5]Nested-LSTMs.md","hash":"9fb4297d8822d133413a8e764692081f9d14c474","modified":1604373971654},{"_id":"source/_posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing.md","hash":"2af7564cbf24ec89e4469fc0e7fc696eb86f3d9a","modified":1604373971105},{"_id":"source/_posts/[2018.3.20]Event-detection.md","hash":"ec08168a14951d66a84cda970fa4f325b29dc354","modified":1604373967533},{"_id":"source/_posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data.md","hash":"c4b83bb355e71bd98832626f863bb4e2cdf17537","modified":1604373967006},{"_id":"source/_posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization.md","hash":"bcad6c0f7095dd819ed262656d7afb7625bfd26b","modified":1604373966064},{"_id":"source/_posts/[2018.5.10]Knowledge-Graph-Augmented-Neural-Networks-for-NLP.md","hash":"97b81e12e2c7e49fab06e623e6f12310ce76d3d6","modified":1604373965090},{"_id":"source/_posts/[2018.8.22]RegEx-with-NN.md","hash":"048012f06570a0a060e151c0d86001bc801f08cb","modified":1604373964399},{"_id":"source/_posts/[2018.9]nlp-short-reviews-week-1.md","hash":"3acad7f585faaa598bfae6ecb4613055def2aa3f","modified":1538791834775},{"_id":"source/_posts/about.md","hash":"e2a7279868df08390fe0636177fecab26271b195","modified":1637227099227},{"_id":"source/_posts/complexity.md","hash":"884bea5a67590cca6019d9a12442352f0e5aa284","modified":1480502588000},{"_id":"source/_posts/compression.md","hash":"6c09d635d6e901b27df17fe64aa759930e9e4ada","modified":1480502772000},{"_id":"source/_posts/datamining-class-pred.md","hash":"bed4bfa4b40d639d5a22a25c135d2ac7fb65eda2","modified":1482767562000},{"_id":"source/_posts/datamining-pretreatment.md","hash":"c274b6cd8f6388eae82e6786eae0da41b0a20a1c","modified":1481228966000},{"_id":"source/_posts/datamining-qualitative-induction.md","hash":"732e1ebd08758c64a7325b269d7f788671221e1b","modified":1482476111000},{"_id":"source/_posts/graph.md","hash":"fb81eb7a78b150f0133ada157a40e8437d3a135c","modified":1486814910000},{"_id":"source/_posts/hexo-with-latex.md","hash":"8bb31822f906a82bc52db60a54d08e6e85c9a94e","modified":1480542334000},{"_id":"source/_posts/intro-about-KG.md","hash":"08ca28433ce06cdfe5188e402e7eacb3390067ac","modified":1530451665722},{"_id":"source/_posts/learning-OS-and-building-LorriOS.md","hash":"62c7e96f2a44fe667c35e3074986dc8c60551010","modified":1492441961000},{"_id":"source/_posts/machine-learning.md","hash":"5304e7f0e49d9060a8bf3556e1c5a940e249d37c","modified":1481576860000},{"_id":"source/_posts/management-of-the-firm.md","hash":"720d9655a8216e9732a4af5e20cc76e88e7cde5b","modified":1481664406000},{"_id":"source/_posts/participe-present-et-gerondif.md","hash":"754b1e438f2f1c593b0a33993af2b8788db80232","modified":1481664512000},{"_id":"source/_posts/proba-ch1.md","hash":"dba3dc8fb9720b16d533037452729bd83b6599c5","modified":1480591178000},{"_id":"source/_posts/proba-ch2.md","hash":"62fb75ec16f81340349fa4d5e9a5f4b4a0e4b3e1","modified":1520973329109},{"_id":"source/_posts/proba-ch3.md","hash":"c83c5d8f3c8b9631fb804fa14bca32b7c8f3df98","modified":1480606117000},{"_id":"source/_posts/proba-ch4.md","hash":"7cd6e2d4d3b7fb4e5f8547082cea9611aa13e542","modified":1482153097000},{"_id":"source/_posts/proba-ch5.md","hash":"ccd8f72481bdf1b8ad53605254a3d3d43c3f3787","modified":1481454317000},{"_id":"source/_posts/proba-ch6.md","hash":"25e4cdddd7bcf976a6491622111db0c7a195e2c3","modified":1480712994000},{"_id":"source/_posts/projet-enjeu-plugin-chrome-101.md","hash":"2b82eded6334d7d1afd6a427c6db2bf71431b4de","modified":1480504742000},{"_id":"source/_posts/resume-Jue.Wang.pdf","hash":"2b91dcdc01832142e6a24bb6f8e38723c789d510","modified":1628407300267},{"_id":"source/_posts/update-rss.md","hash":"73fae345273458857740a546b139668f7321678e","modified":1604373960854},{"_id":"source/_posts/vps-cheatsheet.md","hash":"450b60f634d03a66cc2525700dc35ec9d3d56d1a","modified":1581255832811},{"_id":"source/categories/index.md","hash":"d923d9de1c6af59ea3bd15493b0182e94ea9a4a2","modified":1520718469441},{"_id":"themes/theme-icarus/layout/comment/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1628406127087},{"_id":"themes/theme-icarus/layout/donate/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1628406127089},{"_id":"themes/theme-icarus/layout/misc/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1628406127090},{"_id":"themes/theme-icarus/layout/search/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1628406127090},{"_id":"themes/theme-icarus/layout/share/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1628406127090},{"_id":"themes/theme-icarus/include/schema/comment/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1628406127077},{"_id":"themes/theme-icarus/include/schema/donate/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1628406127079},{"_id":"themes/theme-icarus/include/schema/misc/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1628406127080},{"_id":"themes/theme-icarus/include/schema/search/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1628406127080},{"_id":"themes/theme-icarus/include/schema/share/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1628406127080},{"_id":"themes/theme-icarus/.eslintignore","hash":"5410a1bef9807f666cd92a0d2020f700e67e4096","modified":1628406127075},{"_id":"themes/theme-icarus/.eslintrc.json","hash":"43c7740158c8690242720b4ff7fa11426fc20c79","modified":1628406127075},{"_id":"themes/theme-icarus/.npmignore","hash":"42242c8da7a020a3295e7dd3d18bf022cb08b661","modified":1628406127075},{"_id":"themes/theme-icarus/CONTRIBUTING.md","hash":"70254c6778c1e41bb2ff222bbf3a70b2239b9bc1","modified":1628406127075},{"_id":"themes/theme-icarus/LICENSE","hash":"86037e5335a49321fa73b7815cab542057fac944","modified":1628406127075},{"_id":"themes/theme-icarus/README.md","hash":"32f9f4fc8cd7ec60b30544bd2e558b593519ae5d","modified":1628406127076},{"_id":"themes/theme-icarus/_config.yml","hash":"eb2e6af82c5bd306ef8638e00f843650445f6967","modified":1628406456256},{"_id":"themes/theme-icarus/package.json","hash":"653d306a010f669192883483414da500d48cf592","modified":1628406127091},{"_id":"themes/theme-icarus/include/config.js","hash":"1ff0f174e9670074ad2bee890d5b6da486800c9a","modified":1628406127076},{"_id":"themes/theme-icarus/include/dependency.js","hash":"d30dbcefd58619f6705d6369b644bc7ba44d2421","modified":1628406127076},{"_id":"themes/theme-icarus/include/register.js","hash":"a974b56a1fbb254f1ae048cc2221363faaccec25","modified":1628406127077},{"_id":"themes/theme-icarus/layout/.DS_Store","hash":"df2fbeb1400acda0909a32c1cf6bf492f1121e07","modified":1637223606753},{"_id":"themes/theme-icarus/layout/archive.jsx","hash":"05677e93d4a43f417dbbf0d63ca37a99e6349e3b","modified":1628406127087},{"_id":"themes/theme-icarus/layout/categories.jsx","hash":"b8ad43e28a4990d222bfbb95b032f88555492347","modified":1628406127087},{"_id":"themes/theme-icarus/layout/category.jsx","hash":"fd15e4eac32de9ac8687aeb3dbe179ab61375700","modified":1628406127087},{"_id":"themes/theme-icarus/layout/index.jsx","hash":"0a84a2348394fa9fc5080dd396bd28d357594f47","modified":1628406127089},{"_id":"themes/theme-icarus/layout/layout.jsx","hash":"a5829907b219e95266f7ed5ee6203e60e2273f93","modified":1628406127089},{"_id":"themes/theme-icarus/layout/page.jsx","hash":"d26c2db57e5a88d6483a03aeb51cda9d191d8cea","modified":1628406127090},{"_id":"themes/theme-icarus/layout/post.jsx","hash":"d26c2db57e5a88d6483a03aeb51cda9d191d8cea","modified":1628406127090},{"_id":"themes/theme-icarus/layout/tag.jsx","hash":"d2f18cac32ca2725d34ccff3f2051c623be6c892","modified":1628406127091},{"_id":"themes/theme-icarus/layout/tags.jsx","hash":"2c42cb64778235dd220c563a27a92108ddc50cc4","modified":1628406127091},{"_id":"themes/theme-icarus/languages/de.yml","hash":"78421f09961ca0b24756a0688fb2cb2e2696e25f","modified":1628406127085},{"_id":"themes/theme-icarus/languages/en.yml","hash":"3d674204d9f723c829226da745afddd180c1131d","modified":1628406127085},{"_id":"themes/theme-icarus/languages/es.yml","hash":"38579b8fad4b6997362acc770615bcd85ff20f68","modified":1628406127085},{"_id":"themes/theme-icarus/languages/fr.yml","hash":"06d5c819d6108a42b28cff7b52e5410d0bed55d1","modified":1628406127085},{"_id":"themes/theme-icarus/languages/id.yml","hash":"5e48b1d62378cadeb64b88349477726a5c1bae47","modified":1628406127085},{"_id":"themes/theme-icarus/languages/ja.yml","hash":"801d9930fef48d6a3f80470d5bed4f3eb78147e6","modified":1628406127085},{"_id":"themes/theme-icarus/languages/ko.yml","hash":"e3374265377809c1518114cf352b595840c0b416","modified":1628406127086},{"_id":"themes/theme-icarus/languages/pl.yml","hash":"2e7debb44cd91096f30efc87bf8d6b1d0d0214c9","modified":1628406127086},{"_id":"themes/theme-icarus/languages/pt-BR.yml","hash":"ee8f73350e4c6e2f63b7fc72b34472a6b1e21244","modified":1628406127086},{"_id":"themes/theme-icarus/languages/ru.yml","hash":"9d91358c2acbe7a0f2a25daf7f65b999ff32d068","modified":1628406127086},{"_id":"themes/theme-icarus/languages/tk.yml","hash":"ca583168bd2025124a1cd0e977da475d7a7496fd","modified":1628406127086},{"_id":"themes/theme-icarus/languages/tr.yml","hash":"74e438bb42619666050192d6f3dc39023777eee2","modified":1628406127086},{"_id":"themes/theme-icarus/languages/vn.yml","hash":"5f2fffa642110c81d8f529949711c9d19ad6bbbe","modified":1628406127086},{"_id":"themes/theme-icarus/languages/zh-CN.yml","hash":"02475ba14afc70dfeaf5678467cee307835e4efa","modified":1628406127087},{"_id":"themes/theme-icarus/languages/zh-TW.yml","hash":"a6826e0c8cdb9ad286324b682b466a9e2ad78e6f","modified":1628406127087},{"_id":"themes/theme-icarus/scripts/index.js","hash":"0c666db6fcb4ffc4d300f4e108c00ee42b1cbbe6","modified":1628406127091},{"_id":"themes/theme-icarus/include/migration/head.js","hash":"269ba172013cbd2f10b9bc51af0496628081329b","modified":1628406127076},{"_id":"themes/theme-icarus/include/migration/v2_v3.js","hash":"3ccb2d2ce11018bebd7172da66faecc3983bff00","modified":1628406127076},{"_id":"themes/theme-icarus/include/migration/v3_v4.js","hash":"9faf2184d7fe87debfbe007f3fc9079dcbcafcfe","modified":1628406127077},{"_id":"themes/theme-icarus/include/schema/config.json","hash":"ac633f9d349bca4f089d59d2c3738b57376f1b31","modified":1628406127079},{"_id":"themes/theme-icarus/include/style/article.styl","hash":"105c983871b6c9148d97a0f756886e56411572bd","modified":1628406127080},{"_id":"themes/theme-icarus/include/style/card.styl","hash":"f78674422eb408cd17c17bbdc3ee1ebe4a453e05","modified":1628406127082},{"_id":"themes/theme-icarus/include/style/base.styl","hash":"2bca6ad099949d52236c87db8db1002ffb99774c","modified":1628406127081},{"_id":"themes/theme-icarus/include/style/button.styl","hash":"0fb35b4786be1b387c751fa2849bc71523fcedd4","modified":1628406127082},{"_id":"themes/theme-icarus/include/style/codeblock.styl","hash":"30bee4cf6792e9665eb648cc20b352d9eaff1207","modified":1628406127083},{"_id":"themes/theme-icarus/include/style/donate.styl","hash":"8d0af00628c13134b5f30a558608e7bebf18c2ec","modified":1628406127083},{"_id":"themes/theme-icarus/include/style/footer.styl","hash":"a4ad715dee38b249538ac6cce94efc9b355a904b","modified":1628406127083},{"_id":"themes/theme-icarus/include/style/navbar.styl","hash":"ecc73c8ad504c0fa4bb910eb51500c14e0a8d662","modified":1628406127083},{"_id":"themes/theme-icarus/include/style/helper.styl","hash":"9f3393e6122cc9f351091bfab960674e962da343","modified":1628406127083},{"_id":"themes/theme-icarus/include/style/pagination.styl","hash":"b81bcd7ff915b4e9299533addc01bc4575ec35e3","modified":1628406127084},{"_id":"themes/theme-icarus/include/style/plugin.styl","hash":"679b61b5fc5b3281735a21c37aeb64229d9c51ea","modified":1628406127084},{"_id":"themes/theme-icarus/include/style/responsive.styl","hash":"207083fe287612cddee6608b541861b14ac8de81","modified":1628406127084},{"_id":"themes/theme-icarus/include/style/search.styl","hash":"416737e1da4e7e907bd03609b0fee9e2aacfe56c","modified":1628406127084},{"_id":"themes/theme-icarus/include/style/timeline.styl","hash":"ea61798a09bffdda07efb93c2ff800b63bddc4c4","modified":1628406127084},{"_id":"themes/theme-icarus/include/style/widget.styl","hash":"c746902251136544eb3fe523235b3183f4189460","modified":1628406127084},{"_id":"themes/theme-icarus/include/util/console.js","hash":"59cf9d277d3ac85a496689bd811b1c316001641d","modified":1628406127084},{"_id":"themes/theme-icarus/layout/common/comment.jsx","hash":"427089c33002707b76e2f38709459a6824fd0f9b","modified":1628406127088},{"_id":"themes/theme-icarus/layout/common/donates.jsx","hash":"889fb0a7ccc502f0a43b4a18eb330e351e50493c","modified":1628406127088},{"_id":"themes/theme-icarus/layout/common/article.jsx","hash":"16513ab1745533d0f4cdbdee323339ebab6d02c1","modified":1628406127087},{"_id":"themes/theme-icarus/layout/common/footer.jsx","hash":"a52571e3a3ed7314164798f58932cc1cd997d0b8","modified":1642474663120},{"_id":"themes/theme-icarus/layout/common/head.jsx","hash":"5625c4040a885aaf150f35fe9d07d844d7f94a27","modified":1628406127088},{"_id":"themes/theme-icarus/layout/common/navbar.jsx","hash":"fcd9fd4624dee49207ef09ea2a1c63f524f3710c","modified":1628406127088},{"_id":"themes/theme-icarus/layout/common/plugins.jsx","hash":"f6826c1a5f5f59f4a0aa00c63bdb0ad4ff4eab69","modified":1628406127088},{"_id":"themes/theme-icarus/layout/common/scripts.jsx","hash":"0fe1fddab431fb9f63906d8c480d5cd6b33abc32","modified":1628406127089},{"_id":"themes/theme-icarus/layout/common/search.jsx","hash":"6f244a37293031670a2964fe424ecd062e591d7b","modified":1628406127089},{"_id":"themes/theme-icarus/layout/common/share.jsx","hash":"c9fb0319ad5e5a10ad3636b26a6c2afed14c590f","modified":1628406127089},{"_id":"themes/theme-icarus/layout/common/widgets.jsx","hash":"689cf4a6b79337b11d1d56afa9dda09223a809a1","modified":1628406127089},{"_id":"themes/theme-icarus/layout/plugin/animejs.jsx","hash":"e2aa27c3501a58ef1e91e511557b77395c2c02aa","modified":1628406127090},{"_id":"themes/theme-icarus/layout/plugin/back_to_top.jsx","hash":"7fc0c5aaabd7d0eaff04cb68ec139442dc3414e8","modified":1628406127090},{"_id":"themes/theme-icarus/layout/widget/profile.jsx","hash":"0d3a7fd922c12cc45d2c8d26a8f4d3a9a6ed0ae0","modified":1628406127091},{"_id":"themes/theme-icarus/source/css/cyberpunk.styl","hash":"ae17d3528df0c3f089df14a06b7bd82f1bc5fed9","modified":1628406127091},{"_id":"themes/theme-icarus/source/css/default.styl","hash":"b01da3028e5a1267a40aaae5c86a11187a2259e3","modified":1628406127092},{"_id":"themes/theme-icarus/source/css/style.styl","hash":"5b9815586e993a6ccbe8cdcfc0c65ea38fc315ac","modified":1628406127092},{"_id":"themes/theme-icarus/source/img/favicon.svg","hash":"16fd847265845063a16596761cddb32926073dd2","modified":1628406127094},{"_id":"themes/theme-icarus/source/img/logo.svg","hash":"e9b5c1438ddb576693a15d0713b2a1d9ceda4be9","modified":1628406127094},{"_id":"themes/theme-icarus/source/img/razor-bottom-black.svg","hash":"a3eda07b1c605b456da9cdf335a1075db5e5d72c","modified":1628406127095},{"_id":"themes/theme-icarus/source/img/og_image.png","hash":"b03f163096ca9c350ec962feee9836277b5c2509","modified":1628406127094},{"_id":"themes/theme-icarus/source/img/razor-top-black.svg","hash":"201f1171a43ce667a39091fe47c0f278857f18f0","modified":1628406127095},{"_id":"themes/theme-icarus/source/js/.eslintrc.json","hash":"6bf0641cb69dffac97f69baea192d7fa3ab612cb","modified":1628406127095},{"_id":"themes/theme-icarus/source/js/animation.js","hash":"12cedd5caaf9109eed97e50eeab8f883f6e49be3","modified":1628406127095},{"_id":"themes/theme-icarus/source/js/back_to_top.js","hash":"d91f10c08c726135a13dfa1f422c49d8764ef03f","modified":1628406127095},{"_id":"themes/theme-icarus/source/js/column.js","hash":"0baee024ab67474c073a4c41b495f3e7f0df4505","modified":1628406127095},{"_id":"themes/theme-icarus/source/js/main.js","hash":"13e4b1c4fa287f3db61aae329ad093a81992f23d","modified":1628406127095},{"_id":"themes/theme-icarus/include/schema/common/article.json","hash":"8d78149f44629d0848921c6fb9c008b03cef3116","modified":1628406127077},{"_id":"themes/theme-icarus/include/schema/common/comment.json","hash":"7d744391a8abee9a2c450b6fdd36a3866a488025","modified":1628406127077},{"_id":"themes/theme-icarus/include/schema/common/donates.json","hash":"ae86e6f177bedf4afbe638502c12635027539305","modified":1628406127078},{"_id":"themes/theme-icarus/include/schema/common/footer.json","hash":"09d706cbb94d6da9a0d15c719ce7139325cae1c7","modified":1628406127078},{"_id":"themes/theme-icarus/include/schema/common/head.json","hash":"98889f059c635e6bdbd51effd04cf1cf44968a66","modified":1628406127078},{"_id":"themes/theme-icarus/include/schema/common/navbar.json","hash":"6691e587284c4cf450e0288680d5ff0f3565f090","modified":1628406127078},{"_id":"themes/theme-icarus/include/schema/common/plugins.json","hash":"6036a805749816416850d944f7d64aaae62e5e75","modified":1628406127078},{"_id":"themes/theme-icarus/include/schema/common/providers.json","hash":"97ec953d497fb53594227ae98acaef8a8baa91da","modified":1628406127078},{"_id":"themes/theme-icarus/include/schema/common/search.json","hash":"985fbcbf47054af714ead1a124869d54f2a8b607","modified":1628406127079},{"_id":"themes/theme-icarus/include/schema/common/share.json","hash":"cf4f9ff4fb27c3541b35f57db355c228fa6873e4","modified":1628406127079},{"_id":"themes/theme-icarus/include/schema/common/sidebar.json","hash":"eb241beaec4c73e3085dfb3139ce72e827e20549","modified":1628406127079},{"_id":"themes/theme-icarus/include/schema/common/widgets.json","hash":"cadd9dc942740ecd5037d3943e72f8b6a8399bbe","modified":1628406127079},{"_id":"themes/theme-icarus/include/schema/plugin/animejs.json","hash":"e62ab6e20bd8862efa1ed32e7c0db0f8acbcfdec","modified":1628406127080},{"_id":"themes/theme-icarus/include/schema/plugin/back_to_top.json","hash":"dc0febab7e7b67075d0ad3f80f5ec8b798b68dea","modified":1628406127080},{"_id":"themes/theme-icarus/include/schema/widget/profile.json","hash":"690ee1b0791cab47ea03cf42b5b4932ed2aa5675","modified":1628406127080},{"_id":"themes/theme-icarus/source/img/avatar.png","hash":"63ac9405ec373c2d98e05ecd18a7c748d8b6b68a","modified":1628406127094},{"_id":"public/baidusitemap.xml","hash":"58277b18b56241552052776d3963833aeb33f3b9","modified":1637227270456},{"_id":"public/atom.xml","hash":"ab549dc29efc4c4626e6b68cc347f66efdaf957f","modified":1637227270456},{"_id":"public/sitemap.xml","hash":"c66bb644b7a1160d66597891050ca10df1719028","modified":1637227270456},{"_id":"public/js/algolia.js","hash":"a8df0c0abeeb4ee1d2d720161f3aea7339380704","modified":1637227270456},{"_id":"public/js/google_cse.js","hash":"1a9881669dfdeb2b3214074eee0d3e01e52db2c4","modified":1637227270456},{"_id":"public/js/insight.js","hash":"86bbdb7305d9bf19ad62d2ca2cf169fc8d9f9d31","modified":1637227270456},{"_id":"public/js/toc.js","hash":"da6fb757a1b083b8ed138bf29aad3a7bf8ec4f11","modified":1637227270456},{"_id":"public/content.json","hash":"6f678daf59a5118d82b903100c5c5f550dd50fd5","modified":1637227270456},{"_id":"public/manifest.json","hash":"5aab5ffbd3a78aba97c48ca85b69aee90f63cdd7","modified":1637227270456},{"_id":"public/about-zh/index.html","hash":"0a47c5d35cfc9ed0d22b4d83fe79fd89369d2e8e","modified":1637227270456},{"_id":"public/tags/index.html","hash":"8998e5a97394427b8f2a8387e4211cdc5e568b8c","modified":1637227270456},{"_id":"public/about/index.html","hash":"3e6297185ee6ba00e6604d3bc77628506754a7f5","modified":1637227270456},{"_id":"public/categories/index.html","hash":"5ae6a68122b95f59d094fe0aa6d4224a0c953bf2","modified":1637227270456},{"_id":"public/posts/about/index.html","hash":"b0be535960f32153ac36d48f6eba5906fc4e360d","modified":1637227270456},{"_id":"public/posts/vps-cheatsheet/index.html","hash":"6c13f45d753d4a28637e957eb826b1cb4569106a","modified":1637227270456},{"_id":"public/posts/[2018.9]nlp-short-reviews-week-1/index.html","hash":"65ee96e04c2ad06357a26c0a8e4b9b1fe7e4650a","modified":1637227270456},{"_id":"public/posts/[2018.8.22]RegEx-with-NN/index.html","hash":"7084ded1598d8bd578f6842071e570f4ccab8c69","modified":1637227270456},{"_id":"public/posts/intro-about-KG/index.html","hash":"4fe558151772c51ffd3016946e090ace32697207","modified":1637227270456},{"_id":"public/posts/update-rss/index.html","hash":"281dd22ff5a267ca007832b37dc6d406d2817da9","modified":1637227270456},{"_id":"public/posts/[2018.5.10]Knowledge-Graph-Augmented-Neural-Networks-for-NLP/index.html","hash":"f223d0b4ae199b79a588a9456514b08523eade5b","modified":1637227270456},{"_id":"public/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/index.html","hash":"c87b065ce9f7495fb0f06e2854a77b0c4e4a882e","modified":1637227270456},{"_id":"public/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/index.html","hash":"5bbdc951622ae07614d292a8a5b07f2a4bda55f2","modified":1637227270456},{"_id":"public/posts/[2018.3.20]Event-detection/index.html","hash":"24a39005038b037e9e6dc70f77d744ea7e004e62","modified":1637227270456},{"_id":"public/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/index.html","hash":"0322990f27c6fb1851e2ce58179f08717608cd95","modified":1637227270456},{"_id":"public/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/index.html","hash":"5f1b26bf453172c239fdffd7b1ccc59bbeff9271","modified":1637227270456},{"_id":"public/posts/[2018.2.5]Nested-LSTMs/index.html","hash":"1849d1395df6013d02f2120689f829b7eb01ebab","modified":1637227270456},{"_id":"public/posts/[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction/index.html","hash":"7d29dc048dc76e86e65af96b265657bea67e767f","modified":1637227270456},{"_id":"public/posts/[2018.1.14]Models-for-relation-extraction/index.html","hash":"ffd0142ec8369d491aa00822c2f0f822778b06a4","modified":1637227270456},{"_id":"public/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/index.html","hash":"57773bb33a97004ec2bd04a7cb37f91a3b380730","modified":1637227270456},{"_id":"public/posts/[2018.1.21]Event-detection-and-co-referentce/index.html","hash":"cd08eda5f2f4f9f626c79177736e2f0b0bcba68e","modified":1637227270456},{"_id":"public/posts/[2017.12.17]Relation-Classification-via-Attention-Model/index.html","hash":"a822903be38b2a41e5eb3c0465f6a5255037132b","modified":1637227270456},{"_id":"public/posts/[2017.12.10]Entity-resolution/index.html","hash":"9d5fb61394cc2f636eeb5d2161a549333e18a2f6","modified":1637227270456},{"_id":"public/posts/Note-of-NLP/index.html","hash":"f70a4d47a19198865fe44d74c02bd49990606dc0","modified":1637227270456},{"_id":"public/posts/Generative-Adversarial-Network/index.html","hash":"547f6345fac2ea5f5234559658d633943f0b5369","modified":1637227270456},{"_id":"public/posts/Note-of-knowledge-graph/index.html","hash":"17b46574d708a43ea5a5f94de7c53111bf7f5562","modified":1637227270456},{"_id":"public/posts/MongoDB-Docker-and-Python/index.html","hash":"669b6831db5b19c56cd5926c26c1f7328f1a6647","modified":1637227270456},{"_id":"public/posts/Hands-on-Scrapy/index.html","hash":"e7997077e0f5f83beaf87b398070ff4403d33436","modified":1637227270456},{"_id":"public/posts/OS-notes/index.html","hash":"6ee694c13055ed89c29b0f6477a1b6ca52d6d6d7","modified":1637227270456},{"_id":"public/posts/EDP-finite-element-method/index.html","hash":"a27c3d5f9c1b8eac95c530f0db6b8125dfa1236c","modified":1637227270456},{"_id":"public/posts/EDP-basic-models/index.html","hash":"ee5b9fa0d51131e23a825b27bfd91a0754420275","modified":1637227270456},{"_id":"public/posts/Sobolev-space/index.html","hash":"28a3fca9830b4a4e7baa8ec01580dcb08845cf04","modified":1637227270456},{"_id":"public/posts/Hilbert-space/index.html","hash":"c844210eb37271dc29d929654955359999f0efcc","modified":1637227270456},{"_id":"public/posts/EDP-basic-matrix-review/index.html","hash":"51f9616ab1a1e908e4109e219c46562bcb05f751","modified":1637227270456},{"_id":"public/posts/Note-of-statistic/index.html","hash":"374230c86209783c333463144bbe2a85fa84ca28","modified":1637227270456},{"_id":"public/posts/Bayes-estimation/index.html","hash":"a3c402d0bd04411e484f4bc2f0b89e508a5213a6","modified":1637227270456},{"_id":"public/posts/learning-OS-and-building-LorriOS/index.html","hash":"3f9c6a26d0b7c4ae64f3c8018aa980f92afd7718","modified":1637227270456},{"_id":"public/posts/datamining-class-pred/index.html","hash":"6f7c1b131d9b753a8d0b3d0ee5aa1163cada6188","modified":1637227270456},{"_id":"public/posts/QuadTree/index.html","hash":"cad8cc22c092199c223f1d1d395ebdf73af053e9","modified":1637227270456},{"_id":"public/posts/management-of-the-firm/index.html","hash":"28ea1682b05d61abf41da918c70ce3ec8e13ca4d","modified":1637227270456},{"_id":"public/posts/machine-learning/index.html","hash":"a5a651419545df707e1bd695bf297ea50fa8a97b","modified":1637227270456},{"_id":"public/posts/ML-CNN/index.html","hash":"22b6e07aaf5d5f1b8b614a4edbcf4a389bf51ce4","modified":1637227270456},{"_id":"public/posts/datamining-qualitative-induction/index.html","hash":"0aa7ec6ded26c2ffd1e0535f41fcfbeb53d363ff","modified":1637227270456},{"_id":"public/posts/datamining-pretreatment/index.html","hash":"19e6dc1cb9b8e2bde4fd79ca7451386f8e5dd8b8","modified":1637227270456},{"_id":"public/posts/Note-of-datamining/index.html","hash":"644aede4350c55c56778f411ab9c1f030585fe7c","modified":1637227270456},{"_id":"public/posts/participe-present-et-gerondif/index.html","hash":"cc3e140f64feb3a153ebcabe1ced5a1476364434","modified":1637227270456},{"_id":"public/posts/proba-ch6/index.html","hash":"36743c09560ae1cb1b732c7bd4cfeeb79798aa8f","modified":1637227270456},{"_id":"public/posts/proba-ch5/index.html","hash":"e7bbdd8b79fe6f21644d7856d6deb887f44c5c28","modified":1637227270456},{"_id":"public/posts/proba-ch4/index.html","hash":"2e5f6f0794b9f2a3787ccf6d806c87754fbb6e20","modified":1637227270456},{"_id":"public/posts/proba-ch3/index.html","hash":"8094d48220f99c63a8834e90ce65509b3348514b","modified":1637227270456},{"_id":"public/posts/proba-ch2/index.html","hash":"523ee4cd3f06e27459d705dfe7c6e10020de55f0","modified":1637227270456},{"_id":"public/posts/proba-ch1/index.html","hash":"9a88069ea2801c640e8e00f78e659252427ac495","modified":1637227270456},{"_id":"public/posts/hexo-with-latex/index.html","hash":"461cb5050efce08d7c9ff46d8a92baa4305918e9","modified":1637227270456},{"_id":"public/posts/Note-of-probability/index.html","hash":"f16ace3eb8e09e4c824f6313a755b7420e376f63","modified":1637227270456},{"_id":"public/posts/projet-enjeu-plugin-chrome-101/index.html","hash":"0d7ca2669e3a47f9a145e09ff76050842a136382","modified":1637227270456},{"_id":"public/posts/graph/index.html","hash":"f01cbfb396d581e1add5a6260f4e8d97bc33e38d","modified":1637227270456},{"_id":"public/posts/compression/index.html","hash":"c527fa4b3ed050d747ef06967d7608f928333ab5","modified":1637227270456},{"_id":"public/posts/Method-of-programming-facing-to-exams/index.html","hash":"75dc56094462fc1e27d6a660e6a2024e9f3819d9","modified":1637227270456},{"_id":"public/posts/Note-of-learning-Algo/index.html","hash":"bb339417b544780ed0a5efbaa64438febf3e222b","modified":1637227270456},{"_id":"public/posts/complexity/index.html","hash":"3b0bf0f0a630a872110f0459f914bf94659e80ee","modified":1637227270456},{"_id":"public/archives/index.html","hash":"e010e702cbfd6b906539ecdc02a8af5d0a712491","modified":1637227270456},{"_id":"public/archives/page/2/index.html","hash":"d5edf94ccd31986fe83474773ea66b2e6a5358a8","modified":1637227270456},{"_id":"public/archives/page/3/index.html","hash":"ba0d4077d22db465c359e6e9a83be69d962d7642","modified":1637227270456},{"_id":"public/archives/page/4/index.html","hash":"0e99555b44b10533831455cfed1ffb80327751b8","modified":1637227270456},{"_id":"public/archives/page/5/index.html","hash":"49a1d0e0256df52ee3acf07a96dfa3aa247a0fea","modified":1637227270456},{"_id":"public/archives/page/6/index.html","hash":"578e682c9021c2c07d97bd9e35d7932d25609b3c","modified":1637227270456},{"_id":"public/archives/2016/index.html","hash":"f942c1d8bf38bf30086918a9d0060af8ceb6f9cb","modified":1637227270456},{"_id":"public/archives/2016/page/2/index.html","hash":"2535018e0c5003282bbc154d11b1580ba2f46c89","modified":1637227270456},{"_id":"public/archives/2016/page/3/index.html","hash":"84e0e9161d49b6df76443b1133b76ce102009b9f","modified":1637227270456},{"_id":"public/archives/2016/11/index.html","hash":"f178094b7a25a657b490676045f8d7c9d5b39b82","modified":1637227270456},{"_id":"public/archives/2016/12/index.html","hash":"ac4156d6190c59e967bdcdc695764a43f2ba44e9","modified":1637227270456},{"_id":"public/archives/2016/12/page/2/index.html","hash":"8b6aed56e946c68da1ac3951404936c5729ac547","modified":1637227270456},{"_id":"public/archives/2017/index.html","hash":"402dbf08420a59f038bdc3ed598c023e8eee57fa","modified":1637227270456},{"_id":"public/archives/2017/page/2/index.html","hash":"25dbb783450869d5ce1b631a471a830b9a102735","modified":1637227270456},{"_id":"public/archives/2017/01/index.html","hash":"f61ce1d9f5e1be64194e247a0c0b3b418deb62d9","modified":1637227270456},{"_id":"public/archives/2017/02/index.html","hash":"6d6540b80210582150f9a792fd8576e6ae59e108","modified":1637227270456},{"_id":"public/archives/2017/03/index.html","hash":"8c6d6d22505af94fc58ed6bcd16450650eb721f4","modified":1637227270456},{"_id":"public/archives/2017/04/index.html","hash":"4d7109ced1c274e6dac1b6c82372206b7b53f1c2","modified":1637227270456},{"_id":"public/archives/2017/06/index.html","hash":"19c1ac5d3502d76f2bff662e3fead267ad317f18","modified":1637227270456},{"_id":"public/archives/2017/12/index.html","hash":"c2a91e1a27b76e47f297f8c7f79dae3f1ea6780e","modified":1637227270456},{"_id":"public/archives/2018/index.html","hash":"0d84ae82701be3a647c1fa1a6587993980a5227b","modified":1637227270456},{"_id":"public/archives/2018/page/2/index.html","hash":"b4baf28f3cdb45141df83fb5ef3a969c1f2ccc25","modified":1637227270456},{"_id":"public/archives/2018/01/index.html","hash":"43bcd08a922bb87ce34a0e61dd5eda2505280edc","modified":1637227270456},{"_id":"public/archives/2018/02/index.html","hash":"7f7a64dd9ed67f56a778e6886f67400ddecced34","modified":1637227270456},{"_id":"public/archives/2018/03/index.html","hash":"f1dcfe05bf38a7315ba314337a2de3f8ed6736a7","modified":1637227270456},{"_id":"public/archives/2018/04/index.html","hash":"914fafa7eb8ea4c3c785203478901b07facebadb","modified":1637227270456},{"_id":"public/archives/2018/05/index.html","hash":"894b2e4aaa0e96f84c26149d0a88440508747ff6","modified":1637227270456},{"_id":"public/archives/2018/07/index.html","hash":"306fd2bacbc11c63aae88cf7ef36465ba17bb912","modified":1637227270456},{"_id":"public/archives/2018/08/index.html","hash":"ffae337111ef6fe0c124ccb53f312d00f35200af","modified":1637227270456},{"_id":"public/archives/2018/09/index.html","hash":"e1885ca3fb22e569a0b4e46dfa914f2506bcd18b","modified":1637227270456},{"_id":"public/archives/2020/index.html","hash":"fa7a5e9bf24dcdcbad82eec4d26802fc9e05da72","modified":1637227270456},{"_id":"public/archives/2020/02/index.html","hash":"ae34ba24afe3b89b6d0ecedfe0788e4d79122c63","modified":1637227270456},{"_id":"public/archives/2021/index.html","hash":"fd05b0a349dfe9af27b94df8568686ebfa4d0391","modified":1637227270456},{"_id":"public/archives/2021/06/index.html","hash":"3670caad27a00922c7af7197ddae6f332443a8a2","modified":1637227270456},{"_id":"public/categories/math/index.html","hash":"65838d132a2dc8f037d6a7e98ad2f870f20d6abc","modified":1637227270456},{"_id":"public/categories/math/page/2/index.html","hash":"9cd0ca258e2b300f2a502efc26363e4207b1a3c3","modified":1637227270456},{"_id":"public/categories/math/unfinished/index.html","hash":"65910e03110066551ea50b2ed3c08759f439d4e3","modified":1637227270456},{"_id":"public/categories/programming/index.html","hash":"928dfa46c900e9b60c5bddb83e174e734b7e9e22","modified":1637227270456},{"_id":"public/categories/programming/page/2/index.html","hash":"c3827abc70bb24783574bfe52b9f385b43cc43b0","modified":1637227270456},{"_id":"public/categories/programming/unfinished/index.html","hash":"d7a0f8377dde9d7335945eba65bbe9d13cfb8f80","modified":1637227270456},{"_id":"public/categories/research/index.html","hash":"128b3ed951cc7e9c3a61a5c718b8c29f166f94b9","modified":1637227270456},{"_id":"public/categories/research/page/2/index.html","hash":"8cc33083a00e1e879aa783700be200ee9e08ef8b","modified":1637227270456},{"_id":"public/categories/other/index.html","hash":"c84241fbae7c4193e94d2e240dcb20e14c662b02","modified":1637227270456},{"_id":"public/categories/francais/index.html","hash":"4cc03f5288a384475f53af75588c95c6c981e770","modified":1637227270456},{"_id":"public/index.html","hash":"624cb8b75e4c77de6677b2ce19238531ad1af5ec","modified":1637227270456},{"_id":"public/page/2/index.html","hash":"f08268bb1a323dcc4c36617c085b47103007fa06","modified":1637227270456},{"_id":"public/page/3/index.html","hash":"11d42c4fc3f4e4b99bf5f2c6e1c826e4d4908a60","modified":1637227270456},{"_id":"public/page/4/index.html","hash":"08998a47ce202ec814059e77ff1db6241ad307b1","modified":1637227270456},{"_id":"public/page/5/index.html","hash":"2b8254f86f43b4d354e2417f3a501a7f00e4f9ab","modified":1637227270456},{"_id":"public/page/6/index.html","hash":"514d23b4e3c0718ab3a17d42984bc0d2b822c558","modified":1637227270456},{"_id":"public/tags/Bayes/index.html","hash":"ed60aba2cd013a07969e8025d197431692c8bb1f","modified":1637227270456},{"_id":"public/tags/statistic/index.html","hash":"d5128782175d1895cd91cc70db1d285324e98d11","modified":1637227270456},{"_id":"public/tags/EDP/index.html","hash":"196aedf054ec4c07c7da8c92d94cfc78a3d062f0","modified":1637227270456},{"_id":"public/tags/matrix/index.html","hash":"2b626b53d705691d34c38ba4d99e975f336a4256","modified":1637227270456},{"_id":"public/tags/math/index.html","hash":"9dbfe39ed7c70e461c90809b124c9f47fd5a739e","modified":1637227270456},{"_id":"public/tags/math/page/2/index.html","hash":"3e532d46dc3194d3cd89e615f6e2138a35f02ef9","modified":1637227270456},{"_id":"public/tags/FEM/index.html","hash":"6642207f918745aa3a3faf5dcebff253e6b2d4b8","modified":1637227270456},{"_id":"public/tags/scrapy/index.html","hash":"0878cf3f0bc601a235f0557fd486fe2716e41513","modified":1637227270456},{"_id":"public/tags/python/index.html","hash":"51e5d0d8b2cc1a4525dcf4d3cc97b5ddf98659a5","modified":1637227270456},{"_id":"public/tags/spider/index.html","hash":"7f3b7ada967b9c660733ae1366c16e52a476c636","modified":1637227270456},{"_id":"public/tags/crawl/index.html","hash":"e123c78f2ba17560a8c58b7175142bfbb3747592","modified":1637227270456},{"_id":"public/tags/Hilbert/index.html","hash":"7246970b207d47a8125da5d09b4f38b01978ab7c","modified":1637227270456},{"_id":"public/tags/analyse/index.html","hash":"df4fe3db9bea8979025ca3af3c92ba9bef2a9294","modified":1637227270456},{"_id":"public/tags/GAN/index.html","hash":"d3597e060ee7eef2c49635f45a38a4aa4f1a2589","modified":1637227270456},{"_id":"public/tags/deep-learning/index.html","hash":"0784e8e4864b0cab96bfa5337d5637b85aecee30","modified":1637227270456},{"_id":"public/tags/machine-learning/index.html","hash":"9f17d51a01f5761833a4d1cb8b8a4de0e3eace6d","modified":1637227270456},{"_id":"public/tags/programming/index.html","hash":"b17206a84d435825909229fb0d091ab8617a504b","modified":1637227270456},{"_id":"public/tags/algo/index.html","hash":"2a01f2d7acd090c4961cef799e18ab513a73dd9f","modified":1637227270456},{"_id":"public/tags/CNN/index.html","hash":"ba7db07b11e3a72201f52c857c595ae2817a0263","modified":1637227270456},{"_id":"public/tags/mongo/index.html","hash":"a5e8569189ac733444e5b58f3feb352e706cb524","modified":1637227270456},{"_id":"public/tags/mongodb/index.html","hash":"3f93a843c1f3c216b6a220cf39794abd1536364f","modified":1637227270456},{"_id":"public/tags/docker/index.html","hash":"21579ea30b34b2240e914fd7783771aeb384240a","modified":1637227270456},{"_id":"public/tags/nlp/index.html","hash":"15aa927b00103284ce22868df7825417f3ca9081","modified":1637227270456},{"_id":"public/tags/datamining/index.html","hash":"c7bc8c12dd1e2a0e8a452107ecb888feb0f8dd6e","modified":1637227270456},{"_id":"public/tags/knowledge-graph/index.html","hash":"5382492c8072e8f2f03e767d81642bd8aca3ef64","modified":1637227270456},{"_id":"public/tags/probability/index.html","hash":"e59e3c4b2d04bc52fe0f389365cd37304e7d127d","modified":1637227270456},{"_id":"public/tags/OS/index.html","hash":"306cefb6096554f339dd69ddb2f22d0c604f521d","modified":1637227270456},{"_id":"public/tags/data-structure/index.html","hash":"0413a4c509bfe1b4951a1747f230a7c48909d35c","modified":1637227270456},{"_id":"public/tags/Sobolev/index.html","hash":"5cfaa8acac5361116237a45aaca9d5c0832225da","modified":1637227270456},{"_id":"public/tags/entity-resolution/index.html","hash":"80d1e8a13da332037872d1fa7f35fb72195aa955","modified":1637227270456},{"_id":"public/tags/sequence-labeling/index.html","hash":"bf1fddbc59aa9579437cd20b0b25d4f54f281664","modified":1637227270456},{"_id":"public/tags/relation-extraction/index.html","hash":"19ca9d64971a3b3921ce4dd9c9014576f5a0bf6a","modified":1637227270456},{"_id":"public/tags/LSTM/index.html","hash":"1de03a7f9a132423f508e0862ee40776adf6eb7f","modified":1637227270456},{"_id":"public/tags/RNN/index.html","hash":"2e2047d3282f2cb1966906b5a042f42904deb4e4","modified":1637227270456},{"_id":"public/tags/distant-supervision/index.html","hash":"cdc4e45a923f471c96af934bff71d906aeb41424","modified":1637227270456},{"_id":"public/tags/relation-classification/index.html","hash":"e5ea69955bb1dc82a226c9dfbaf37765a21e8bde","modified":1637227270456},{"_id":"public/tags/attention/index.html","hash":"421ca8cf77fa0dfbae873eb477cac0e0da7af917","modified":1637227270456},{"_id":"public/tags/event-detection/index.html","hash":"9b19e175b559d5e61dcf0fcea88c62cf97fc16ff","modified":1637227270456},{"_id":"public/tags/co-reference/index.html","hash":"9aee0d15d24eafe9f8833ab19899add4bb1011a2","modified":1637227270456},{"_id":"public/tags/convolution/index.html","hash":"09db848bca8e94a7205ced14c2c35a53ca554daa","modified":1637227270456},{"_id":"public/tags/BiLSTM/index.html","hash":"4801f0b5c840758589e357b7856cc6ee342d3e5f","modified":1637227270456},{"_id":"public/tags/event-extraction/index.html","hash":"1368925a9c50a76bb6c09298bb59f794a9e3154b","modified":1637227270456},{"_id":"public/tags/limited-supervision/index.html","hash":"bbdfc23142fdc2cd16878122b9668f522d2f0413","modified":1637227270456},{"_id":"public/tags/weak-supervision/index.html","hash":"07a177c4419f7508a56b927af57f6b3951496ff1","modified":1637227270456},{"_id":"public/tags/KGC/index.html","hash":"e7f46b00342dd2f15766c90b6d308180da7c6bce","modified":1637227270456},{"_id":"public/tags/neural-network/index.html","hash":"8b589634dc30fee557b0af0354a74a15e40963bb","modified":1637227270456},{"_id":"public/tags/reinforcement-learning/index.html","hash":"752711243308034be0cbd726e83a26d92c7cc011","modified":1637227270456},{"_id":"public/tags/summarization/index.html","hash":"26111dc4ce507870d366739e37b80c632812e66b","modified":1637227270456},{"_id":"public/tags/NLP/index.html","hash":"2c7e56a9abf9f9ff2f600d0e1e57ae8d31e4201f","modified":1637227270456},{"_id":"public/tags/regular-expression/index.html","hash":"c87ceb5a1312917298a9d6b2abe752c60b329189","modified":1637227270456},{"_id":"public/tags/review/index.html","hash":"b74c59d37a6ebcf3e827781546a8476f78f3ad2c","modified":1637227270456},{"_id":"public/tags/complexity/index.html","hash":"c6f067eb6f23be6b4c546dde82c94bbb9ff2d709","modified":1637227270456},{"_id":"public/tags/compression/index.html","hash":"1d8b0b9d030f1e1a561722f5c85eab8ca5692045","modified":1637227270456},{"_id":"public/tags/classification/index.html","hash":"20c54e2cd8e9408f331b8cdde17bf7defcda1ec6","modified":1637227270456},{"_id":"public/tags/prediction/index.html","hash":"4cc24f0693710e3fa0f0a735f7f0c36426661bea","modified":1637227270456},{"_id":"public/tags/qualitative-induction/index.html","hash":"f48c5a78e0a22b69700d5a4f5d911171a0127252","modified":1637227270456},{"_id":"public/tags/graph/index.html","hash":"c758d40eb9df4ad867374e458305b45a9e6660fe","modified":1637227270456},{"_id":"public/tags/hexo/index.html","hash":"e885e6a3b7f58cfa7a1b2f454653de1a9838651d","modified":1637227270456},{"_id":"public/tags/latex/index.html","hash":"03748f3b6dc80f3d80971f603193fd124e4ca0e2","modified":1637227270456},{"_id":"public/tags/mathjax/index.html","hash":"6c7c67401d02b8bc26aa0a81c53b27a74cbb61e5","modified":1637227270456},{"_id":"public/tags/marked/index.html","hash":"2f920b347377d29b5dc45892df8a4e95008727c7","modified":1637227270456},{"_id":"public/tags/knowledge-reasoning/index.html","hash":"c9e784eea78d0f09488f9b111a214b77f8194fbe","modified":1637227270456},{"_id":"public/tags/kernel/index.html","hash":"d8833f5064d5820378c272bd8bdd38fde87a8d51","modified":1637227270456},{"_id":"public/tags/management/index.html","hash":"c32f4d519e85f441f1bc04e690b16324b513bb59","modified":1637227270456},{"_id":"public/tags/firm/index.html","hash":"393c6d03177c0c73b15b7859ba13ea057c5699f7","modified":1637227270456},{"_id":"public/tags/francais/index.html","hash":"a65daad94c30312163d3f2b23e8eb080d4946344","modified":1637227270456},{"_id":"public/tags/language/index.html","hash":"cfb4179cda180be6f33ab6fdb1d23c668dbe6c36","modified":1637227270456},{"_id":"public/tags/aleatoire/index.html","hash":"299147b84e8ec88f1f53e97d30aa84964c88f83e","modified":1637227270456},{"_id":"public/tags/vector/index.html","hash":"df1a997b7500d94873afef3268d170a69f4cc7bc","modified":1637227270456},{"_id":"public/tags/web/index.html","hash":"dcc443fd7cb0f2fc39494482efa36f91d81d3b9c","modified":1637227270456},{"_id":"public/tags/chrome/index.html","hash":"d85cc862fa17249c31ed4a7091f3fa84eb03c77f","modified":1637227270456},{"_id":"public/tags/rss/index.html","hash":"172d3a410cf65f012f80d3306fea1b6d3517291c","modified":1637227270456},{"_id":"public/tags/vps/index.html","hash":"67715f6f801ac8a29235060991a693aa54735444","modified":1637227270456},{"_id":"public/baidu_verify_TcRbK06C8G.html","hash":"e68267f32b8f78af851edccc9c01f6597fcc50b8","modified":1637227270456},{"_id":"public/CNAME","hash":"584ac3adaeb479e7552e94ce2d7b42f3814d180e","modified":1637227270456},{"_id":"public/about/resume-Jue.Wang.pdf","hash":"2b91dcdc01832142e6a24bb6f8e38723c789d510","modified":1637227270456},{"_id":"public/baidu_verify_eUOah4Iuy2.html","hash":"fe1eac761615be2ba4f62006849696dffa0e9b9f","modified":1637227270456},{"_id":"public/img/logo.svg","hash":"e9b5c1438ddb576693a15d0713b2a1d9ceda4be9","modified":1637227270456},{"_id":"public/img/og_image.png","hash":"b03f163096ca9c350ec962feee9836277b5c2509","modified":1637227270456},{"_id":"public/img/favicon.svg","hash":"16fd847265845063a16596761cddb32926073dd2","modified":1637227270456},{"_id":"public/img/razor-bottom-black.svg","hash":"a3eda07b1c605b456da9cdf335a1075db5e5d72c","modified":1637227270456},{"_id":"public/img/razor-top-black.svg","hash":"201f1171a43ce667a39091fe47c0f278857f18f0","modified":1637227270456},{"_id":"public/js/animation.js","hash":"12cedd5caaf9109eed97e50eeab8f883f6e49be3","modified":1637227270456},{"_id":"public/js/column.js","hash":"0baee024ab67474c073a4c41b495f3e7f0df4505","modified":1637227270456},{"_id":"public/js/back_to_top.js","hash":"d91f10c08c726135a13dfa1f422c49d8764ef03f","modified":1637227270456},{"_id":"public/js/main.js","hash":"13e4b1c4fa287f3db61aae329ad093a81992f23d","modified":1637227270456},{"_id":"public/css/cyberpunk.css","hash":"073797b87e28376604d586c48beb66f6fe9cb504","modified":1637227270456},{"_id":"public/css/default.css","hash":"49786c0fefcaa20821d9853a4a6ca81904322793","modified":1637227270456},{"_id":"public/css/style.css","hash":"49786c0fefcaa20821d9853a4a6ca81904322793","modified":1637227270456},{"_id":"public/img/avatar.png","hash":"63ac9405ec373c2d98e05ecd18a7c748d8b6b68a","modified":1637227270456}],"Category":[{"name":"math","_id":"ckw4qufor0004gwtldwdudn8l"},{"name":"unfinished","parent":"ckw4qufor0004gwtldwdudn8l","_id":"ckw4qufp2000mgwtl0radcg7s"},{"name":"programming","_id":"ckw4qufp5000tgwtl6ywt3c4w"},{"name":"unfinished","parent":"ckw4qufp5000tgwtl6ywt3c4w","_id":"ckw4qufpn001zgwtlf76efbqh"},{"name":"research","_id":"ckw4qufpt002igwtl4r5aa33y"},{"name":"other","_id":"ckw4qufqj004ogwtlejyf8alu"},{"name":"francais","_id":"ckw4qufqq0054gwtl0f4z5t7h"}],"Data":[{"_id":"recommended_posts","data":{"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"}],"https://LorrinWWW.github.io/posts/complexity/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/hexo-with-latex/":[{"title":"区块链时代的最正确的打赏姿势——在Hexo博客配置Coinhive网页挖矿","permalink":"http://www.davidfnck.com/blockchain/mine-xmr-in-hexo-by-coinhive.html/"}],"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/":[{"title":"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-statistic/":[{"title":"贝叶斯估计 Bayes estimation","permalink":"https://LorrinWWW.github.io/posts/Bayes-estimation/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"}],"https://LorrinWWW.github.io/posts/[2018.2.5]Nested-LSTMs/":[{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"}],"https://LorrinWWW.github.io/posts/datamining-pretreatment/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Several models for knowledge graph representing and completing 几个知识图谱模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/"}],"https://LorrinWWW.github.io/posts/compression/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/Sobolev-space/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/proba-ch5/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch3/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/proba-ch1/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/ML-CNN/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/EDP-basic-models/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/machine-learning/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/datamining-class-pred/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction/":[],"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/":[{"title":"OS notes","permalink":"https://LorrinWWW.github.io/posts/OS-notes/"}],"https://LorrinWWW.github.io/posts/projet-enjeu-plugin-chrome-101/":[{"title":"Chrome油猴脚本","permalink":"http://gubangzhong.cn/2017/06/13/Chrome油猴脚本/"}],"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/":[{"title":"MongoDB, Docker and Python","permalink":"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/":[{"title":"Hands on Scrapy","permalink":"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/OS-notes/":[{"title":"learning OS and building LorriOS","permalink":"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/"}],"https://LorrinWWW.github.io/posts/Bayes-estimation/":[{"title":"Note of statistic","permalink":"https://LorrinWWW.github.io/posts/Note-of-statistic/"}],"https://LorrinWWW.github.io/posts/management-of-the-firm/":[],"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"}],"https://LorrinWWW.github.io/posts/datamining-qualitative-induction/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-finite-element-method/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/QuadTree/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/proba-ch4/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch2/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/":[{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/Note-of-NLP/":[{"title":"Generative Adversarial Network","permalink":"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/"},{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"}],"https://LorrinWWW.github.io/posts/Note-of-datamining/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"数据挖掘-分类与预测","permalink":"https://LorrinWWW.github.io/posts/datamining-class-pred/"},{"title":"数据挖掘-预处理","permalink":"https://LorrinWWW.github.io/posts/datamining-pretreatment/"}],"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/Note-of-probability/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/participe-present-et-gerondif/":[],"https://LorrinWWW.github.io/posts/Hilbert-space/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/proba-ch6/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}]}}],"Page":[{"title":"王珏","date":"2020-12-28T08:00:00.000Z","_content":"\nHi ~ 我目前是浙江大学数据智能实验室（[Data Intelligence Lab](http://59.111.103.237:8081/)）的博士生，导师是[寿黎但](https://person.zju.edu.cn/en/should)教授。\n\n我主要研究 Natural Language Processing 和 Data Mining 相关工作。 具体来说，我的研究兴趣在于 Information Extraction（比如实体识别、关系抽取等），低资源下的NLP（比如弱/半监督学习算法），以及高效NLP算法（比如知识蒸馏等）。可以通过[电子邮件](mailto:zjuwangjue@gmail.com)来联系我。\n\n这是我的[简历](/about/resume-Jue.Wang.pdf)。\n\n## 近期动态\n\n- Jun 2021: I graduated from [CentraleSupélec](https://www.centralesupelec.fr/) with diplôme d'Ingénieur (master degree), cheers!\n- Dec 2020: As the first author, I had one long paper accepted to AAAI 2021.\n- Sep 2020: As the first author, I had one long paper accepted to EMNLP 2020.\n- Apr 2020: As the first author, I had one long paper accepted to ACL 2020.\n- Feb 2020: I had a remote internship at [StatNLP](https://statnlp-research.github.io/) under the guidance of [Prof. Wei Lu](https://istd.sutd.edu.sg/people/faculty/lu-wei).\n- Aug 2019: I was enrolled in ByteCamp hosted by [ByteDance](https://bytedance.com/en), where I mainly deal with Multimodal Classification.\n- July 2019: We got one demo paper accepted to SIGIR 2019. I attended the conference as the assistant presenter.\n- Jun 2018 to Dec 2018: I did an internship in [Rokid](https://www.rokid.com/), where I mainly deal with Spoken Language Understanding.\n- Jun 2017 to Aug 2018: I did an research internship in [Data Intelligence Lab](http://59.111.103.237:8081/).\n\n## 教育\n\n- **Zhejiang University**, PhD student in Computer Science (Current), Sep 2018 - Jun 2023 (Expected)\n- **Université Paris Saclay (CentraleSupélec)**, Master (Engineer) in General Engineering, Sep 2016 - May 2021\n- **Zhejiang University**, Bachelor in Electrical Engineering, Sep 2014 - Jun 2018\n\n## 联系\n\nCollege of Computer Science and Technology, Zhejiang University\n\n38 Zheda Rd, Xihu Qu, Hangzhou, Zhejiang, 310027\n\nEmail: zjuwangjue@gmail.com\n\n\n\n---\n\n[Blog](https://blog.lorrin.info)([RSS](https://blog.lorrin.info/atom.xml)), [Github](https://github.com/LorrinWWW), [知乎](https://www.zhihu.com/people/wang-jue-9/activities), 欢迎关注～","source":"about-zh/index.md","raw":"---\ntitle: \"王珏\"\ndate: 2020-12-28 16:00:00\n---\n\nHi ~ 我目前是浙江大学数据智能实验室（[Data Intelligence Lab](http://59.111.103.237:8081/)）的博士生，导师是[寿黎但](https://person.zju.edu.cn/en/should)教授。\n\n我主要研究 Natural Language Processing 和 Data Mining 相关工作。 具体来说，我的研究兴趣在于 Information Extraction（比如实体识别、关系抽取等），低资源下的NLP（比如弱/半监督学习算法），以及高效NLP算法（比如知识蒸馏等）。可以通过[电子邮件](mailto:zjuwangjue@gmail.com)来联系我。\n\n这是我的[简历](/about/resume-Jue.Wang.pdf)。\n\n## 近期动态\n\n- Jun 2021: I graduated from [CentraleSupélec](https://www.centralesupelec.fr/) with diplôme d'Ingénieur (master degree), cheers!\n- Dec 2020: As the first author, I had one long paper accepted to AAAI 2021.\n- Sep 2020: As the first author, I had one long paper accepted to EMNLP 2020.\n- Apr 2020: As the first author, I had one long paper accepted to ACL 2020.\n- Feb 2020: I had a remote internship at [StatNLP](https://statnlp-research.github.io/) under the guidance of [Prof. Wei Lu](https://istd.sutd.edu.sg/people/faculty/lu-wei).\n- Aug 2019: I was enrolled in ByteCamp hosted by [ByteDance](https://bytedance.com/en), where I mainly deal with Multimodal Classification.\n- July 2019: We got one demo paper accepted to SIGIR 2019. I attended the conference as the assistant presenter.\n- Jun 2018 to Dec 2018: I did an internship in [Rokid](https://www.rokid.com/), where I mainly deal with Spoken Language Understanding.\n- Jun 2017 to Aug 2018: I did an research internship in [Data Intelligence Lab](http://59.111.103.237:8081/).\n\n## 教育\n\n- **Zhejiang University**, PhD student in Computer Science (Current), Sep 2018 - Jun 2023 (Expected)\n- **Université Paris Saclay (CentraleSupélec)**, Master (Engineer) in General Engineering, Sep 2016 - May 2021\n- **Zhejiang University**, Bachelor in Electrical Engineering, Sep 2014 - Jun 2018\n\n## 联系\n\nCollege of Computer Science and Technology, Zhejiang University\n\n38 Zheda Rd, Xihu Qu, Hangzhou, Zhejiang, 310027\n\nEmail: zjuwangjue@gmail.com\n\n\n\n---\n\n[Blog](https://blog.lorrin.info)([RSS](https://blog.lorrin.info/atom.xml)), [Github](https://github.com/LorrinWWW), [知乎](https://www.zhihu.com/people/wang-jue-9/activities), 欢迎关注～","updated":"2021-06-09T01:49:23.109Z","path":"about-zh/index.html","comments":1,"layout":"page","_id":"ckw4qufoj0000gwtl2hx86yv1","content":"<p>Hi ~ 我目前是浙江大学数据智能实验室（<a href=\"http://59.111.103.237:8081/\">Data Intelligence Lab</a>）的博士生，导师是<a href=\"https://person.zju.edu.cn/en/should\">寿黎但</a>教授。</p>\n<p>我主要研究 Natural Language Processing 和 Data Mining 相关工作。 具体来说，我的研究兴趣在于 Information Extraction（比如实体识别、关系抽取等），低资源下的NLP（比如弱/半监督学习算法），以及高效NLP算法（比如知识蒸馏等）。可以通过<a href=\"mailto:zjuwangjue@gmail.com\">电子邮件</a>来联系我。</p>\n<p>这是我的<a href=\"/about/resume-Jue.Wang.pdf\">简历</a>。</p>\n<h2 id=\"近期动态\"><a href=\"#近期动态\" class=\"headerlink\" title=\"近期动态\"></a>近期动态</h2><ul>\n<li>Jun 2021: I graduated from <a href=\"https://www.centralesupelec.fr/\">CentraleSupélec</a> with diplôme d’Ingénieur (master degree), cheers!</li>\n<li>Dec 2020: As the first author, I had one long paper accepted to AAAI 2021.</li>\n<li>Sep 2020: As the first author, I had one long paper accepted to EMNLP 2020.</li>\n<li>Apr 2020: As the first author, I had one long paper accepted to ACL 2020.</li>\n<li>Feb 2020: I had a remote internship at <a href=\"https://statnlp-research.github.io/\">StatNLP</a> under the guidance of <a href=\"https://istd.sutd.edu.sg/people/faculty/lu-wei\">Prof. Wei Lu</a>.</li>\n<li>Aug 2019: I was enrolled in ByteCamp hosted by <a href=\"https://bytedance.com/en\">ByteDance</a>, where I mainly deal with Multimodal Classification.</li>\n<li>July 2019: We got one demo paper accepted to SIGIR 2019. I attended the conference as the assistant presenter.</li>\n<li>Jun 2018 to Dec 2018: I did an internship in <a href=\"https://www.rokid.com/\">Rokid</a>, where I mainly deal with Spoken Language Understanding.</li>\n<li>Jun 2017 to Aug 2018: I did an research internship in <a href=\"http://59.111.103.237:8081/\">Data Intelligence Lab</a>.</li>\n</ul>\n<h2 id=\"教育\"><a href=\"#教育\" class=\"headerlink\" title=\"教育\"></a>教育</h2><ul>\n<li><strong>Zhejiang University</strong>, PhD student in Computer Science (Current), Sep 2018 - Jun 2023 (Expected)</li>\n<li><strong>Université Paris Saclay (CentraleSupélec)</strong>, Master (Engineer) in General Engineering, Sep 2016 - May 2021</li>\n<li><strong>Zhejiang University</strong>, Bachelor in Electrical Engineering, Sep 2014 - Jun 2018</li>\n</ul>\n<h2 id=\"联系\"><a href=\"#联系\" class=\"headerlink\" title=\"联系\"></a>联系</h2><p>College of Computer Science and Technology, Zhejiang University</p>\n<p>38 Zheda Rd, Xihu Qu, Hangzhou, Zhejiang, 310027</p>\n<p>Email: <a href=\"mailto:zjuwangjue@gmail.com\">zjuwangjue@gmail.com</a></p>\n<hr>\n<p><a href=\"https://blog.lorrin.info/\">Blog</a>(<a href=\"https://blog.lorrin.info/atom.xml\">RSS</a>), <a href=\"https://github.com/LorrinWWW\">Github</a>, <a href=\"https://www.zhihu.com/people/wang-jue-9/activities\">知乎</a>, 欢迎关注～</p>\n","site":{"data":{"recommended_posts":{"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"}],"https://LorrinWWW.github.io/posts/complexity/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/hexo-with-latex/":[{"title":"区块链时代的最正确的打赏姿势——在Hexo博客配置Coinhive网页挖矿","permalink":"http://www.davidfnck.com/blockchain/mine-xmr-in-hexo-by-coinhive.html/"}],"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/":[{"title":"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-statistic/":[{"title":"贝叶斯估计 Bayes estimation","permalink":"https://LorrinWWW.github.io/posts/Bayes-estimation/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"}],"https://LorrinWWW.github.io/posts/[2018.2.5]Nested-LSTMs/":[{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"}],"https://LorrinWWW.github.io/posts/datamining-pretreatment/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Several models for knowledge graph representing and completing 几个知识图谱模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/"}],"https://LorrinWWW.github.io/posts/compression/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/Sobolev-space/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/proba-ch5/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch3/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/proba-ch1/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/ML-CNN/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/EDP-basic-models/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/machine-learning/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/datamining-class-pred/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction/":[],"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/":[{"title":"OS notes","permalink":"https://LorrinWWW.github.io/posts/OS-notes/"}],"https://LorrinWWW.github.io/posts/projet-enjeu-plugin-chrome-101/":[{"title":"Chrome油猴脚本","permalink":"http://gubangzhong.cn/2017/06/13/Chrome油猴脚本/"}],"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/":[{"title":"MongoDB, Docker and Python","permalink":"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/":[{"title":"Hands on Scrapy","permalink":"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/OS-notes/":[{"title":"learning OS and building LorriOS","permalink":"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/"}],"https://LorrinWWW.github.io/posts/Bayes-estimation/":[{"title":"Note of statistic","permalink":"https://LorrinWWW.github.io/posts/Note-of-statistic/"}],"https://LorrinWWW.github.io/posts/management-of-the-firm/":[],"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"}],"https://LorrinWWW.github.io/posts/datamining-qualitative-induction/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-finite-element-method/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/QuadTree/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/proba-ch4/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch2/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/":[{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/Note-of-NLP/":[{"title":"Generative Adversarial Network","permalink":"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/"},{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"}],"https://LorrinWWW.github.io/posts/Note-of-datamining/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"数据挖掘-分类与预测","permalink":"https://LorrinWWW.github.io/posts/datamining-class-pred/"},{"title":"数据挖掘-预处理","permalink":"https://LorrinWWW.github.io/posts/datamining-pretreatment/"}],"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/Note-of-probability/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/participe-present-et-gerondif/":[],"https://LorrinWWW.github.io/posts/Hilbert-space/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/proba-ch6/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}]}}},"excerpt":"","more":"<p>Hi ~ 我目前是浙江大学数据智能实验室（<a href=\"http://59.111.103.237:8081/\">Data Intelligence Lab</a>）的博士生，导师是<a href=\"https://person.zju.edu.cn/en/should\">寿黎但</a>教授。</p>\n<p>我主要研究 Natural Language Processing 和 Data Mining 相关工作。 具体来说，我的研究兴趣在于 Information Extraction（比如实体识别、关系抽取等），低资源下的NLP（比如弱/半监督学习算法），以及高效NLP算法（比如知识蒸馏等）。可以通过<a href=\"mailto:zjuwangjue@gmail.com\">电子邮件</a>来联系我。</p>\n<p>这是我的<a href=\"/about/resume-Jue.Wang.pdf\">简历</a>。</p>\n<h2 id=\"近期动态\"><a href=\"#近期动态\" class=\"headerlink\" title=\"近期动态\"></a>近期动态</h2><ul>\n<li>Jun 2021: I graduated from <a href=\"https://www.centralesupelec.fr/\">CentraleSupélec</a> with diplôme d’Ingénieur (master degree), cheers!</li>\n<li>Dec 2020: As the first author, I had one long paper accepted to AAAI 2021.</li>\n<li>Sep 2020: As the first author, I had one long paper accepted to EMNLP 2020.</li>\n<li>Apr 2020: As the first author, I had one long paper accepted to ACL 2020.</li>\n<li>Feb 2020: I had a remote internship at <a href=\"https://statnlp-research.github.io/\">StatNLP</a> under the guidance of <a href=\"https://istd.sutd.edu.sg/people/faculty/lu-wei\">Prof. Wei Lu</a>.</li>\n<li>Aug 2019: I was enrolled in ByteCamp hosted by <a href=\"https://bytedance.com/en\">ByteDance</a>, where I mainly deal with Multimodal Classification.</li>\n<li>July 2019: We got one demo paper accepted to SIGIR 2019. I attended the conference as the assistant presenter.</li>\n<li>Jun 2018 to Dec 2018: I did an internship in <a href=\"https://www.rokid.com/\">Rokid</a>, where I mainly deal with Spoken Language Understanding.</li>\n<li>Jun 2017 to Aug 2018: I did an research internship in <a href=\"http://59.111.103.237:8081/\">Data Intelligence Lab</a>.</li>\n</ul>\n<h2 id=\"教育\"><a href=\"#教育\" class=\"headerlink\" title=\"教育\"></a>教育</h2><ul>\n<li><strong>Zhejiang University</strong>, PhD student in Computer Science (Current), Sep 2018 - Jun 2023 (Expected)</li>\n<li><strong>Université Paris Saclay (CentraleSupélec)</strong>, Master (Engineer) in General Engineering, Sep 2016 - May 2021</li>\n<li><strong>Zhejiang University</strong>, Bachelor in Electrical Engineering, Sep 2014 - Jun 2018</li>\n</ul>\n<h2 id=\"联系\"><a href=\"#联系\" class=\"headerlink\" title=\"联系\"></a>联系</h2><p>College of Computer Science and Technology, Zhejiang University</p>\n<p>38 Zheda Rd, Xihu Qu, Hangzhou, Zhejiang, 310027</p>\n<p>Email: <a href=\"mailto:&#x7a;&#106;&#117;&#x77;&#97;&#x6e;&#103;&#106;&#x75;&#101;&#64;&#103;&#109;&#x61;&#105;&#108;&#x2e;&#x63;&#111;&#x6d;\">&#x7a;&#106;&#117;&#x77;&#97;&#x6e;&#103;&#106;&#x75;&#101;&#64;&#103;&#109;&#x61;&#105;&#108;&#x2e;&#x63;&#111;&#x6d;</a></p>\n<hr>\n<p><a href=\"https://blog.lorrin.info/\">Blog</a>(<a href=\"https://blog.lorrin.info/atom.xml\">RSS</a>), <a href=\"https://github.com/LorrinWWW\">Github</a>, <a href=\"https://www.zhihu.com/people/wang-jue-9/activities\">知乎</a>, 欢迎关注～</p>\n"},{"title":"Jue Wang","date":"2021-06-01T08:00:00.000Z","_content":"\nHello, I am a PhD student in [Data Intelligence Lab](http://59.111.103.237:8081/) of Zhejiang University, advised by [Prof. Lidan Shou](https://person.zju.edu.cn/en/should).\n\nI work on Natural Language Processing and Data Mining. More specifically, my research interests lie in Information Extraction (e.g., Named Entity Recognition and Relation Extraction), NLP in low-resource scenarios (e.g., Weak/Semi-Supervised Learning), and Efficient Algorithms for NLP (e.g., Knowledge Distillation and Network Pruning). If you want to get in touch, please [send me an email](mailto:zjuwangjue@gmail.com). \n\nMy [resume](resume-Jue.Wang.pdf). \n\n## Updates\n\n- Jun 2021: I graduated from [CentraleSupélec](https://www.centralesupelec.fr/) with diplôme d'Ingénieur (master degree), cheers!\n- Dec 2020: As the first author, I had one long paper accepted to AAAI 2021.\n- Sep 2020: As the first author, I had one long paper accepted to EMNLP 2020.\n- Apr 2020: As the first author, I had one long paper accepted to ACL 2020.\n- Feb 2020: I had a remote internship at [StatNLP](https://statnlp-research.github.io/) under the guidance of [Prof. Wei Lu](https://istd.sutd.edu.sg/people/faculty/lu-wei).\n- Aug 2019: I was enrolled in ByteCamp hosted by [ByteDance](https://bytedance.com/en), where I mainly deal with Multimodal Classification.\n- July 2019: We got one demo paper accepted to SIGIR 2019. I attended the conference as the assistant presenter.\n- Jun 2018 to Dec 2018: I did an internship in [Rokid](https://www.rokid.com/), where I mainly deal with Spoken Language Understanding.\n- Jun 2017 to Aug 2018: I did an research internship in [Data Intelligence Lab](http://59.111.103.237:8081/).\n\n## Education\n\n- **Zhejiang University**, PhD student in Computer Science (Current), Sep 2018 - Jun 2023 (Expected)\n- **Université Paris Saclay (CentraleSupélec)**, Master (Engineer) in General Engineering, Sep 2016 - May 2021\n- **Zhejiang University**, Bachelor in Electrical Engineering, Sep 2014 - Jun 2018\n\n## Contact\n\nCollege of Computer Science and Technology, Zhejiang University\n\n38 Zheda Rd, Xihu Qu, Hangzhou, Zhejiang, 310027\n\nEmail: zjuwangjue@gmail.com\n\n\n\n([中文版](/about-zh))\n\n---\n\n[Blog](https://blog.lorrin.info)([RSS](https://blog.lorrin.info/atom.xml)), [Github](https://github.com/LorrinWWW), [知乎](https://www.zhihu.com/people/wang-jue-9/activities), 欢迎关注～","source":"about/index.md","raw":"---\ntitle: \"Jue Wang\"\ndate: 2021-6-1 16:00:00\n---\n\nHello, I am a PhD student in [Data Intelligence Lab](http://59.111.103.237:8081/) of Zhejiang University, advised by [Prof. Lidan Shou](https://person.zju.edu.cn/en/should).\n\nI work on Natural Language Processing and Data Mining. More specifically, my research interests lie in Information Extraction (e.g., Named Entity Recognition and Relation Extraction), NLP in low-resource scenarios (e.g., Weak/Semi-Supervised Learning), and Efficient Algorithms for NLP (e.g., Knowledge Distillation and Network Pruning). If you want to get in touch, please [send me an email](mailto:zjuwangjue@gmail.com). \n\nMy [resume](resume-Jue.Wang.pdf). \n\n## Updates\n\n- Jun 2021: I graduated from [CentraleSupélec](https://www.centralesupelec.fr/) with diplôme d'Ingénieur (master degree), cheers!\n- Dec 2020: As the first author, I had one long paper accepted to AAAI 2021.\n- Sep 2020: As the first author, I had one long paper accepted to EMNLP 2020.\n- Apr 2020: As the first author, I had one long paper accepted to ACL 2020.\n- Feb 2020: I had a remote internship at [StatNLP](https://statnlp-research.github.io/) under the guidance of [Prof. Wei Lu](https://istd.sutd.edu.sg/people/faculty/lu-wei).\n- Aug 2019: I was enrolled in ByteCamp hosted by [ByteDance](https://bytedance.com/en), where I mainly deal with Multimodal Classification.\n- July 2019: We got one demo paper accepted to SIGIR 2019. I attended the conference as the assistant presenter.\n- Jun 2018 to Dec 2018: I did an internship in [Rokid](https://www.rokid.com/), where I mainly deal with Spoken Language Understanding.\n- Jun 2017 to Aug 2018: I did an research internship in [Data Intelligence Lab](http://59.111.103.237:8081/).\n\n## Education\n\n- **Zhejiang University**, PhD student in Computer Science (Current), Sep 2018 - Jun 2023 (Expected)\n- **Université Paris Saclay (CentraleSupélec)**, Master (Engineer) in General Engineering, Sep 2016 - May 2021\n- **Zhejiang University**, Bachelor in Electrical Engineering, Sep 2014 - Jun 2018\n\n## Contact\n\nCollege of Computer Science and Technology, Zhejiang University\n\n38 Zheda Rd, Xihu Qu, Hangzhou, Zhejiang, 310027\n\nEmail: zjuwangjue@gmail.com\n\n\n\n([中文版](/about-zh))\n\n---\n\n[Blog](https://blog.lorrin.info)([RSS](https://blog.lorrin.info/atom.xml)), [Github](https://github.com/LorrinWWW), [知乎](https://www.zhihu.com/people/wang-jue-9/activities), 欢迎关注～","updated":"2021-11-18T09:10:27.259Z","path":"about/index.html","comments":1,"layout":"page","_id":"ckw4qufop0002gwtl1mor8n7u","content":"<p>Hello, I am a PhD student in <a href=\"http://59.111.103.237:8081/\">Data Intelligence Lab</a> of Zhejiang University, advised by <a href=\"https://person.zju.edu.cn/en/should\">Prof. Lidan Shou</a>.</p>\n<p>I work on Natural Language Processing and Data Mining. More specifically, my research interests lie in Information Extraction (e.g., Named Entity Recognition and Relation Extraction), NLP in low-resource scenarios (e.g., Weak/Semi-Supervised Learning), and Efficient Algorithms for NLP (e.g., Knowledge Distillation and Network Pruning). If you want to get in touch, please <a href=\"mailto:zjuwangjue@gmail.com\">send me an email</a>. </p>\n<p>My <a href=\"resume-Jue.Wang.pdf\">resume</a>. </p>\n<h2 id=\"Updates\"><a href=\"#Updates\" class=\"headerlink\" title=\"Updates\"></a>Updates</h2><ul>\n<li>Jun 2021: I graduated from <a href=\"https://www.centralesupelec.fr/\">CentraleSupélec</a> with diplôme d’Ingénieur (master degree), cheers!</li>\n<li>Dec 2020: As the first author, I had one long paper accepted to AAAI 2021.</li>\n<li>Sep 2020: As the first author, I had one long paper accepted to EMNLP 2020.</li>\n<li>Apr 2020: As the first author, I had one long paper accepted to ACL 2020.</li>\n<li>Feb 2020: I had a remote internship at <a href=\"https://statnlp-research.github.io/\">StatNLP</a> under the guidance of <a href=\"https://istd.sutd.edu.sg/people/faculty/lu-wei\">Prof. Wei Lu</a>.</li>\n<li>Aug 2019: I was enrolled in ByteCamp hosted by <a href=\"https://bytedance.com/en\">ByteDance</a>, where I mainly deal with Multimodal Classification.</li>\n<li>July 2019: We got one demo paper accepted to SIGIR 2019. I attended the conference as the assistant presenter.</li>\n<li>Jun 2018 to Dec 2018: I did an internship in <a href=\"https://www.rokid.com/\">Rokid</a>, where I mainly deal with Spoken Language Understanding.</li>\n<li>Jun 2017 to Aug 2018: I did an research internship in <a href=\"http://59.111.103.237:8081/\">Data Intelligence Lab</a>.</li>\n</ul>\n<h2 id=\"Education\"><a href=\"#Education\" class=\"headerlink\" title=\"Education\"></a>Education</h2><ul>\n<li><strong>Zhejiang University</strong>, PhD student in Computer Science (Current), Sep 2018 - Jun 2023 (Expected)</li>\n<li><strong>Université Paris Saclay (CentraleSupélec)</strong>, Master (Engineer) in General Engineering, Sep 2016 - May 2021</li>\n<li><strong>Zhejiang University</strong>, Bachelor in Electrical Engineering, Sep 2014 - Jun 2018</li>\n</ul>\n<h2 id=\"Contact\"><a href=\"#Contact\" class=\"headerlink\" title=\"Contact\"></a>Contact</h2><p>College of Computer Science and Technology, Zhejiang University</p>\n<p>38 Zheda Rd, Xihu Qu, Hangzhou, Zhejiang, 310027</p>\n<p>Email: <a href=\"mailto:zjuwangjue@gmail.com\">zjuwangjue@gmail.com</a></p>\n<p>(<a href=\"/about-zh\">中文版</a>)</p>\n<hr>\n<p><a href=\"https://blog.lorrin.info/\">Blog</a>(<a href=\"https://blog.lorrin.info/atom.xml\">RSS</a>), <a href=\"https://github.com/LorrinWWW\">Github</a>, <a href=\"https://www.zhihu.com/people/wang-jue-9/activities\">知乎</a>, 欢迎关注～</p>\n","site":{"data":{"recommended_posts":{"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"}],"https://LorrinWWW.github.io/posts/complexity/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/hexo-with-latex/":[{"title":"区块链时代的最正确的打赏姿势——在Hexo博客配置Coinhive网页挖矿","permalink":"http://www.davidfnck.com/blockchain/mine-xmr-in-hexo-by-coinhive.html/"}],"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/":[{"title":"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-statistic/":[{"title":"贝叶斯估计 Bayes estimation","permalink":"https://LorrinWWW.github.io/posts/Bayes-estimation/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"}],"https://LorrinWWW.github.io/posts/[2018.2.5]Nested-LSTMs/":[{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"}],"https://LorrinWWW.github.io/posts/datamining-pretreatment/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Several models for knowledge graph representing and completing 几个知识图谱模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/"}],"https://LorrinWWW.github.io/posts/compression/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/Sobolev-space/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/proba-ch5/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch3/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/proba-ch1/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/ML-CNN/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/EDP-basic-models/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/machine-learning/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/datamining-class-pred/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction/":[],"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/":[{"title":"OS notes","permalink":"https://LorrinWWW.github.io/posts/OS-notes/"}],"https://LorrinWWW.github.io/posts/projet-enjeu-plugin-chrome-101/":[{"title":"Chrome油猴脚本","permalink":"http://gubangzhong.cn/2017/06/13/Chrome油猴脚本/"}],"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/":[{"title":"MongoDB, Docker and Python","permalink":"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/":[{"title":"Hands on Scrapy","permalink":"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/OS-notes/":[{"title":"learning OS and building LorriOS","permalink":"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/"}],"https://LorrinWWW.github.io/posts/Bayes-estimation/":[{"title":"Note of statistic","permalink":"https://LorrinWWW.github.io/posts/Note-of-statistic/"}],"https://LorrinWWW.github.io/posts/management-of-the-firm/":[],"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"}],"https://LorrinWWW.github.io/posts/datamining-qualitative-induction/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-finite-element-method/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/QuadTree/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/proba-ch4/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch2/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/":[{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/Note-of-NLP/":[{"title":"Generative Adversarial Network","permalink":"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/"},{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"}],"https://LorrinWWW.github.io/posts/Note-of-datamining/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"数据挖掘-分类与预测","permalink":"https://LorrinWWW.github.io/posts/datamining-class-pred/"},{"title":"数据挖掘-预处理","permalink":"https://LorrinWWW.github.io/posts/datamining-pretreatment/"}],"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/Note-of-probability/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/participe-present-et-gerondif/":[],"https://LorrinWWW.github.io/posts/Hilbert-space/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/proba-ch6/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}]}}},"excerpt":"","more":"<p>Hello, I am a PhD student in <a href=\"http://59.111.103.237:8081/\">Data Intelligence Lab</a> of Zhejiang University, advised by <a href=\"https://person.zju.edu.cn/en/should\">Prof. Lidan Shou</a>.</p>\n<p>I work on Natural Language Processing and Data Mining. More specifically, my research interests lie in Information Extraction (e.g., Named Entity Recognition and Relation Extraction), NLP in low-resource scenarios (e.g., Weak/Semi-Supervised Learning), and Efficient Algorithms for NLP (e.g., Knowledge Distillation and Network Pruning). If you want to get in touch, please <a href=\"mailto:zjuwangjue@gmail.com\">send me an email</a>. </p>\n<p>My <a href=\"resume-Jue.Wang.pdf\">resume</a>. </p>\n<h2 id=\"Updates\"><a href=\"#Updates\" class=\"headerlink\" title=\"Updates\"></a>Updates</h2><ul>\n<li>Jun 2021: I graduated from <a href=\"https://www.centralesupelec.fr/\">CentraleSupélec</a> with diplôme d’Ingénieur (master degree), cheers!</li>\n<li>Dec 2020: As the first author, I had one long paper accepted to AAAI 2021.</li>\n<li>Sep 2020: As the first author, I had one long paper accepted to EMNLP 2020.</li>\n<li>Apr 2020: As the first author, I had one long paper accepted to ACL 2020.</li>\n<li>Feb 2020: I had a remote internship at <a href=\"https://statnlp-research.github.io/\">StatNLP</a> under the guidance of <a href=\"https://istd.sutd.edu.sg/people/faculty/lu-wei\">Prof. Wei Lu</a>.</li>\n<li>Aug 2019: I was enrolled in ByteCamp hosted by <a href=\"https://bytedance.com/en\">ByteDance</a>, where I mainly deal with Multimodal Classification.</li>\n<li>July 2019: We got one demo paper accepted to SIGIR 2019. I attended the conference as the assistant presenter.</li>\n<li>Jun 2018 to Dec 2018: I did an internship in <a href=\"https://www.rokid.com/\">Rokid</a>, where I mainly deal with Spoken Language Understanding.</li>\n<li>Jun 2017 to Aug 2018: I did an research internship in <a href=\"http://59.111.103.237:8081/\">Data Intelligence Lab</a>.</li>\n</ul>\n<h2 id=\"Education\"><a href=\"#Education\" class=\"headerlink\" title=\"Education\"></a>Education</h2><ul>\n<li><strong>Zhejiang University</strong>, PhD student in Computer Science (Current), Sep 2018 - Jun 2023 (Expected)</li>\n<li><strong>Université Paris Saclay (CentraleSupélec)</strong>, Master (Engineer) in General Engineering, Sep 2016 - May 2021</li>\n<li><strong>Zhejiang University</strong>, Bachelor in Electrical Engineering, Sep 2014 - Jun 2018</li>\n</ul>\n<h2 id=\"Contact\"><a href=\"#Contact\" class=\"headerlink\" title=\"Contact\"></a>Contact</h2><p>College of Computer Science and Technology, Zhejiang University</p>\n<p>38 Zheda Rd, Xihu Qu, Hangzhou, Zhejiang, 310027</p>\n<p>Email: <a href=\"mailto:&#122;&#106;&#117;&#119;&#x61;&#x6e;&#103;&#x6a;&#x75;&#x65;&#x40;&#103;&#x6d;&#97;&#105;&#x6c;&#x2e;&#99;&#111;&#109;\">&#122;&#106;&#117;&#119;&#x61;&#x6e;&#103;&#x6a;&#x75;&#x65;&#x40;&#103;&#x6d;&#97;&#105;&#x6c;&#x2e;&#99;&#111;&#109;</a></p>\n<p>(<a href=\"/about-zh\">中文版</a>)</p>\n<hr>\n<p><a href=\"https://blog.lorrin.info/\">Blog</a>(<a href=\"https://blog.lorrin.info/atom.xml\">RSS</a>), <a href=\"https://github.com/LorrinWWW\">Github</a>, <a href=\"https://www.zhihu.com/people/wang-jue-9/activities\">知乎</a>, 欢迎关注～</p>\n"},{"title":"tags","date":"2016-11-27T05:15:12.000Z","type":"tags","layout":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2016-11-27 13:15:12\ntype: \"tags\"\nlayout: \"tags\"\n---\n","updated":"2018-03-10T21:47:27.796Z","path":"tags/index.html","comments":1,"_id":"ckw4qufos0006gwtl0lkj9jd7","content":"","site":{"data":{"recommended_posts":{"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"}],"https://LorrinWWW.github.io/posts/complexity/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/hexo-with-latex/":[{"title":"区块链时代的最正确的打赏姿势——在Hexo博客配置Coinhive网页挖矿","permalink":"http://www.davidfnck.com/blockchain/mine-xmr-in-hexo-by-coinhive.html/"}],"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/":[{"title":"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-statistic/":[{"title":"贝叶斯估计 Bayes estimation","permalink":"https://LorrinWWW.github.io/posts/Bayes-estimation/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"}],"https://LorrinWWW.github.io/posts/[2018.2.5]Nested-LSTMs/":[{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"}],"https://LorrinWWW.github.io/posts/datamining-pretreatment/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Several models for knowledge graph representing and completing 几个知识图谱模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/"}],"https://LorrinWWW.github.io/posts/compression/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/Sobolev-space/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/proba-ch5/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch3/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/proba-ch1/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/ML-CNN/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/EDP-basic-models/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/machine-learning/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/datamining-class-pred/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction/":[],"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/":[{"title":"OS notes","permalink":"https://LorrinWWW.github.io/posts/OS-notes/"}],"https://LorrinWWW.github.io/posts/projet-enjeu-plugin-chrome-101/":[{"title":"Chrome油猴脚本","permalink":"http://gubangzhong.cn/2017/06/13/Chrome油猴脚本/"}],"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/":[{"title":"MongoDB, Docker and Python","permalink":"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/":[{"title":"Hands on Scrapy","permalink":"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/OS-notes/":[{"title":"learning OS and building LorriOS","permalink":"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/"}],"https://LorrinWWW.github.io/posts/Bayes-estimation/":[{"title":"Note of statistic","permalink":"https://LorrinWWW.github.io/posts/Note-of-statistic/"}],"https://LorrinWWW.github.io/posts/management-of-the-firm/":[],"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"}],"https://LorrinWWW.github.io/posts/datamining-qualitative-induction/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-finite-element-method/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/QuadTree/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/proba-ch4/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch2/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/":[{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/Note-of-NLP/":[{"title":"Generative Adversarial Network","permalink":"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/"},{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"}],"https://LorrinWWW.github.io/posts/Note-of-datamining/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"数据挖掘-分类与预测","permalink":"https://LorrinWWW.github.io/posts/datamining-class-pred/"},{"title":"数据挖掘-预处理","permalink":"https://LorrinWWW.github.io/posts/datamining-pretreatment/"}],"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/Note-of-probability/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/participe-present-et-gerondif/":[],"https://LorrinWWW.github.io/posts/Hilbert-space/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/proba-ch6/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}]}}},"excerpt":"","more":""},{"title":"categories","date":"2016-11-27T05:13:12.000Z","type":"categories","layout":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2016-11-27 13:13:12\ntype: \"categories\" \nlayout: \"categories\"\n---\n","updated":"2018-03-10T21:47:49.441Z","path":"categories/index.html","comments":1,"_id":"ckw4qufou0008gwtl4chcawnh","content":"","site":{"data":{"recommended_posts":{"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"}],"https://LorrinWWW.github.io/posts/complexity/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/hexo-with-latex/":[{"title":"区块链时代的最正确的打赏姿势——在Hexo博客配置Coinhive网页挖矿","permalink":"http://www.davidfnck.com/blockchain/mine-xmr-in-hexo-by-coinhive.html/"}],"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/":[{"title":"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-statistic/":[{"title":"贝叶斯估计 Bayes estimation","permalink":"https://LorrinWWW.github.io/posts/Bayes-estimation/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"}],"https://LorrinWWW.github.io/posts/[2018.2.5]Nested-LSTMs/":[{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"}],"https://LorrinWWW.github.io/posts/datamining-pretreatment/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Several models for knowledge graph representing and completing 几个知识图谱模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/"}],"https://LorrinWWW.github.io/posts/compression/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/Sobolev-space/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/proba-ch5/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch3/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/proba-ch1/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/ML-CNN/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/EDP-basic-models/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/machine-learning/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/datamining-class-pred/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction/":[],"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/":[{"title":"OS notes","permalink":"https://LorrinWWW.github.io/posts/OS-notes/"}],"https://LorrinWWW.github.io/posts/projet-enjeu-plugin-chrome-101/":[{"title":"Chrome油猴脚本","permalink":"http://gubangzhong.cn/2017/06/13/Chrome油猴脚本/"}],"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/":[{"title":"MongoDB, Docker and Python","permalink":"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/":[{"title":"Hands on Scrapy","permalink":"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/OS-notes/":[{"title":"learning OS and building LorriOS","permalink":"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/"}],"https://LorrinWWW.github.io/posts/Bayes-estimation/":[{"title":"Note of statistic","permalink":"https://LorrinWWW.github.io/posts/Note-of-statistic/"}],"https://LorrinWWW.github.io/posts/management-of-the-firm/":[],"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"}],"https://LorrinWWW.github.io/posts/datamining-qualitative-induction/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-finite-element-method/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/QuadTree/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/proba-ch4/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch2/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/":[{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/Note-of-NLP/":[{"title":"Generative Adversarial Network","permalink":"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/"},{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"}],"https://LorrinWWW.github.io/posts/Note-of-datamining/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"数据挖掘-分类与预测","permalink":"https://LorrinWWW.github.io/posts/datamining-class-pred/"},{"title":"数据挖掘-预处理","permalink":"https://LorrinWWW.github.io/posts/datamining-pretreatment/"}],"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/Note-of-probability/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/participe-present-et-gerondif/":[],"https://LorrinWWW.github.io/posts/Hilbert-space/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/proba-ch6/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}]}}},"excerpt":"","more":""}],"Post":[{"title":"贝叶斯估计 Bayes estimation","date":"2017-01-07T08:46:31.000Z","_content":"\n在机器学习中，贝叶斯这个名字被不止一次的提到。正好又看到了贝叶斯参数估计，简单记录方法，可能没有严谨的数学推导，见谅。\n\n首先我们必须提及的就是历史上的两大学派——经典统计学派和贝叶斯统计学派。\n\n这里我们假设要估计的参数为$\\theta$，简单来说，经典统计学派认为$\\theta$是一个未知但固定的常数，而贝叶斯学派认为$\\theta$是一个变量。\n\n## 贝叶斯公式\n\n我们为什么需要贝叶斯估计呢？\n\n我们不妨看一个例子，经典统计学派我们使用极大似然估计法。\n\n假如学生在做一道题，当一个学生会做这道题时，他的正确率是98%。当他不会做这道题时，答题的正确率为5%。现在，有一个学生的对这道题测验结果为错误，问这个人会做这道题吗？ \n既然会做并做对的概率为98%，不会做但做对的概率为5%，如果用最大似然估计的方法，我们认为这个人已经感染了病毒。 \n\n但是如果如果这道题十分容易呢？比如，题目是1+1=？，事实上，就算是幼儿园的小孩也会做。那么这个估计结果其实是有时偏颇的。\n\n此时我们用贝叶斯方法进行估计，如果我们得知有一个先验概率，比如整体学生中只有1%的人会感染此种病毒，那么由贝叶斯公式： \n$$\np(y_i|x) = \\frac{p(x|y_i)p(y_i)}{p(x)}\n$$\n\n$$\np(不会做|做错)=\\frac{p(做错|不会做)p(不会做)}{p(做错)}=\\frac{0.95\\times 0.01}{0.95 \\times 0.01 + 0.02 \\times 0.99} = 0.324\n$$\n\n这么看来，还是会做的概率比较高，只不过这个学生比较粗心罢了。\n\n## 利用贝叶斯进行参数估计\n\n贝叶斯估计中，需要对参数有一个先验估计，并记录其分布为：\n$$\n\\theta: \\pi(\\theta)\n$$\n已知样本为\n$$\n\\widetilde{X} = \\{ X_1,...,X_n \\}\n$$\n参数的联合分布为\n$$\np(\\widetilde{x},\\theta) = p(\\widetilde{x} |\\theta)\\pi(\\theta)\n$$\n所以\n$$\n\\pi(\\theta|\\widetilde{x} ) = \\frac{p(\\widetilde{x} ,\\theta)}{p(\\widetilde{x})} \\\\\n= \\frac{p(\\widetilde{x} |\\theta)\\pi(\\theta)}{\\int{p(\\widetilde{x} |\\theta)\\pi(\\theta) d\\theta}}\n$$\n特别的，若T是一个充分统计量\n$$\np(\\widetilde{x}|\\theta) = p(\\widetilde{x} |T = t)p_T(t|\\theta) \\varpropto p_T(t|\\theta)\n$$\n从而\n$$\n\\pi(\\theta|\\widetilde{x} ) = \\pi(\\theta|t )\n$$\n这样，参数的分布就已经得到了，接下来一般通过三种方法进行估计：\n\n- 后验分布的众数进行估计\n- 后验分布的中位数进行估计\n- 后验分布的期望进行估计\n\n总结一下，贝叶斯估计就是，认为被估计参数是一个随机变量，通过已有的数据得到一个先验分布，结合这个先验分布再通过现有的条件，最后得到一个较为合理的后验分布。\n\n注：\n\n- 如果x的方差趋于无穷，意味着样本没有任何意义，估计结果等于先验估计。\n- 如果样本数量趋于无穷，则估计结果与先验估计无关。这说明先验估计实际上是为了弥补样本的不足。","source":"_posts/Bayes-estimation.md","raw":"---\ntitle: 贝叶斯估计 Bayes estimation\ndate: 2017-01-07 16:46:31\ncategories: [math]\ntags: [Bayes, statistic]\n---\n\n在机器学习中，贝叶斯这个名字被不止一次的提到。正好又看到了贝叶斯参数估计，简单记录方法，可能没有严谨的数学推导，见谅。\n\n首先我们必须提及的就是历史上的两大学派——经典统计学派和贝叶斯统计学派。\n\n这里我们假设要估计的参数为$\\theta$，简单来说，经典统计学派认为$\\theta$是一个未知但固定的常数，而贝叶斯学派认为$\\theta$是一个变量。\n\n## 贝叶斯公式\n\n我们为什么需要贝叶斯估计呢？\n\n我们不妨看一个例子，经典统计学派我们使用极大似然估计法。\n\n假如学生在做一道题，当一个学生会做这道题时，他的正确率是98%。当他不会做这道题时，答题的正确率为5%。现在，有一个学生的对这道题测验结果为错误，问这个人会做这道题吗？ \n既然会做并做对的概率为98%，不会做但做对的概率为5%，如果用最大似然估计的方法，我们认为这个人已经感染了病毒。 \n\n但是如果如果这道题十分容易呢？比如，题目是1+1=？，事实上，就算是幼儿园的小孩也会做。那么这个估计结果其实是有时偏颇的。\n\n此时我们用贝叶斯方法进行估计，如果我们得知有一个先验概率，比如整体学生中只有1%的人会感染此种病毒，那么由贝叶斯公式： \n$$\np(y_i|x) = \\frac{p(x|y_i)p(y_i)}{p(x)}\n$$\n\n$$\np(不会做|做错)=\\frac{p(做错|不会做)p(不会做)}{p(做错)}=\\frac{0.95\\times 0.01}{0.95 \\times 0.01 + 0.02 \\times 0.99} = 0.324\n$$\n\n这么看来，还是会做的概率比较高，只不过这个学生比较粗心罢了。\n\n## 利用贝叶斯进行参数估计\n\n贝叶斯估计中，需要对参数有一个先验估计，并记录其分布为：\n$$\n\\theta: \\pi(\\theta)\n$$\n已知样本为\n$$\n\\widetilde{X} = \\{ X_1,...,X_n \\}\n$$\n参数的联合分布为\n$$\np(\\widetilde{x},\\theta) = p(\\widetilde{x} |\\theta)\\pi(\\theta)\n$$\n所以\n$$\n\\pi(\\theta|\\widetilde{x} ) = \\frac{p(\\widetilde{x} ,\\theta)}{p(\\widetilde{x})} \\\\\n= \\frac{p(\\widetilde{x} |\\theta)\\pi(\\theta)}{\\int{p(\\widetilde{x} |\\theta)\\pi(\\theta) d\\theta}}\n$$\n特别的，若T是一个充分统计量\n$$\np(\\widetilde{x}|\\theta) = p(\\widetilde{x} |T = t)p_T(t|\\theta) \\varpropto p_T(t|\\theta)\n$$\n从而\n$$\n\\pi(\\theta|\\widetilde{x} ) = \\pi(\\theta|t )\n$$\n这样，参数的分布就已经得到了，接下来一般通过三种方法进行估计：\n\n- 后验分布的众数进行估计\n- 后验分布的中位数进行估计\n- 后验分布的期望进行估计\n\n总结一下，贝叶斯估计就是，认为被估计参数是一个随机变量，通过已有的数据得到一个先验分布，结合这个先验分布再通过现有的条件，最后得到一个较为合理的后验分布。\n\n注：\n\n- 如果x的方差趋于无穷，意味着样本没有任何意义，估计结果等于先验估计。\n- 如果样本数量趋于无穷，则估计结果与先验估计无关。这说明先验估计实际上是为了弥补样本的不足。","slug":"Bayes-estimation","published":1,"updated":"2017-01-07T16:48:18.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckw4qufom0001gwtl4ehk83to","content":"<p>在机器学习中，贝叶斯这个名字被不止一次的提到。正好又看到了贝叶斯参数估计，简单记录方法，可能没有严谨的数学推导，见谅。</p>\n<p>首先我们必须提及的就是历史上的两大学派——经典统计学派和贝叶斯统计学派。</p>\n<p>这里我们假设要估计的参数为$\\theta$，简单来说，经典统计学派认为$\\theta$是一个未知但固定的常数，而贝叶斯学派认为$\\theta$是一个变量。</p>\n<h2 id=\"贝叶斯公式\"><a href=\"#贝叶斯公式\" class=\"headerlink\" title=\"贝叶斯公式\"></a>贝叶斯公式</h2><p>我们为什么需要贝叶斯估计呢？</p>\n<p>我们不妨看一个例子，经典统计学派我们使用极大似然估计法。</p>\n<p>假如学生在做一道题，当一个学生会做这道题时，他的正确率是98%。当他不会做这道题时，答题的正确率为5%。现在，有一个学生的对这道题测验结果为错误，问这个人会做这道题吗？<br>既然会做并做对的概率为98%，不会做但做对的概率为5%，如果用最大似然估计的方法，我们认为这个人已经感染了病毒。 </p>\n<p>但是如果如果这道题十分容易呢？比如，题目是1+1=？，事实上，就算是幼儿园的小孩也会做。那么这个估计结果其实是有时偏颇的。</p>\n<p>此时我们用贝叶斯方法进行估计，如果我们得知有一个先验概率，比如整体学生中只有1%的人会感染此种病毒，那么由贝叶斯公式：<br>$$<br>p(y_i|x) = \\frac{p(x|y_i)p(y_i)}{p(x)}<br>$$</p>\n<p>$$<br>p(不会做|做错)=\\frac{p(做错|不会做)p(不会做)}{p(做错)}=\\frac{0.95\\times 0.01}{0.95 \\times 0.01 + 0.02 \\times 0.99} = 0.324<br>$$</p>\n<p>这么看来，还是会做的概率比较高，只不过这个学生比较粗心罢了。</p>\n<h2 id=\"利用贝叶斯进行参数估计\"><a href=\"#利用贝叶斯进行参数估计\" class=\"headerlink\" title=\"利用贝叶斯进行参数估计\"></a>利用贝叶斯进行参数估计</h2><p>贝叶斯估计中，需要对参数有一个先验估计，并记录其分布为：<br>$$<br>\\theta: \\pi(\\theta)<br>$$<br>已知样本为<br>$$<br>\\widetilde{X} = { X_1,…,X_n }<br>$$<br>参数的联合分布为<br>$$<br>p(\\widetilde{x},\\theta) = p(\\widetilde{x} |\\theta)\\pi(\\theta)<br>$$<br>所以<br>$$<br>\\pi(\\theta|\\widetilde{x} ) = \\frac{p(\\widetilde{x} ,\\theta)}{p(\\widetilde{x})} \\<br>= \\frac{p(\\widetilde{x} |\\theta)\\pi(\\theta)}{\\int{p(\\widetilde{x} |\\theta)\\pi(\\theta) d\\theta}}<br>$$<br>特别的，若T是一个充分统计量<br>$$<br>p(\\widetilde{x}|\\theta) = p(\\widetilde{x} |T = t)p_T(t|\\theta) \\varpropto p_T(t|\\theta)<br>$$<br>从而<br>$$<br>\\pi(\\theta|\\widetilde{x} ) = \\pi(\\theta|t )<br>$$<br>这样，参数的分布就已经得到了，接下来一般通过三种方法进行估计：</p>\n<ul>\n<li>后验分布的众数进行估计</li>\n<li>后验分布的中位数进行估计</li>\n<li>后验分布的期望进行估计</li>\n</ul>\n<p>总结一下，贝叶斯估计就是，认为被估计参数是一个随机变量，通过已有的数据得到一个先验分布，结合这个先验分布再通过现有的条件，最后得到一个较为合理的后验分布。</p>\n<p>注：</p>\n<ul>\n<li>如果x的方差趋于无穷，意味着样本没有任何意义，估计结果等于先验估计。</li>\n<li>如果样本数量趋于无穷，则估计结果与先验估计无关。这说明先验估计实际上是为了弥补样本的不足。</li>\n</ul>\n","site":{"data":{"recommended_posts":{"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"}],"https://LorrinWWW.github.io/posts/complexity/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/hexo-with-latex/":[{"title":"区块链时代的最正确的打赏姿势——在Hexo博客配置Coinhive网页挖矿","permalink":"http://www.davidfnck.com/blockchain/mine-xmr-in-hexo-by-coinhive.html/"}],"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/":[{"title":"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-statistic/":[{"title":"贝叶斯估计 Bayes estimation","permalink":"https://LorrinWWW.github.io/posts/Bayes-estimation/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"}],"https://LorrinWWW.github.io/posts/[2018.2.5]Nested-LSTMs/":[{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"}],"https://LorrinWWW.github.io/posts/datamining-pretreatment/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Several models for knowledge graph representing and completing 几个知识图谱模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/"}],"https://LorrinWWW.github.io/posts/compression/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/Sobolev-space/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/proba-ch5/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch3/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/proba-ch1/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/ML-CNN/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/EDP-basic-models/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/machine-learning/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/datamining-class-pred/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction/":[],"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/":[{"title":"OS notes","permalink":"https://LorrinWWW.github.io/posts/OS-notes/"}],"https://LorrinWWW.github.io/posts/projet-enjeu-plugin-chrome-101/":[{"title":"Chrome油猴脚本","permalink":"http://gubangzhong.cn/2017/06/13/Chrome油猴脚本/"}],"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/":[{"title":"MongoDB, Docker and Python","permalink":"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/":[{"title":"Hands on Scrapy","permalink":"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/OS-notes/":[{"title":"learning OS and building LorriOS","permalink":"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/"}],"https://LorrinWWW.github.io/posts/Bayes-estimation/":[{"title":"Note of statistic","permalink":"https://LorrinWWW.github.io/posts/Note-of-statistic/"}],"https://LorrinWWW.github.io/posts/management-of-the-firm/":[],"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"}],"https://LorrinWWW.github.io/posts/datamining-qualitative-induction/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-finite-element-method/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/QuadTree/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/proba-ch4/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch2/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/":[{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/Note-of-NLP/":[{"title":"Generative Adversarial Network","permalink":"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/"},{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"}],"https://LorrinWWW.github.io/posts/Note-of-datamining/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"数据挖掘-分类与预测","permalink":"https://LorrinWWW.github.io/posts/datamining-class-pred/"},{"title":"数据挖掘-预处理","permalink":"https://LorrinWWW.github.io/posts/datamining-pretreatment/"}],"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/Note-of-probability/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/participe-present-et-gerondif/":[],"https://LorrinWWW.github.io/posts/Hilbert-space/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/proba-ch6/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}]}}},"excerpt":"","more":"<p>在机器学习中，贝叶斯这个名字被不止一次的提到。正好又看到了贝叶斯参数估计，简单记录方法，可能没有严谨的数学推导，见谅。</p>\n<p>首先我们必须提及的就是历史上的两大学派——经典统计学派和贝叶斯统计学派。</p>\n<p>这里我们假设要估计的参数为$\\theta$，简单来说，经典统计学派认为$\\theta$是一个未知但固定的常数，而贝叶斯学派认为$\\theta$是一个变量。</p>\n<h2 id=\"贝叶斯公式\"><a href=\"#贝叶斯公式\" class=\"headerlink\" title=\"贝叶斯公式\"></a>贝叶斯公式</h2><p>我们为什么需要贝叶斯估计呢？</p>\n<p>我们不妨看一个例子，经典统计学派我们使用极大似然估计法。</p>\n<p>假如学生在做一道题，当一个学生会做这道题时，他的正确率是98%。当他不会做这道题时，答题的正确率为5%。现在，有一个学生的对这道题测验结果为错误，问这个人会做这道题吗？<br>既然会做并做对的概率为98%，不会做但做对的概率为5%，如果用最大似然估计的方法，我们认为这个人已经感染了病毒。 </p>\n<p>但是如果如果这道题十分容易呢？比如，题目是1+1=？，事实上，就算是幼儿园的小孩也会做。那么这个估计结果其实是有时偏颇的。</p>\n<p>此时我们用贝叶斯方法进行估计，如果我们得知有一个先验概率，比如整体学生中只有1%的人会感染此种病毒，那么由贝叶斯公式：<br>$$<br>p(y_i|x) = \\frac{p(x|y_i)p(y_i)}{p(x)}<br>$$</p>\n<p>$$<br>p(不会做|做错)=\\frac{p(做错|不会做)p(不会做)}{p(做错)}=\\frac{0.95\\times 0.01}{0.95 \\times 0.01 + 0.02 \\times 0.99} = 0.324<br>$$</p>\n<p>这么看来，还是会做的概率比较高，只不过这个学生比较粗心罢了。</p>\n<h2 id=\"利用贝叶斯进行参数估计\"><a href=\"#利用贝叶斯进行参数估计\" class=\"headerlink\" title=\"利用贝叶斯进行参数估计\"></a>利用贝叶斯进行参数估计</h2><p>贝叶斯估计中，需要对参数有一个先验估计，并记录其分布为：<br>$$<br>\\theta: \\pi(\\theta)<br>$$<br>已知样本为<br>$$<br>\\widetilde{X} = { X_1,…,X_n }<br>$$<br>参数的联合分布为<br>$$<br>p(\\widetilde{x},\\theta) = p(\\widetilde{x} |\\theta)\\pi(\\theta)<br>$$<br>所以<br>$$<br>\\pi(\\theta|\\widetilde{x} ) = \\frac{p(\\widetilde{x} ,\\theta)}{p(\\widetilde{x})} \\<br>= \\frac{p(\\widetilde{x} |\\theta)\\pi(\\theta)}{\\int{p(\\widetilde{x} |\\theta)\\pi(\\theta) d\\theta}}<br>$$<br>特别的，若T是一个充分统计量<br>$$<br>p(\\widetilde{x}|\\theta) = p(\\widetilde{x} |T = t)p_T(t|\\theta) \\varpropto p_T(t|\\theta)<br>$$<br>从而<br>$$<br>\\pi(\\theta|\\widetilde{x} ) = \\pi(\\theta|t )<br>$$<br>这样，参数的分布就已经得到了，接下来一般通过三种方法进行估计：</p>\n<ul>\n<li>后验分布的众数进行估计</li>\n<li>后验分布的中位数进行估计</li>\n<li>后验分布的期望进行估计</li>\n</ul>\n<p>总结一下，贝叶斯估计就是，认为被估计参数是一个随机变量，通过已有的数据得到一个先验分布，结合这个先验分布再通过现有的条件，最后得到一个较为合理的后验分布。</p>\n<p>注：</p>\n<ul>\n<li>如果x的方差趋于无穷，意味着样本没有任何意义，估计结果等于先验估计。</li>\n<li>如果样本数量趋于无穷，则估计结果与先验估计无关。这说明先验估计实际上是为了弥补样本的不足。</li>\n</ul>\n"},{"title":"EDP基础：矩阵的复习","date":"2017-02-07T02:22:27.000Z","_content":"\n## 注意点：\n\n1. A est une matrice hermitienne <=> A=A* et donc A est normale\n2. A est une matrice  unitaire (酉矩阵) <=> A^-1 = A* et donc A est normale\n3. A est une matrice orthogonale <=> A=A^T\n\n\n\n## Schur定理（矩陣三角化）\n\n$$\nA = [a_{ij}]_{n\\times n} , 特征值\\lambda_i , 对应特征向量x_i \\\\\nS = [x_1,...,x_n], D = diag[\\lambda_1,...,\\lambda_n] \\\\\n则,A是可对角化矩阵 \\\\\nA = SDS^{-1}\n$$\n\n如果**A**是*n*阶的复方阵，则存在*n*阶酉矩阵U，*n*阶上三角矩阵T，使得：\n$$\nA = UTU^{-1}\n$$\n\n## 酉矩阵\n\n$$\nUU^* = I \\\\\ni.e. \\to U^*= U^{-1}\n$$\n\nU是酉矩阵。\n\n\n\n## Cholesky分解\n\n条件：\n\n1. une matrice hermitienne：矩阵中的元素共轭对称（复数域的定义，类比于实数对称矩阵）。Hermitiank意味着对于任意向量x和y，(x*)Ay共轭相等\n2. Positive-definite：正定矩阵A意味着，对于任何向量x，(x^T)Ax总是大于零(复数域是(x*)Ax>0)\n\n则存在L为下三角矩阵，使得 A = LL*。\n\n\n\n## QR分解\n\n目标：A = QR，*Q*是正交矩阵（意味着*Q*T*Q* = *I*）而*R*是上三角矩阵。\n\n","source":"_posts/EDP-basic-matrix-review.md","raw":"---\ntitle: 'EDP基础：矩阵的复习'\ndate: 2017-02-07 10:22:27\ncategories: [math]\ntags: [EDP, matrix]\n---\n\n## 注意点：\n\n1. A est une matrice hermitienne <=> A=A* et donc A est normale\n2. A est une matrice  unitaire (酉矩阵) <=> A^-1 = A* et donc A est normale\n3. A est une matrice orthogonale <=> A=A^T\n\n\n\n## Schur定理（矩陣三角化）\n\n$$\nA = [a_{ij}]_{n\\times n} , 特征值\\lambda_i , 对应特征向量x_i \\\\\nS = [x_1,...,x_n], D = diag[\\lambda_1,...,\\lambda_n] \\\\\n则,A是可对角化矩阵 \\\\\nA = SDS^{-1}\n$$\n\n如果**A**是*n*阶的复方阵，则存在*n*阶酉矩阵U，*n*阶上三角矩阵T，使得：\n$$\nA = UTU^{-1}\n$$\n\n## 酉矩阵\n\n$$\nUU^* = I \\\\\ni.e. \\to U^*= U^{-1}\n$$\n\nU是酉矩阵。\n\n\n\n## Cholesky分解\n\n条件：\n\n1. une matrice hermitienne：矩阵中的元素共轭对称（复数域的定义，类比于实数对称矩阵）。Hermitiank意味着对于任意向量x和y，(x*)Ay共轭相等\n2. Positive-definite：正定矩阵A意味着，对于任何向量x，(x^T)Ax总是大于零(复数域是(x*)Ax>0)\n\n则存在L为下三角矩阵，使得 A = LL*。\n\n\n\n## QR分解\n\n目标：A = QR，*Q*是正交矩阵（意味着*Q*T*Q* = *I*）而*R*是上三角矩阵。\n\n","slug":"EDP-basic-matrix-review","published":1,"updated":"2017-02-07T10:40:09.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckw4qufop0003gwtl3dk5e0gb","content":"<h2 id=\"注意点：\"><a href=\"#注意点：\" class=\"headerlink\" title=\"注意点：\"></a>注意点：</h2><ol>\n<li>A est une matrice hermitienne &lt;=&gt; A=A* et donc A est normale</li>\n<li>A est une matrice  unitaire (酉矩阵) &lt;=&gt; A^-1 = A* et donc A est normale</li>\n<li>A est une matrice orthogonale &lt;=&gt; A=A^T</li>\n</ol>\n<h2 id=\"Schur定理（矩陣三角化）\"><a href=\"#Schur定理（矩陣三角化）\" class=\"headerlink\" title=\"Schur定理（矩陣三角化）\"></a>Schur定理（矩陣三角化）</h2><p>$$<br>A = [a_{ij}]_{n\\times n} , 特征值\\lambda_i , 对应特征向量x_i \\<br>S = [x_1,…,x_n], D = diag[\\lambda_1,…,\\lambda_n] \\<br>则,A是可对角化矩阵 \\<br>A = SDS^{-1}<br>$$</p>\n<p>如果<strong>A</strong>是<em>n</em>阶的复方阵，则存在<em>n</em>阶酉矩阵U，<em>n</em>阶上三角矩阵T，使得：<br>$$<br>A = UTU^{-1}<br>$$</p>\n<h2 id=\"酉矩阵\"><a href=\"#酉矩阵\" class=\"headerlink\" title=\"酉矩阵\"></a>酉矩阵</h2><p>$$<br>UU^* = I \\<br>i.e. \\to U^*= U^{-1}<br>$$</p>\n<p>U是酉矩阵。</p>\n<h2 id=\"Cholesky分解\"><a href=\"#Cholesky分解\" class=\"headerlink\" title=\"Cholesky分解\"></a>Cholesky分解</h2><p>条件：</p>\n<ol>\n<li>une matrice hermitienne：矩阵中的元素共轭对称（复数域的定义，类比于实数对称矩阵）。Hermitiank意味着对于任意向量x和y，(x*)Ay共轭相等</li>\n<li>Positive-definite：正定矩阵A意味着，对于任何向量x，(x^T)Ax总是大于零(复数域是(x*)Ax&gt;0)</li>\n</ol>\n<p>则存在L为下三角矩阵，使得 A = LL*。</p>\n<h2 id=\"QR分解\"><a href=\"#QR分解\" class=\"headerlink\" title=\"QR分解\"></a>QR分解</h2><p>目标：A = QR，<em>Q</em>是正交矩阵（意味着<em>Q</em>T<em>Q</em>&nbsp;=&nbsp;<em>I</em>）而<em>R</em>是上三角矩阵。</p>\n","site":{"data":{"recommended_posts":{"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"}],"https://LorrinWWW.github.io/posts/complexity/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/hexo-with-latex/":[{"title":"区块链时代的最正确的打赏姿势——在Hexo博客配置Coinhive网页挖矿","permalink":"http://www.davidfnck.com/blockchain/mine-xmr-in-hexo-by-coinhive.html/"}],"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/":[{"title":"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-statistic/":[{"title":"贝叶斯估计 Bayes estimation","permalink":"https://LorrinWWW.github.io/posts/Bayes-estimation/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"}],"https://LorrinWWW.github.io/posts/[2018.2.5]Nested-LSTMs/":[{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"}],"https://LorrinWWW.github.io/posts/datamining-pretreatment/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Several models for knowledge graph representing and completing 几个知识图谱模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/"}],"https://LorrinWWW.github.io/posts/compression/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/Sobolev-space/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/proba-ch5/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch3/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/proba-ch1/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/ML-CNN/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/EDP-basic-models/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/machine-learning/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/datamining-class-pred/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction/":[],"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/":[{"title":"OS notes","permalink":"https://LorrinWWW.github.io/posts/OS-notes/"}],"https://LorrinWWW.github.io/posts/projet-enjeu-plugin-chrome-101/":[{"title":"Chrome油猴脚本","permalink":"http://gubangzhong.cn/2017/06/13/Chrome油猴脚本/"}],"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/":[{"title":"MongoDB, Docker and Python","permalink":"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/":[{"title":"Hands on Scrapy","permalink":"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/OS-notes/":[{"title":"learning OS and building LorriOS","permalink":"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/"}],"https://LorrinWWW.github.io/posts/Bayes-estimation/":[{"title":"Note of statistic","permalink":"https://LorrinWWW.github.io/posts/Note-of-statistic/"}],"https://LorrinWWW.github.io/posts/management-of-the-firm/":[],"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"}],"https://LorrinWWW.github.io/posts/datamining-qualitative-induction/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-finite-element-method/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/QuadTree/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/proba-ch4/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch2/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/":[{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/Note-of-NLP/":[{"title":"Generative Adversarial Network","permalink":"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/"},{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"}],"https://LorrinWWW.github.io/posts/Note-of-datamining/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"数据挖掘-分类与预测","permalink":"https://LorrinWWW.github.io/posts/datamining-class-pred/"},{"title":"数据挖掘-预处理","permalink":"https://LorrinWWW.github.io/posts/datamining-pretreatment/"}],"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/Note-of-probability/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/participe-present-et-gerondif/":[],"https://LorrinWWW.github.io/posts/Hilbert-space/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/proba-ch6/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}]}}},"excerpt":"","more":"<h2 id=\"注意点：\"><a href=\"#注意点：\" class=\"headerlink\" title=\"注意点：\"></a>注意点：</h2><ol>\n<li>A est une matrice hermitienne &lt;=&gt; A=A* et donc A est normale</li>\n<li>A est une matrice  unitaire (酉矩阵) &lt;=&gt; A^-1 = A* et donc A est normale</li>\n<li>A est une matrice orthogonale &lt;=&gt; A=A^T</li>\n</ol>\n<h2 id=\"Schur定理（矩陣三角化）\"><a href=\"#Schur定理（矩陣三角化）\" class=\"headerlink\" title=\"Schur定理（矩陣三角化）\"></a>Schur定理（矩陣三角化）</h2><p>$$<br>A = [a_{ij}]_{n\\times n} , 特征值\\lambda_i , 对应特征向量x_i \\<br>S = [x_1,…,x_n], D = diag[\\lambda_1,…,\\lambda_n] \\<br>则,A是可对角化矩阵 \\<br>A = SDS^{-1}<br>$$</p>\n<p>如果<strong>A</strong>是<em>n</em>阶的复方阵，则存在<em>n</em>阶酉矩阵U，<em>n</em>阶上三角矩阵T，使得：<br>$$<br>A = UTU^{-1}<br>$$</p>\n<h2 id=\"酉矩阵\"><a href=\"#酉矩阵\" class=\"headerlink\" title=\"酉矩阵\"></a>酉矩阵</h2><p>$$<br>UU^* = I \\<br>i.e. \\to U^*= U^{-1}<br>$$</p>\n<p>U是酉矩阵。</p>\n<h2 id=\"Cholesky分解\"><a href=\"#Cholesky分解\" class=\"headerlink\" title=\"Cholesky分解\"></a>Cholesky分解</h2><p>条件：</p>\n<ol>\n<li>une matrice hermitienne：矩阵中的元素共轭对称（复数域的定义，类比于实数对称矩阵）。Hermitiank意味着对于任意向量x和y，(x*)Ay共轭相等</li>\n<li>Positive-definite：正定矩阵A意味着，对于任何向量x，(x^T)Ax总是大于零(复数域是(x*)Ax&gt;0)</li>\n</ol>\n<p>则存在L为下三角矩阵，使得 A = LL*。</p>\n<h2 id=\"QR分解\"><a href=\"#QR分解\" class=\"headerlink\" title=\"QR分解\"></a>QR分解</h2><p>目标：A = QR，<em>Q</em>是正交矩阵（意味着<em>Q</em>T<em>Q</em> = <em>I</em>）而<em>R</em>是上三角矩阵。</p>\n"},{"title":"EDP basic models","date":"2017-03-06T02:04:58.000Z","_content":"\n# Examples of moedels\n\n1.    Problem of Cauchy\n\n      **Nomenclature** : problem of Cauchy\n\n      **Equation** : system of 2 differential equations of order 2\n\n      **Conditions** : initials (all the data from the same point) i.e. fix time\n\n      $$\n      \\begin{equation}\n        \\begin{cases}\n          \\frac{dS}{dt}(t) = F(S,R)  ,  \\\\\n          \\frac{dR}{dt}(t) = G(S,R) ,  \\\\\n          S(0) = S^0 \\\\\n          R(0) = R^0\n        \\end{cases}\n      \\end{equation}\n      $$\n\n2.    Problem of Dirichlet\n\n      **Nomenclature** : problem of Dirichlet\n\n      **Equation** : scalar differential equation of order 2.\n\n      **Conditions** : prescribed value at the edge\n\n      $$\n      \\begin{equation}\n        \\begin{cases}\n      - \\frac{d}{dx} ( k \\frac{d\\theta}{dx})(x) + \\theta(x) = f(x) 0<x<L, \\\\\n        \\theta(0) = \\theta_0, \\theta(L) = \\theta_L \\\\\n     \\end{cases}\n   \\end{equation}\n$$\n\n   **Remark:**\n\n   - 如果k是常数，解为指数函数；若否，我们可以用两种数值计算：les differences finies et les elements finis.\n   - 可以使用*射击法*，认为$\\theta(L)$由$\\theta'(0)$得到，接下来就是验证是否能找到一个好的$\\theta'(0)$使$\\theta(L)=\\theta(L) = \\theta_L$\n\n### Physic examples\n\n- Fluide environnant au repos\n$$\n  \\rho c_v \\partial_t{\\theta} - div(k\\cdot\\overrightarrow{grad}(\\theta)) = f\n$$\n  - init condition:\n$$\n    \\theta(0,.) = \\theta_0\n    $$\n\n-   condition at the edges could be:\n\n    - condition of Dirichlet:\n      $$\n      \\theta(t,.) = g(t)\n      $$\n\n    - Condition of Neumann:\n      $$\n      \\overrightarrow{grad}(\\theta)(t,.) \\cdot \\vec{n}= h(t)\n      $$\n\n    - Condition of Dirichlet and Neumann: Mix\n\n-   Fluide environnant en mouvement a une vitesse $\\vec u(t,x)$\n    $$\n    \\rho c_v \\partial_t{\\theta} - div(\\rho c_v \\theta \\vec u) - div(k\\cdot\\overrightarrow{grad}(\\theta)) = f\n    $$\n    si le fluide est incompressible i.e $\\partial_t \\rho = 0$ soit encore $div(\\vec u) = 0$ (直观上理解，若不可压缩，则速度场一定是连续的，不然一定会在局部压缩), alors:\n    $$\n    \\rho c_v \\partial_t{\\theta} - \\vec u \\cdot div(\\rho c_v \\theta ) - div(k\\cdot\\overrightarrow{grad}(\\theta)) = f\n    $$\n    ​\n\n    ​","source":"_posts/EDP-basic-models.md","raw":"---\ntitle: EDP basic models\ncategories: [math]\ntags:\n  - math\n  - EDP\ndate: 2017-03-06 10:04:58\n---\n\n# Examples of moedels\n\n1.    Problem of Cauchy\n\n      **Nomenclature** : problem of Cauchy\n\n      **Equation** : system of 2 differential equations of order 2\n\n      **Conditions** : initials (all the data from the same point) i.e. fix time\n\n      $$\n      \\begin{equation}\n        \\begin{cases}\n          \\frac{dS}{dt}(t) = F(S,R)  ,  \\\\\n          \\frac{dR}{dt}(t) = G(S,R) ,  \\\\\n          S(0) = S^0 \\\\\n          R(0) = R^0\n        \\end{cases}\n      \\end{equation}\n      $$\n\n2.    Problem of Dirichlet\n\n      **Nomenclature** : problem of Dirichlet\n\n      **Equation** : scalar differential equation of order 2.\n\n      **Conditions** : prescribed value at the edge\n\n      $$\n      \\begin{equation}\n        \\begin{cases}\n      - \\frac{d}{dx} ( k \\frac{d\\theta}{dx})(x) + \\theta(x) = f(x) 0<x<L, \\\\\n        \\theta(0) = \\theta_0, \\theta(L) = \\theta_L \\\\\n     \\end{cases}\n   \\end{equation}\n$$\n\n   **Remark:**\n\n   - 如果k是常数，解为指数函数；若否，我们可以用两种数值计算：les differences finies et les elements finis.\n   - 可以使用*射击法*，认为$\\theta(L)$由$\\theta'(0)$得到，接下来就是验证是否能找到一个好的$\\theta'(0)$使$\\theta(L)=\\theta(L) = \\theta_L$\n\n### Physic examples\n\n- Fluide environnant au repos\n$$\n  \\rho c_v \\partial_t{\\theta} - div(k\\cdot\\overrightarrow{grad}(\\theta)) = f\n$$\n  - init condition:\n$$\n    \\theta(0,.) = \\theta_0\n    $$\n\n-   condition at the edges could be:\n\n    - condition of Dirichlet:\n      $$\n      \\theta(t,.) = g(t)\n      $$\n\n    - Condition of Neumann:\n      $$\n      \\overrightarrow{grad}(\\theta)(t,.) \\cdot \\vec{n}= h(t)\n      $$\n\n    - Condition of Dirichlet and Neumann: Mix\n\n-   Fluide environnant en mouvement a une vitesse $\\vec u(t,x)$\n    $$\n    \\rho c_v \\partial_t{\\theta} - div(\\rho c_v \\theta \\vec u) - div(k\\cdot\\overrightarrow{grad}(\\theta)) = f\n    $$\n    si le fluide est incompressible i.e $\\partial_t \\rho = 0$ soit encore $div(\\vec u) = 0$ (直观上理解，若不可压缩，则速度场一定是连续的，不然一定会在局部压缩), alors:\n    $$\n    \\rho c_v \\partial_t{\\theta} - \\vec u \\cdot div(\\rho c_v \\theta ) - div(k\\cdot\\overrightarrow{grad}(\\theta)) = f\n    $$\n    ​\n\n    ​","slug":"EDP-basic-models","published":1,"updated":"2017-03-06T11:11:26.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckw4qufot0007gwtldi2mbss0","content":"<h1 id=\"Examples-of-moedels\"><a href=\"#Examples-of-moedels\" class=\"headerlink\" title=\"Examples of moedels\"></a>Examples of moedels</h1><ol>\n<li><p>Problem of Cauchy</p>\n<p>   <strong>Nomenclature</strong> : problem of Cauchy</p>\n<p>   <strong>Equation</strong> : system of 2 differential equations of order 2</p>\n<p>   <strong>Conditions</strong> : initials (all the data from the same point) i.e. fix time</p>\n<p>   $$<br>   \\begin{equation}</p>\n<pre><code> \\begin{cases}\n   \\frac{dS}{dt}(t) = F(S,R)  ,  \\\\\n   \\frac{dR}{dt}(t) = G(S,R) ,  \\\\\n   S(0) = S^0 \\\\\n   R(0) = R^0\n \\end{cases}\n</code></pre>\n<p>   \\end{equation}<br>   $$</p>\n</li>\n<li><p>Problem of Dirichlet</p>\n<p>   <strong>Nomenclature</strong> : problem of Dirichlet</p>\n<p>   <strong>Equation</strong> : scalar differential equation of order 2.</p>\n<p>   <strong>Conditions</strong> : prescribed value at the edge</p>\n<p>   $$<br>   \\begin{equation}</p>\n<pre><code> \\begin{cases}\n</code></pre>\n<ul>\n<li>\\frac{d}{dx} ( k \\frac{d\\theta}{dx})(x) + \\theta(x) = f(x) 0&lt;x&lt;L, \\<br>\\theta(0) = \\theta_0, \\theta(L) = \\theta_L \\<br>\\end{cases}<br>\\end{equation}<br>$$</li>\n</ul>\n</li>\n</ol>\n<p>   <strong>Remark:</strong></p>\n<ul>\n<li>如果k是常数，解为指数函数；若否，我们可以用两种数值计算：les differences finies et les elements finis.</li>\n<li>可以使用<em>射击法</em>，认为$\\theta(L)$由$\\theta’(0)$得到，接下来就是验证是否能找到一个好的$\\theta’(0)$使$\\theta(L)=\\theta(L) = \\theta_L$</li>\n</ul>\n<h3 id=\"Physic-examples\"><a href=\"#Physic-examples\" class=\"headerlink\" title=\"Physic examples\"></a>Physic examples</h3><ul>\n<li><p>Fluide environnant au repos<br>$$<br>\\rho c_v \\partial_t{\\theta} - div(k\\cdot\\overrightarrow{grad}(\\theta)) = f<br>$$</p>\n<ul>\n<li>init condition:<br>$$<br>\\theta(0,.) = \\theta_0<br>$$</li>\n</ul>\n</li>\n<li><p>condition at the edges could be:</p>\n<ul>\n<li><p>condition of Dirichlet:<br>$$<br>\\theta(t,.) = g(t)<br>$$</p>\n</li>\n<li><p>Condition of Neumann:<br>$$<br>\\overrightarrow{grad}(\\theta)(t,.) \\cdot \\vec{n}= h(t)<br>$$</p>\n</li>\n<li><p>Condition of Dirichlet and Neumann: Mix</p>\n</li>\n</ul>\n</li>\n<li><p>Fluide environnant en mouvement a une vitesse $\\vec u(t,x)$<br>  $$<br>  \\rho c_v \\partial_t{\\theta} - div(\\rho c_v \\theta \\vec u) - div(k\\cdot\\overrightarrow{grad}(\\theta)) = f<br>  $$<br>  si le fluide est incompressible i.e $\\partial_t \\rho = 0$ soit encore $div(\\vec u) = 0$ (直观上理解，若不可压缩，则速度场一定是连续的，不然一定会在局部压缩), alors:<br>  $$<br>  \\rho c_v \\partial_t{\\theta} - \\vec u \\cdot div(\\rho c_v \\theta ) - div(k\\cdot\\overrightarrow{grad}(\\theta)) = f<br>  $$<br>  ​</p>\n<p>  ​</p>\n</li>\n</ul>\n","site":{"data":{"recommended_posts":{"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"}],"https://LorrinWWW.github.io/posts/complexity/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/hexo-with-latex/":[{"title":"区块链时代的最正确的打赏姿势——在Hexo博客配置Coinhive网页挖矿","permalink":"http://www.davidfnck.com/blockchain/mine-xmr-in-hexo-by-coinhive.html/"}],"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/":[{"title":"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-statistic/":[{"title":"贝叶斯估计 Bayes estimation","permalink":"https://LorrinWWW.github.io/posts/Bayes-estimation/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"}],"https://LorrinWWW.github.io/posts/[2018.2.5]Nested-LSTMs/":[{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"}],"https://LorrinWWW.github.io/posts/datamining-pretreatment/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Several models for knowledge graph representing and completing 几个知识图谱模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/"}],"https://LorrinWWW.github.io/posts/compression/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/Sobolev-space/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/proba-ch5/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch3/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/proba-ch1/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/ML-CNN/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/EDP-basic-models/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/machine-learning/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/datamining-class-pred/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction/":[],"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/":[{"title":"OS notes","permalink":"https://LorrinWWW.github.io/posts/OS-notes/"}],"https://LorrinWWW.github.io/posts/projet-enjeu-plugin-chrome-101/":[{"title":"Chrome油猴脚本","permalink":"http://gubangzhong.cn/2017/06/13/Chrome油猴脚本/"}],"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/":[{"title":"MongoDB, Docker and Python","permalink":"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/":[{"title":"Hands on Scrapy","permalink":"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/OS-notes/":[{"title":"learning OS and building LorriOS","permalink":"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/"}],"https://LorrinWWW.github.io/posts/Bayes-estimation/":[{"title":"Note of statistic","permalink":"https://LorrinWWW.github.io/posts/Note-of-statistic/"}],"https://LorrinWWW.github.io/posts/management-of-the-firm/":[],"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"}],"https://LorrinWWW.github.io/posts/datamining-qualitative-induction/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-finite-element-method/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/QuadTree/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/proba-ch4/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch2/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/":[{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/Note-of-NLP/":[{"title":"Generative Adversarial Network","permalink":"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/"},{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"}],"https://LorrinWWW.github.io/posts/Note-of-datamining/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"数据挖掘-分类与预测","permalink":"https://LorrinWWW.github.io/posts/datamining-class-pred/"},{"title":"数据挖掘-预处理","permalink":"https://LorrinWWW.github.io/posts/datamining-pretreatment/"}],"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/Note-of-probability/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/participe-present-et-gerondif/":[],"https://LorrinWWW.github.io/posts/Hilbert-space/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/proba-ch6/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}]}}},"excerpt":"","more":"<h1 id=\"Examples-of-moedels\"><a href=\"#Examples-of-moedels\" class=\"headerlink\" title=\"Examples of moedels\"></a>Examples of moedels</h1><ol>\n<li><p>Problem of Cauchy</p>\n<p>   <strong>Nomenclature</strong> : problem of Cauchy</p>\n<p>   <strong>Equation</strong> : system of 2 differential equations of order 2</p>\n<p>   <strong>Conditions</strong> : initials (all the data from the same point) i.e. fix time</p>\n<p>   $$<br>   \\begin{equation}</p>\n<pre><code> \\begin&#123;cases&#125;\n   \\frac&#123;dS&#125;&#123;dt&#125;(t) = F(S,R)  ,  \\\\\n   \\frac&#123;dR&#125;&#123;dt&#125;(t) = G(S,R) ,  \\\\\n   S(0) = S^0 \\\\\n   R(0) = R^0\n \\end&#123;cases&#125;\n</code></pre>\n<p>   \\end{equation}<br>   $$</p>\n</li>\n<li><p>Problem of Dirichlet</p>\n<p>   <strong>Nomenclature</strong> : problem of Dirichlet</p>\n<p>   <strong>Equation</strong> : scalar differential equation of order 2.</p>\n<p>   <strong>Conditions</strong> : prescribed value at the edge</p>\n<p>   $$<br>   \\begin{equation}</p>\n<pre><code> \\begin&#123;cases&#125;\n</code></pre>\n<ul>\n<li>\\frac{d}{dx} ( k \\frac{d\\theta}{dx})(x) + \\theta(x) = f(x) 0&lt;x&lt;L, \\<br>\\theta(0) = \\theta_0, \\theta(L) = \\theta_L \\<br>\\end{cases}<br>\\end{equation}<br>$$</li>\n</ul>\n</li>\n</ol>\n<p>   <strong>Remark:</strong></p>\n<ul>\n<li>如果k是常数，解为指数函数；若否，我们可以用两种数值计算：les differences finies et les elements finis.</li>\n<li>可以使用<em>射击法</em>，认为$\\theta(L)$由$\\theta’(0)$得到，接下来就是验证是否能找到一个好的$\\theta’(0)$使$\\theta(L)=\\theta(L) = \\theta_L$</li>\n</ul>\n<h3 id=\"Physic-examples\"><a href=\"#Physic-examples\" class=\"headerlink\" title=\"Physic examples\"></a>Physic examples</h3><ul>\n<li><p>Fluide environnant au repos<br>$$<br>\\rho c_v \\partial_t{\\theta} - div(k\\cdot\\overrightarrow{grad}(\\theta)) = f<br>$$</p>\n<ul>\n<li>init condition:<br>$$<br>\\theta(0,.) = \\theta_0<br>$$</li>\n</ul>\n</li>\n<li><p>condition at the edges could be:</p>\n<ul>\n<li><p>condition of Dirichlet:<br>$$<br>\\theta(t,.) = g(t)<br>$$</p>\n</li>\n<li><p>Condition of Neumann:<br>$$<br>\\overrightarrow{grad}(\\theta)(t,.) \\cdot \\vec{n}= h(t)<br>$$</p>\n</li>\n<li><p>Condition of Dirichlet and Neumann: Mix</p>\n</li>\n</ul>\n</li>\n<li><p>Fluide environnant en mouvement a une vitesse $\\vec u(t,x)$<br>  $$<br>  \\rho c_v \\partial_t{\\theta} - div(\\rho c_v \\theta \\vec u) - div(k\\cdot\\overrightarrow{grad}(\\theta)) = f<br>  $$<br>  si le fluide est incompressible i.e $\\partial_t \\rho = 0$ soit encore $div(\\vec u) = 0$ (直观上理解，若不可压缩，则速度场一定是连续的，不然一定会在局部压缩), alors:<br>  $$<br>  \\rho c_v \\partial_t{\\theta} - \\vec u \\cdot div(\\rho c_v \\theta ) - div(k\\cdot\\overrightarrow{grad}(\\theta)) = f<br>  $$<br>  ​</p>\n<p>  ​</p>\n</li>\n</ul>\n"},{"title":"EDP finite element method","date":"2017-03-31T14:42:08.000Z","_content":"\n","source":"_posts/EDP-finite-element-method.md","raw":"---\ntitle: EDP finite element method\ncategories: [math, unfinished]\ntags:\n  - math\n  - FEM\ndate: 2017-03-31 22:42:08\n---\n\n","slug":"EDP-finite-element-method","published":1,"updated":"2017-03-31T20:48:32.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckw4qufou0009gwtlab327nnk","content":"","site":{"data":{"recommended_posts":{"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"}],"https://LorrinWWW.github.io/posts/complexity/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/hexo-with-latex/":[{"title":"区块链时代的最正确的打赏姿势——在Hexo博客配置Coinhive网页挖矿","permalink":"http://www.davidfnck.com/blockchain/mine-xmr-in-hexo-by-coinhive.html/"}],"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/":[{"title":"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-statistic/":[{"title":"贝叶斯估计 Bayes estimation","permalink":"https://LorrinWWW.github.io/posts/Bayes-estimation/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"}],"https://LorrinWWW.github.io/posts/[2018.2.5]Nested-LSTMs/":[{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"}],"https://LorrinWWW.github.io/posts/datamining-pretreatment/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Several models for knowledge graph representing and completing 几个知识图谱模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/"}],"https://LorrinWWW.github.io/posts/compression/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/Sobolev-space/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/proba-ch5/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch3/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/proba-ch1/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/ML-CNN/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/EDP-basic-models/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/machine-learning/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/datamining-class-pred/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction/":[],"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/":[{"title":"OS notes","permalink":"https://LorrinWWW.github.io/posts/OS-notes/"}],"https://LorrinWWW.github.io/posts/projet-enjeu-plugin-chrome-101/":[{"title":"Chrome油猴脚本","permalink":"http://gubangzhong.cn/2017/06/13/Chrome油猴脚本/"}],"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/":[{"title":"MongoDB, Docker and Python","permalink":"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/":[{"title":"Hands on Scrapy","permalink":"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/OS-notes/":[{"title":"learning OS and building LorriOS","permalink":"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/"}],"https://LorrinWWW.github.io/posts/Bayes-estimation/":[{"title":"Note of statistic","permalink":"https://LorrinWWW.github.io/posts/Note-of-statistic/"}],"https://LorrinWWW.github.io/posts/management-of-the-firm/":[],"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"}],"https://LorrinWWW.github.io/posts/datamining-qualitative-induction/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-finite-element-method/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/QuadTree/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/proba-ch4/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch2/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/":[{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/Note-of-NLP/":[{"title":"Generative Adversarial Network","permalink":"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/"},{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"}],"https://LorrinWWW.github.io/posts/Note-of-datamining/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"数据挖掘-分类与预测","permalink":"https://LorrinWWW.github.io/posts/datamining-class-pred/"},{"title":"数据挖掘-预处理","permalink":"https://LorrinWWW.github.io/posts/datamining-pretreatment/"}],"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/Note-of-probability/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/participe-present-et-gerondif/":[],"https://LorrinWWW.github.io/posts/Hilbert-space/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/proba-ch6/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}]}}},"excerpt":"","more":""},{"title":"Hands on Scrapy","date":"2017-06-21T12:00:27.000Z","_content":"\n# 各个结构\n\n## 新建项目\n\n```bash\nscrapy startproject <project_name> [project_dir]\n```\n\n文件结构：\n\n```\n├── author.json\n├── runSpider.py\n├── scrapy.cfg\n└── projectname\n    ├── __init__.py\n    ├── items.py\n    ├── middlewares.py\n    ├── pipelines.py\n    ├── settings.py      // 设置\n    └── spiders          // 具体的爬虫\n        ├── __init__.py\n        ├── spider0.py\n        └── spider1.py\n```\n\n## 爬虫主体\n\n示例如下，一个带登录的爬虫。\n\n```python\nfrom scrapy.spiders import CrawlSpider\nfrom scrapy.selector import Selector\nfrom scrapy.http import Request, FormRequest\n\nfrom tutorial.settings import *\n\nclass SampleSpider(CrawlSpider):\n    name = 'sampleSpider'\n    allowed_domains = ['sample.com']\n    start_url = 'https://sample.xxx.xx.x.x.x.xxx'\n\n    def __init__(self):\n        self.headers = HEADER\n    \n    def start_requests(self):\n        return [Request(\n            \"https://sample.com/signin\",\n            meta = {'cookiejar' : 1},\n            callback = self.post_login\n        )]\n\n    def post_login(self, response):\n        return [FormRequest(\n            'https://www.sample.com/login/',\n            method='POST',\n            meta={'cookiejar': response.meta['cookiejar']},\n            formdata = {\n                'email':'xxxx',\n                'password':'yyyy',\n            },\n            callback = self.after_login\n        )]\n\n    def after_login(self, response):\n        return [Request(\n            self.start_url,\n            meta={'cookiejar': response.meta['cookiejar']},\n            callback=self.parse,\n            errback=self.parse_err,\n        )]\n    \n    def parse(self, response):\n        # do something\n        pass\n\n    def parse_err(self, response):\n        print('eeeerrrrrrooooooorrrrrr!!!!!!')\n\n```\n\n## 选择器\n\n#### 自带选择器：\n\n- CSS Selector\n- XPath\n\n一般情况下CSS选择期即可满足要求，语法忘了google即可。\n\n```python\nquery = 'h1.title::text'\nresponse.css(query).extract()[0]\nresponse.css(query).extract_first()\n# response.css(query).extract_first().strip() # 一般用strip清理多余的空格等\n```\n\n#### BeautifulSoup\n\n方便，但是慢。不能解析js。\n\n#### lxml\n\n用于解析XML，当然也可用于HTML。\n\n## Item\n\n内建的一种抽象数据结构类，自行定义所需要的。\n\n使用时可以研究下ItemLoader。\n\n#### Item Pipeline\n\nItem被返回是被送往Item Pipeline进行进一步处理，即每次return Item等操作时，都会调用pipeline中的相应函数。一般有以下用途：\n\n- 验证Item\n\n  ```python\n  from scrapy.exceptions import DropItem\n\n  class PricePipeline(object):\n\n      vat_factor = 1.15\n\n      def process_item(self, item, spider):\n          if item['price']:\n              if item['price_excludes_vat']:\n                  item['price'] = item['price'] * self.vat_factor\n              return item\n          else:\n              raise DropItem(\"Missing price in %s\" % item)\n  ```\n\n- 写到JSON文件\n\n  ```python\n  import json\n\n  class JsonWriterPipeline(object):\n\n      def open_spider(self, spider):\n          self.file = open('items.jl', 'w')\n\n      def close_spider(self, spider):\n          self.file.close()\n\n      def process_item(self, item, spider):\n          line = json.dumps(dict(item)) + \"\\n\"\n          self.file.write(line)\n          return item\n  ```\n\n- 写到MongoDB\n\n  ```python\n  import pymongo\n\n  class MongoPipeline(object):\n\n      collection_name = 'scrapy_items'\n\n      def __init__(self, mongo_uri, mongo_db):\n          self.mongo_uri = mongo_uri\n          self.mongo_db = mongo_db\n\n      @classmethod\n      def from_crawler(cls, crawler):\n          return cls(\n              mongo_uri=crawler.settings.get('MONGO_URI'),\n              mongo_db=crawler.settings.get('MONGO_DATABASE', 'items')\n          )\n\n      def open_spider(self, spider):\n          self.client = pymongo.MongoClient(self.mongo_uri)\n          self.db = self.client[self.mongo_db]\n\n      def close_spider(self, spider):\n          self.client.close()\n\n      def process_item(self, item, spider):\n          self.db[self.collection_name].insert_one(dict(item))\n          return item\n  ```\n\n等。\n\n# 例子\n\n## 登陆\n\n- POST表单模拟登录\n\n  1. 首先要找到表单，在打开chrome的控制台，切换到\"network\"。\n  2. 手动登录，并停止network录制(不然可能随着页面的跳转，记录都被刷掉了)。\n  3. 依次查找，主要看headers中的form data，是否有和登录相关的信息\n  4. 模拟发送该表单(注意Request url)\n\n  具体写法见上面爬虫的例子\n\n  如果有验证码的情况，简单的可以用pil，复杂的可能就比较困难了，可以考虑使用cookie模拟登录。\n\n- 利用Cookie模拟登录\n\n  1. 同样思路，打开network，并手动登录\n  2. 在headers中找cookies\n  3. 将cookies整理成字典形式，作为Request的参数即可\n\n\n## CrawlSpider\n\n区别于普通的Spider，CrawSpider更适合于用于爬全网，注意不能覆盖parse函数。\n\nCrawlSpider通过Rule来限定搜索区域，parse会根据follow、callback的情况作出不同反应。\n\n如果需要登录，则使用表单登录后，注意最后一次callback需设置为parse，否则程序将会停止，CrawlSpider失效。","source":"_posts/Hands-on-Scrapy.md","raw":"---\ntitle: Hands on Scrapy\ndate: 2017-06-21 20:00:27\ncategories: [programming]\ntags: [scrapy, python, spider, crawl]\n---\n\n# 各个结构\n\n## 新建项目\n\n```bash\nscrapy startproject <project_name> [project_dir]\n```\n\n文件结构：\n\n```\n├── author.json\n├── runSpider.py\n├── scrapy.cfg\n└── projectname\n    ├── __init__.py\n    ├── items.py\n    ├── middlewares.py\n    ├── pipelines.py\n    ├── settings.py      // 设置\n    └── spiders          // 具体的爬虫\n        ├── __init__.py\n        ├── spider0.py\n        └── spider1.py\n```\n\n## 爬虫主体\n\n示例如下，一个带登录的爬虫。\n\n```python\nfrom scrapy.spiders import CrawlSpider\nfrom scrapy.selector import Selector\nfrom scrapy.http import Request, FormRequest\n\nfrom tutorial.settings import *\n\nclass SampleSpider(CrawlSpider):\n    name = 'sampleSpider'\n    allowed_domains = ['sample.com']\n    start_url = 'https://sample.xxx.xx.x.x.x.xxx'\n\n    def __init__(self):\n        self.headers = HEADER\n    \n    def start_requests(self):\n        return [Request(\n            \"https://sample.com/signin\",\n            meta = {'cookiejar' : 1},\n            callback = self.post_login\n        )]\n\n    def post_login(self, response):\n        return [FormRequest(\n            'https://www.sample.com/login/',\n            method='POST',\n            meta={'cookiejar': response.meta['cookiejar']},\n            formdata = {\n                'email':'xxxx',\n                'password':'yyyy',\n            },\n            callback = self.after_login\n        )]\n\n    def after_login(self, response):\n        return [Request(\n            self.start_url,\n            meta={'cookiejar': response.meta['cookiejar']},\n            callback=self.parse,\n            errback=self.parse_err,\n        )]\n    \n    def parse(self, response):\n        # do something\n        pass\n\n    def parse_err(self, response):\n        print('eeeerrrrrrooooooorrrrrr!!!!!!')\n\n```\n\n## 选择器\n\n#### 自带选择器：\n\n- CSS Selector\n- XPath\n\n一般情况下CSS选择期即可满足要求，语法忘了google即可。\n\n```python\nquery = 'h1.title::text'\nresponse.css(query).extract()[0]\nresponse.css(query).extract_first()\n# response.css(query).extract_first().strip() # 一般用strip清理多余的空格等\n```\n\n#### BeautifulSoup\n\n方便，但是慢。不能解析js。\n\n#### lxml\n\n用于解析XML，当然也可用于HTML。\n\n## Item\n\n内建的一种抽象数据结构类，自行定义所需要的。\n\n使用时可以研究下ItemLoader。\n\n#### Item Pipeline\n\nItem被返回是被送往Item Pipeline进行进一步处理，即每次return Item等操作时，都会调用pipeline中的相应函数。一般有以下用途：\n\n- 验证Item\n\n  ```python\n  from scrapy.exceptions import DropItem\n\n  class PricePipeline(object):\n\n      vat_factor = 1.15\n\n      def process_item(self, item, spider):\n          if item['price']:\n              if item['price_excludes_vat']:\n                  item['price'] = item['price'] * self.vat_factor\n              return item\n          else:\n              raise DropItem(\"Missing price in %s\" % item)\n  ```\n\n- 写到JSON文件\n\n  ```python\n  import json\n\n  class JsonWriterPipeline(object):\n\n      def open_spider(self, spider):\n          self.file = open('items.jl', 'w')\n\n      def close_spider(self, spider):\n          self.file.close()\n\n      def process_item(self, item, spider):\n          line = json.dumps(dict(item)) + \"\\n\"\n          self.file.write(line)\n          return item\n  ```\n\n- 写到MongoDB\n\n  ```python\n  import pymongo\n\n  class MongoPipeline(object):\n\n      collection_name = 'scrapy_items'\n\n      def __init__(self, mongo_uri, mongo_db):\n          self.mongo_uri = mongo_uri\n          self.mongo_db = mongo_db\n\n      @classmethod\n      def from_crawler(cls, crawler):\n          return cls(\n              mongo_uri=crawler.settings.get('MONGO_URI'),\n              mongo_db=crawler.settings.get('MONGO_DATABASE', 'items')\n          )\n\n      def open_spider(self, spider):\n          self.client = pymongo.MongoClient(self.mongo_uri)\n          self.db = self.client[self.mongo_db]\n\n      def close_spider(self, spider):\n          self.client.close()\n\n      def process_item(self, item, spider):\n          self.db[self.collection_name].insert_one(dict(item))\n          return item\n  ```\n\n等。\n\n# 例子\n\n## 登陆\n\n- POST表单模拟登录\n\n  1. 首先要找到表单，在打开chrome的控制台，切换到\"network\"。\n  2. 手动登录，并停止network录制(不然可能随着页面的跳转，记录都被刷掉了)。\n  3. 依次查找，主要看headers中的form data，是否有和登录相关的信息\n  4. 模拟发送该表单(注意Request url)\n\n  具体写法见上面爬虫的例子\n\n  如果有验证码的情况，简单的可以用pil，复杂的可能就比较困难了，可以考虑使用cookie模拟登录。\n\n- 利用Cookie模拟登录\n\n  1. 同样思路，打开network，并手动登录\n  2. 在headers中找cookies\n  3. 将cookies整理成字典形式，作为Request的参数即可\n\n\n## CrawlSpider\n\n区别于普通的Spider，CrawSpider更适合于用于爬全网，注意不能覆盖parse函数。\n\nCrawlSpider通过Rule来限定搜索区域，parse会根据follow、callback的情况作出不同反应。\n\n如果需要登录，则使用表单登录后，注意最后一次callback需设置为parse，否则程序将会停止，CrawlSpider失效。","slug":"Hands-on-Scrapy","published":1,"updated":"2017-06-22T14:15:11.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckw4qufov000agwtleg6gd9g5","content":"<h1 id=\"各个结构\"><a href=\"#各个结构\" class=\"headerlink\" title=\"各个结构\"></a>各个结构</h1><h2 id=\"新建项目\"><a href=\"#新建项目\" class=\"headerlink\" title=\"新建项目\"></a>新建项目</h2><figure class=\"highlight bash\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">scrapy startproject &lt;project_name&gt; [project_dir]</span><br></pre></td></tr></tbody></table></figure>\n\n<p>文件结构：</p>\n<figure class=\"highlight plaintext\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">├── author.json</span><br><span class=\"line\">├── runSpider.py</span><br><span class=\"line\">├── scrapy.cfg</span><br><span class=\"line\">└── projectname</span><br><span class=\"line\">    ├── __init__.py</span><br><span class=\"line\">    ├── items.py</span><br><span class=\"line\">    ├── middlewares.py</span><br><span class=\"line\">    ├── pipelines.py</span><br><span class=\"line\">    ├── settings.py      // 设置</span><br><span class=\"line\">    └── spiders          // 具体的爬虫</span><br><span class=\"line\">        ├── __init__.py</span><br><span class=\"line\">        ├── spider0.py</span><br><span class=\"line\">        └── spider1.py</span><br></pre></td></tr></tbody></table></figure>\n\n<h2 id=\"爬虫主体\"><a href=\"#爬虫主体\" class=\"headerlink\" title=\"爬虫主体\"></a>爬虫主体</h2><p>示例如下，一个带登录的爬虫。</p>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> scrapy.spiders <span class=\"keyword\">import</span> CrawlSpider</span><br><span class=\"line\"><span class=\"keyword\">from</span> scrapy.selector <span class=\"keyword\">import</span> Selector</span><br><span class=\"line\"><span class=\"keyword\">from</span> scrapy.http <span class=\"keyword\">import</span> Request, FormRequest</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">from</span> tutorial.settings <span class=\"keyword\">import</span> *</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">SampleSpider</span>(<span class=\"params\">CrawlSpider</span>):</span></span><br><span class=\"line\">    name = <span class=\"string\">'sampleSpider'</span></span><br><span class=\"line\">    allowed_domains = [<span class=\"string\">'sample.com'</span>]</span><br><span class=\"line\">    start_url = <span class=\"string\">'https://sample.xxx.xx.x.x.x.xxx'</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\">self</span>):</span></span><br><span class=\"line\">        self.headers = HEADER</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">start_requests</span>(<span class=\"params\">self</span>):</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> [Request(</span><br><span class=\"line\">            <span class=\"string\">\"https://sample.com/signin\"</span>,</span><br><span class=\"line\">            meta = {<span class=\"string\">'cookiejar'</span> : <span class=\"number\">1</span>},</span><br><span class=\"line\">            callback = self.post_login</span><br><span class=\"line\">        )]</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">post_login</span>(<span class=\"params\">self, response</span>):</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> [FormRequest(</span><br><span class=\"line\">            <span class=\"string\">'https://www.sample.com/login/'</span>,</span><br><span class=\"line\">            method=<span class=\"string\">'POST'</span>,</span><br><span class=\"line\">            meta={<span class=\"string\">'cookiejar'</span>: response.meta[<span class=\"string\">'cookiejar'</span>]},</span><br><span class=\"line\">            formdata = {</span><br><span class=\"line\">                <span class=\"string\">'email'</span>:<span class=\"string\">'xxxx'</span>,</span><br><span class=\"line\">                <span class=\"string\">'password'</span>:<span class=\"string\">'yyyy'</span>,</span><br><span class=\"line\">            },</span><br><span class=\"line\">            callback = self.after_login</span><br><span class=\"line\">        )]</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">after_login</span>(<span class=\"params\">self, response</span>):</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> [Request(</span><br><span class=\"line\">            self.start_url,</span><br><span class=\"line\">            meta={<span class=\"string\">'cookiejar'</span>: response.meta[<span class=\"string\">'cookiejar'</span>]},</span><br><span class=\"line\">            callback=self.parse,</span><br><span class=\"line\">            errback=self.parse_err,</span><br><span class=\"line\">        )]</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">parse</span>(<span class=\"params\">self, response</span>):</span></span><br><span class=\"line\">        <span class=\"comment\"># do something</span></span><br><span class=\"line\">        <span class=\"keyword\">pass</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">parse_err</span>(<span class=\"params\">self, response</span>):</span></span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">'eeeerrrrrrooooooorrrrrr!!!!!!'</span>)</span><br><span class=\"line\"></span><br></pre></td></tr></tbody></table></figure>\n\n<h2 id=\"选择器\"><a href=\"#选择器\" class=\"headerlink\" title=\"选择器\"></a>选择器</h2><h4 id=\"自带选择器：\"><a href=\"#自带选择器：\" class=\"headerlink\" title=\"自带选择器：\"></a>自带选择器：</h4><ul>\n<li>CSS Selector</li>\n<li>XPath</li>\n</ul>\n<p>一般情况下CSS选择期即可满足要求，语法忘了google即可。</p>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">query = <span class=\"string\">'h1.title::text'</span></span><br><span class=\"line\">response.css(query).extract()[<span class=\"number\">0</span>]</span><br><span class=\"line\">response.css(query).extract_first()</span><br><span class=\"line\"><span class=\"comment\"># response.css(query).extract_first().strip() # 一般用strip清理多余的空格等</span></span><br></pre></td></tr></tbody></table></figure>\n\n<h4 id=\"BeautifulSoup\"><a href=\"#BeautifulSoup\" class=\"headerlink\" title=\"BeautifulSoup\"></a>BeautifulSoup</h4><p>方便，但是慢。不能解析js。</p>\n<h4 id=\"lxml\"><a href=\"#lxml\" class=\"headerlink\" title=\"lxml\"></a>lxml</h4><p>用于解析XML，当然也可用于HTML。</p>\n<h2 id=\"Item\"><a href=\"#Item\" class=\"headerlink\" title=\"Item\"></a>Item</h2><p>内建的一种抽象数据结构类，自行定义所需要的。</p>\n<p>使用时可以研究下ItemLoader。</p>\n<h4 id=\"Item-Pipeline\"><a href=\"#Item-Pipeline\" class=\"headerlink\" title=\"Item Pipeline\"></a>Item Pipeline</h4><p>Item被返回是被送往Item Pipeline进行进一步处理，即每次return Item等操作时，都会调用pipeline中的相应函数。一般有以下用途：</p>\n<ul>\n<li><p>验证Item</p>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> scrapy.exceptions <span class=\"keyword\">import</span> DropItem</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">PricePipeline</span>(<span class=\"params\"><span class=\"built_in\">object</span></span>):</span></span><br><span class=\"line\"></span><br><span class=\"line\">    vat_factor = <span class=\"number\">1.15</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">process_item</span>(<span class=\"params\">self, item, spider</span>):</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> item[<span class=\"string\">'price'</span>]:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> item[<span class=\"string\">'price_excludes_vat'</span>]:</span><br><span class=\"line\">                item[<span class=\"string\">'price'</span>] = item[<span class=\"string\">'price'</span>] * self.vat_factor</span><br><span class=\"line\">            <span class=\"keyword\">return</span> item</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"keyword\">raise</span> DropItem(<span class=\"string\">\"Missing price in %s\"</span> % item)</span><br></pre></td></tr></tbody></table></figure></li>\n<li><p>写到JSON文件</p>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> json</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">JsonWriterPipeline</span>(<span class=\"params\"><span class=\"built_in\">object</span></span>):</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">open_spider</span>(<span class=\"params\">self, spider</span>):</span></span><br><span class=\"line\">        self.file = <span class=\"built_in\">open</span>(<span class=\"string\">'items.jl'</span>, <span class=\"string\">'w'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">close_spider</span>(<span class=\"params\">self, spider</span>):</span></span><br><span class=\"line\">        self.file.close()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">process_item</span>(<span class=\"params\">self, item, spider</span>):</span></span><br><span class=\"line\">        line = json.dumps(<span class=\"built_in\">dict</span>(item)) + <span class=\"string\">\"\\n\"</span></span><br><span class=\"line\">        self.file.write(line)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> item</span><br></pre></td></tr></tbody></table></figure></li>\n<li><p>写到MongoDB</p>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> pymongo</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MongoPipeline</span>(<span class=\"params\"><span class=\"built_in\">object</span></span>):</span></span><br><span class=\"line\"></span><br><span class=\"line\">    collection_name = <span class=\"string\">'scrapy_items'</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\">self, mongo_uri, mongo_db</span>):</span></span><br><span class=\"line\">        self.mongo_uri = mongo_uri</span><br><span class=\"line\">        self.mongo_db = mongo_db</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">    @classmethod</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">from_crawler</span>(<span class=\"params\">cls, crawler</span>):</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> cls(</span><br><span class=\"line\">            mongo_uri=crawler.settings.get(<span class=\"string\">'MONGO_URI'</span>),</span><br><span class=\"line\">            mongo_db=crawler.settings.get(<span class=\"string\">'MONGO_DATABASE'</span>, <span class=\"string\">'items'</span>)</span><br><span class=\"line\">        )</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">open_spider</span>(<span class=\"params\">self, spider</span>):</span></span><br><span class=\"line\">        self.client = pymongo.MongoClient(self.mongo_uri)</span><br><span class=\"line\">        self.db = self.client[self.mongo_db]</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">close_spider</span>(<span class=\"params\">self, spider</span>):</span></span><br><span class=\"line\">        self.client.close()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">process_item</span>(<span class=\"params\">self, item, spider</span>):</span></span><br><span class=\"line\">        self.db[self.collection_name].insert_one(<span class=\"built_in\">dict</span>(item))</span><br><span class=\"line\">        <span class=\"keyword\">return</span> item</span><br></pre></td></tr></tbody></table></figure></li>\n</ul>\n<p>等。</p>\n<h1 id=\"例子\"><a href=\"#例子\" class=\"headerlink\" title=\"例子\"></a>例子</h1><h2 id=\"登陆\"><a href=\"#登陆\" class=\"headerlink\" title=\"登陆\"></a>登陆</h2><ul>\n<li><p>POST表单模拟登录</p>\n<ol>\n<li>首先要找到表单，在打开chrome的控制台，切换到”network”。</li>\n<li>手动登录，并停止network录制(不然可能随着页面的跳转，记录都被刷掉了)。</li>\n<li>依次查找，主要看headers中的form data，是否有和登录相关的信息</li>\n<li>模拟发送该表单(注意Request url)</li>\n</ol>\n<p>具体写法见上面爬虫的例子</p>\n<p>如果有验证码的情况，简单的可以用pil，复杂的可能就比较困难了，可以考虑使用cookie模拟登录。</p>\n</li>\n<li><p>利用Cookie模拟登录</p>\n<ol>\n<li>同样思路，打开network，并手动登录</li>\n<li>在headers中找cookies</li>\n<li>将cookies整理成字典形式，作为Request的参数即可</li>\n</ol>\n</li>\n</ul>\n<h2 id=\"CrawlSpider\"><a href=\"#CrawlSpider\" class=\"headerlink\" title=\"CrawlSpider\"></a>CrawlSpider</h2><p>区别于普通的Spider，CrawSpider更适合于用于爬全网，注意不能覆盖parse函数。</p>\n<p>CrawlSpider通过Rule来限定搜索区域，parse会根据follow、callback的情况作出不同反应。</p>\n<p>如果需要登录，则使用表单登录后，注意最后一次callback需设置为parse，否则程序将会停止，CrawlSpider失效。</p>\n","site":{"data":{"recommended_posts":{"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"}],"https://LorrinWWW.github.io/posts/complexity/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/hexo-with-latex/":[{"title":"区块链时代的最正确的打赏姿势——在Hexo博客配置Coinhive网页挖矿","permalink":"http://www.davidfnck.com/blockchain/mine-xmr-in-hexo-by-coinhive.html/"}],"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/":[{"title":"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-statistic/":[{"title":"贝叶斯估计 Bayes estimation","permalink":"https://LorrinWWW.github.io/posts/Bayes-estimation/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"}],"https://LorrinWWW.github.io/posts/[2018.2.5]Nested-LSTMs/":[{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"}],"https://LorrinWWW.github.io/posts/datamining-pretreatment/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Several models for knowledge graph representing and completing 几个知识图谱模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/"}],"https://LorrinWWW.github.io/posts/compression/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/Sobolev-space/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/proba-ch5/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch3/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/proba-ch1/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/ML-CNN/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/EDP-basic-models/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/machine-learning/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/datamining-class-pred/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction/":[],"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/":[{"title":"OS notes","permalink":"https://LorrinWWW.github.io/posts/OS-notes/"}],"https://LorrinWWW.github.io/posts/projet-enjeu-plugin-chrome-101/":[{"title":"Chrome油猴脚本","permalink":"http://gubangzhong.cn/2017/06/13/Chrome油猴脚本/"}],"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/":[{"title":"MongoDB, Docker and Python","permalink":"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/":[{"title":"Hands on Scrapy","permalink":"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/OS-notes/":[{"title":"learning OS and building LorriOS","permalink":"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/"}],"https://LorrinWWW.github.io/posts/Bayes-estimation/":[{"title":"Note of statistic","permalink":"https://LorrinWWW.github.io/posts/Note-of-statistic/"}],"https://LorrinWWW.github.io/posts/management-of-the-firm/":[],"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"}],"https://LorrinWWW.github.io/posts/datamining-qualitative-induction/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-finite-element-method/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/QuadTree/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/proba-ch4/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch2/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/":[{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/Note-of-NLP/":[{"title":"Generative Adversarial Network","permalink":"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/"},{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"}],"https://LorrinWWW.github.io/posts/Note-of-datamining/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"数据挖掘-分类与预测","permalink":"https://LorrinWWW.github.io/posts/datamining-class-pred/"},{"title":"数据挖掘-预处理","permalink":"https://LorrinWWW.github.io/posts/datamining-pretreatment/"}],"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/Note-of-probability/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/participe-present-et-gerondif/":[],"https://LorrinWWW.github.io/posts/Hilbert-space/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/proba-ch6/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}]}}},"excerpt":"","more":"<h1 id=\"各个结构\"><a href=\"#各个结构\" class=\"headerlink\" title=\"各个结构\"></a>各个结构</h1><h2 id=\"新建项目\"><a href=\"#新建项目\" class=\"headerlink\" title=\"新建项目\"></a>新建项目</h2><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">scrapy startproject &lt;project_name&gt; [project_dir]</span><br></pre></td></tr></table></figure>\n\n<p>文件结构：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">├── author.json</span><br><span class=\"line\">├── runSpider.py</span><br><span class=\"line\">├── scrapy.cfg</span><br><span class=\"line\">└── projectname</span><br><span class=\"line\">    ├── __init__.py</span><br><span class=\"line\">    ├── items.py</span><br><span class=\"line\">    ├── middlewares.py</span><br><span class=\"line\">    ├── pipelines.py</span><br><span class=\"line\">    ├── settings.py      // 设置</span><br><span class=\"line\">    └── spiders          // 具体的爬虫</span><br><span class=\"line\">        ├── __init__.py</span><br><span class=\"line\">        ├── spider0.py</span><br><span class=\"line\">        └── spider1.py</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"爬虫主体\"><a href=\"#爬虫主体\" class=\"headerlink\" title=\"爬虫主体\"></a>爬虫主体</h2><p>示例如下，一个带登录的爬虫。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> scrapy.spiders <span class=\"keyword\">import</span> CrawlSpider</span><br><span class=\"line\"><span class=\"keyword\">from</span> scrapy.selector <span class=\"keyword\">import</span> Selector</span><br><span class=\"line\"><span class=\"keyword\">from</span> scrapy.http <span class=\"keyword\">import</span> Request, FormRequest</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">from</span> tutorial.settings <span class=\"keyword\">import</span> *</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">SampleSpider</span>(<span class=\"params\">CrawlSpider</span>):</span></span><br><span class=\"line\">    name = <span class=\"string\">&#x27;sampleSpider&#x27;</span></span><br><span class=\"line\">    allowed_domains = [<span class=\"string\">&#x27;sample.com&#x27;</span>]</span><br><span class=\"line\">    start_url = <span class=\"string\">&#x27;https://sample.xxx.xx.x.x.x.xxx&#x27;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\">self</span>):</span></span><br><span class=\"line\">        self.headers = HEADER</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">start_requests</span>(<span class=\"params\">self</span>):</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> [Request(</span><br><span class=\"line\">            <span class=\"string\">&quot;https://sample.com/signin&quot;</span>,</span><br><span class=\"line\">            meta = &#123;<span class=\"string\">&#x27;cookiejar&#x27;</span> : <span class=\"number\">1</span>&#125;,</span><br><span class=\"line\">            callback = self.post_login</span><br><span class=\"line\">        )]</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">post_login</span>(<span class=\"params\">self, response</span>):</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> [FormRequest(</span><br><span class=\"line\">            <span class=\"string\">&#x27;https://www.sample.com/login/&#x27;</span>,</span><br><span class=\"line\">            method=<span class=\"string\">&#x27;POST&#x27;</span>,</span><br><span class=\"line\">            meta=&#123;<span class=\"string\">&#x27;cookiejar&#x27;</span>: response.meta[<span class=\"string\">&#x27;cookiejar&#x27;</span>]&#125;,</span><br><span class=\"line\">            formdata = &#123;</span><br><span class=\"line\">                <span class=\"string\">&#x27;email&#x27;</span>:<span class=\"string\">&#x27;xxxx&#x27;</span>,</span><br><span class=\"line\">                <span class=\"string\">&#x27;password&#x27;</span>:<span class=\"string\">&#x27;yyyy&#x27;</span>,</span><br><span class=\"line\">            &#125;,</span><br><span class=\"line\">            callback = self.after_login</span><br><span class=\"line\">        )]</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">after_login</span>(<span class=\"params\">self, response</span>):</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> [Request(</span><br><span class=\"line\">            self.start_url,</span><br><span class=\"line\">            meta=&#123;<span class=\"string\">&#x27;cookiejar&#x27;</span>: response.meta[<span class=\"string\">&#x27;cookiejar&#x27;</span>]&#125;,</span><br><span class=\"line\">            callback=self.parse,</span><br><span class=\"line\">            errback=self.parse_err,</span><br><span class=\"line\">        )]</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">parse</span>(<span class=\"params\">self, response</span>):</span></span><br><span class=\"line\">        <span class=\"comment\"># do something</span></span><br><span class=\"line\">        <span class=\"keyword\">pass</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">parse_err</span>(<span class=\"params\">self, response</span>):</span></span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;eeeerrrrrrooooooorrrrrr!!!!!!&#x27;</span>)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"选择器\"><a href=\"#选择器\" class=\"headerlink\" title=\"选择器\"></a>选择器</h2><h4 id=\"自带选择器：\"><a href=\"#自带选择器：\" class=\"headerlink\" title=\"自带选择器：\"></a>自带选择器：</h4><ul>\n<li>CSS Selector</li>\n<li>XPath</li>\n</ul>\n<p>一般情况下CSS选择期即可满足要求，语法忘了google即可。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">query = <span class=\"string\">&#x27;h1.title::text&#x27;</span></span><br><span class=\"line\">response.css(query).extract()[<span class=\"number\">0</span>]</span><br><span class=\"line\">response.css(query).extract_first()</span><br><span class=\"line\"><span class=\"comment\"># response.css(query).extract_first().strip() # 一般用strip清理多余的空格等</span></span><br></pre></td></tr></table></figure>\n\n<h4 id=\"BeautifulSoup\"><a href=\"#BeautifulSoup\" class=\"headerlink\" title=\"BeautifulSoup\"></a>BeautifulSoup</h4><p>方便，但是慢。不能解析js。</p>\n<h4 id=\"lxml\"><a href=\"#lxml\" class=\"headerlink\" title=\"lxml\"></a>lxml</h4><p>用于解析XML，当然也可用于HTML。</p>\n<h2 id=\"Item\"><a href=\"#Item\" class=\"headerlink\" title=\"Item\"></a>Item</h2><p>内建的一种抽象数据结构类，自行定义所需要的。</p>\n<p>使用时可以研究下ItemLoader。</p>\n<h4 id=\"Item-Pipeline\"><a href=\"#Item-Pipeline\" class=\"headerlink\" title=\"Item Pipeline\"></a>Item Pipeline</h4><p>Item被返回是被送往Item Pipeline进行进一步处理，即每次return Item等操作时，都会调用pipeline中的相应函数。一般有以下用途：</p>\n<ul>\n<li><p>验证Item</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> scrapy.exceptions <span class=\"keyword\">import</span> DropItem</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">PricePipeline</span>(<span class=\"params\"><span class=\"built_in\">object</span></span>):</span></span><br><span class=\"line\"></span><br><span class=\"line\">    vat_factor = <span class=\"number\">1.15</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">process_item</span>(<span class=\"params\">self, item, spider</span>):</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> item[<span class=\"string\">&#x27;price&#x27;</span>]:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> item[<span class=\"string\">&#x27;price_excludes_vat&#x27;</span>]:</span><br><span class=\"line\">                item[<span class=\"string\">&#x27;price&#x27;</span>] = item[<span class=\"string\">&#x27;price&#x27;</span>] * self.vat_factor</span><br><span class=\"line\">            <span class=\"keyword\">return</span> item</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            <span class=\"keyword\">raise</span> DropItem(<span class=\"string\">&quot;Missing price in %s&quot;</span> % item)</span><br></pre></td></tr></table></figure></li>\n<li><p>写到JSON文件</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> json</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">JsonWriterPipeline</span>(<span class=\"params\"><span class=\"built_in\">object</span></span>):</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">open_spider</span>(<span class=\"params\">self, spider</span>):</span></span><br><span class=\"line\">        self.file = <span class=\"built_in\">open</span>(<span class=\"string\">&#x27;items.jl&#x27;</span>, <span class=\"string\">&#x27;w&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">close_spider</span>(<span class=\"params\">self, spider</span>):</span></span><br><span class=\"line\">        self.file.close()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">process_item</span>(<span class=\"params\">self, item, spider</span>):</span></span><br><span class=\"line\">        line = json.dumps(<span class=\"built_in\">dict</span>(item)) + <span class=\"string\">&quot;\\n&quot;</span></span><br><span class=\"line\">        self.file.write(line)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> item</span><br></pre></td></tr></table></figure></li>\n<li><p>写到MongoDB</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> pymongo</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">MongoPipeline</span>(<span class=\"params\"><span class=\"built_in\">object</span></span>):</span></span><br><span class=\"line\"></span><br><span class=\"line\">    collection_name = <span class=\"string\">&#x27;scrapy_items&#x27;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\">self, mongo_uri, mongo_db</span>):</span></span><br><span class=\"line\">        self.mongo_uri = mongo_uri</span><br><span class=\"line\">        self.mongo_db = mongo_db</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">    @classmethod</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">from_crawler</span>(<span class=\"params\">cls, crawler</span>):</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> cls(</span><br><span class=\"line\">            mongo_uri=crawler.settings.get(<span class=\"string\">&#x27;MONGO_URI&#x27;</span>),</span><br><span class=\"line\">            mongo_db=crawler.settings.get(<span class=\"string\">&#x27;MONGO_DATABASE&#x27;</span>, <span class=\"string\">&#x27;items&#x27;</span>)</span><br><span class=\"line\">        )</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">open_spider</span>(<span class=\"params\">self, spider</span>):</span></span><br><span class=\"line\">        self.client = pymongo.MongoClient(self.mongo_uri)</span><br><span class=\"line\">        self.db = self.client[self.mongo_db]</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">close_spider</span>(<span class=\"params\">self, spider</span>):</span></span><br><span class=\"line\">        self.client.close()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">process_item</span>(<span class=\"params\">self, item, spider</span>):</span></span><br><span class=\"line\">        self.db[self.collection_name].insert_one(<span class=\"built_in\">dict</span>(item))</span><br><span class=\"line\">        <span class=\"keyword\">return</span> item</span><br></pre></td></tr></table></figure></li>\n</ul>\n<p>等。</p>\n<h1 id=\"例子\"><a href=\"#例子\" class=\"headerlink\" title=\"例子\"></a>例子</h1><h2 id=\"登陆\"><a href=\"#登陆\" class=\"headerlink\" title=\"登陆\"></a>登陆</h2><ul>\n<li><p>POST表单模拟登录</p>\n<ol>\n<li>首先要找到表单，在打开chrome的控制台，切换到”network”。</li>\n<li>手动登录，并停止network录制(不然可能随着页面的跳转，记录都被刷掉了)。</li>\n<li>依次查找，主要看headers中的form data，是否有和登录相关的信息</li>\n<li>模拟发送该表单(注意Request url)</li>\n</ol>\n<p>具体写法见上面爬虫的例子</p>\n<p>如果有验证码的情况，简单的可以用pil，复杂的可能就比较困难了，可以考虑使用cookie模拟登录。</p>\n</li>\n<li><p>利用Cookie模拟登录</p>\n<ol>\n<li>同样思路，打开network，并手动登录</li>\n<li>在headers中找cookies</li>\n<li>将cookies整理成字典形式，作为Request的参数即可</li>\n</ol>\n</li>\n</ul>\n<h2 id=\"CrawlSpider\"><a href=\"#CrawlSpider\" class=\"headerlink\" title=\"CrawlSpider\"></a>CrawlSpider</h2><p>区别于普通的Spider，CrawSpider更适合于用于爬全网，注意不能覆盖parse函数。</p>\n<p>CrawlSpider通过Rule来限定搜索区域，parse会根据follow、callback的情况作出不同反应。</p>\n<p>如果需要登录，则使用表单登录后，注意最后一次callback需设置为parse，否则程序将会停止，CrawlSpider失效。</p>\n"},{"title":"Hilbert space","date":"2017-02-11T05:10:06.000Z","_content":"\n# 希尔伯特空间 Hilbert space\n\n## 感性认识\n\n摘自wiki:\n\n>在数学里，**希尔伯特空间**即**完备的内积空间**，也就是说一个带有内积的完备向量空间。\n\n要弄清楚希尔伯特空间，我们需要从几个基本的地方出发。\n\n### 几个形容词\n\n- 线性\n\n  有线性结构的。\n\n- 赋范\n\n  空间中用于度量“长度”，引入范数。\n\n- 完备(complet)\n\n  其上所有的柯西序列会收敛到此空间里的一点，序列的极限依然在此空间内。通俗的说，我们认为这个空间是不缺点、有皮的。\n\n- 内积\n\n  补充“角度”概念，引入内积。\n\n### 几类空间\n\n- 线性空间\n\n- 度量空间\n\n  最基本的空间。前者有线性结构，后者有度量空间，二者没有交集。\n\n- 赋范线性空间\n\n  引入范数\n\n- 内积空间\n\n  内积空间是赋范线性空间。\n\n- 希尔伯特空间\n\n  希尔伯特空间是完备的内积空间\n\n((线性空间 + 范数  = 赋范空间 + 线性结构) + 内积 = 内积空间) + 完备性 = 希尔伯特空间\n\n## 可以由以下空间获得\n\n- 欧几里得空间\n\n  定义内积。\n\n- 序列空间\n\n- 勒贝格空间\n\n  特指L^2空间，定义其内积:\n\n  ​\n  $$\n  (f|g) = \\int{\\overline{f}g}\n  $$\n  还需证明其完备性.\n\n- 索伯列夫空间\n\n  一般表示为H^s或W^(s,2)。在偏微分方程中常用。\n\n## 希尔伯特空间的基\n\n- 所有基的范数为1\n- 彼此正交\n- 其线性扩张稠密：即其中的所有元素的有限的线性组合是H的一个稠密子集。\n\n## 相关\n\n- 内积(produit scalaire)\n\n  满足以下四个条件：\n\n$$\n\\forall (x,y) \\in H^2 \\qquad \\varphi(y,x) = \\overline{\\varphi(x,y)} \\\\\n\\forall (x,y,z) \\in H^3, \\forall(\\lambda,\\mu) \\in \\mathbb{C}^2 \\qquad \\varphi(z,\\lambda x+\\mu y) = \\lambda\\varphi(z,x) + \\mu \\varphi(z,y) \\\\\n\\forall x \\in H^2 \\qquad \\varphi(x,x) \\ge 0 \\\\\n\\varphi (x,x) = 0 \\Longrightarrow x = 0\n$$\n","source":"_posts/Hilbert-space.md","raw":"---\ntitle: Hilbert space\ndate: 2017-02-11 13:10:06\ncategories: [math]\ntags: [Hilbert, math, analyse]\n---\n\n# 希尔伯特空间 Hilbert space\n\n## 感性认识\n\n摘自wiki:\n\n>在数学里，**希尔伯特空间**即**完备的内积空间**，也就是说一个带有内积的完备向量空间。\n\n要弄清楚希尔伯特空间，我们需要从几个基本的地方出发。\n\n### 几个形容词\n\n- 线性\n\n  有线性结构的。\n\n- 赋范\n\n  空间中用于度量“长度”，引入范数。\n\n- 完备(complet)\n\n  其上所有的柯西序列会收敛到此空间里的一点，序列的极限依然在此空间内。通俗的说，我们认为这个空间是不缺点、有皮的。\n\n- 内积\n\n  补充“角度”概念，引入内积。\n\n### 几类空间\n\n- 线性空间\n\n- 度量空间\n\n  最基本的空间。前者有线性结构，后者有度量空间，二者没有交集。\n\n- 赋范线性空间\n\n  引入范数\n\n- 内积空间\n\n  内积空间是赋范线性空间。\n\n- 希尔伯特空间\n\n  希尔伯特空间是完备的内积空间\n\n((线性空间 + 范数  = 赋范空间 + 线性结构) + 内积 = 内积空间) + 完备性 = 希尔伯特空间\n\n## 可以由以下空间获得\n\n- 欧几里得空间\n\n  定义内积。\n\n- 序列空间\n\n- 勒贝格空间\n\n  特指L^2空间，定义其内积:\n\n  ​\n  $$\n  (f|g) = \\int{\\overline{f}g}\n  $$\n  还需证明其完备性.\n\n- 索伯列夫空间\n\n  一般表示为H^s或W^(s,2)。在偏微分方程中常用。\n\n## 希尔伯特空间的基\n\n- 所有基的范数为1\n- 彼此正交\n- 其线性扩张稠密：即其中的所有元素的有限的线性组合是H的一个稠密子集。\n\n## 相关\n\n- 内积(produit scalaire)\n\n  满足以下四个条件：\n\n$$\n\\forall (x,y) \\in H^2 \\qquad \\varphi(y,x) = \\overline{\\varphi(x,y)} \\\\\n\\forall (x,y,z) \\in H^3, \\forall(\\lambda,\\mu) \\in \\mathbb{C}^2 \\qquad \\varphi(z,\\lambda x+\\mu y) = \\lambda\\varphi(z,x) + \\mu \\varphi(z,y) \\\\\n\\forall x \\in H^2 \\qquad \\varphi(x,x) \\ge 0 \\\\\n\\varphi (x,x) = 0 \\Longrightarrow x = 0\n$$\n","slug":"Hilbert-space","published":1,"updated":"2017-02-11T12:57:32.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckw4qufox000dgwtldq2v1h0p","content":"<h1 id=\"希尔伯特空间-Hilbert-space\"><a href=\"#希尔伯特空间-Hilbert-space\" class=\"headerlink\" title=\"希尔伯特空间 Hilbert space\"></a>希尔伯特空间 Hilbert space</h1><h2 id=\"感性认识\"><a href=\"#感性认识\" class=\"headerlink\" title=\"感性认识\"></a>感性认识</h2><p>摘自wiki:</p>\n<blockquote>\n<p>在数学里，<strong>希尔伯特空间</strong>即<strong>完备的内积空间</strong>，也就是说一个带有内积的完备向量空间。</p>\n</blockquote>\n<p>要弄清楚希尔伯特空间，我们需要从几个基本的地方出发。</p>\n<h3 id=\"几个形容词\"><a href=\"#几个形容词\" class=\"headerlink\" title=\"几个形容词\"></a>几个形容词</h3><ul>\n<li><p>线性</p>\n<p>有线性结构的。</p>\n</li>\n<li><p>赋范</p>\n<p>空间中用于度量“长度”，引入范数。</p>\n</li>\n<li><p>完备(complet)</p>\n<p>其上所有的柯西序列会收敛到此空间里的一点，序列的极限依然在此空间内。通俗的说，我们认为这个空间是不缺点、有皮的。</p>\n</li>\n<li><p>内积</p>\n<p>补充“角度”概念，引入内积。</p>\n</li>\n</ul>\n<h3 id=\"几类空间\"><a href=\"#几类空间\" class=\"headerlink\" title=\"几类空间\"></a>几类空间</h3><ul>\n<li><p>线性空间</p>\n</li>\n<li><p>度量空间</p>\n<p>最基本的空间。前者有线性结构，后者有度量空间，二者没有交集。</p>\n</li>\n<li><p>赋范线性空间</p>\n<p>引入范数</p>\n</li>\n<li><p>内积空间</p>\n<p>内积空间是赋范线性空间。</p>\n</li>\n<li><p>希尔伯特空间</p>\n<p>希尔伯特空间是完备的内积空间</p>\n</li>\n</ul>\n<p>((线性空间 + 范数  = 赋范空间 + 线性结构) + 内积 = 内积空间) + 完备性 = 希尔伯特空间</p>\n<h2 id=\"可以由以下空间获得\"><a href=\"#可以由以下空间获得\" class=\"headerlink\" title=\"可以由以下空间获得\"></a>可以由以下空间获得</h2><ul>\n<li><p>欧几里得空间</p>\n<p>定义内积。</p>\n</li>\n<li><p>序列空间</p>\n</li>\n<li><p>勒贝格空间</p>\n<p>特指L^2空间，定义其内积:</p>\n<p>​<br>$$<br>(f|g) = \\int{\\overline{f}g}<br>$$<br>还需证明其完备性.</p>\n</li>\n<li><p>索伯列夫空间</p>\n<p>一般表示为H^s或W^(s,2)。在偏微分方程中常用。</p>\n</li>\n</ul>\n<h2 id=\"希尔伯特空间的基\"><a href=\"#希尔伯特空间的基\" class=\"headerlink\" title=\"希尔伯特空间的基\"></a>希尔伯特空间的基</h2><ul>\n<li>所有基的范数为1</li>\n<li>彼此正交</li>\n<li>其线性扩张稠密：即其中的所有元素的有限的线性组合是H的一个稠密子集。</li>\n</ul>\n<h2 id=\"相关\"><a href=\"#相关\" class=\"headerlink\" title=\"相关\"></a>相关</h2><ul>\n<li><p>内积(produit scalaire)</p>\n<p>满足以下四个条件：</p>\n</li>\n</ul>\n<p>$$<br>\\forall (x,y) \\in H^2 \\qquad \\varphi(y,x) = \\overline{\\varphi(x,y)} \\<br>\\forall (x,y,z) \\in H^3, \\forall(\\lambda,\\mu) \\in \\mathbb{C}^2 \\qquad \\varphi(z,\\lambda x+\\mu y) = \\lambda\\varphi(z,x) + \\mu \\varphi(z,y) \\<br>\\forall x \\in H^2 \\qquad \\varphi(x,x) \\ge 0 \\<br>\\varphi (x,x) = 0 \\Longrightarrow x = 0<br>$$</p>\n","site":{"data":{"recommended_posts":{"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"}],"https://LorrinWWW.github.io/posts/complexity/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/hexo-with-latex/":[{"title":"区块链时代的最正确的打赏姿势——在Hexo博客配置Coinhive网页挖矿","permalink":"http://www.davidfnck.com/blockchain/mine-xmr-in-hexo-by-coinhive.html/"}],"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/":[{"title":"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-statistic/":[{"title":"贝叶斯估计 Bayes estimation","permalink":"https://LorrinWWW.github.io/posts/Bayes-estimation/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"}],"https://LorrinWWW.github.io/posts/[2018.2.5]Nested-LSTMs/":[{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"}],"https://LorrinWWW.github.io/posts/datamining-pretreatment/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Several models for knowledge graph representing and completing 几个知识图谱模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/"}],"https://LorrinWWW.github.io/posts/compression/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/Sobolev-space/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/proba-ch5/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch3/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/proba-ch1/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/ML-CNN/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/EDP-basic-models/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/machine-learning/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/datamining-class-pred/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction/":[],"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/":[{"title":"OS notes","permalink":"https://LorrinWWW.github.io/posts/OS-notes/"}],"https://LorrinWWW.github.io/posts/projet-enjeu-plugin-chrome-101/":[{"title":"Chrome油猴脚本","permalink":"http://gubangzhong.cn/2017/06/13/Chrome油猴脚本/"}],"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/":[{"title":"MongoDB, Docker and Python","permalink":"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/":[{"title":"Hands on Scrapy","permalink":"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/OS-notes/":[{"title":"learning OS and building LorriOS","permalink":"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/"}],"https://LorrinWWW.github.io/posts/Bayes-estimation/":[{"title":"Note of statistic","permalink":"https://LorrinWWW.github.io/posts/Note-of-statistic/"}],"https://LorrinWWW.github.io/posts/management-of-the-firm/":[],"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"}],"https://LorrinWWW.github.io/posts/datamining-qualitative-induction/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-finite-element-method/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/QuadTree/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/proba-ch4/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch2/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/":[{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/Note-of-NLP/":[{"title":"Generative Adversarial Network","permalink":"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/"},{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"}],"https://LorrinWWW.github.io/posts/Note-of-datamining/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"数据挖掘-分类与预测","permalink":"https://LorrinWWW.github.io/posts/datamining-class-pred/"},{"title":"数据挖掘-预处理","permalink":"https://LorrinWWW.github.io/posts/datamining-pretreatment/"}],"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/Note-of-probability/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/participe-present-et-gerondif/":[],"https://LorrinWWW.github.io/posts/Hilbert-space/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/proba-ch6/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}]}}},"excerpt":"","more":"<h1 id=\"希尔伯特空间-Hilbert-space\"><a href=\"#希尔伯特空间-Hilbert-space\" class=\"headerlink\" title=\"希尔伯特空间 Hilbert space\"></a>希尔伯特空间 Hilbert space</h1><h2 id=\"感性认识\"><a href=\"#感性认识\" class=\"headerlink\" title=\"感性认识\"></a>感性认识</h2><p>摘自wiki:</p>\n<blockquote>\n<p>在数学里，<strong>希尔伯特空间</strong>即<strong>完备的内积空间</strong>，也就是说一个带有内积的完备向量空间。</p>\n</blockquote>\n<p>要弄清楚希尔伯特空间，我们需要从几个基本的地方出发。</p>\n<h3 id=\"几个形容词\"><a href=\"#几个形容词\" class=\"headerlink\" title=\"几个形容词\"></a>几个形容词</h3><ul>\n<li><p>线性</p>\n<p>有线性结构的。</p>\n</li>\n<li><p>赋范</p>\n<p>空间中用于度量“长度”，引入范数。</p>\n</li>\n<li><p>完备(complet)</p>\n<p>其上所有的柯西序列会收敛到此空间里的一点，序列的极限依然在此空间内。通俗的说，我们认为这个空间是不缺点、有皮的。</p>\n</li>\n<li><p>内积</p>\n<p>补充“角度”概念，引入内积。</p>\n</li>\n</ul>\n<h3 id=\"几类空间\"><a href=\"#几类空间\" class=\"headerlink\" title=\"几类空间\"></a>几类空间</h3><ul>\n<li><p>线性空间</p>\n</li>\n<li><p>度量空间</p>\n<p>最基本的空间。前者有线性结构，后者有度量空间，二者没有交集。</p>\n</li>\n<li><p>赋范线性空间</p>\n<p>引入范数</p>\n</li>\n<li><p>内积空间</p>\n<p>内积空间是赋范线性空间。</p>\n</li>\n<li><p>希尔伯特空间</p>\n<p>希尔伯特空间是完备的内积空间</p>\n</li>\n</ul>\n<p>((线性空间 + 范数  = 赋范空间 + 线性结构) + 内积 = 内积空间) + 完备性 = 希尔伯特空间</p>\n<h2 id=\"可以由以下空间获得\"><a href=\"#可以由以下空间获得\" class=\"headerlink\" title=\"可以由以下空间获得\"></a>可以由以下空间获得</h2><ul>\n<li><p>欧几里得空间</p>\n<p>定义内积。</p>\n</li>\n<li><p>序列空间</p>\n</li>\n<li><p>勒贝格空间</p>\n<p>特指L^2空间，定义其内积:</p>\n<p>​<br>$$<br>(f|g) = \\int{\\overline{f}g}<br>$$<br>还需证明其完备性.</p>\n</li>\n<li><p>索伯列夫空间</p>\n<p>一般表示为H^s或W^(s,2)。在偏微分方程中常用。</p>\n</li>\n</ul>\n<h2 id=\"希尔伯特空间的基\"><a href=\"#希尔伯特空间的基\" class=\"headerlink\" title=\"希尔伯特空间的基\"></a>希尔伯特空间的基</h2><ul>\n<li>所有基的范数为1</li>\n<li>彼此正交</li>\n<li>其线性扩张稠密：即其中的所有元素的有限的线性组合是H的一个稠密子集。</li>\n</ul>\n<h2 id=\"相关\"><a href=\"#相关\" class=\"headerlink\" title=\"相关\"></a>相关</h2><ul>\n<li><p>内积(produit scalaire)</p>\n<p>满足以下四个条件：</p>\n</li>\n</ul>\n<p>$$<br>\\forall (x,y) \\in H^2 \\qquad \\varphi(y,x) = \\overline{\\varphi(x,y)} \\<br>\\forall (x,y,z) \\in H^3, \\forall(\\lambda,\\mu) \\in \\mathbb{C}^2 \\qquad \\varphi(z,\\lambda x+\\mu y) = \\lambda\\varphi(z,x) + \\mu \\varphi(z,y) \\<br>\\forall x \\in H^2 \\qquad \\varphi(x,x) \\ge 0 \\<br>\\varphi (x,x) = 0 \\Longrightarrow x = 0<br>$$</p>\n"},{"title":"Generative Adversarial Network","date":"2017-06-25T06:41:15.000Z","typora-copy-images-to":"ipic","_content":"\n# Generative Adversarial Network\n\n## Generater\n\n1. Auto encoder\n\n   input => nn encoder => code => nn decoder => output\n\n   Output compared with input as close as possible\n\n   [code => nn decoder => output] := a generater\n\n2. VAE\n\n   Auto-encoder Variational Bayes:\n\n   input => nn encoder \n\n   => {\n\n   ​    code : [$m_i$],\n\n   ​    variation : [$\\sigma_i$],\n\n   ​    error : [$e_i$],\n\n   } \n\n   => {$c_i = exp(\\sigma_i) \\times e_i + m_i$}\n\n   => nn decoder => output\n\n   The goal is to monimize the expression as followed:\n   $$\n   \\sum(exp(\\sigma_i) - (1+\\sigma_i) + (m_i)^2)\n   $$\n\n\n\n\n\n## GAN\n\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202530.png\" width=\"70%\">\n\n相当于是由一个生成器和分类器(true or false)\n\n极大似然$P_{data}(x; \\theta) , P_G(x;\\theta)$\n\n- Generator G\n\n  - G is a function, input z, output x\n  - Given a prior distribution $P_{prior}(z)$, a probability distribution $P_G(x)$ is defined by function G\n\n- Discriminator D\n\n  - D is a function, input x, output scalar\n  - Evaluate the \"difference\" between $P_G(x)$ and $P_{data}(x)$\n\n- Function V(G, D)\n  $$\n  G^* = {arg} {min}_G {max}_D V(G,D)\n  $$\n\n\n\n\n从G*中可以看出，D是在给定G的情况下，尽其所能地提高V，即发现$P_{data}$和$P_G$的最多的差异。而G则是使该值尽量减小。在G和D的博弈中模型逐渐完善。\n\n给定V\n$$\nV = E_{x~P_{data}}[logD(x)]+ E_{x~P_G}[log(1-D(x))] \\\\\n= \\int_x P_{data}(x)logD(x)dx +\\int_xP_G(x)log(1-D(X))dx \\\\\n= \\int_x[P_{data}(x)logD(x) + P_G(x)log(1-D(x))]dx\n$$\n要让V的大小可以由积分内的式子决定，即\n$$\nP_{data}(x)logD(x) + P_G(x)log(1-D(x))\n$$\ni.e. find D* maximizing: $f(D) = alog(D)+blog(1-D)$\n$$\n=> D^* = \\frac{a}{a+b} = \\frac{P_{data}(x)}{P_{data}(x)+P_{G}(x)}\n$$\n所以\n$$\nmax_D V(G,D) = V(G, D^*) \\\\\n= -2log2 + \\int_x P_{data}(x) log\\frac{P_{data}(x)}{(P_{data}(x)+P_{G}(x))/2}dx \\\\ + \\int_x P_{data}(x) log\\frac{P_{G}(x)}{(P_{data}(x)+P_{G}(x))/2}dx \\\\\n= -2log2 + KL(P_{data}(x) ||\\frac{P_{data}(x)+P_{G}(x)}{2}) \\\\ \n+ KL(P_{G}(x) ||\\frac{P_{data}(x)+P_{G}(x)}{2}) \\\\\n= -2log2 + 2JSD(P_{data}(X||P_G(x))\n$$\n其中\n$$\nKL := KL divergence \\\\\nJSD(P||Q) = \\frac{1}{2}(KL(P||M) + KL(Q||M)), M= \\frac{P+Q}{2}\n$$\n所以$max_D(G,D)$，当且仅当$P_G = P_{data}$\n\n### **总结一下算法**\n\n- Given $G_0$\n- Find $D_0^*$ maximizing $V(G_0,D)$\n- $\\theta_G \\leftarrow \\theta_G - \\eta \\partial V(G, D_0^*)/ \\partial \\theta_G $ => Obtain G1\n- Find $D_1^*$ maximizing $V(G_1,D)$\n- ...\n\n#### 实际操作\n\n我们知道实际上是不能求期望，即作不能作积分的。因此需要一定的近似。\n\n我们需要将其离散化，取m个样本，V可以写为\n$$\nV = \\frac{1}{m}\\sum logD(x_i) + \\frac{1}{m} \\sum log(1-D(x_i^G)) \\\\\nwhere \\{x_1, ..., x_m\\}  from P_{data}(x), \\{x_1^G,...,x_m^G\\} from P_G(x)\n$$\n","source":"_posts/Generative-Adversarial-Network.md","raw":"---\ntitle: Generative Adversarial Network\ndate: 2017-06-25 14:41:15\ncategories: [programming]\ntags: [GAN, deep-learning]\ntypora-copy-images-to: ipic\n---\n\n# Generative Adversarial Network\n\n## Generater\n\n1. Auto encoder\n\n   input => nn encoder => code => nn decoder => output\n\n   Output compared with input as close as possible\n\n   [code => nn decoder => output] := a generater\n\n2. VAE\n\n   Auto-encoder Variational Bayes:\n\n   input => nn encoder \n\n   => {\n\n   ​    code : [$m_i$],\n\n   ​    variation : [$\\sigma_i$],\n\n   ​    error : [$e_i$],\n\n   } \n\n   => {$c_i = exp(\\sigma_i) \\times e_i + m_i$}\n\n   => nn decoder => output\n\n   The goal is to monimize the expression as followed:\n   $$\n   \\sum(exp(\\sigma_i) - (1+\\sigma_i) + (m_i)^2)\n   $$\n\n\n\n\n\n## GAN\n\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202530.png\" width=\"70%\">\n\n相当于是由一个生成器和分类器(true or false)\n\n极大似然$P_{data}(x; \\theta) , P_G(x;\\theta)$\n\n- Generator G\n\n  - G is a function, input z, output x\n  - Given a prior distribution $P_{prior}(z)$, a probability distribution $P_G(x)$ is defined by function G\n\n- Discriminator D\n\n  - D is a function, input x, output scalar\n  - Evaluate the \"difference\" between $P_G(x)$ and $P_{data}(x)$\n\n- Function V(G, D)\n  $$\n  G^* = {arg} {min}_G {max}_D V(G,D)\n  $$\n\n\n\n\n从G*中可以看出，D是在给定G的情况下，尽其所能地提高V，即发现$P_{data}$和$P_G$的最多的差异。而G则是使该值尽量减小。在G和D的博弈中模型逐渐完善。\n\n给定V\n$$\nV = E_{x~P_{data}}[logD(x)]+ E_{x~P_G}[log(1-D(x))] \\\\\n= \\int_x P_{data}(x)logD(x)dx +\\int_xP_G(x)log(1-D(X))dx \\\\\n= \\int_x[P_{data}(x)logD(x) + P_G(x)log(1-D(x))]dx\n$$\n要让V的大小可以由积分内的式子决定，即\n$$\nP_{data}(x)logD(x) + P_G(x)log(1-D(x))\n$$\ni.e. find D* maximizing: $f(D) = alog(D)+blog(1-D)$\n$$\n=> D^* = \\frac{a}{a+b} = \\frac{P_{data}(x)}{P_{data}(x)+P_{G}(x)}\n$$\n所以\n$$\nmax_D V(G,D) = V(G, D^*) \\\\\n= -2log2 + \\int_x P_{data}(x) log\\frac{P_{data}(x)}{(P_{data}(x)+P_{G}(x))/2}dx \\\\ + \\int_x P_{data}(x) log\\frac{P_{G}(x)}{(P_{data}(x)+P_{G}(x))/2}dx \\\\\n= -2log2 + KL(P_{data}(x) ||\\frac{P_{data}(x)+P_{G}(x)}{2}) \\\\ \n+ KL(P_{G}(x) ||\\frac{P_{data}(x)+P_{G}(x)}{2}) \\\\\n= -2log2 + 2JSD(P_{data}(X||P_G(x))\n$$\n其中\n$$\nKL := KL divergence \\\\\nJSD(P||Q) = \\frac{1}{2}(KL(P||M) + KL(Q||M)), M= \\frac{P+Q}{2}\n$$\n所以$max_D(G,D)$，当且仅当$P_G = P_{data}$\n\n### **总结一下算法**\n\n- Given $G_0$\n- Find $D_0^*$ maximizing $V(G_0,D)$\n- $\\theta_G \\leftarrow \\theta_G - \\eta \\partial V(G, D_0^*)/ \\partial \\theta_G $ => Obtain G1\n- Find $D_1^*$ maximizing $V(G_1,D)$\n- ...\n\n#### 实际操作\n\n我们知道实际上是不能求期望，即作不能作积分的。因此需要一定的近似。\n\n我们需要将其离散化，取m个样本，V可以写为\n$$\nV = \\frac{1}{m}\\sum logD(x_i) + \\frac{1}{m} \\sum log(1-D(x_i^G)) \\\\\nwhere \\{x_1, ..., x_m\\}  from P_{data}(x), \\{x_1^G,...,x_m^G\\} from P_G(x)\n$$\n","slug":"Generative-Adversarial-Network","published":1,"updated":"2020-11-03T03:26:03.479Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckw4qufoy000egwtlcr1j4206","content":"<h1 id=\"Generative-Adversarial-Network\"><a href=\"#Generative-Adversarial-Network\" class=\"headerlink\" title=\"Generative Adversarial Network\"></a>Generative Adversarial Network</h1><h2 id=\"Generater\"><a href=\"#Generater\" class=\"headerlink\" title=\"Generater\"></a>Generater</h2><ol>\n<li><p>Auto encoder</p>\n<p>input =&gt; nn encoder =&gt; code =&gt; nn decoder =&gt; output</p>\n<p>Output compared with input as close as possible</p>\n<p>[code =&gt; nn decoder =&gt; output] := a generater</p>\n</li>\n<li><p>VAE</p>\n<p>Auto-encoder Variational Bayes:</p>\n<p>input =&gt; nn encoder </p>\n<p>=&gt; {</p>\n<p>​    code : [$m_i$],</p>\n<p>​    variation : [$\\sigma_i$],</p>\n<p>​    error : [$e_i$],</p>\n<p>} </p>\n<p>=&gt; {$c_i = exp(\\sigma_i) \\times e_i + m_i$}</p>\n<p>=&gt; nn decoder =&gt; output</p>\n<p>The goal is to monimize the expression as followed:<br>$$<br>\\sum(exp(\\sigma_i) - (1+\\sigma_i) + (m_i)^2)<br>$$</p>\n</li>\n</ol>\n<h2 id=\"GAN\"><a href=\"#GAN\" class=\"headerlink\" title=\"GAN\"></a>GAN</h2><img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202530.png\" width=\"70%\">\n\n<p>相当于是由一个生成器和分类器(true or false)</p>\n<p>极大似然$P_{data}(x; \\theta) , P_G(x;\\theta)$</p>\n<ul>\n<li><p>Generator G</p>\n<ul>\n<li>G is a function, input z, output x</li>\n<li>Given a prior distribution $P_{prior}(z)$, a probability distribution $P_G(x)$ is defined by function G</li>\n</ul>\n</li>\n<li><p>Discriminator D</p>\n<ul>\n<li>D is a function, input x, output scalar</li>\n<li>Evaluate the “difference” between $P_G(x)$ and $P_{data}(x)$</li>\n</ul>\n</li>\n<li><p>Function V(G, D)<br>$$<br>G^* = {arg} {min}_G {max}_D V(G,D)<br>$$</p>\n</li>\n</ul>\n<p>从G*中可以看出，D是在给定G的情况下，尽其所能地提高V，即发现$P_{data}$和$P_G$的最多的差异。而G则是使该值尽量减小。在G和D的博弈中模型逐渐完善。</p>\n<p>给定V<br>$$<br>V = E_{x<del>P_{data}}[logD(x)]+ E_{x</del>P_G}[log(1-D(x))] \\<br>= \\int_x P_{data}(x)logD(x)dx +\\int_xP_G(x)log(1-D(X))dx \\<br>= \\int_x[P_{data}(x)logD(x) + P_G(x)log(1-D(x))]dx<br>$$<br>要让V的大小可以由积分内的式子决定，即<br>$$<br>P_{data}(x)logD(x) + P_G(x)log(1-D(x))<br>$$<br>i.e. find D* maximizing: $f(D) = alog(D)+blog(1-D)$<br>$$<br>=&gt; D^* = \\frac{a}{a+b} = \\frac{P_{data}(x)}{P_{data}(x)+P_{G}(x)}<br>$$<br>所以<br>$$<br>max_D V(G,D) = V(G, D^*) \\<br>= -2log2 + \\int_x P_{data}(x) log\\frac{P_{data}(x)}{(P_{data}(x)+P_{G}(x))/2}dx \\ + \\int_x P_{data}(x) log\\frac{P_{G}(x)}{(P_{data}(x)+P_{G}(x))/2}dx \\<br>= -2log2 + KL(P_{data}(x) ||\\frac{P_{data}(x)+P_{G}(x)}{2}) \\ </p>\n<ul>\n<li>KL(P_{G}(x) ||\\frac{P_{data}(x)+P_{G}(x)}{2}) \\<br>= -2log2 + 2JSD(P_{data}(X||P_G(x))<br>$$<br>其中<br>$$<br>KL := KL divergence \\<br>JSD(P||Q) = \\frac{1}{2}(KL(P||M) + KL(Q||M)), M= \\frac{P+Q}{2}<br>$$<br>所以$max_D(G,D)$，当且仅当$P_G = P_{data}$</li>\n</ul>\n<h3 id=\"总结一下算法\"><a href=\"#总结一下算法\" class=\"headerlink\" title=\"总结一下算法\"></a><strong>总结一下算法</strong></h3><ul>\n<li>Given $G_0$</li>\n<li>Find $D_0^*$ maximizing $V(G_0,D)$</li>\n<li>$\\theta_G \\leftarrow \\theta_G - \\eta \\partial V(G, D_0^*)/ \\partial \\theta_G $ =&gt; Obtain G1</li>\n<li>Find $D_1^*$ maximizing $V(G_1,D)$</li>\n<li>…</li>\n</ul>\n<h4 id=\"实际操作\"><a href=\"#实际操作\" class=\"headerlink\" title=\"实际操作\"></a>实际操作</h4><p>我们知道实际上是不能求期望，即作不能作积分的。因此需要一定的近似。</p>\n<p>我们需要将其离散化，取m个样本，V可以写为<br>$$<br>V = \\frac{1}{m}\\sum logD(x_i) + \\frac{1}{m} \\sum log(1-D(x_i^G)) \\<br>where {x_1, …, x_m}  from P_{data}(x), {x_1^G,…,x_m^G} from P_G(x)<br>$$</p>\n","site":{"data":{"recommended_posts":{"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"}],"https://LorrinWWW.github.io/posts/complexity/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/hexo-with-latex/":[{"title":"区块链时代的最正确的打赏姿势——在Hexo博客配置Coinhive网页挖矿","permalink":"http://www.davidfnck.com/blockchain/mine-xmr-in-hexo-by-coinhive.html/"}],"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/":[{"title":"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-statistic/":[{"title":"贝叶斯估计 Bayes estimation","permalink":"https://LorrinWWW.github.io/posts/Bayes-estimation/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"}],"https://LorrinWWW.github.io/posts/[2018.2.5]Nested-LSTMs/":[{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"}],"https://LorrinWWW.github.io/posts/datamining-pretreatment/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Several models for knowledge graph representing and completing 几个知识图谱模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/"}],"https://LorrinWWW.github.io/posts/compression/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/Sobolev-space/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/proba-ch5/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch3/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/proba-ch1/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/ML-CNN/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/EDP-basic-models/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/machine-learning/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/datamining-class-pred/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction/":[],"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/":[{"title":"OS notes","permalink":"https://LorrinWWW.github.io/posts/OS-notes/"}],"https://LorrinWWW.github.io/posts/projet-enjeu-plugin-chrome-101/":[{"title":"Chrome油猴脚本","permalink":"http://gubangzhong.cn/2017/06/13/Chrome油猴脚本/"}],"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/":[{"title":"MongoDB, Docker and Python","permalink":"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/":[{"title":"Hands on Scrapy","permalink":"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/OS-notes/":[{"title":"learning OS and building LorriOS","permalink":"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/"}],"https://LorrinWWW.github.io/posts/Bayes-estimation/":[{"title":"Note of statistic","permalink":"https://LorrinWWW.github.io/posts/Note-of-statistic/"}],"https://LorrinWWW.github.io/posts/management-of-the-firm/":[],"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"}],"https://LorrinWWW.github.io/posts/datamining-qualitative-induction/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-finite-element-method/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/QuadTree/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/proba-ch4/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch2/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/":[{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/Note-of-NLP/":[{"title":"Generative Adversarial Network","permalink":"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/"},{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"}],"https://LorrinWWW.github.io/posts/Note-of-datamining/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"数据挖掘-分类与预测","permalink":"https://LorrinWWW.github.io/posts/datamining-class-pred/"},{"title":"数据挖掘-预处理","permalink":"https://LorrinWWW.github.io/posts/datamining-pretreatment/"}],"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/Note-of-probability/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/participe-present-et-gerondif/":[],"https://LorrinWWW.github.io/posts/Hilbert-space/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/proba-ch6/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}]}}},"excerpt":"","more":"<h1 id=\"Generative-Adversarial-Network\"><a href=\"#Generative-Adversarial-Network\" class=\"headerlink\" title=\"Generative Adversarial Network\"></a>Generative Adversarial Network</h1><h2 id=\"Generater\"><a href=\"#Generater\" class=\"headerlink\" title=\"Generater\"></a>Generater</h2><ol>\n<li><p>Auto encoder</p>\n<p>input =&gt; nn encoder =&gt; code =&gt; nn decoder =&gt; output</p>\n<p>Output compared with input as close as possible</p>\n<p>[code =&gt; nn decoder =&gt; output] := a generater</p>\n</li>\n<li><p>VAE</p>\n<p>Auto-encoder Variational Bayes:</p>\n<p>input =&gt; nn encoder </p>\n<p>=&gt; {</p>\n<p>​    code : [$m_i$],</p>\n<p>​    variation : [$\\sigma_i$],</p>\n<p>​    error : [$e_i$],</p>\n<p>} </p>\n<p>=&gt; {$c_i = exp(\\sigma_i) \\times e_i + m_i$}</p>\n<p>=&gt; nn decoder =&gt; output</p>\n<p>The goal is to monimize the expression as followed:<br>$$<br>\\sum(exp(\\sigma_i) - (1+\\sigma_i) + (m_i)^2)<br>$$</p>\n</li>\n</ol>\n<h2 id=\"GAN\"><a href=\"#GAN\" class=\"headerlink\" title=\"GAN\"></a>GAN</h2><img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202530.png\" width=\"70%\">\n\n<p>相当于是由一个生成器和分类器(true or false)</p>\n<p>极大似然$P_{data}(x; \\theta) , P_G(x;\\theta)$</p>\n<ul>\n<li><p>Generator G</p>\n<ul>\n<li>G is a function, input z, output x</li>\n<li>Given a prior distribution $P_{prior}(z)$, a probability distribution $P_G(x)$ is defined by function G</li>\n</ul>\n</li>\n<li><p>Discriminator D</p>\n<ul>\n<li>D is a function, input x, output scalar</li>\n<li>Evaluate the “difference” between $P_G(x)$ and $P_{data}(x)$</li>\n</ul>\n</li>\n<li><p>Function V(G, D)<br>$$<br>G^* = {arg} {min}_G {max}_D V(G,D)<br>$$</p>\n</li>\n</ul>\n<p>从G*中可以看出，D是在给定G的情况下，尽其所能地提高V，即发现$P_{data}$和$P_G$的最多的差异。而G则是使该值尽量减小。在G和D的博弈中模型逐渐完善。</p>\n<p>给定V<br>$$<br>V = E_{x<del>P_{data}}[logD(x)]+ E_{x</del>P_G}[log(1-D(x))] \\<br>= \\int_x P_{data}(x)logD(x)dx +\\int_xP_G(x)log(1-D(X))dx \\<br>= \\int_x[P_{data}(x)logD(x) + P_G(x)log(1-D(x))]dx<br>$$<br>要让V的大小可以由积分内的式子决定，即<br>$$<br>P_{data}(x)logD(x) + P_G(x)log(1-D(x))<br>$$<br>i.e. find D* maximizing: $f(D) = alog(D)+blog(1-D)$<br>$$<br>=&gt; D^* = \\frac{a}{a+b} = \\frac{P_{data}(x)}{P_{data}(x)+P_{G}(x)}<br>$$<br>所以<br>$$<br>max_D V(G,D) = V(G, D^*) \\<br>= -2log2 + \\int_x P_{data}(x) log\\frac{P_{data}(x)}{(P_{data}(x)+P_{G}(x))/2}dx \\ + \\int_x P_{data}(x) log\\frac{P_{G}(x)}{(P_{data}(x)+P_{G}(x))/2}dx \\<br>= -2log2 + KL(P_{data}(x) ||\\frac{P_{data}(x)+P_{G}(x)}{2}) \\ </p>\n<ul>\n<li>KL(P_{G}(x) ||\\frac{P_{data}(x)+P_{G}(x)}{2}) \\<br>= -2log2 + 2JSD(P_{data}(X||P_G(x))<br>$$<br>其中<br>$$<br>KL := KL divergence \\<br>JSD(P||Q) = \\frac{1}{2}(KL(P||M) + KL(Q||M)), M= \\frac{P+Q}{2}<br>$$<br>所以$max_D(G,D)$，当且仅当$P_G = P_{data}$</li>\n</ul>\n<h3 id=\"总结一下算法\"><a href=\"#总结一下算法\" class=\"headerlink\" title=\"总结一下算法\"></a><strong>总结一下算法</strong></h3><ul>\n<li>Given $G_0$</li>\n<li>Find $D_0^*$ maximizing $V(G_0,D)$</li>\n<li>$\\theta_G \\leftarrow \\theta_G - \\eta \\partial V(G, D_0^*)/ \\partial \\theta_G $ =&gt; Obtain G1</li>\n<li>Find $D_1^*$ maximizing $V(G_1,D)$</li>\n<li>…</li>\n</ul>\n<h4 id=\"实际操作\"><a href=\"#实际操作\" class=\"headerlink\" title=\"实际操作\"></a>实际操作</h4><p>我们知道实际上是不能求期望，即作不能作积分的。因此需要一定的近似。</p>\n<p>我们需要将其离散化，取m个样本，V可以写为<br>$$<br>V = \\frac{1}{m}\\sum logD(x_i) + \\frac{1}{m} \\sum log(1-D(x_i^G)) \\<br>where {x_1, …, x_m}  from P_{data}(x), {x_1^G,…,x_m^G} from P_G(x)<br>$$</p>\n"},{"title":"ML CNN","date":"2016-12-14T02:34:43.000Z","_content":"\n笔记向\n\n目前来说，深度学习中比较火的两大类是卷积神经网络(CNN)和递归神经网络(RNN)。\n\n# 全连接层\n\n全连接层一般由两个部分— 线性部分和非线性部分组成。\n\n## 线性部分\n\n输入：\n$$\nx = [x_0,x_1,...,x_n]^T\n$$\n输出：\n$$\nz = [z_0,z_1,...,z_m]^T\n$$\n参数为一个矩阵\n$$\nW_{m \\times n}\n$$\n偏执项：\n$$\nb = [b_0,b_1,...,b_m]^T\n$$\n于是：\n$$\nW * x + b = z\n$$\n\n## 非线性部分\n\n根据我们线性代数的知识，如果一系列变换都是线形变换，那么我们可以使用一次线形变换来代替之前的所有变换。也就是说，若没有非线性部分，那么**多元**神经元在这里也就没有意义了(因为只需要一层神经元就可以代替其他所有的)。\n\n当然，非线性部分也不是为了存在而存在的，用途我们会在后面有更深的理解。\n\n非线性部分的模型也不是唯一的，但是我们常常使用以下几种。\n\n1. sigmoid\n\n   记得就在前几天，在数理统计的书上也看到了这个函数— sigmoid。\n\n   先写出它的形式：\n   $$\n   f(x) = \\frac{1}{1+e^{-x}}\n   $$\n   这个函数把一个取值在R上的变量，变成了一个取值在(0, 1)上的变量。\n\n2. 双曲正切\n   $$\n   f(x) = \\frac{e^x - e^{-x}}{e^x+e^{-x}}\n   $$\n   它的取值范围是(-1,1)。可以看出，它是有正有负的，而sigmoid是全为正的。\n\n","source":"_posts/ML-CNN.md","raw":"---\ntitle: ML CNN\ndate: 2016-12-14 10:34:43\ncategories: [programming, unfinished]\ntags: [machine-learning, programming, algo, CNN]\n---\n\n笔记向\n\n目前来说，深度学习中比较火的两大类是卷积神经网络(CNN)和递归神经网络(RNN)。\n\n# 全连接层\n\n全连接层一般由两个部分— 线性部分和非线性部分组成。\n\n## 线性部分\n\n输入：\n$$\nx = [x_0,x_1,...,x_n]^T\n$$\n输出：\n$$\nz = [z_0,z_1,...,z_m]^T\n$$\n参数为一个矩阵\n$$\nW_{m \\times n}\n$$\n偏执项：\n$$\nb = [b_0,b_1,...,b_m]^T\n$$\n于是：\n$$\nW * x + b = z\n$$\n\n## 非线性部分\n\n根据我们线性代数的知识，如果一系列变换都是线形变换，那么我们可以使用一次线形变换来代替之前的所有变换。也就是说，若没有非线性部分，那么**多元**神经元在这里也就没有意义了(因为只需要一层神经元就可以代替其他所有的)。\n\n当然，非线性部分也不是为了存在而存在的，用途我们会在后面有更深的理解。\n\n非线性部分的模型也不是唯一的，但是我们常常使用以下几种。\n\n1. sigmoid\n\n   记得就在前几天，在数理统计的书上也看到了这个函数— sigmoid。\n\n   先写出它的形式：\n   $$\n   f(x) = \\frac{1}{1+e^{-x}}\n   $$\n   这个函数把一个取值在R上的变量，变成了一个取值在(0, 1)上的变量。\n\n2. 双曲正切\n   $$\n   f(x) = \\frac{e^x - e^{-x}}{e^x+e^{-x}}\n   $$\n   它的取值范围是(-1,1)。可以看出，它是有正有负的，而sigmoid是全为正的。\n\n","slug":"ML-CNN","published":1,"updated":"2020-02-09T14:06:32.526Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckw4qufp0000igwtlg2ur61jm","content":"<p>笔记向</p>\n<p>目前来说，深度学习中比较火的两大类是卷积神经网络(CNN)和递归神经网络(RNN)。</p>\n<h1 id=\"全连接层\"><a href=\"#全连接层\" class=\"headerlink\" title=\"全连接层\"></a>全连接层</h1><p>全连接层一般由两个部分— 线性部分和非线性部分组成。</p>\n<h2 id=\"线性部分\"><a href=\"#线性部分\" class=\"headerlink\" title=\"线性部分\"></a>线性部分</h2><p>输入：<br>$$<br>x = [x_0,x_1,…,x_n]^T<br>$$<br>输出：<br>$$<br>z = [z_0,z_1,…,z_m]^T<br>$$<br>参数为一个矩阵<br>$$<br>W_{m \\times n}<br>$$<br>偏执项：<br>$$<br>b = [b_0,b_1,…,b_m]^T<br>$$<br>于是：<br>$$<br>W * x + b = z<br>$$</p>\n<h2 id=\"非线性部分\"><a href=\"#非线性部分\" class=\"headerlink\" title=\"非线性部分\"></a>非线性部分</h2><p>根据我们线性代数的知识，如果一系列变换都是线形变换，那么我们可以使用一次线形变换来代替之前的所有变换。也就是说，若没有非线性部分，那么<strong>多元</strong>神经元在这里也就没有意义了(因为只需要一层神经元就可以代替其他所有的)。</p>\n<p>当然，非线性部分也不是为了存在而存在的，用途我们会在后面有更深的理解。</p>\n<p>非线性部分的模型也不是唯一的，但是我们常常使用以下几种。</p>\n<ol>\n<li><p>sigmoid</p>\n<p>记得就在前几天，在数理统计的书上也看到了这个函数— sigmoid。</p>\n<p>先写出它的形式：<br>$$<br>f(x) = \\frac{1}{1+e^{-x}}<br>$$<br>这个函数把一个取值在R上的变量，变成了一个取值在(0, 1)上的变量。</p>\n</li>\n<li><p>双曲正切<br>$$<br>f(x) = \\frac{e^x - e^{-x}}{e^x+e^{-x}}<br>$$<br>它的取值范围是(-1,1)。可以看出，它是有正有负的，而sigmoid是全为正的。</p>\n</li>\n</ol>\n","site":{"data":{"recommended_posts":{"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"}],"https://LorrinWWW.github.io/posts/complexity/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/hexo-with-latex/":[{"title":"区块链时代的最正确的打赏姿势——在Hexo博客配置Coinhive网页挖矿","permalink":"http://www.davidfnck.com/blockchain/mine-xmr-in-hexo-by-coinhive.html/"}],"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/":[{"title":"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-statistic/":[{"title":"贝叶斯估计 Bayes estimation","permalink":"https://LorrinWWW.github.io/posts/Bayes-estimation/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"}],"https://LorrinWWW.github.io/posts/[2018.2.5]Nested-LSTMs/":[{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"}],"https://LorrinWWW.github.io/posts/datamining-pretreatment/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Several models for knowledge graph representing and completing 几个知识图谱模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/"}],"https://LorrinWWW.github.io/posts/compression/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/Sobolev-space/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/proba-ch5/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch3/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/proba-ch1/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/ML-CNN/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/EDP-basic-models/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/machine-learning/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/datamining-class-pred/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction/":[],"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/":[{"title":"OS notes","permalink":"https://LorrinWWW.github.io/posts/OS-notes/"}],"https://LorrinWWW.github.io/posts/projet-enjeu-plugin-chrome-101/":[{"title":"Chrome油猴脚本","permalink":"http://gubangzhong.cn/2017/06/13/Chrome油猴脚本/"}],"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/":[{"title":"MongoDB, Docker and Python","permalink":"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/":[{"title":"Hands on Scrapy","permalink":"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/OS-notes/":[{"title":"learning OS and building LorriOS","permalink":"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/"}],"https://LorrinWWW.github.io/posts/Bayes-estimation/":[{"title":"Note of statistic","permalink":"https://LorrinWWW.github.io/posts/Note-of-statistic/"}],"https://LorrinWWW.github.io/posts/management-of-the-firm/":[],"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"}],"https://LorrinWWW.github.io/posts/datamining-qualitative-induction/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-finite-element-method/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/QuadTree/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/proba-ch4/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch2/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/":[{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/Note-of-NLP/":[{"title":"Generative Adversarial Network","permalink":"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/"},{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"}],"https://LorrinWWW.github.io/posts/Note-of-datamining/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"数据挖掘-分类与预测","permalink":"https://LorrinWWW.github.io/posts/datamining-class-pred/"},{"title":"数据挖掘-预处理","permalink":"https://LorrinWWW.github.io/posts/datamining-pretreatment/"}],"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/Note-of-probability/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/participe-present-et-gerondif/":[],"https://LorrinWWW.github.io/posts/Hilbert-space/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/proba-ch6/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}]}}},"excerpt":"","more":"<p>笔记向</p>\n<p>目前来说，深度学习中比较火的两大类是卷积神经网络(CNN)和递归神经网络(RNN)。</p>\n<h1 id=\"全连接层\"><a href=\"#全连接层\" class=\"headerlink\" title=\"全连接层\"></a>全连接层</h1><p>全连接层一般由两个部分— 线性部分和非线性部分组成。</p>\n<h2 id=\"线性部分\"><a href=\"#线性部分\" class=\"headerlink\" title=\"线性部分\"></a>线性部分</h2><p>输入：<br>$$<br>x = [x_0,x_1,…,x_n]^T<br>$$<br>输出：<br>$$<br>z = [z_0,z_1,…,z_m]^T<br>$$<br>参数为一个矩阵<br>$$<br>W_{m \\times n}<br>$$<br>偏执项：<br>$$<br>b = [b_0,b_1,…,b_m]^T<br>$$<br>于是：<br>$$<br>W * x + b = z<br>$$</p>\n<h2 id=\"非线性部分\"><a href=\"#非线性部分\" class=\"headerlink\" title=\"非线性部分\"></a>非线性部分</h2><p>根据我们线性代数的知识，如果一系列变换都是线形变换，那么我们可以使用一次线形变换来代替之前的所有变换。也就是说，若没有非线性部分，那么<strong>多元</strong>神经元在这里也就没有意义了(因为只需要一层神经元就可以代替其他所有的)。</p>\n<p>当然，非线性部分也不是为了存在而存在的，用途我们会在后面有更深的理解。</p>\n<p>非线性部分的模型也不是唯一的，但是我们常常使用以下几种。</p>\n<ol>\n<li><p>sigmoid</p>\n<p>记得就在前几天，在数理统计的书上也看到了这个函数— sigmoid。</p>\n<p>先写出它的形式：<br>$$<br>f(x) = \\frac{1}{1+e^{-x}}<br>$$<br>这个函数把一个取值在R上的变量，变成了一个取值在(0, 1)上的变量。</p>\n</li>\n<li><p>双曲正切<br>$$<br>f(x) = \\frac{e^x - e^{-x}}{e^x+e^{-x}}<br>$$<br>它的取值范围是(-1,1)。可以看出，它是有正有负的，而sigmoid是全为正的。</p>\n</li>\n</ol>\n"},{"title":"面向考试常用编程思想 Method of programming facing to exams","date":"2016-11-27T05:49:31.000Z","_content":"\n1. 穷举法\n\n   略\n\n2. 贪心法\n\n   略\n\n3. 分治法\n\n   ```Python\n   def merge(A, B):\n       # merge two small solved problems into one.\n       return merged\n\n   def divideConquer(S, divide, combine):\n       if len(S) == 1: return S\n       # divide a grand problems\n       L, R = divide(S)\n       A = divideConquer(L, divide, combine)\n       B = divideConquer(R, divide, combine)\n       return merge(A, B)\n   ```\n\n   上面的情形只供参考，参数等视具体情况而定。\n\n4. 动态规划\n\n   比较经典的就是背包问题。[代码来源](http://blog.csdn.net/littlethunder/article/details/26575417)\n\n   ```Python\n   def bag(n,c,w,v):  \n       res=[[-1 for j in range(c+1)] for i in range(n+1)]  \n       for j in range(c+1):  \n           res[0][j]=0  \n       for i in range(1,n+1):  \n           for j in range(1,c+1):  \n               res[i][j]=res[i-1][j]  \n               if j>=w[i-1] and res[i][j]<res[i-1][j-w[i-1]]+v[i-1]:  \n                   res[i][j]=res[i-1][j-w[i-1]]+v[i-1]  \n       return res  \n     \n   def show(n,c,w,res):  \n       print('最大价值为:',res[n][c])  \n       x=[False for i in range(n)]  \n       j=c  \n       for i in range(1,n+1):  \n           if res[i][j]>res[i-1][j]:  \n               x[i-1]=True  \n               j-=w[i-1]  \n       print('选择的物品为:')  \n       for i in range(n):  \n           if x[i]:  \n               print('第',i,'个,',end='')  \n       print('')  \n     \n   if __name__=='__main__':  \n       n=5  \n       c=10  \n       w=[2,2,6,5,4]  \n       v=[6,3,5,4,6]  \n       res=bag(n,c,w,v)  \n       show(n,c,w,res)\n   ```\n\n   ​","source":"_posts/Method-of-programming-facing-to-exams.md","raw":"---\ntitle: 面向考试常用编程思想 Method of programming facing to exams\ndate: 2016-11-27 13:49:31\ncategories: programming\ntags: [algo, programming]\n---\n\n1. 穷举法\n\n   略\n\n2. 贪心法\n\n   略\n\n3. 分治法\n\n   ```Python\n   def merge(A, B):\n       # merge two small solved problems into one.\n       return merged\n\n   def divideConquer(S, divide, combine):\n       if len(S) == 1: return S\n       # divide a grand problems\n       L, R = divide(S)\n       A = divideConquer(L, divide, combine)\n       B = divideConquer(R, divide, combine)\n       return merge(A, B)\n   ```\n\n   上面的情形只供参考，参数等视具体情况而定。\n\n4. 动态规划\n\n   比较经典的就是背包问题。[代码来源](http://blog.csdn.net/littlethunder/article/details/26575417)\n\n   ```Python\n   def bag(n,c,w,v):  \n       res=[[-1 for j in range(c+1)] for i in range(n+1)]  \n       for j in range(c+1):  \n           res[0][j]=0  \n       for i in range(1,n+1):  \n           for j in range(1,c+1):  \n               res[i][j]=res[i-1][j]  \n               if j>=w[i-1] and res[i][j]<res[i-1][j-w[i-1]]+v[i-1]:  \n                   res[i][j]=res[i-1][j-w[i-1]]+v[i-1]  \n       return res  \n     \n   def show(n,c,w,res):  \n       print('最大价值为:',res[n][c])  \n       x=[False for i in range(n)]  \n       j=c  \n       for i in range(1,n+1):  \n           if res[i][j]>res[i-1][j]:  \n               x[i-1]=True  \n               j-=w[i-1]  \n       print('选择的物品为:')  \n       for i in range(n):  \n           if x[i]:  \n               print('第',i,'个,',end='')  \n       print('')  \n     \n   if __name__=='__main__':  \n       n=5  \n       c=10  \n       w=[2,2,6,5,4]  \n       v=[6,3,5,4,6]  \n       res=bag(n,c,w,v)  \n       show(n,c,w,res)\n   ```\n\n   ​","slug":"Method-of-programming-facing-to-exams","published":1,"updated":"2016-11-30T10:45:24.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckw4qufp2000kgwtl01ad8zyn","content":"<ol>\n<li><p>穷举法</p>\n<p>略</p>\n</li>\n<li><p>贪心法</p>\n<p>略</p>\n</li>\n<li><p>分治法</p>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">merge</span>(<span class=\"params\">A, B</span>):</span></span><br><span class=\"line\">    <span class=\"comment\"># merge two small solved problems into one.</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> merged</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">divideConquer</span>(<span class=\"params\">S, divide, combine</span>):</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(S) == <span class=\"number\">1</span>: <span class=\"keyword\">return</span> S</span><br><span class=\"line\">    <span class=\"comment\"># divide a grand problems</span></span><br><span class=\"line\">    L, R = divide(S)</span><br><span class=\"line\">    A = divideConquer(L, divide, combine)</span><br><span class=\"line\">    B = divideConquer(R, divide, combine)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> merge(A, B)</span><br></pre></td></tr></tbody></table></figure>\n\n<p>上面的情形只供参考，参数等视具体情况而定。</p>\n</li>\n<li><p>动态规划</p>\n<p>比较经典的就是背包问题。<a href=\"http://blog.csdn.net/littlethunder/article/details/26575417\">代码来源</a></p>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">bag</span>(<span class=\"params\">n,c,w,v</span>):</span>  </span><br><span class=\"line\">    res=[[-<span class=\"number\">1</span> <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(c+<span class=\"number\">1</span>)] <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(n+<span class=\"number\">1</span>)]  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(c+<span class=\"number\">1</span>):  </span><br><span class=\"line\">        res[<span class=\"number\">0</span>][j]=<span class=\"number\">0</span>  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">1</span>,n+<span class=\"number\">1</span>):  </span><br><span class=\"line\">        <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">1</span>,c+<span class=\"number\">1</span>):  </span><br><span class=\"line\">            res[i][j]=res[i-<span class=\"number\">1</span>][j]  </span><br><span class=\"line\">            <span class=\"keyword\">if</span> j&gt;=w[i-<span class=\"number\">1</span>] <span class=\"keyword\">and</span> res[i][j]&lt;res[i-<span class=\"number\">1</span>][j-w[i-<span class=\"number\">1</span>]]+v[i-<span class=\"number\">1</span>]:  </span><br><span class=\"line\">                res[i][j]=res[i-<span class=\"number\">1</span>][j-w[i-<span class=\"number\">1</span>]]+v[i-<span class=\"number\">1</span>]  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> res  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">show</span>(<span class=\"params\">n,c,w,res</span>):</span>  </span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">'最大价值为:'</span>,res[n][c])  </span><br><span class=\"line\">    x=[<span class=\"literal\">False</span> <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(n)]  </span><br><span class=\"line\">    j=c  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">1</span>,n+<span class=\"number\">1</span>):  </span><br><span class=\"line\">        <span class=\"keyword\">if</span> res[i][j]&gt;res[i-<span class=\"number\">1</span>][j]:  </span><br><span class=\"line\">            x[i-<span class=\"number\">1</span>]=<span class=\"literal\">True</span>  </span><br><span class=\"line\">            j-=w[i-<span class=\"number\">1</span>]  </span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">'选择的物品为:'</span>)  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(n):  </span><br><span class=\"line\">        <span class=\"keyword\">if</span> x[i]:  </span><br><span class=\"line\">            <span class=\"built_in\">print</span>(<span class=\"string\">'第'</span>,i,<span class=\"string\">'个,'</span>,end=<span class=\"string\">''</span>)  </span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">''</span>)  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">'__main__'</span>:  </span><br><span class=\"line\">    n=<span class=\"number\">5</span>  </span><br><span class=\"line\">    c=<span class=\"number\">10</span>  </span><br><span class=\"line\">    w=[<span class=\"number\">2</span>,<span class=\"number\">2</span>,<span class=\"number\">6</span>,<span class=\"number\">5</span>,<span class=\"number\">4</span>]  </span><br><span class=\"line\">    v=[<span class=\"number\">6</span>,<span class=\"number\">3</span>,<span class=\"number\">5</span>,<span class=\"number\">4</span>,<span class=\"number\">6</span>]  </span><br><span class=\"line\">    res=bag(n,c,w,v)  </span><br><span class=\"line\">    show(n,c,w,res)</span><br></pre></td></tr></tbody></table></figure>\n\n<p>​</p>\n</li>\n</ol>\n","site":{"data":{"recommended_posts":{"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"}],"https://LorrinWWW.github.io/posts/complexity/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/hexo-with-latex/":[{"title":"区块链时代的最正确的打赏姿势——在Hexo博客配置Coinhive网页挖矿","permalink":"http://www.davidfnck.com/blockchain/mine-xmr-in-hexo-by-coinhive.html/"}],"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/":[{"title":"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-statistic/":[{"title":"贝叶斯估计 Bayes estimation","permalink":"https://LorrinWWW.github.io/posts/Bayes-estimation/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"}],"https://LorrinWWW.github.io/posts/[2018.2.5]Nested-LSTMs/":[{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"}],"https://LorrinWWW.github.io/posts/datamining-pretreatment/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Several models for knowledge graph representing and completing 几个知识图谱模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/"}],"https://LorrinWWW.github.io/posts/compression/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/Sobolev-space/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/proba-ch5/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch3/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/proba-ch1/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/ML-CNN/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/EDP-basic-models/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/machine-learning/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/datamining-class-pred/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction/":[],"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/":[{"title":"OS notes","permalink":"https://LorrinWWW.github.io/posts/OS-notes/"}],"https://LorrinWWW.github.io/posts/projet-enjeu-plugin-chrome-101/":[{"title":"Chrome油猴脚本","permalink":"http://gubangzhong.cn/2017/06/13/Chrome油猴脚本/"}],"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/":[{"title":"MongoDB, Docker and Python","permalink":"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/":[{"title":"Hands on Scrapy","permalink":"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/OS-notes/":[{"title":"learning OS and building LorriOS","permalink":"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/"}],"https://LorrinWWW.github.io/posts/Bayes-estimation/":[{"title":"Note of statistic","permalink":"https://LorrinWWW.github.io/posts/Note-of-statistic/"}],"https://LorrinWWW.github.io/posts/management-of-the-firm/":[],"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"}],"https://LorrinWWW.github.io/posts/datamining-qualitative-induction/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-finite-element-method/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/QuadTree/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/proba-ch4/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch2/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/":[{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/Note-of-NLP/":[{"title":"Generative Adversarial Network","permalink":"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/"},{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"}],"https://LorrinWWW.github.io/posts/Note-of-datamining/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"数据挖掘-分类与预测","permalink":"https://LorrinWWW.github.io/posts/datamining-class-pred/"},{"title":"数据挖掘-预处理","permalink":"https://LorrinWWW.github.io/posts/datamining-pretreatment/"}],"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/Note-of-probability/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/participe-present-et-gerondif/":[],"https://LorrinWWW.github.io/posts/Hilbert-space/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/proba-ch6/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}]}}},"excerpt":"","more":"<ol>\n<li><p>穷举法</p>\n<p>略</p>\n</li>\n<li><p>贪心法</p>\n<p>略</p>\n</li>\n<li><p>分治法</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">merge</span>(<span class=\"params\">A, B</span>):</span></span><br><span class=\"line\">    <span class=\"comment\"># merge two small solved problems into one.</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> merged</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">divideConquer</span>(<span class=\"params\">S, divide, combine</span>):</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> <span class=\"built_in\">len</span>(S) == <span class=\"number\">1</span>: <span class=\"keyword\">return</span> S</span><br><span class=\"line\">    <span class=\"comment\"># divide a grand problems</span></span><br><span class=\"line\">    L, R = divide(S)</span><br><span class=\"line\">    A = divideConquer(L, divide, combine)</span><br><span class=\"line\">    B = divideConquer(R, divide, combine)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> merge(A, B)</span><br></pre></td></tr></table></figure>\n\n<p>上面的情形只供参考，参数等视具体情况而定。</p>\n</li>\n<li><p>动态规划</p>\n<p>比较经典的就是背包问题。<a href=\"http://blog.csdn.net/littlethunder/article/details/26575417\">代码来源</a></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">bag</span>(<span class=\"params\">n,c,w,v</span>):</span>  </span><br><span class=\"line\">    res=[[-<span class=\"number\">1</span> <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(c+<span class=\"number\">1</span>)] <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(n+<span class=\"number\">1</span>)]  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(c+<span class=\"number\">1</span>):  </span><br><span class=\"line\">        res[<span class=\"number\">0</span>][j]=<span class=\"number\">0</span>  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">1</span>,n+<span class=\"number\">1</span>):  </span><br><span class=\"line\">        <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">1</span>,c+<span class=\"number\">1</span>):  </span><br><span class=\"line\">            res[i][j]=res[i-<span class=\"number\">1</span>][j]  </span><br><span class=\"line\">            <span class=\"keyword\">if</span> j&gt;=w[i-<span class=\"number\">1</span>] <span class=\"keyword\">and</span> res[i][j]&lt;res[i-<span class=\"number\">1</span>][j-w[i-<span class=\"number\">1</span>]]+v[i-<span class=\"number\">1</span>]:  </span><br><span class=\"line\">                res[i][j]=res[i-<span class=\"number\">1</span>][j-w[i-<span class=\"number\">1</span>]]+v[i-<span class=\"number\">1</span>]  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> res  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">show</span>(<span class=\"params\">n,c,w,res</span>):</span>  </span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;最大价值为:&#x27;</span>,res[n][c])  </span><br><span class=\"line\">    x=[<span class=\"literal\">False</span> <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(n)]  </span><br><span class=\"line\">    j=c  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">1</span>,n+<span class=\"number\">1</span>):  </span><br><span class=\"line\">        <span class=\"keyword\">if</span> res[i][j]&gt;res[i-<span class=\"number\">1</span>][j]:  </span><br><span class=\"line\">            x[i-<span class=\"number\">1</span>]=<span class=\"literal\">True</span>  </span><br><span class=\"line\">            j-=w[i-<span class=\"number\">1</span>]  </span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;选择的物品为:&#x27;</span>)  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(n):  </span><br><span class=\"line\">        <span class=\"keyword\">if</span> x[i]:  </span><br><span class=\"line\">            <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;第&#x27;</span>,i,<span class=\"string\">&#x27;个,&#x27;</span>,end=<span class=\"string\">&#x27;&#x27;</span>)  </span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;&#x27;</span>)  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__==<span class=\"string\">&#x27;__main__&#x27;</span>:  </span><br><span class=\"line\">    n=<span class=\"number\">5</span>  </span><br><span class=\"line\">    c=<span class=\"number\">10</span>  </span><br><span class=\"line\">    w=[<span class=\"number\">2</span>,<span class=\"number\">2</span>,<span class=\"number\">6</span>,<span class=\"number\">5</span>,<span class=\"number\">4</span>]  </span><br><span class=\"line\">    v=[<span class=\"number\">6</span>,<span class=\"number\">3</span>,<span class=\"number\">5</span>,<span class=\"number\">4</span>,<span class=\"number\">6</span>]  </span><br><span class=\"line\">    res=bag(n,c,w,v)  </span><br><span class=\"line\">    show(n,c,w,res)</span><br></pre></td></tr></table></figure>\n\n<p>​</p>\n</li>\n</ol>\n"},{"title":"MongoDB, Docker and Python","date":"2017-06-22T06:53:44.000Z","_content":"\n## 安装\n\n需要预先安装\n\n- Docker\n- Python\n  - pymongo\n\n## Docker\n\n创建容器。若只使用一次，可以加上--rm；若要后台运行，加上-d\n\n27017是mongodb的默认端口\n\n```Bash\ndocker run --name my-mongo -it -p 27017:27017 mongo\n```\n\n创建完成后需要使用时：\n\n```Shell\ndocker start my-mongo\n```\n\n## Python\n\n```python\nfrom pymongo import MongoClient\n\n# connection\nclient = MongoClient()\nclient.server_info()\ndb = client.test\n\n# loop cursor\ncursor = db.cars.find()\nfor doc in cursor:\n    print(doc)\n\n# or just find one\ndb.cars.find_one()\n```","source":"_posts/MongoDB-Docker-and-Python.md","raw":"---\ntitle: 'MongoDB, Docker and Python'\ndate: 2017-06-22 14:53:44\ncategories: [programming]\ntags: [mongo, mongodb, docker, python]\n---\n\n## 安装\n\n需要预先安装\n\n- Docker\n- Python\n  - pymongo\n\n## Docker\n\n创建容器。若只使用一次，可以加上--rm；若要后台运行，加上-d\n\n27017是mongodb的默认端口\n\n```Bash\ndocker run --name my-mongo -it -p 27017:27017 mongo\n```\n\n创建完成后需要使用时：\n\n```Shell\ndocker start my-mongo\n```\n\n## Python\n\n```python\nfrom pymongo import MongoClient\n\n# connection\nclient = MongoClient()\nclient.server_info()\ndb = client.test\n\n# loop cursor\ncursor = db.cars.find()\nfor doc in cursor:\n    print(doc)\n\n# or just find one\ndb.cars.find_one()\n```","slug":"MongoDB-Docker-and-Python","published":1,"updated":"2017-06-22T07:07:59.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckw4qufp3000pgwtl2fv8cajr","content":"<h2 id=\"安装\"><a href=\"#安装\" class=\"headerlink\" title=\"安装\"></a>安装</h2><p>需要预先安装</p>\n<ul>\n<li>Docker</li>\n<li>Python<ul>\n<li>pymongo</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"Docker\"><a href=\"#Docker\" class=\"headerlink\" title=\"Docker\"></a>Docker</h2><p>创建容器。若只使用一次，可以加上–rm；若要后台运行，加上-d</p>\n<p>27017是mongodb的默认端口</p>\n<figure class=\"highlight bash\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker run --name my-mongo -it -p 27017:27017 mongo</span><br></pre></td></tr></tbody></table></figure>\n\n<p>创建完成后需要使用时：</p>\n<figure class=\"highlight shell\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker start my-mongo</span><br></pre></td></tr></tbody></table></figure>\n\n<h2 id=\"Python\"><a href=\"#Python\" class=\"headerlink\" title=\"Python\"></a>Python</h2><figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> pymongo <span class=\"keyword\">import</span> MongoClient</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># connection</span></span><br><span class=\"line\">client = MongoClient()</span><br><span class=\"line\">client.server_info()</span><br><span class=\"line\">db = client.test</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># loop cursor</span></span><br><span class=\"line\">cursor = db.cars.find()</span><br><span class=\"line\"><span class=\"keyword\">for</span> doc <span class=\"keyword\">in</span> cursor:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(doc)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># or just find one</span></span><br><span class=\"line\">db.cars.find_one()</span><br></pre></td></tr></tbody></table></figure>","site":{"data":{"recommended_posts":{"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"}],"https://LorrinWWW.github.io/posts/complexity/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/hexo-with-latex/":[{"title":"区块链时代的最正确的打赏姿势——在Hexo博客配置Coinhive网页挖矿","permalink":"http://www.davidfnck.com/blockchain/mine-xmr-in-hexo-by-coinhive.html/"}],"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/":[{"title":"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-statistic/":[{"title":"贝叶斯估计 Bayes estimation","permalink":"https://LorrinWWW.github.io/posts/Bayes-estimation/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"}],"https://LorrinWWW.github.io/posts/[2018.2.5]Nested-LSTMs/":[{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"}],"https://LorrinWWW.github.io/posts/datamining-pretreatment/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Several models for knowledge graph representing and completing 几个知识图谱模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/"}],"https://LorrinWWW.github.io/posts/compression/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/Sobolev-space/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/proba-ch5/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch3/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/proba-ch1/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/ML-CNN/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/EDP-basic-models/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/machine-learning/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/datamining-class-pred/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction/":[],"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/":[{"title":"OS notes","permalink":"https://LorrinWWW.github.io/posts/OS-notes/"}],"https://LorrinWWW.github.io/posts/projet-enjeu-plugin-chrome-101/":[{"title":"Chrome油猴脚本","permalink":"http://gubangzhong.cn/2017/06/13/Chrome油猴脚本/"}],"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/":[{"title":"MongoDB, Docker and Python","permalink":"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/":[{"title":"Hands on Scrapy","permalink":"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/OS-notes/":[{"title":"learning OS and building LorriOS","permalink":"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/"}],"https://LorrinWWW.github.io/posts/Bayes-estimation/":[{"title":"Note of statistic","permalink":"https://LorrinWWW.github.io/posts/Note-of-statistic/"}],"https://LorrinWWW.github.io/posts/management-of-the-firm/":[],"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"}],"https://LorrinWWW.github.io/posts/datamining-qualitative-induction/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-finite-element-method/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/QuadTree/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/proba-ch4/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch2/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/":[{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/Note-of-NLP/":[{"title":"Generative Adversarial Network","permalink":"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/"},{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"}],"https://LorrinWWW.github.io/posts/Note-of-datamining/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"数据挖掘-分类与预测","permalink":"https://LorrinWWW.github.io/posts/datamining-class-pred/"},{"title":"数据挖掘-预处理","permalink":"https://LorrinWWW.github.io/posts/datamining-pretreatment/"}],"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/Note-of-probability/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/participe-present-et-gerondif/":[],"https://LorrinWWW.github.io/posts/Hilbert-space/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/proba-ch6/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}]}}},"excerpt":"","more":"<h2 id=\"安装\"><a href=\"#安装\" class=\"headerlink\" title=\"安装\"></a>安装</h2><p>需要预先安装</p>\n<ul>\n<li>Docker</li>\n<li>Python<ul>\n<li>pymongo</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"Docker\"><a href=\"#Docker\" class=\"headerlink\" title=\"Docker\"></a>Docker</h2><p>创建容器。若只使用一次，可以加上–rm；若要后台运行，加上-d</p>\n<p>27017是mongodb的默认端口</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker run --name my-mongo -it -p 27017:27017 mongo</span><br></pre></td></tr></table></figure>\n\n<p>创建完成后需要使用时：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker start my-mongo</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"Python\"><a href=\"#Python\" class=\"headerlink\" title=\"Python\"></a>Python</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> pymongo <span class=\"keyword\">import</span> MongoClient</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># connection</span></span><br><span class=\"line\">client = MongoClient()</span><br><span class=\"line\">client.server_info()</span><br><span class=\"line\">db = client.test</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># loop cursor</span></span><br><span class=\"line\">cursor = db.cars.find()</span><br><span class=\"line\"><span class=\"keyword\">for</span> doc <span class=\"keyword\">in</span> cursor:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(doc)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># or just find one</span></span><br><span class=\"line\">db.cars.find_one()</span><br></pre></td></tr></table></figure>"},{"title":"Note of NLP","date":"2017-06-26T12:52:45.000Z","_content":"\n# NLP\n\n1. \"One-hot\" representation\n\n   每一个词都作为一个特征，用一个很大的向量来描述文章。\n   $$\n   [0,0,0,0,0,0,0,0,0,1,0,0,0]\n   $$\n\n2. Main idea of word2vec\n\n   Two algorithms\n\n   1. Skip-grams\n   2. Continuous bag of words (CBOW)\n\n   Two training methods\n\n   1. Hierarchical softmax\n   2. Negative sampling\n\n## 基于深度学习的关系提取\n\n>[Zeng et al. 2014] 提出采用卷积神经网络进行关系抽取。他们采用词汇向量和词的位置向量作为卷积神经网络的输入，通过卷积层、池化层和非线性层得到句子表示。通过考虑实体的位置向量和其他相关的词汇特征，句子中的实体信息能够被较好地考虑到关系抽取中。后来，[Santos et al. 2015]还提出了一种新的卷积神经网络进行关系抽取，其中采用了新的损失函数，能够有效地提高不同关系类别之间的区分性。\n>\n>[Miwa et al. 2016] 提出了一种基于端到端神经网络的关系抽取模型。该模型使用双向 LSTM（Long-Short Term Memory，长短时记忆模型）和树形 LSTM 同时对实体和句子进行建模。目前，基于卷积神经网络的方法在关系抽取的标准数据集 SemEval-2010 Task 8 上取得了最好的效果。\n>\n>--基于深度学习的关系抽取技术进展_刘知远_熊德意\n\nRNN在NLP中应用较多，有关它的文章：[The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) 译文[递归神经网络不可思议的有效性](http://www.csdn.net/article/2015-08-28/2825569)\n\n其中目前比较流行的是LSTM，有关它的文章：[Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) 译文 [理解LSTM网络](http://blog.csdn.net/jerr__y/article/details/58598296)\n\n[LSTM的tensorflow简单实现](http://blog.csdn.net/jerr__y/article/details/61195257)\n\n### GAN ?\n\nACGAN可用于分类问题，Discriminator输出正伪的同时还会输出类别。适合类别数量已给定的情况。\n\nInfoGAN。\n\n半监督GAN。","source":"_posts/Note-of-NLP.md","raw":"---\ntitle: Note of NLP\ndate: 2017-06-26 20:52:45\ncategories: [programming]\ntags: [nlp, machine-learning, deep-learning]\n---\n\n# NLP\n\n1. \"One-hot\" representation\n\n   每一个词都作为一个特征，用一个很大的向量来描述文章。\n   $$\n   [0,0,0,0,0,0,0,0,0,1,0,0,0]\n   $$\n\n2. Main idea of word2vec\n\n   Two algorithms\n\n   1. Skip-grams\n   2. Continuous bag of words (CBOW)\n\n   Two training methods\n\n   1. Hierarchical softmax\n   2. Negative sampling\n\n## 基于深度学习的关系提取\n\n>[Zeng et al. 2014] 提出采用卷积神经网络进行关系抽取。他们采用词汇向量和词的位置向量作为卷积神经网络的输入，通过卷积层、池化层和非线性层得到句子表示。通过考虑实体的位置向量和其他相关的词汇特征，句子中的实体信息能够被较好地考虑到关系抽取中。后来，[Santos et al. 2015]还提出了一种新的卷积神经网络进行关系抽取，其中采用了新的损失函数，能够有效地提高不同关系类别之间的区分性。\n>\n>[Miwa et al. 2016] 提出了一种基于端到端神经网络的关系抽取模型。该模型使用双向 LSTM（Long-Short Term Memory，长短时记忆模型）和树形 LSTM 同时对实体和句子进行建模。目前，基于卷积神经网络的方法在关系抽取的标准数据集 SemEval-2010 Task 8 上取得了最好的效果。\n>\n>--基于深度学习的关系抽取技术进展_刘知远_熊德意\n\nRNN在NLP中应用较多，有关它的文章：[The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) 译文[递归神经网络不可思议的有效性](http://www.csdn.net/article/2015-08-28/2825569)\n\n其中目前比较流行的是LSTM，有关它的文章：[Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) 译文 [理解LSTM网络](http://blog.csdn.net/jerr__y/article/details/58598296)\n\n[LSTM的tensorflow简单实现](http://blog.csdn.net/jerr__y/article/details/61195257)\n\n### GAN ?\n\nACGAN可用于分类问题，Discriminator输出正伪的同时还会输出类别。适合类别数量已给定的情况。\n\nInfoGAN。\n\n半监督GAN。","slug":"Note-of-NLP","published":1,"updated":"2017-06-28T09:13:58.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckw4qufp4000rgwtlg65o8sme","content":"<h1 id=\"NLP\"><a href=\"#NLP\" class=\"headerlink\" title=\"NLP\"></a>NLP</h1><ol>\n<li><p>“One-hot” representation</p>\n<p>每一个词都作为一个特征，用一个很大的向量来描述文章。<br>$$<br>[0,0,0,0,0,0,0,0,0,1,0,0,0]<br>$$</p>\n</li>\n<li><p>Main idea of word2vec</p>\n<p>Two algorithms</p>\n<ol>\n<li>Skip-grams</li>\n<li>Continuous bag of words (CBOW)</li>\n</ol>\n<p>Two training methods</p>\n<ol>\n<li>Hierarchical softmax</li>\n<li>Negative sampling</li>\n</ol>\n</li>\n</ol>\n<h2 id=\"基于深度学习的关系提取\"><a href=\"#基于深度学习的关系提取\" class=\"headerlink\" title=\"基于深度学习的关系提取\"></a>基于深度学习的关系提取</h2><blockquote>\n<p>[Zeng et al. 2014] 提出采用卷积神经网络进行关系抽取。他们采用词汇向量和词的位置向量作为卷积神经网络的输入，通过卷积层、池化层和非线性层得到句子表示。通过考虑实体的位置向量和其他相关的词汇特征，句子中的实体信息能够被较好地考虑到关系抽取中。后来，[Santos et al. 2015]还提出了一种新的卷积神经网络进行关系抽取，其中采用了新的损失函数，能够有效地提高不同关系类别之间的区分性。</p>\n<p>[Miwa et al. 2016] 提出了一种基于端到端神经网络的关系抽取模型。该模型使用双向 LSTM（Long-Short Term Memory，长短时记忆模型）和树形 LSTM 同时对实体和句子进行建模。目前，基于卷积神经网络的方法在关系抽取的标准数据集 SemEval-2010 Task 8 上取得了最好的效果。</p>\n<p>–基于深度学习的关系抽取技术进展_刘知远_熊德意</p>\n</blockquote>\n<p>RNN在NLP中应用较多，有关它的文章：<a href=\"http://karpathy.github.io/2015/05/21/rnn-effectiveness/\">The Unreasonable Effectiveness of Recurrent Neural Networks</a> 译文<a href=\"http://www.csdn.net/article/2015-08-28/2825569\">递归神经网络不可思议的有效性</a></p>\n<p>其中目前比较流行的是LSTM，有关它的文章：<a href=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/\">Understanding LSTM Networks</a> 译文 <a href=\"http://blog.csdn.net/jerr__y/article/details/58598296\">理解LSTM网络</a></p>\n<p><a href=\"http://blog.csdn.net/jerr__y/article/details/61195257\">LSTM的tensorflow简单实现</a></p>\n<h3 id=\"GAN\"><a href=\"#GAN\" class=\"headerlink\" title=\"GAN ?\"></a>GAN ?</h3><p>ACGAN可用于分类问题，Discriminator输出正伪的同时还会输出类别。适合类别数量已给定的情况。</p>\n<p>InfoGAN。</p>\n<p>半监督GAN。</p>\n","site":{"data":{"recommended_posts":{"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"}],"https://LorrinWWW.github.io/posts/complexity/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/hexo-with-latex/":[{"title":"区块链时代的最正确的打赏姿势——在Hexo博客配置Coinhive网页挖矿","permalink":"http://www.davidfnck.com/blockchain/mine-xmr-in-hexo-by-coinhive.html/"}],"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/":[{"title":"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-statistic/":[{"title":"贝叶斯估计 Bayes estimation","permalink":"https://LorrinWWW.github.io/posts/Bayes-estimation/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"}],"https://LorrinWWW.github.io/posts/[2018.2.5]Nested-LSTMs/":[{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"}],"https://LorrinWWW.github.io/posts/datamining-pretreatment/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Several models for knowledge graph representing and completing 几个知识图谱模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/"}],"https://LorrinWWW.github.io/posts/compression/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/Sobolev-space/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/proba-ch5/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch3/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/proba-ch1/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/ML-CNN/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/EDP-basic-models/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/machine-learning/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/datamining-class-pred/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction/":[],"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/":[{"title":"OS notes","permalink":"https://LorrinWWW.github.io/posts/OS-notes/"}],"https://LorrinWWW.github.io/posts/projet-enjeu-plugin-chrome-101/":[{"title":"Chrome油猴脚本","permalink":"http://gubangzhong.cn/2017/06/13/Chrome油猴脚本/"}],"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/":[{"title":"MongoDB, Docker and Python","permalink":"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/":[{"title":"Hands on Scrapy","permalink":"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/OS-notes/":[{"title":"learning OS and building LorriOS","permalink":"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/"}],"https://LorrinWWW.github.io/posts/Bayes-estimation/":[{"title":"Note of statistic","permalink":"https://LorrinWWW.github.io/posts/Note-of-statistic/"}],"https://LorrinWWW.github.io/posts/management-of-the-firm/":[],"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"}],"https://LorrinWWW.github.io/posts/datamining-qualitative-induction/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-finite-element-method/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/QuadTree/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/proba-ch4/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch2/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/":[{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/Note-of-NLP/":[{"title":"Generative Adversarial Network","permalink":"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/"},{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"}],"https://LorrinWWW.github.io/posts/Note-of-datamining/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"数据挖掘-分类与预测","permalink":"https://LorrinWWW.github.io/posts/datamining-class-pred/"},{"title":"数据挖掘-预处理","permalink":"https://LorrinWWW.github.io/posts/datamining-pretreatment/"}],"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/Note-of-probability/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/participe-present-et-gerondif/":[],"https://LorrinWWW.github.io/posts/Hilbert-space/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/proba-ch6/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}]}}},"excerpt":"","more":"<h1 id=\"NLP\"><a href=\"#NLP\" class=\"headerlink\" title=\"NLP\"></a>NLP</h1><ol>\n<li><p>“One-hot” representation</p>\n<p>每一个词都作为一个特征，用一个很大的向量来描述文章。<br>$$<br>[0,0,0,0,0,0,0,0,0,1,0,0,0]<br>$$</p>\n</li>\n<li><p>Main idea of word2vec</p>\n<p>Two algorithms</p>\n<ol>\n<li>Skip-grams</li>\n<li>Continuous bag of words (CBOW)</li>\n</ol>\n<p>Two training methods</p>\n<ol>\n<li>Hierarchical softmax</li>\n<li>Negative sampling</li>\n</ol>\n</li>\n</ol>\n<h2 id=\"基于深度学习的关系提取\"><a href=\"#基于深度学习的关系提取\" class=\"headerlink\" title=\"基于深度学习的关系提取\"></a>基于深度学习的关系提取</h2><blockquote>\n<p>[Zeng et al. 2014] 提出采用卷积神经网络进行关系抽取。他们采用词汇向量和词的位置向量作为卷积神经网络的输入，通过卷积层、池化层和非线性层得到句子表示。通过考虑实体的位置向量和其他相关的词汇特征，句子中的实体信息能够被较好地考虑到关系抽取中。后来，[Santos et al. 2015]还提出了一种新的卷积神经网络进行关系抽取，其中采用了新的损失函数，能够有效地提高不同关系类别之间的区分性。</p>\n<p>[Miwa et al. 2016] 提出了一种基于端到端神经网络的关系抽取模型。该模型使用双向 LSTM（Long-Short Term Memory，长短时记忆模型）和树形 LSTM 同时对实体和句子进行建模。目前，基于卷积神经网络的方法在关系抽取的标准数据集 SemEval-2010 Task 8 上取得了最好的效果。</p>\n<p>–基于深度学习的关系抽取技术进展_刘知远_熊德意</p>\n</blockquote>\n<p>RNN在NLP中应用较多，有关它的文章：<a href=\"http://karpathy.github.io/2015/05/21/rnn-effectiveness/\">The Unreasonable Effectiveness of Recurrent Neural Networks</a> 译文<a href=\"http://www.csdn.net/article/2015-08-28/2825569\">递归神经网络不可思议的有效性</a></p>\n<p>其中目前比较流行的是LSTM，有关它的文章：<a href=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/\">Understanding LSTM Networks</a> 译文 <a href=\"http://blog.csdn.net/jerr__y/article/details/58598296\">理解LSTM网络</a></p>\n<p><a href=\"http://blog.csdn.net/jerr__y/article/details/61195257\">LSTM的tensorflow简单实现</a></p>\n<h3 id=\"GAN\"><a href=\"#GAN\" class=\"headerlink\" title=\"GAN ?\"></a>GAN ?</h3><p>ACGAN可用于分类问题，Discriminator输出正伪的同时还会输出类别。适合类别数量已给定的情况。</p>\n<p>InfoGAN。</p>\n<p>半监督GAN。</p>\n"},{"title":"数据挖掘笔记 Note of datamining","date":"2016-12-06T07:35:24.000Z","_content":"\n# 构成\n\n一般说的数据挖掘指的是知识挖掘，构成如下：\n\n1. 数据清洗 - data clearing\n2. 数据集成 - data integration\n3. 数据转换 - data transformation\n4. 数据挖掘 - data mining\n5. 模式评估 - pattern evaluation\n6. 知识表示 - knowledge presentation\n\n# 分析\n\n1. 关联分析\n\n   age(X, \"20-29\") ^ income(X, \"20K-30K\") => buys(X, \"MP3\")\n\n   [support = 2%, confidence = 60%]\n\n2. 分类预测\n\n   分类挖掘的主要表示手段有：\n\n   - 分类规则\n   - 决策树\n   - 数学公式\n   - 神经网络\n\n3. 聚类分析\n\n   与分类预测的主要区别是，后者是分类已知，属于监督学习方法；前者无事先确定类别归属，属于无监督学习方法。\n\n   聚类分析中，首先需要根据“各聚集内部数据对象间的相似度最大化；而各聚集对象间相似度最小化”的基本聚类分析原则，以及度量数据对象之间相似度的计算公式，将聚类分析的数据对象划分为若干组。\n\n4. 异类分析\n\n   不符合大多数数据对象所构成的规律(模型)的数据对象就被称为异类。\n\n   我们往往将异类作为噪声或者意外而将其排除，但是某些情况下这些异类反而更有价值，对异类数据的分析处理通常就称为异类挖掘。\n\n   例如，通过对银行账户的异类分析，发现信用卡诈骗等。\n\n5. 演化分析\n\n   数据演化分析就是对随时间变化的数据对象的变化规律和趋势进行建模描述。\n\n   例如股票市场。\n\n# 数据挖掘结果评估\n\n一个数据挖掘系统在完成一次分析后会获得大量的模式或规则，其中只有很小的一部分是有实际使用价值的。\n\n通常，有以下几个标准\n\n1. 易于用户理解;\n2. 对新数据或测试数据能够确定有效程度;\n3. 具有潜在价值;\n4. 新奇的\n\n# 数据挖掘系统分类\n\n数据挖掘系统可以按照三种标准进行划分，它们是数据库类型、所挖掘的知识和所使用的技术。","source":"_posts/Note-of-datamining.md","raw":"---\ntitle: 数据挖掘笔记 Note of datamining\ndate: 2016-12-6 15:35:24\ncategories: [programming, unfinished]\ntags: [datamining]\n---\n\n# 构成\n\n一般说的数据挖掘指的是知识挖掘，构成如下：\n\n1. 数据清洗 - data clearing\n2. 数据集成 - data integration\n3. 数据转换 - data transformation\n4. 数据挖掘 - data mining\n5. 模式评估 - pattern evaluation\n6. 知识表示 - knowledge presentation\n\n# 分析\n\n1. 关联分析\n\n   age(X, \"20-29\") ^ income(X, \"20K-30K\") => buys(X, \"MP3\")\n\n   [support = 2%, confidence = 60%]\n\n2. 分类预测\n\n   分类挖掘的主要表示手段有：\n\n   - 分类规则\n   - 决策树\n   - 数学公式\n   - 神经网络\n\n3. 聚类分析\n\n   与分类预测的主要区别是，后者是分类已知，属于监督学习方法；前者无事先确定类别归属，属于无监督学习方法。\n\n   聚类分析中，首先需要根据“各聚集内部数据对象间的相似度最大化；而各聚集对象间相似度最小化”的基本聚类分析原则，以及度量数据对象之间相似度的计算公式，将聚类分析的数据对象划分为若干组。\n\n4. 异类分析\n\n   不符合大多数数据对象所构成的规律(模型)的数据对象就被称为异类。\n\n   我们往往将异类作为噪声或者意外而将其排除，但是某些情况下这些异类反而更有价值，对异类数据的分析处理通常就称为异类挖掘。\n\n   例如，通过对银行账户的异类分析，发现信用卡诈骗等。\n\n5. 演化分析\n\n   数据演化分析就是对随时间变化的数据对象的变化规律和趋势进行建模描述。\n\n   例如股票市场。\n\n# 数据挖掘结果评估\n\n一个数据挖掘系统在完成一次分析后会获得大量的模式或规则，其中只有很小的一部分是有实际使用价值的。\n\n通常，有以下几个标准\n\n1. 易于用户理解;\n2. 对新数据或测试数据能够确定有效程度;\n3. 具有潜在价值;\n4. 新奇的\n\n# 数据挖掘系统分类\n\n数据挖掘系统可以按照三种标准进行划分，它们是数据库类型、所挖掘的知识和所使用的技术。","slug":"Note-of-datamining","published":1,"updated":"2016-12-06T21:04:53.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckw4qufp6000vgwtl7kika36r","content":"<h1 id=\"构成\"><a href=\"#构成\" class=\"headerlink\" title=\"构成\"></a>构成</h1><p>一般说的数据挖掘指的是知识挖掘，构成如下：</p>\n<ol>\n<li>数据清洗 - data clearing</li>\n<li>数据集成 - data integration</li>\n<li>数据转换 - data transformation</li>\n<li>数据挖掘 - data mining</li>\n<li>模式评估 - pattern evaluation</li>\n<li>知识表示 - knowledge presentation</li>\n</ol>\n<h1 id=\"分析\"><a href=\"#分析\" class=\"headerlink\" title=\"分析\"></a>分析</h1><ol>\n<li><p>关联分析</p>\n<p>age(X, “20-29”) ^ income(X, “20K-30K”) =&gt; buys(X, “MP3”)</p>\n<p>[support = 2%, confidence = 60%]</p>\n</li>\n<li><p>分类预测</p>\n<p>分类挖掘的主要表示手段有：</p>\n<ul>\n<li>分类规则</li>\n<li>决策树</li>\n<li>数学公式</li>\n<li>神经网络</li>\n</ul>\n</li>\n<li><p>聚类分析</p>\n<p>与分类预测的主要区别是，后者是分类已知，属于监督学习方法；前者无事先确定类别归属，属于无监督学习方法。</p>\n<p>聚类分析中，首先需要根据“各聚集内部数据对象间的相似度最大化；而各聚集对象间相似度最小化”的基本聚类分析原则，以及度量数据对象之间相似度的计算公式，将聚类分析的数据对象划分为若干组。</p>\n</li>\n<li><p>异类分析</p>\n<p>不符合大多数数据对象所构成的规律(模型)的数据对象就被称为异类。</p>\n<p>我们往往将异类作为噪声或者意外而将其排除，但是某些情况下这些异类反而更有价值，对异类数据的分析处理通常就称为异类挖掘。</p>\n<p>例如，通过对银行账户的异类分析，发现信用卡诈骗等。</p>\n</li>\n<li><p>演化分析</p>\n<p>数据演化分析就是对随时间变化的数据对象的变化规律和趋势进行建模描述。</p>\n<p>例如股票市场。</p>\n</li>\n</ol>\n<h1 id=\"数据挖掘结果评估\"><a href=\"#数据挖掘结果评估\" class=\"headerlink\" title=\"数据挖掘结果评估\"></a>数据挖掘结果评估</h1><p>一个数据挖掘系统在完成一次分析后会获得大量的模式或规则，其中只有很小的一部分是有实际使用价值的。</p>\n<p>通常，有以下几个标准</p>\n<ol>\n<li>易于用户理解;</li>\n<li>对新数据或测试数据能够确定有效程度;</li>\n<li>具有潜在价值;</li>\n<li>新奇的</li>\n</ol>\n<h1 id=\"数据挖掘系统分类\"><a href=\"#数据挖掘系统分类\" class=\"headerlink\" title=\"数据挖掘系统分类\"></a>数据挖掘系统分类</h1><p>数据挖掘系统可以按照三种标准进行划分，它们是数据库类型、所挖掘的知识和所使用的技术。</p>\n","site":{"data":{"recommended_posts":{"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"}],"https://LorrinWWW.github.io/posts/complexity/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/hexo-with-latex/":[{"title":"区块链时代的最正确的打赏姿势——在Hexo博客配置Coinhive网页挖矿","permalink":"http://www.davidfnck.com/blockchain/mine-xmr-in-hexo-by-coinhive.html/"}],"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/":[{"title":"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-statistic/":[{"title":"贝叶斯估计 Bayes estimation","permalink":"https://LorrinWWW.github.io/posts/Bayes-estimation/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"}],"https://LorrinWWW.github.io/posts/[2018.2.5]Nested-LSTMs/":[{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"}],"https://LorrinWWW.github.io/posts/datamining-pretreatment/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Several models for knowledge graph representing and completing 几个知识图谱模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/"}],"https://LorrinWWW.github.io/posts/compression/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/Sobolev-space/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/proba-ch5/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch3/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/proba-ch1/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/ML-CNN/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/EDP-basic-models/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/machine-learning/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/datamining-class-pred/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction/":[],"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/":[{"title":"OS notes","permalink":"https://LorrinWWW.github.io/posts/OS-notes/"}],"https://LorrinWWW.github.io/posts/projet-enjeu-plugin-chrome-101/":[{"title":"Chrome油猴脚本","permalink":"http://gubangzhong.cn/2017/06/13/Chrome油猴脚本/"}],"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/":[{"title":"MongoDB, Docker and Python","permalink":"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/":[{"title":"Hands on Scrapy","permalink":"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/OS-notes/":[{"title":"learning OS and building LorriOS","permalink":"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/"}],"https://LorrinWWW.github.io/posts/Bayes-estimation/":[{"title":"Note of statistic","permalink":"https://LorrinWWW.github.io/posts/Note-of-statistic/"}],"https://LorrinWWW.github.io/posts/management-of-the-firm/":[],"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"}],"https://LorrinWWW.github.io/posts/datamining-qualitative-induction/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-finite-element-method/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/QuadTree/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/proba-ch4/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch2/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/":[{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/Note-of-NLP/":[{"title":"Generative Adversarial Network","permalink":"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/"},{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"}],"https://LorrinWWW.github.io/posts/Note-of-datamining/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"数据挖掘-分类与预测","permalink":"https://LorrinWWW.github.io/posts/datamining-class-pred/"},{"title":"数据挖掘-预处理","permalink":"https://LorrinWWW.github.io/posts/datamining-pretreatment/"}],"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/Note-of-probability/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/participe-present-et-gerondif/":[],"https://LorrinWWW.github.io/posts/Hilbert-space/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/proba-ch6/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}]}}},"excerpt":"","more":"<h1 id=\"构成\"><a href=\"#构成\" class=\"headerlink\" title=\"构成\"></a>构成</h1><p>一般说的数据挖掘指的是知识挖掘，构成如下：</p>\n<ol>\n<li>数据清洗 - data clearing</li>\n<li>数据集成 - data integration</li>\n<li>数据转换 - data transformation</li>\n<li>数据挖掘 - data mining</li>\n<li>模式评估 - pattern evaluation</li>\n<li>知识表示 - knowledge presentation</li>\n</ol>\n<h1 id=\"分析\"><a href=\"#分析\" class=\"headerlink\" title=\"分析\"></a>分析</h1><ol>\n<li><p>关联分析</p>\n<p>age(X, “20-29”) ^ income(X, “20K-30K”) =&gt; buys(X, “MP3”)</p>\n<p>[support = 2%, confidence = 60%]</p>\n</li>\n<li><p>分类预测</p>\n<p>分类挖掘的主要表示手段有：</p>\n<ul>\n<li>分类规则</li>\n<li>决策树</li>\n<li>数学公式</li>\n<li>神经网络</li>\n</ul>\n</li>\n<li><p>聚类分析</p>\n<p>与分类预测的主要区别是，后者是分类已知，属于监督学习方法；前者无事先确定类别归属，属于无监督学习方法。</p>\n<p>聚类分析中，首先需要根据“各聚集内部数据对象间的相似度最大化；而各聚集对象间相似度最小化”的基本聚类分析原则，以及度量数据对象之间相似度的计算公式，将聚类分析的数据对象划分为若干组。</p>\n</li>\n<li><p>异类分析</p>\n<p>不符合大多数数据对象所构成的规律(模型)的数据对象就被称为异类。</p>\n<p>我们往往将异类作为噪声或者意外而将其排除，但是某些情况下这些异类反而更有价值，对异类数据的分析处理通常就称为异类挖掘。</p>\n<p>例如，通过对银行账户的异类分析，发现信用卡诈骗等。</p>\n</li>\n<li><p>演化分析</p>\n<p>数据演化分析就是对随时间变化的数据对象的变化规律和趋势进行建模描述。</p>\n<p>例如股票市场。</p>\n</li>\n</ol>\n<h1 id=\"数据挖掘结果评估\"><a href=\"#数据挖掘结果评估\" class=\"headerlink\" title=\"数据挖掘结果评估\"></a>数据挖掘结果评估</h1><p>一个数据挖掘系统在完成一次分析后会获得大量的模式或规则，其中只有很小的一部分是有实际使用价值的。</p>\n<p>通常，有以下几个标准</p>\n<ol>\n<li>易于用户理解;</li>\n<li>对新数据或测试数据能够确定有效程度;</li>\n<li>具有潜在价值;</li>\n<li>新奇的</li>\n</ol>\n<h1 id=\"数据挖掘系统分类\"><a href=\"#数据挖掘系统分类\" class=\"headerlink\" title=\"数据挖掘系统分类\"></a>数据挖掘系统分类</h1><p>数据挖掘系统可以按照三种标准进行划分，它们是数据库类型、所挖掘的知识和所使用的技术。</p>\n"},{"title":"Note of knowledge graph","date":"2017-06-25T03:21:18.000Z","_content":"\n# 知识图谱 Knowledge graph\n\n知识图谱的概念由Google提出，目前成为一大热点。总体而言，英文知识图谱的文献相对齐全。中英文知识图谱的差别主要体现于对自然语言的处理，对于中文而言，首先准确的分词，其次要应对中文想对自由的语法和语言形式。\n\n下面是一些笔记。\n\n知识图谱的构建：\n\n## 1. 信息抽取 information extraction\n\n我们需要能够自动化地从结构化、半结构化和无结构数据中抽取实体(entity)、关系(relationship)以及实体属性等。\n\n1. 实体抽取 entity extraction\n   - 实体提取：\n     - liu等人，Knn算法和条件随机场模型\n     - lin等人，字典和最大熵算法\n   - 实体分类：\n     - 2012，ling等人，借鉴freebase归纳实体分类方法，条件随机场模型进行实体边界识别，自适应感知机算法实现对实体的自动分类，结果优于Stanford NER等主流命名实体识别系统\n     - 预定义实体分类并不好，新思路：对于给定实体，采用统计机器学习等方法，从目标数据集中抽取与之俱有相似的上下文特征等实体，从而实现实体的分类和聚类。参考whitelaw的解决方案\n     - ​\n2. 关系抽取 relationship extraction","source":"_posts/Note-of-knowledge-graph.md","raw":"---\ntitle: Note of knowledge graph\ndate: 2017-06-25 11:21:18\ncategories: [programming]\ntags: [knowledge-graph, machine-learning, datamining]\n---\n\n# 知识图谱 Knowledge graph\n\n知识图谱的概念由Google提出，目前成为一大热点。总体而言，英文知识图谱的文献相对齐全。中英文知识图谱的差别主要体现于对自然语言的处理，对于中文而言，首先准确的分词，其次要应对中文想对自由的语法和语言形式。\n\n下面是一些笔记。\n\n知识图谱的构建：\n\n## 1. 信息抽取 information extraction\n\n我们需要能够自动化地从结构化、半结构化和无结构数据中抽取实体(entity)、关系(relationship)以及实体属性等。\n\n1. 实体抽取 entity extraction\n   - 实体提取：\n     - liu等人，Knn算法和条件随机场模型\n     - lin等人，字典和最大熵算法\n   - 实体分类：\n     - 2012，ling等人，借鉴freebase归纳实体分类方法，条件随机场模型进行实体边界识别，自适应感知机算法实现对实体的自动分类，结果优于Stanford NER等主流命名实体识别系统\n     - 预定义实体分类并不好，新思路：对于给定实体，采用统计机器学习等方法，从目标数据集中抽取与之俱有相似的上下文特征等实体，从而实现实体的分类和聚类。参考whitelaw的解决方案\n     - ​\n2. 关系抽取 relationship extraction","slug":"Note-of-knowledge-graph","published":1,"updated":"2017-06-25T05:40:19.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckw4qufp7000xgwtl0rnhe8o8","content":"<h1 id=\"知识图谱-Knowledge-graph\"><a href=\"#知识图谱-Knowledge-graph\" class=\"headerlink\" title=\"知识图谱 Knowledge graph\"></a>知识图谱 Knowledge graph</h1><p>知识图谱的概念由Google提出，目前成为一大热点。总体而言，英文知识图谱的文献相对齐全。中英文知识图谱的差别主要体现于对自然语言的处理，对于中文而言，首先准确的分词，其次要应对中文想对自由的语法和语言形式。</p>\n<p>下面是一些笔记。</p>\n<p>知识图谱的构建：</p>\n<h2 id=\"1-信息抽取-information-extraction\"><a href=\"#1-信息抽取-information-extraction\" class=\"headerlink\" title=\"1. 信息抽取 information extraction\"></a>1. 信息抽取 information extraction</h2><p>我们需要能够自动化地从结构化、半结构化和无结构数据中抽取实体(entity)、关系(relationship)以及实体属性等。</p>\n<ol>\n<li>实体抽取 entity extraction<ul>\n<li>实体提取：<ul>\n<li>liu等人，Knn算法和条件随机场模型</li>\n<li>lin等人，字典和最大熵算法</li>\n</ul>\n</li>\n<li>实体分类：<ul>\n<li>2012，ling等人，借鉴freebase归纳实体分类方法，条件随机场模型进行实体边界识别，自适应感知机算法实现对实体的自动分类，结果优于Stanford NER等主流命名实体识别系统</li>\n<li>预定义实体分类并不好，新思路：对于给定实体，采用统计机器学习等方法，从目标数据集中抽取与之俱有相似的上下文特征等实体，从而实现实体的分类和聚类。参考whitelaw的解决方案</li>\n<li>​</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>关系抽取 relationship extraction</li>\n</ol>\n","site":{"data":{"recommended_posts":{"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"}],"https://LorrinWWW.github.io/posts/complexity/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/hexo-with-latex/":[{"title":"区块链时代的最正确的打赏姿势——在Hexo博客配置Coinhive网页挖矿","permalink":"http://www.davidfnck.com/blockchain/mine-xmr-in-hexo-by-coinhive.html/"}],"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/":[{"title":"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-statistic/":[{"title":"贝叶斯估计 Bayes estimation","permalink":"https://LorrinWWW.github.io/posts/Bayes-estimation/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"}],"https://LorrinWWW.github.io/posts/[2018.2.5]Nested-LSTMs/":[{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"}],"https://LorrinWWW.github.io/posts/datamining-pretreatment/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Several models for knowledge graph representing and completing 几个知识图谱模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/"}],"https://LorrinWWW.github.io/posts/compression/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/Sobolev-space/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/proba-ch5/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch3/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/proba-ch1/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/ML-CNN/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/EDP-basic-models/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/machine-learning/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/datamining-class-pred/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction/":[],"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/":[{"title":"OS notes","permalink":"https://LorrinWWW.github.io/posts/OS-notes/"}],"https://LorrinWWW.github.io/posts/projet-enjeu-plugin-chrome-101/":[{"title":"Chrome油猴脚本","permalink":"http://gubangzhong.cn/2017/06/13/Chrome油猴脚本/"}],"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/":[{"title":"MongoDB, Docker and Python","permalink":"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/":[{"title":"Hands on Scrapy","permalink":"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/OS-notes/":[{"title":"learning OS and building LorriOS","permalink":"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/"}],"https://LorrinWWW.github.io/posts/Bayes-estimation/":[{"title":"Note of statistic","permalink":"https://LorrinWWW.github.io/posts/Note-of-statistic/"}],"https://LorrinWWW.github.io/posts/management-of-the-firm/":[],"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"}],"https://LorrinWWW.github.io/posts/datamining-qualitative-induction/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-finite-element-method/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/QuadTree/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/proba-ch4/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch2/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/":[{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/Note-of-NLP/":[{"title":"Generative Adversarial Network","permalink":"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/"},{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"}],"https://LorrinWWW.github.io/posts/Note-of-datamining/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"数据挖掘-分类与预测","permalink":"https://LorrinWWW.github.io/posts/datamining-class-pred/"},{"title":"数据挖掘-预处理","permalink":"https://LorrinWWW.github.io/posts/datamining-pretreatment/"}],"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/Note-of-probability/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/participe-present-et-gerondif/":[],"https://LorrinWWW.github.io/posts/Hilbert-space/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/proba-ch6/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}]}}},"excerpt":"","more":"<h1 id=\"知识图谱-Knowledge-graph\"><a href=\"#知识图谱-Knowledge-graph\" class=\"headerlink\" title=\"知识图谱 Knowledge graph\"></a>知识图谱 Knowledge graph</h1><p>知识图谱的概念由Google提出，目前成为一大热点。总体而言，英文知识图谱的文献相对齐全。中英文知识图谱的差别主要体现于对自然语言的处理，对于中文而言，首先准确的分词，其次要应对中文想对自由的语法和语言形式。</p>\n<p>下面是一些笔记。</p>\n<p>知识图谱的构建：</p>\n<h2 id=\"1-信息抽取-information-extraction\"><a href=\"#1-信息抽取-information-extraction\" class=\"headerlink\" title=\"1. 信息抽取 information extraction\"></a>1. 信息抽取 information extraction</h2><p>我们需要能够自动化地从结构化、半结构化和无结构数据中抽取实体(entity)、关系(relationship)以及实体属性等。</p>\n<ol>\n<li>实体抽取 entity extraction<ul>\n<li>实体提取：<ul>\n<li>liu等人，Knn算法和条件随机场模型</li>\n<li>lin等人，字典和最大熵算法</li>\n</ul>\n</li>\n<li>实体分类：<ul>\n<li>2012，ling等人，借鉴freebase归纳实体分类方法，条件随机场模型进行实体边界识别，自适应感知机算法实现对实体的自动分类，结果优于Stanford NER等主流命名实体识别系统</li>\n<li>预定义实体分类并不好，新思路：对于给定实体，采用统计机器学习等方法，从目标数据集中抽取与之俱有相似的上下文特征等实体，从而实现实体的分类和聚类。参考whitelaw的解决方案</li>\n<li>​</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>关系抽取 relationship extraction</li>\n</ol>\n"},{"title":"算法笔记 Note of learning Algo","date":"2016-11-27T04:38:32.000Z","_content":"\n# Data Structure\n\n{% post_link complexity How to calcul the complexity?  %} \n\n{% post_link compression How to make a compression?  %}\n\n1. The data : int, double, etc.\n2. The basic data structure\n   - Container\n   - Table \n   - Stack \n   - Queue \n   - List\n3. Tree\n   - Arbres binaires - 二叉树\n     - ABR - 二叉树的查询\n   - Arbres n-aires\n4. Dictionary \n   - Hash table - Table de hachage - 哈希表\n5. **Heap - Tas - 堆 **\n   - 二叉堆\n   - 堆排序\n6. Find-Union\n\n# Method of programming\n\n{% post_link Method-of-programming-facing-to-exams Savoir plus %}\n\n1. Echaustive / force brute - 穷举法\n2. Try-error - Essai-erreur 试错法\n3. Glouton - 贪心法\n4. Recursif - recursive - 递归\n5. Divede merge - Diviser pour regner - 分治\n6. Dynamic - Dynamique - 动态规划\n\n# Graph\n\n{% post_link graph Savoir plus %}\n\n1. Traversal - parcours - 遍历\n   - BFS\n   - DFS\n2. Critical path method - plus court chemin\n   - Dijkstra\n   - Floyd\n   - Bellman-Ford\n3. Tree - arbre\n   - Prim\n   - Kruskal\n\n*backtracking\n\n*Branch and bound\n\n# Others\n\n1. 排序\n   - 选择，冒泡\n   - 分治（fusion）\n   - 快速\n   - tas ","source":"_posts/Note-of-learning-Algo.md","raw":"---\ntitle: 算法笔记 Note of learning Algo\ndate: 2016-11-27 12:38:32\ncategories: programming\ntags: [algo, programming]\n---\n\n# Data Structure\n\n{% post_link complexity How to calcul the complexity?  %} \n\n{% post_link compression How to make a compression?  %}\n\n1. The data : int, double, etc.\n2. The basic data structure\n   - Container\n   - Table \n   - Stack \n   - Queue \n   - List\n3. Tree\n   - Arbres binaires - 二叉树\n     - ABR - 二叉树的查询\n   - Arbres n-aires\n4. Dictionary \n   - Hash table - Table de hachage - 哈希表\n5. **Heap - Tas - 堆 **\n   - 二叉堆\n   - 堆排序\n6. Find-Union\n\n# Method of programming\n\n{% post_link Method-of-programming-facing-to-exams Savoir plus %}\n\n1. Echaustive / force brute - 穷举法\n2. Try-error - Essai-erreur 试错法\n3. Glouton - 贪心法\n4. Recursif - recursive - 递归\n5. Divede merge - Diviser pour regner - 分治\n6. Dynamic - Dynamique - 动态规划\n\n# Graph\n\n{% post_link graph Savoir plus %}\n\n1. Traversal - parcours - 遍历\n   - BFS\n   - DFS\n2. Critical path method - plus court chemin\n   - Dijkstra\n   - Floyd\n   - Bellman-Ford\n3. Tree - arbre\n   - Prim\n   - Kruskal\n\n*backtracking\n\n*Branch and bound\n\n# Others\n\n1. 排序\n   - 选择，冒泡\n   - 分治（fusion）\n   - 快速\n   - tas ","slug":"Note-of-learning-Algo","published":1,"updated":"2016-11-30T10:56:38.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckw4qufp80011gwtl9j3b7d15","content":"<h1 id=\"Data-Structure\"><a href=\"#Data-Structure\" class=\"headerlink\" title=\"Data Structure\"></a>Data Structure</h1><a href=\"/posts/complexity/\" title=\"How to calcul the complexity?\">How to calcul the complexity?</a> \n\n<a href=\"/posts/compression/\" title=\"How to make a compression?\">How to make a compression?</a>\n\n<ol>\n<li>The data : int, double, etc.</li>\n<li>The basic data structure<ul>\n<li>Container</li>\n<li>Table </li>\n<li>Stack </li>\n<li>Queue </li>\n<li>List</li>\n</ul>\n</li>\n<li>Tree<ul>\n<li>Arbres binaires - 二叉树<ul>\n<li>ABR - 二叉树的查询</li>\n</ul>\n</li>\n<li>Arbres n-aires</li>\n</ul>\n</li>\n<li>Dictionary <ul>\n<li>Hash table - Table de hachage - 哈希表</li>\n</ul>\n</li>\n<li>**Heap - Tas - 堆 **<ul>\n<li>二叉堆</li>\n<li>堆排序</li>\n</ul>\n</li>\n<li>Find-Union</li>\n</ol>\n<h1 id=\"Method-of-programming\"><a href=\"#Method-of-programming\" class=\"headerlink\" title=\"Method of programming\"></a>Method of programming</h1><a href=\"/posts/Method-of-programming-facing-to-exams/\" title=\"Savoir plus\">Savoir plus</a>\n\n<ol>\n<li>Echaustive / force brute - 穷举法</li>\n<li>Try-error - Essai-erreur 试错法</li>\n<li>Glouton - 贪心法</li>\n<li>Recursif - recursive - 递归</li>\n<li>Divede merge - Diviser pour regner - 分治</li>\n<li>Dynamic - Dynamique - 动态规划</li>\n</ol>\n<h1 id=\"Graph\"><a href=\"#Graph\" class=\"headerlink\" title=\"Graph\"></a>Graph</h1><a href=\"/posts/graph/\" title=\"Savoir plus\">Savoir plus</a>\n\n<ol>\n<li>Traversal - parcours - 遍历<ul>\n<li>BFS</li>\n<li>DFS</li>\n</ul>\n</li>\n<li>Critical path method - plus court chemin<ul>\n<li>Dijkstra</li>\n<li>Floyd</li>\n<li>Bellman-Ford</li>\n</ul>\n</li>\n<li>Tree - arbre<ul>\n<li>Prim</li>\n<li>Kruskal</li>\n</ul>\n</li>\n</ol>\n<p>*backtracking</p>\n<p>*Branch and bound</p>\n<h1 id=\"Others\"><a href=\"#Others\" class=\"headerlink\" title=\"Others\"></a>Others</h1><ol>\n<li>排序<ul>\n<li>选择，冒泡</li>\n<li>分治（fusion）</li>\n<li>快速</li>\n<li>tas </li>\n</ul>\n</li>\n</ol>\n","site":{"data":{"recommended_posts":{"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"}],"https://LorrinWWW.github.io/posts/complexity/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/hexo-with-latex/":[{"title":"区块链时代的最正确的打赏姿势——在Hexo博客配置Coinhive网页挖矿","permalink":"http://www.davidfnck.com/blockchain/mine-xmr-in-hexo-by-coinhive.html/"}],"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/":[{"title":"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-statistic/":[{"title":"贝叶斯估计 Bayes estimation","permalink":"https://LorrinWWW.github.io/posts/Bayes-estimation/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"}],"https://LorrinWWW.github.io/posts/[2018.2.5]Nested-LSTMs/":[{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"}],"https://LorrinWWW.github.io/posts/datamining-pretreatment/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Several models for knowledge graph representing and completing 几个知识图谱模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/"}],"https://LorrinWWW.github.io/posts/compression/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/Sobolev-space/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/proba-ch5/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch3/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/proba-ch1/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/ML-CNN/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/EDP-basic-models/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/machine-learning/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/datamining-class-pred/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction/":[],"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/":[{"title":"OS notes","permalink":"https://LorrinWWW.github.io/posts/OS-notes/"}],"https://LorrinWWW.github.io/posts/projet-enjeu-plugin-chrome-101/":[{"title":"Chrome油猴脚本","permalink":"http://gubangzhong.cn/2017/06/13/Chrome油猴脚本/"}],"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/":[{"title":"MongoDB, Docker and Python","permalink":"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/":[{"title":"Hands on Scrapy","permalink":"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/OS-notes/":[{"title":"learning OS and building LorriOS","permalink":"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/"}],"https://LorrinWWW.github.io/posts/Bayes-estimation/":[{"title":"Note of statistic","permalink":"https://LorrinWWW.github.io/posts/Note-of-statistic/"}],"https://LorrinWWW.github.io/posts/management-of-the-firm/":[],"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"}],"https://LorrinWWW.github.io/posts/datamining-qualitative-induction/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-finite-element-method/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/QuadTree/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/proba-ch4/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch2/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/":[{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/Note-of-NLP/":[{"title":"Generative Adversarial Network","permalink":"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/"},{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"}],"https://LorrinWWW.github.io/posts/Note-of-datamining/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"数据挖掘-分类与预测","permalink":"https://LorrinWWW.github.io/posts/datamining-class-pred/"},{"title":"数据挖掘-预处理","permalink":"https://LorrinWWW.github.io/posts/datamining-pretreatment/"}],"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/Note-of-probability/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/participe-present-et-gerondif/":[],"https://LorrinWWW.github.io/posts/Hilbert-space/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/proba-ch6/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}]}}},"excerpt":"","more":"<h1 id=\"Data-Structure\"><a href=\"#Data-Structure\" class=\"headerlink\" title=\"Data Structure\"></a>Data Structure</h1><a href=\"/posts/complexity/\" title=\"How to calcul the complexity?\">How to calcul the complexity?</a> \n\n<a href=\"/posts/compression/\" title=\"How to make a compression?\">How to make a compression?</a>\n\n<ol>\n<li>The data : int, double, etc.</li>\n<li>The basic data structure<ul>\n<li>Container</li>\n<li>Table </li>\n<li>Stack </li>\n<li>Queue </li>\n<li>List</li>\n</ul>\n</li>\n<li>Tree<ul>\n<li>Arbres binaires - 二叉树<ul>\n<li>ABR - 二叉树的查询</li>\n</ul>\n</li>\n<li>Arbres n-aires</li>\n</ul>\n</li>\n<li>Dictionary <ul>\n<li>Hash table - Table de hachage - 哈希表</li>\n</ul>\n</li>\n<li>**Heap - Tas - 堆 **<ul>\n<li>二叉堆</li>\n<li>堆排序</li>\n</ul>\n</li>\n<li>Find-Union</li>\n</ol>\n<h1 id=\"Method-of-programming\"><a href=\"#Method-of-programming\" class=\"headerlink\" title=\"Method of programming\"></a>Method of programming</h1><a href=\"/posts/Method-of-programming-facing-to-exams/\" title=\"Savoir plus\">Savoir plus</a>\n\n<ol>\n<li>Echaustive / force brute - 穷举法</li>\n<li>Try-error - Essai-erreur 试错法</li>\n<li>Glouton - 贪心法</li>\n<li>Recursif - recursive - 递归</li>\n<li>Divede merge - Diviser pour regner - 分治</li>\n<li>Dynamic - Dynamique - 动态规划</li>\n</ol>\n<h1 id=\"Graph\"><a href=\"#Graph\" class=\"headerlink\" title=\"Graph\"></a>Graph</h1><a href=\"/posts/graph/\" title=\"Savoir plus\">Savoir plus</a>\n\n<ol>\n<li>Traversal - parcours - 遍历<ul>\n<li>BFS</li>\n<li>DFS</li>\n</ul>\n</li>\n<li>Critical path method - plus court chemin<ul>\n<li>Dijkstra</li>\n<li>Floyd</li>\n<li>Bellman-Ford</li>\n</ul>\n</li>\n<li>Tree - arbre<ul>\n<li>Prim</li>\n<li>Kruskal</li>\n</ul>\n</li>\n</ol>\n<p>*backtracking</p>\n<p>*Branch and bound</p>\n<h1 id=\"Others\"><a href=\"#Others\" class=\"headerlink\" title=\"Others\"></a>Others</h1><ol>\n<li>排序<ul>\n<li>选择，冒泡</li>\n<li>分治（fusion）</li>\n<li>快速</li>\n<li>tas </li>\n</ul>\n</li>\n</ol>\n"},{"title":"概率论笔记 Note of probability","date":"2016-11-30T11:35:53.000Z","_content":"\n该笔记只是记录在法国学的一些东西，因为学的是法语的翻译可能有些摸不着头脑，并且记的凌乱，望见谅。\n\n另外，这里的概率论是基于测度论的概率论，这里假设已经学过测度学了。\n\n# Part 1 公理、概率空间\n\n{% post_link proba-ch1 Savoir plus  %}\n\n# Part 2 概率与随机变量\n\n{% post_link proba-ch2 Savoir plus  %}\n\n# Part 3 实域概率和特征方程\n\n{% post_link proba-ch3 Savoir plus  %}\n\n# Part 4 高斯向量\n\n{% post_link proba-ch4 Savoir plus  %}\n\n# Part 5 数列和随机变量系列\n\n{% post_link proba-ch5 Savoir plus  %}\n\n# Part 6 条件数学期望\n\n{% post_link proba-ch6 Savoir plus  %}\n","source":"_posts/Note-of-probability.md","raw":"---\ntitle: 概率论笔记 Note of probability\ndate: 2016-11-30 19:35:53\ncategories: [math]\ntags: [math, probability]\n---\n\n该笔记只是记录在法国学的一些东西，因为学的是法语的翻译可能有些摸不着头脑，并且记的凌乱，望见谅。\n\n另外，这里的概率论是基于测度论的概率论，这里假设已经学过测度学了。\n\n# Part 1 公理、概率空间\n\n{% post_link proba-ch1 Savoir plus  %}\n\n# Part 2 概率与随机变量\n\n{% post_link proba-ch2 Savoir plus  %}\n\n# Part 3 实域概率和特征方程\n\n{% post_link proba-ch3 Savoir plus  %}\n\n# Part 4 高斯向量\n\n{% post_link proba-ch4 Savoir plus  %}\n\n# Part 5 数列和随机变量系列\n\n{% post_link proba-ch5 Savoir plus  %}\n\n# Part 6 条件数学期望\n\n{% post_link proba-ch6 Savoir plus  %}\n","slug":"Note-of-probability","published":1,"updated":"2016-12-05T14:16:49.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckw4qufp90014gwtlg8dt7wef","content":"<p>该笔记只是记录在法国学的一些东西，因为学的是法语的翻译可能有些摸不着头脑，并且记的凌乱，望见谅。</p>\n<p>另外，这里的概率论是基于测度论的概率论，这里假设已经学过测度学了。</p>\n<h1 id=\"Part-1-公理、概率空间\"><a href=\"#Part-1-公理、概率空间\" class=\"headerlink\" title=\"Part 1 公理、概率空间\"></a>Part 1 公理、概率空间</h1><a href=\"/posts/proba-ch1/\" title=\"Savoir plus\">Savoir plus</a>\n\n<h1 id=\"Part-2-概率与随机变量\"><a href=\"#Part-2-概率与随机变量\" class=\"headerlink\" title=\"Part 2 概率与随机变量\"></a>Part 2 概率与随机变量</h1><a href=\"/posts/proba-ch2/\" title=\"Savoir plus\">Savoir plus</a>\n\n<h1 id=\"Part-3-实域概率和特征方程\"><a href=\"#Part-3-实域概率和特征方程\" class=\"headerlink\" title=\"Part 3 实域概率和特征方程\"></a>Part 3 实域概率和特征方程</h1><a href=\"/posts/proba-ch3/\" title=\"Savoir plus\">Savoir plus</a>\n\n<h1 id=\"Part-4-高斯向量\"><a href=\"#Part-4-高斯向量\" class=\"headerlink\" title=\"Part 4 高斯向量\"></a>Part 4 高斯向量</h1><a href=\"/posts/proba-ch4/\" title=\"Savoir plus\">Savoir plus</a>\n\n<h1 id=\"Part-5-数列和随机变量系列\"><a href=\"#Part-5-数列和随机变量系列\" class=\"headerlink\" title=\"Part 5 数列和随机变量系列\"></a>Part 5 数列和随机变量系列</h1><a href=\"/posts/proba-ch5/\" title=\"Savoir plus\">Savoir plus</a>\n\n<h1 id=\"Part-6-条件数学期望\"><a href=\"#Part-6-条件数学期望\" class=\"headerlink\" title=\"Part 6 条件数学期望\"></a>Part 6 条件数学期望</h1><a href=\"/posts/proba-ch6/\" title=\"Savoir plus\">Savoir plus</a>\n","site":{"data":{"recommended_posts":{"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"}],"https://LorrinWWW.github.io/posts/complexity/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/hexo-with-latex/":[{"title":"区块链时代的最正确的打赏姿势——在Hexo博客配置Coinhive网页挖矿","permalink":"http://www.davidfnck.com/blockchain/mine-xmr-in-hexo-by-coinhive.html/"}],"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/":[{"title":"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-statistic/":[{"title":"贝叶斯估计 Bayes estimation","permalink":"https://LorrinWWW.github.io/posts/Bayes-estimation/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"}],"https://LorrinWWW.github.io/posts/[2018.2.5]Nested-LSTMs/":[{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"}],"https://LorrinWWW.github.io/posts/datamining-pretreatment/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Several models for knowledge graph representing and completing 几个知识图谱模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/"}],"https://LorrinWWW.github.io/posts/compression/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/Sobolev-space/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/proba-ch5/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch3/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/proba-ch1/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/ML-CNN/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/EDP-basic-models/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/machine-learning/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/datamining-class-pred/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction/":[],"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/":[{"title":"OS notes","permalink":"https://LorrinWWW.github.io/posts/OS-notes/"}],"https://LorrinWWW.github.io/posts/projet-enjeu-plugin-chrome-101/":[{"title":"Chrome油猴脚本","permalink":"http://gubangzhong.cn/2017/06/13/Chrome油猴脚本/"}],"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/":[{"title":"MongoDB, Docker and Python","permalink":"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/":[{"title":"Hands on Scrapy","permalink":"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/OS-notes/":[{"title":"learning OS and building LorriOS","permalink":"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/"}],"https://LorrinWWW.github.io/posts/Bayes-estimation/":[{"title":"Note of statistic","permalink":"https://LorrinWWW.github.io/posts/Note-of-statistic/"}],"https://LorrinWWW.github.io/posts/management-of-the-firm/":[],"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"}],"https://LorrinWWW.github.io/posts/datamining-qualitative-induction/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-finite-element-method/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/QuadTree/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/proba-ch4/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch2/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/":[{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/Note-of-NLP/":[{"title":"Generative Adversarial Network","permalink":"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/"},{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"}],"https://LorrinWWW.github.io/posts/Note-of-datamining/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"数据挖掘-分类与预测","permalink":"https://LorrinWWW.github.io/posts/datamining-class-pred/"},{"title":"数据挖掘-预处理","permalink":"https://LorrinWWW.github.io/posts/datamining-pretreatment/"}],"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/Note-of-probability/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/participe-present-et-gerondif/":[],"https://LorrinWWW.github.io/posts/Hilbert-space/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/proba-ch6/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}]}}},"excerpt":"","more":"<p>该笔记只是记录在法国学的一些东西，因为学的是法语的翻译可能有些摸不着头脑，并且记的凌乱，望见谅。</p>\n<p>另外，这里的概率论是基于测度论的概率论，这里假设已经学过测度学了。</p>\n<h1 id=\"Part-1-公理、概率空间\"><a href=\"#Part-1-公理、概率空间\" class=\"headerlink\" title=\"Part 1 公理、概率空间\"></a>Part 1 公理、概率空间</h1><a href=\"/posts/proba-ch1/\" title=\"Savoir plus\">Savoir plus</a>\n\n<h1 id=\"Part-2-概率与随机变量\"><a href=\"#Part-2-概率与随机变量\" class=\"headerlink\" title=\"Part 2 概率与随机变量\"></a>Part 2 概率与随机变量</h1><a href=\"/posts/proba-ch2/\" title=\"Savoir plus\">Savoir plus</a>\n\n<h1 id=\"Part-3-实域概率和特征方程\"><a href=\"#Part-3-实域概率和特征方程\" class=\"headerlink\" title=\"Part 3 实域概率和特征方程\"></a>Part 3 实域概率和特征方程</h1><a href=\"/posts/proba-ch3/\" title=\"Savoir plus\">Savoir plus</a>\n\n<h1 id=\"Part-4-高斯向量\"><a href=\"#Part-4-高斯向量\" class=\"headerlink\" title=\"Part 4 高斯向量\"></a>Part 4 高斯向量</h1><a href=\"/posts/proba-ch4/\" title=\"Savoir plus\">Savoir plus</a>\n\n<h1 id=\"Part-5-数列和随机变量系列\"><a href=\"#Part-5-数列和随机变量系列\" class=\"headerlink\" title=\"Part 5 数列和随机变量系列\"></a>Part 5 数列和随机变量系列</h1><a href=\"/posts/proba-ch5/\" title=\"Savoir plus\">Savoir plus</a>\n\n<h1 id=\"Part-6-条件数学期望\"><a href=\"#Part-6-条件数学期望\" class=\"headerlink\" title=\"Part 6 条件数学期望\"></a>Part 6 条件数学期望</h1><a href=\"/posts/proba-ch6/\" title=\"Savoir plus\">Savoir plus</a>\n"},{"title":"Note of statistic","date":"2017-01-28T10:13:13.000Z","_content":"\n# 各种定理\n\n### 大数定理，中心极限定理\n\n略\n\n### Slutsky\n\nif\n$$\n\\Upsilon_n \\to(loi) \\Upsilon \\text{ et } Z_n \\to (P)c\n$$\nthen\n$$\n\\Upsilon_n + Z_n \\to(L) \\Upsilon+c \\text{ et } \\Upsilon_n Z_n \\to(L) \\Upsilon c\n$$\n\n# 假设检验\n\n## un test de $\\chi^2$\n\n列表：\n\n| Liste | a    | b    | c    |\n| ----- | ---- | ---- | ---- |\n| pi    | xx   | xx   | xx   |\n| npi   | xx   | xx   | xx   |\n| ni    | xx   | xx   | xx   |\n\n计算：\n$$\nT= \\sum_{j=1}^n{\\frac{(n_i-np_i)^2}{np_i}}\n$$\n其服从卡方分布，自由度为n-1，从而进行检验。","source":"_posts/Note-of-statistic.md","raw":"---\ntitle: Note of statistic\ndate: 2017-01-28 18:13:13\ncategories: [math, unfinished]\ntags: [statistic, math]\n---\n\n# 各种定理\n\n### 大数定理，中心极限定理\n\n略\n\n### Slutsky\n\nif\n$$\n\\Upsilon_n \\to(loi) \\Upsilon \\text{ et } Z_n \\to (P)c\n$$\nthen\n$$\n\\Upsilon_n + Z_n \\to(L) \\Upsilon+c \\text{ et } \\Upsilon_n Z_n \\to(L) \\Upsilon c\n$$\n\n# 假设检验\n\n## un test de $\\chi^2$\n\n列表：\n\n| Liste | a    | b    | c    |\n| ----- | ---- | ---- | ---- |\n| pi    | xx   | xx   | xx   |\n| npi   | xx   | xx   | xx   |\n| ni    | xx   | xx   | xx   |\n\n计算：\n$$\nT= \\sum_{j=1}^n{\\frac{(n_i-np_i)^2}{np_i}}\n$$\n其服从卡方分布，自由度为n-1，从而进行检验。","slug":"Note-of-statistic","published":1,"updated":"2017-01-28T18:02:06.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckw4qufpa0017gwtl2o49btu3","content":"<h1 id=\"各种定理\"><a href=\"#各种定理\" class=\"headerlink\" title=\"各种定理\"></a>各种定理</h1><h3 id=\"大数定理，中心极限定理\"><a href=\"#大数定理，中心极限定理\" class=\"headerlink\" title=\"大数定理，中心极限定理\"></a>大数定理，中心极限定理</h3><p>略</p>\n<h3 id=\"Slutsky\"><a href=\"#Slutsky\" class=\"headerlink\" title=\"Slutsky\"></a>Slutsky</h3><p>if<br>$$<br>\\Upsilon_n \\to(loi) \\Upsilon \\text{ et } Z_n \\to (P)c<br>$$<br>then<br>$$<br>\\Upsilon_n + Z_n \\to(L) \\Upsilon+c \\text{ et } \\Upsilon_n Z_n \\to(L) \\Upsilon c<br>$$</p>\n<h1 id=\"假设检验\"><a href=\"#假设检验\" class=\"headerlink\" title=\"假设检验\"></a>假设检验</h1><h2 id=\"un-test-de-chi-2\"><a href=\"#un-test-de-chi-2\" class=\"headerlink\" title=\"un test de $\\chi^2$\"></a>un test de $\\chi^2$</h2><p>列表：</p>\n<table>\n<thead>\n<tr>\n<th>Liste</th>\n<th>a</th>\n<th>b</th>\n<th>c</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>pi</td>\n<td>xx</td>\n<td>xx</td>\n<td>xx</td>\n</tr>\n<tr>\n<td>npi</td>\n<td>xx</td>\n<td>xx</td>\n<td>xx</td>\n</tr>\n<tr>\n<td>ni</td>\n<td>xx</td>\n<td>xx</td>\n<td>xx</td>\n</tr>\n</tbody></table>\n<p>计算：<br>$$<br>T= \\sum_{j=1}^n{\\frac{(n_i-np_i)^2}{np_i}}<br>$$<br>其服从卡方分布，自由度为n-1，从而进行检验。</p>\n","site":{"data":{"recommended_posts":{"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"}],"https://LorrinWWW.github.io/posts/complexity/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/hexo-with-latex/":[{"title":"区块链时代的最正确的打赏姿势——在Hexo博客配置Coinhive网页挖矿","permalink":"http://www.davidfnck.com/blockchain/mine-xmr-in-hexo-by-coinhive.html/"}],"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/":[{"title":"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-statistic/":[{"title":"贝叶斯估计 Bayes estimation","permalink":"https://LorrinWWW.github.io/posts/Bayes-estimation/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"}],"https://LorrinWWW.github.io/posts/[2018.2.5]Nested-LSTMs/":[{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"}],"https://LorrinWWW.github.io/posts/datamining-pretreatment/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Several models for knowledge graph representing and completing 几个知识图谱模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/"}],"https://LorrinWWW.github.io/posts/compression/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/Sobolev-space/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/proba-ch5/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch3/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/proba-ch1/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/ML-CNN/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/EDP-basic-models/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/machine-learning/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/datamining-class-pred/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction/":[],"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/":[{"title":"OS notes","permalink":"https://LorrinWWW.github.io/posts/OS-notes/"}],"https://LorrinWWW.github.io/posts/projet-enjeu-plugin-chrome-101/":[{"title":"Chrome油猴脚本","permalink":"http://gubangzhong.cn/2017/06/13/Chrome油猴脚本/"}],"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/":[{"title":"MongoDB, Docker and Python","permalink":"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/":[{"title":"Hands on Scrapy","permalink":"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/OS-notes/":[{"title":"learning OS and building LorriOS","permalink":"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/"}],"https://LorrinWWW.github.io/posts/Bayes-estimation/":[{"title":"Note of statistic","permalink":"https://LorrinWWW.github.io/posts/Note-of-statistic/"}],"https://LorrinWWW.github.io/posts/management-of-the-firm/":[],"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"}],"https://LorrinWWW.github.io/posts/datamining-qualitative-induction/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-finite-element-method/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/QuadTree/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/proba-ch4/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch2/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/":[{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/Note-of-NLP/":[{"title":"Generative Adversarial Network","permalink":"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/"},{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"}],"https://LorrinWWW.github.io/posts/Note-of-datamining/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"数据挖掘-分类与预测","permalink":"https://LorrinWWW.github.io/posts/datamining-class-pred/"},{"title":"数据挖掘-预处理","permalink":"https://LorrinWWW.github.io/posts/datamining-pretreatment/"}],"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/Note-of-probability/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/participe-present-et-gerondif/":[],"https://LorrinWWW.github.io/posts/Hilbert-space/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/proba-ch6/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}]}}},"excerpt":"","more":"<h1 id=\"各种定理\"><a href=\"#各种定理\" class=\"headerlink\" title=\"各种定理\"></a>各种定理</h1><h3 id=\"大数定理，中心极限定理\"><a href=\"#大数定理，中心极限定理\" class=\"headerlink\" title=\"大数定理，中心极限定理\"></a>大数定理，中心极限定理</h3><p>略</p>\n<h3 id=\"Slutsky\"><a href=\"#Slutsky\" class=\"headerlink\" title=\"Slutsky\"></a>Slutsky</h3><p>if<br>$$<br>\\Upsilon_n \\to(loi) \\Upsilon \\text{ et } Z_n \\to (P)c<br>$$<br>then<br>$$<br>\\Upsilon_n + Z_n \\to(L) \\Upsilon+c \\text{ et } \\Upsilon_n Z_n \\to(L) \\Upsilon c<br>$$</p>\n<h1 id=\"假设检验\"><a href=\"#假设检验\" class=\"headerlink\" title=\"假设检验\"></a>假设检验</h1><h2 id=\"un-test-de-chi-2\"><a href=\"#un-test-de-chi-2\" class=\"headerlink\" title=\"un test de $\\chi^2$\"></a>un test de $\\chi^2$</h2><p>列表：</p>\n<table>\n<thead>\n<tr>\n<th>Liste</th>\n<th>a</th>\n<th>b</th>\n<th>c</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>pi</td>\n<td>xx</td>\n<td>xx</td>\n<td>xx</td>\n</tr>\n<tr>\n<td>npi</td>\n<td>xx</td>\n<td>xx</td>\n<td>xx</td>\n</tr>\n<tr>\n<td>ni</td>\n<td>xx</td>\n<td>xx</td>\n<td>xx</td>\n</tr>\n</tbody></table>\n<p>计算：<br>$$<br>T= \\sum_{j=1}^n{\\frac{(n_i-np_i)^2}{np_i}}<br>$$<br>其服从卡方分布，自由度为n-1，从而进行检验。</p>\n"},{"title":"OS notes","date":"2017-04-15T10:59:08.000Z","_content":"\n# Operating System\n\nThis is the note or keywords of a course in udacity.\n\n## Overview\n\n- Processes and process management\n- Threads and concurrency\n- resource management\n- OS services\n- OS support for distributed services\n- Data cendter and cloud\n\n## Introduction\n\n### OS Elements\n\nAbstractions\n\n- Process, thread, file, socket, memory pape\n\nMechanisms\n\n- Create, schedule, open, write, allocate\n\nPolicies\n\n- Least - recently used (LRU) etc.\n\n\n### System call\n\nTwo ways for a user process to execute a priviledged call.\n\n- User process calls hardware directly, and that will cause a trap, the kernel check if it is legal\n- User process executes system call.\n\nuser/kernel transitions are not cheap!\n\n### OS services\n\n- Scheduler => CPU\n- Mem manager\n- Block device driver\n- file system\n- ...\n\n### Linux Architecture\n\nUser interface : **Users** <= user mode\n\nLibrary interface : **Standards utility programs** (shell, editor, compilors) <= user mode\n\nSystem call interface : **Standard licrary** (open, close, read, write, fork) <= user mode\n\n**Linux operating system** <= kernel mode\n\n**Hardware**\n\n## Processes and Process Management\n\nA process:\n\n- state of execution\n  - Program counter\n  - stack\n- parts and temporary holding area\n  - Data, register\n- May require special hardware\n  - IO devices\n\nProcess == state of a program when executing.\n\n### Process Control Block (PCB)\n\na data structure storing status of a process\n\n- PCB created when process is created\n- Certain filds are updated when process state changes\n- other fields change too frequently\n\n### Context switch\n\nHot cache, cold cache\n\n### CPU scheduler\n\nOS must \n\n- preempt\n- schedule\n- dispatch\n\n### Multi Processes\n\nP1(web server), P2(Database)\n\nInter - process communication (IPC) : \n\n- Message - passing IPC\n\n\n- Shared memory IPC","source":"_posts/OS-notes.md","raw":"---\ntitle: OS notes\ndate: 2017-04-15 18:59:08\ncategories: [programming, unfinished]\ntags: [OS]\n---\n\n# Operating System\n\nThis is the note or keywords of a course in udacity.\n\n## Overview\n\n- Processes and process management\n- Threads and concurrency\n- resource management\n- OS services\n- OS support for distributed services\n- Data cendter and cloud\n\n## Introduction\n\n### OS Elements\n\nAbstractions\n\n- Process, thread, file, socket, memory pape\n\nMechanisms\n\n- Create, schedule, open, write, allocate\n\nPolicies\n\n- Least - recently used (LRU) etc.\n\n\n### System call\n\nTwo ways for a user process to execute a priviledged call.\n\n- User process calls hardware directly, and that will cause a trap, the kernel check if it is legal\n- User process executes system call.\n\nuser/kernel transitions are not cheap!\n\n### OS services\n\n- Scheduler => CPU\n- Mem manager\n- Block device driver\n- file system\n- ...\n\n### Linux Architecture\n\nUser interface : **Users** <= user mode\n\nLibrary interface : **Standards utility programs** (shell, editor, compilors) <= user mode\n\nSystem call interface : **Standard licrary** (open, close, read, write, fork) <= user mode\n\n**Linux operating system** <= kernel mode\n\n**Hardware**\n\n## Processes and Process Management\n\nA process:\n\n- state of execution\n  - Program counter\n  - stack\n- parts and temporary holding area\n  - Data, register\n- May require special hardware\n  - IO devices\n\nProcess == state of a program when executing.\n\n### Process Control Block (PCB)\n\na data structure storing status of a process\n\n- PCB created when process is created\n- Certain filds are updated when process state changes\n- other fields change too frequently\n\n### Context switch\n\nHot cache, cold cache\n\n### CPU scheduler\n\nOS must \n\n- preempt\n- schedule\n- dispatch\n\n### Multi Processes\n\nP1(web server), P2(Database)\n\nInter - process communication (IPC) : \n\n- Message - passing IPC\n\n\n- Shared memory IPC","slug":"OS-notes","published":1,"updated":"2017-04-16T18:40:56.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckw4qufpc001bgwtl809wau5y","content":"<h1 id=\"Operating-System\"><a href=\"#Operating-System\" class=\"headerlink\" title=\"Operating System\"></a>Operating System</h1><p>This is the note or keywords of a course in udacity.</p>\n<h2 id=\"Overview\"><a href=\"#Overview\" class=\"headerlink\" title=\"Overview\"></a>Overview</h2><ul>\n<li>Processes and process management</li>\n<li>Threads and concurrency</li>\n<li>resource management</li>\n<li>OS services</li>\n<li>OS support for distributed services</li>\n<li>Data cendter and cloud</li>\n</ul>\n<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><h3 id=\"OS-Elements\"><a href=\"#OS-Elements\" class=\"headerlink\" title=\"OS Elements\"></a>OS Elements</h3><p>Abstractions</p>\n<ul>\n<li>Process, thread, file, socket, memory pape</li>\n</ul>\n<p>Mechanisms</p>\n<ul>\n<li>Create, schedule, open, write, allocate</li>\n</ul>\n<p>Policies</p>\n<ul>\n<li>Least - recently used (LRU) etc.</li>\n</ul>\n<h3 id=\"System-call\"><a href=\"#System-call\" class=\"headerlink\" title=\"System call\"></a>System call</h3><p>Two ways for a user process to execute a priviledged call.</p>\n<ul>\n<li>User process calls hardware directly, and that will cause a trap, the kernel check if it is legal</li>\n<li>User process executes system call.</li>\n</ul>\n<p>user/kernel transitions are not cheap!</p>\n<h3 id=\"OS-services\"><a href=\"#OS-services\" class=\"headerlink\" title=\"OS services\"></a>OS services</h3><ul>\n<li>Scheduler =&gt; CPU</li>\n<li>Mem manager</li>\n<li>Block device driver</li>\n<li>file system</li>\n<li>…</li>\n</ul>\n<h3 id=\"Linux-Architecture\"><a href=\"#Linux-Architecture\" class=\"headerlink\" title=\"Linux Architecture\"></a>Linux Architecture</h3><p>User interface : <strong>Users</strong> &lt;= user mode</p>\n<p>Library interface : <strong>Standards utility programs</strong> (shell, editor, compilors) &lt;= user mode</p>\n<p>System call interface : <strong>Standard licrary</strong> (open, close, read, write, fork) &lt;= user mode</p>\n<p><strong>Linux operating system</strong> &lt;= kernel mode</p>\n<p><strong>Hardware</strong></p>\n<h2 id=\"Processes-and-Process-Management\"><a href=\"#Processes-and-Process-Management\" class=\"headerlink\" title=\"Processes and Process Management\"></a>Processes and Process Management</h2><p>A process:</p>\n<ul>\n<li>state of execution<ul>\n<li>Program counter</li>\n<li>stack</li>\n</ul>\n</li>\n<li>parts and temporary holding area<ul>\n<li>Data, register</li>\n</ul>\n</li>\n<li>May require special hardware<ul>\n<li>IO devices</li>\n</ul>\n</li>\n</ul>\n<p>Process == state of a program when executing.</p>\n<h3 id=\"Process-Control-Block-PCB\"><a href=\"#Process-Control-Block-PCB\" class=\"headerlink\" title=\"Process Control Block (PCB)\"></a>Process Control Block (PCB)</h3><p>a data structure storing status of a process</p>\n<ul>\n<li>PCB created when process is created</li>\n<li>Certain filds are updated when process state changes</li>\n<li>other fields change too frequently</li>\n</ul>\n<h3 id=\"Context-switch\"><a href=\"#Context-switch\" class=\"headerlink\" title=\"Context switch\"></a>Context switch</h3><p>Hot cache, cold cache</p>\n<h3 id=\"CPU-scheduler\"><a href=\"#CPU-scheduler\" class=\"headerlink\" title=\"CPU scheduler\"></a>CPU scheduler</h3><p>OS must </p>\n<ul>\n<li>preempt</li>\n<li>schedule</li>\n<li>dispatch</li>\n</ul>\n<h3 id=\"Multi-Processes\"><a href=\"#Multi-Processes\" class=\"headerlink\" title=\"Multi Processes\"></a>Multi Processes</h3><p>P1(web server), P2(Database)</p>\n<p>Inter - process communication (IPC) : </p>\n<ul>\n<li>Message - passing IPC</li>\n</ul>\n<ul>\n<li>Shared memory IPC</li>\n</ul>\n","site":{"data":{"recommended_posts":{"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"}],"https://LorrinWWW.github.io/posts/complexity/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/hexo-with-latex/":[{"title":"区块链时代的最正确的打赏姿势——在Hexo博客配置Coinhive网页挖矿","permalink":"http://www.davidfnck.com/blockchain/mine-xmr-in-hexo-by-coinhive.html/"}],"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/":[{"title":"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-statistic/":[{"title":"贝叶斯估计 Bayes estimation","permalink":"https://LorrinWWW.github.io/posts/Bayes-estimation/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"}],"https://LorrinWWW.github.io/posts/[2018.2.5]Nested-LSTMs/":[{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"}],"https://LorrinWWW.github.io/posts/datamining-pretreatment/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Several models for knowledge graph representing and completing 几个知识图谱模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/"}],"https://LorrinWWW.github.io/posts/compression/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/Sobolev-space/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/proba-ch5/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch3/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/proba-ch1/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/ML-CNN/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/EDP-basic-models/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/machine-learning/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/datamining-class-pred/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction/":[],"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/":[{"title":"OS notes","permalink":"https://LorrinWWW.github.io/posts/OS-notes/"}],"https://LorrinWWW.github.io/posts/projet-enjeu-plugin-chrome-101/":[{"title":"Chrome油猴脚本","permalink":"http://gubangzhong.cn/2017/06/13/Chrome油猴脚本/"}],"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/":[{"title":"MongoDB, Docker and Python","permalink":"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/":[{"title":"Hands on Scrapy","permalink":"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/OS-notes/":[{"title":"learning OS and building LorriOS","permalink":"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/"}],"https://LorrinWWW.github.io/posts/Bayes-estimation/":[{"title":"Note of statistic","permalink":"https://LorrinWWW.github.io/posts/Note-of-statistic/"}],"https://LorrinWWW.github.io/posts/management-of-the-firm/":[],"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"}],"https://LorrinWWW.github.io/posts/datamining-qualitative-induction/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-finite-element-method/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/QuadTree/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/proba-ch4/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch2/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/":[{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/Note-of-NLP/":[{"title":"Generative Adversarial Network","permalink":"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/"},{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"}],"https://LorrinWWW.github.io/posts/Note-of-datamining/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"数据挖掘-分类与预测","permalink":"https://LorrinWWW.github.io/posts/datamining-class-pred/"},{"title":"数据挖掘-预处理","permalink":"https://LorrinWWW.github.io/posts/datamining-pretreatment/"}],"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/Note-of-probability/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/participe-present-et-gerondif/":[],"https://LorrinWWW.github.io/posts/Hilbert-space/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/proba-ch6/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}]}}},"excerpt":"","more":"<h1 id=\"Operating-System\"><a href=\"#Operating-System\" class=\"headerlink\" title=\"Operating System\"></a>Operating System</h1><p>This is the note or keywords of a course in udacity.</p>\n<h2 id=\"Overview\"><a href=\"#Overview\" class=\"headerlink\" title=\"Overview\"></a>Overview</h2><ul>\n<li>Processes and process management</li>\n<li>Threads and concurrency</li>\n<li>resource management</li>\n<li>OS services</li>\n<li>OS support for distributed services</li>\n<li>Data cendter and cloud</li>\n</ul>\n<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><h3 id=\"OS-Elements\"><a href=\"#OS-Elements\" class=\"headerlink\" title=\"OS Elements\"></a>OS Elements</h3><p>Abstractions</p>\n<ul>\n<li>Process, thread, file, socket, memory pape</li>\n</ul>\n<p>Mechanisms</p>\n<ul>\n<li>Create, schedule, open, write, allocate</li>\n</ul>\n<p>Policies</p>\n<ul>\n<li>Least - recently used (LRU) etc.</li>\n</ul>\n<h3 id=\"System-call\"><a href=\"#System-call\" class=\"headerlink\" title=\"System call\"></a>System call</h3><p>Two ways for a user process to execute a priviledged call.</p>\n<ul>\n<li>User process calls hardware directly, and that will cause a trap, the kernel check if it is legal</li>\n<li>User process executes system call.</li>\n</ul>\n<p>user/kernel transitions are not cheap!</p>\n<h3 id=\"OS-services\"><a href=\"#OS-services\" class=\"headerlink\" title=\"OS services\"></a>OS services</h3><ul>\n<li>Scheduler =&gt; CPU</li>\n<li>Mem manager</li>\n<li>Block device driver</li>\n<li>file system</li>\n<li>…</li>\n</ul>\n<h3 id=\"Linux-Architecture\"><a href=\"#Linux-Architecture\" class=\"headerlink\" title=\"Linux Architecture\"></a>Linux Architecture</h3><p>User interface : <strong>Users</strong> &lt;= user mode</p>\n<p>Library interface : <strong>Standards utility programs</strong> (shell, editor, compilors) &lt;= user mode</p>\n<p>System call interface : <strong>Standard licrary</strong> (open, close, read, write, fork) &lt;= user mode</p>\n<p><strong>Linux operating system</strong> &lt;= kernel mode</p>\n<p><strong>Hardware</strong></p>\n<h2 id=\"Processes-and-Process-Management\"><a href=\"#Processes-and-Process-Management\" class=\"headerlink\" title=\"Processes and Process Management\"></a>Processes and Process Management</h2><p>A process:</p>\n<ul>\n<li>state of execution<ul>\n<li>Program counter</li>\n<li>stack</li>\n</ul>\n</li>\n<li>parts and temporary holding area<ul>\n<li>Data, register</li>\n</ul>\n</li>\n<li>May require special hardware<ul>\n<li>IO devices</li>\n</ul>\n</li>\n</ul>\n<p>Process == state of a program when executing.</p>\n<h3 id=\"Process-Control-Block-PCB\"><a href=\"#Process-Control-Block-PCB\" class=\"headerlink\" title=\"Process Control Block (PCB)\"></a>Process Control Block (PCB)</h3><p>a data structure storing status of a process</p>\n<ul>\n<li>PCB created when process is created</li>\n<li>Certain filds are updated when process state changes</li>\n<li>other fields change too frequently</li>\n</ul>\n<h3 id=\"Context-switch\"><a href=\"#Context-switch\" class=\"headerlink\" title=\"Context switch\"></a>Context switch</h3><p>Hot cache, cold cache</p>\n<h3 id=\"CPU-scheduler\"><a href=\"#CPU-scheduler\" class=\"headerlink\" title=\"CPU scheduler\"></a>CPU scheduler</h3><p>OS must </p>\n<ul>\n<li>preempt</li>\n<li>schedule</li>\n<li>dispatch</li>\n</ul>\n<h3 id=\"Multi-Processes\"><a href=\"#Multi-Processes\" class=\"headerlink\" title=\"Multi Processes\"></a>Multi Processes</h3><p>P1(web server), P2(Database)</p>\n<p>Inter - process communication (IPC) : </p>\n<ul>\n<li>Message - passing IPC</li>\n</ul>\n<ul>\n<li>Shared memory IPC</li>\n</ul>\n"},{"title":"四叉树 QuadTree","date":"2016-12-13T10:45:37.000Z","_content":"\n在ECP的实验课用到了四叉树，故来记录一下。\n\n四叉树是一种数据结构，每一个节点有四个孩子。一般需要用到四叉树的情况往往是二位平面，通过把区域分成四个区块来定义。\n\n在确定并显示一条曲线的具体位置时，需要使用一种算法。一种比较直观的想法就是，将平面细分成小块，通过计算验证曲线是否在这个小块内部。但是这种算法也有一个十分致命的缺点— 复杂度极高，计算量极大。而事实上，平面上大多数的区域是空白的，我们不需要对其每一个小块进行检测。仿照二分法的思路我们可以按如下方法进行。\n\n```Python\n# 确保使用的是等宽字体，用嵌入代码=_=\n\"\"\"\n3-------2       \n|       |\n|       |\n|       |\n0-------1\n检查曲线是否在内，若是，此节点为枝，分为四个小区域，作为四个儿子；若否，此节点为叶，没有儿子。\n=>\n3---7---2\n|   |   |\n8---4---6\n|   |   |\n0---5---1\n以此分别对四个儿子执行以上操作，直到达到所需精度。\n=>\n...\n\"\"\"\n```\n\n按照这个数据结构，若曲线不在内部，则将节点设为叶，无需继续细分。寻找一个点的话复杂度是O(logn)。\n\n\n\n在实验课的内容里，用python进行数据处理，用paraview进行数据可视化。代码其实挺简单（算法就不难= =），实验课还没结束暂时不附代码了，附一个效果图。\n\n![QTree](https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/QTree.png?imageMogr2/thumbnail/!50p)\n\n","source":"_posts/QuadTree.md","raw":"---\ntitle: 四叉树 QuadTree\ndate: 2016-12-13 18:45:37\ncategories: [programming]\ntags: [algo, data-structure]\n---\n\n在ECP的实验课用到了四叉树，故来记录一下。\n\n四叉树是一种数据结构，每一个节点有四个孩子。一般需要用到四叉树的情况往往是二位平面，通过把区域分成四个区块来定义。\n\n在确定并显示一条曲线的具体位置时，需要使用一种算法。一种比较直观的想法就是，将平面细分成小块，通过计算验证曲线是否在这个小块内部。但是这种算法也有一个十分致命的缺点— 复杂度极高，计算量极大。而事实上，平面上大多数的区域是空白的，我们不需要对其每一个小块进行检测。仿照二分法的思路我们可以按如下方法进行。\n\n```Python\n# 确保使用的是等宽字体，用嵌入代码=_=\n\"\"\"\n3-------2       \n|       |\n|       |\n|       |\n0-------1\n检查曲线是否在内，若是，此节点为枝，分为四个小区域，作为四个儿子；若否，此节点为叶，没有儿子。\n=>\n3---7---2\n|   |   |\n8---4---6\n|   |   |\n0---5---1\n以此分别对四个儿子执行以上操作，直到达到所需精度。\n=>\n...\n\"\"\"\n```\n\n按照这个数据结构，若曲线不在内部，则将节点设为叶，无需继续细分。寻找一个点的话复杂度是O(logn)。\n\n\n\n在实验课的内容里，用python进行数据处理，用paraview进行数据可视化。代码其实挺简单（算法就不难= =），实验课还没结束暂时不附代码了，附一个效果图。\n\n![QTree](https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/QTree.png?imageMogr2/thumbnail/!50p)\n\n","slug":"QuadTree","published":1,"updated":"2020-11-03T03:26:02.624Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckw4qufpd001fgwtlc4g6f3la","content":"<p>在ECP的实验课用到了四叉树，故来记录一下。</p>\n<p>四叉树是一种数据结构，每一个节点有四个孩子。一般需要用到四叉树的情况往往是二位平面，通过把区域分成四个区块来定义。</p>\n<p>在确定并显示一条曲线的具体位置时，需要使用一种算法。一种比较直观的想法就是，将平面细分成小块，通过计算验证曲线是否在这个小块内部。但是这种算法也有一个十分致命的缺点— 复杂度极高，计算量极大。而事实上，平面上大多数的区域是空白的，我们不需要对其每一个小块进行检测。仿照二分法的思路我们可以按如下方法进行。</p>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 确保使用的是等宽字体，用嵌入代码=_=</span></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">3-------2       </span></span><br><span class=\"line\"><span class=\"string\">|       |</span></span><br><span class=\"line\"><span class=\"string\">|       |</span></span><br><span class=\"line\"><span class=\"string\">|       |</span></span><br><span class=\"line\"><span class=\"string\">0-------1</span></span><br><span class=\"line\"><span class=\"string\">检查曲线是否在内，若是，此节点为枝，分为四个小区域，作为四个儿子；若否，此节点为叶，没有儿子。</span></span><br><span class=\"line\"><span class=\"string\">=&gt;</span></span><br><span class=\"line\"><span class=\"string\">3---7---2</span></span><br><span class=\"line\"><span class=\"string\">|   |   |</span></span><br><span class=\"line\"><span class=\"string\">8---4---6</span></span><br><span class=\"line\"><span class=\"string\">|   |   |</span></span><br><span class=\"line\"><span class=\"string\">0---5---1</span></span><br><span class=\"line\"><span class=\"string\">以此分别对四个儿子执行以上操作，直到达到所需精度。</span></span><br><span class=\"line\"><span class=\"string\">=&gt;</span></span><br><span class=\"line\"><span class=\"string\">...</span></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br></pre></td></tr></tbody></table></figure>\n\n<p>按照这个数据结构，若曲线不在内部，则将节点设为叶，无需继续细分。寻找一个点的话复杂度是O(logn)。</p>\n<p>在实验课的内容里，用python进行数据处理，用paraview进行数据可视化。代码其实挺简单（算法就不难= =），实验课还没结束暂时不附代码了，附一个效果图。</p>\n<p><img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/QTree.png?imageMogr2/thumbnail/!50p\" alt=\"QTree\"></p>\n","site":{"data":{"recommended_posts":{"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"}],"https://LorrinWWW.github.io/posts/complexity/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/hexo-with-latex/":[{"title":"区块链时代的最正确的打赏姿势——在Hexo博客配置Coinhive网页挖矿","permalink":"http://www.davidfnck.com/blockchain/mine-xmr-in-hexo-by-coinhive.html/"}],"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/":[{"title":"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-statistic/":[{"title":"贝叶斯估计 Bayes estimation","permalink":"https://LorrinWWW.github.io/posts/Bayes-estimation/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"}],"https://LorrinWWW.github.io/posts/[2018.2.5]Nested-LSTMs/":[{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"}],"https://LorrinWWW.github.io/posts/datamining-pretreatment/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Several models for knowledge graph representing and completing 几个知识图谱模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/"}],"https://LorrinWWW.github.io/posts/compression/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/Sobolev-space/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/proba-ch5/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch3/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/proba-ch1/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/ML-CNN/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/EDP-basic-models/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/machine-learning/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/datamining-class-pred/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction/":[],"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/":[{"title":"OS notes","permalink":"https://LorrinWWW.github.io/posts/OS-notes/"}],"https://LorrinWWW.github.io/posts/projet-enjeu-plugin-chrome-101/":[{"title":"Chrome油猴脚本","permalink":"http://gubangzhong.cn/2017/06/13/Chrome油猴脚本/"}],"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/":[{"title":"MongoDB, Docker and Python","permalink":"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/":[{"title":"Hands on Scrapy","permalink":"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/OS-notes/":[{"title":"learning OS and building LorriOS","permalink":"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/"}],"https://LorrinWWW.github.io/posts/Bayes-estimation/":[{"title":"Note of statistic","permalink":"https://LorrinWWW.github.io/posts/Note-of-statistic/"}],"https://LorrinWWW.github.io/posts/management-of-the-firm/":[],"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"}],"https://LorrinWWW.github.io/posts/datamining-qualitative-induction/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-finite-element-method/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/QuadTree/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/proba-ch4/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch2/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/":[{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/Note-of-NLP/":[{"title":"Generative Adversarial Network","permalink":"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/"},{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"}],"https://LorrinWWW.github.io/posts/Note-of-datamining/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"数据挖掘-分类与预测","permalink":"https://LorrinWWW.github.io/posts/datamining-class-pred/"},{"title":"数据挖掘-预处理","permalink":"https://LorrinWWW.github.io/posts/datamining-pretreatment/"}],"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/Note-of-probability/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/participe-present-et-gerondif/":[],"https://LorrinWWW.github.io/posts/Hilbert-space/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/proba-ch6/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}]}}},"excerpt":"","more":"<p>在ECP的实验课用到了四叉树，故来记录一下。</p>\n<p>四叉树是一种数据结构，每一个节点有四个孩子。一般需要用到四叉树的情况往往是二位平面，通过把区域分成四个区块来定义。</p>\n<p>在确定并显示一条曲线的具体位置时，需要使用一种算法。一种比较直观的想法就是，将平面细分成小块，通过计算验证曲线是否在这个小块内部。但是这种算法也有一个十分致命的缺点— 复杂度极高，计算量极大。而事实上，平面上大多数的区域是空白的，我们不需要对其每一个小块进行检测。仿照二分法的思路我们可以按如下方法进行。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 确保使用的是等宽字体，用嵌入代码=_=</span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">3-------2       </span></span><br><span class=\"line\"><span class=\"string\">|       |</span></span><br><span class=\"line\"><span class=\"string\">|       |</span></span><br><span class=\"line\"><span class=\"string\">|       |</span></span><br><span class=\"line\"><span class=\"string\">0-------1</span></span><br><span class=\"line\"><span class=\"string\">检查曲线是否在内，若是，此节点为枝，分为四个小区域，作为四个儿子；若否，此节点为叶，没有儿子。</span></span><br><span class=\"line\"><span class=\"string\">=&gt;</span></span><br><span class=\"line\"><span class=\"string\">3---7---2</span></span><br><span class=\"line\"><span class=\"string\">|   |   |</span></span><br><span class=\"line\"><span class=\"string\">8---4---6</span></span><br><span class=\"line\"><span class=\"string\">|   |   |</span></span><br><span class=\"line\"><span class=\"string\">0---5---1</span></span><br><span class=\"line\"><span class=\"string\">以此分别对四个儿子执行以上操作，直到达到所需精度。</span></span><br><span class=\"line\"><span class=\"string\">=&gt;</span></span><br><span class=\"line\"><span class=\"string\">...</span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>\n\n<p>按照这个数据结构，若曲线不在内部，则将节点设为叶，无需继续细分。寻找一个点的话复杂度是O(logn)。</p>\n<p>在实验课的内容里，用python进行数据处理，用paraview进行数据可视化。代码其实挺简单（算法就不难= =），实验课还没结束暂时不附代码了，附一个效果图。</p>\n<p><img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/QTree.png?imageMogr2/thumbnail/!50p\" alt=\"QTree\"></p>\n"},{"title":"Sobolev space","date":"2017-02-11T06:37:53.000Z","_content":"\n# 索伯列夫空间 Sobolev space\n\n## 感性认识\n\n摘自wiki：\n\n>数学上，一个索博列夫空间是一个由函数组成的赋范向量空间，对于某个给定的p ≥ 1，它对一个函数f和它的直到某个k阶导数加上有限Lp范数的这个条件。\n\n我们通常在C1(函数1阶可导并且1阶导数连续)上研究微分方程的解，但是在偏微分方程上并不是特别方便，人们便开始研究索伯列夫空间。\n\n## 范数\n\n$$\n||f||_{k,p} = (\\sum_{i=0}^k{||f^{(i)}||_p^p})^{1/p} \\\\\n=||f^{(i)}||_p +||f||_p\n$$\n\n赋予了范数的$W^{k,p}$是一个完备空间。\n\n## $H^k$\n\n一些记号：\n$$\nH^k = W^{k,2} \\\\\nH^1(\\Omega) := \\{ v\\in L^2(\\Omega) : \\nabla v \\in (L^2(\\Omega))^d \\} \\\\ \\\\\n(.,.)_{H^1} : (u,v) \\in H^1 \\times H^1 \\mapsto (u,v)_{L^2} +(\\nabla u,\\nabla v)_{L^2} \\\\\n\\qquad \\qquad \\qquad = \\int_\\Omega {uv} + \\sum_{i=1}^d{\\int_ \\Omega {\\partial_{x_i}{u} \\partial_{x_i}{v}}}\n$$\n\n范数和半范数：\n$$\n||.||_{H^1}: v \\mapsto \\sqrt{||v||_{L^2}^2+||\\nabla v||_{L^2}^2} \\\\\n|.|_{H^1(\\Omega)} := ||\\nabla .||_{L^2(\\Omega)} \\\\\n||.||_{H_0^1} := |.|_{H^1}\n$$\n","source":"_posts/Sobolev-space.md","raw":"---\ntitle: Sobolev space\ndate: 2017-02-11 14:37:53\ncategories: [math, unfinished]\ntags: [Sobolev, analyse, math, EDP]\n---\n\n# 索伯列夫空间 Sobolev space\n\n## 感性认识\n\n摘自wiki：\n\n>数学上，一个索博列夫空间是一个由函数组成的赋范向量空间，对于某个给定的p ≥ 1，它对一个函数f和它的直到某个k阶导数加上有限Lp范数的这个条件。\n\n我们通常在C1(函数1阶可导并且1阶导数连续)上研究微分方程的解，但是在偏微分方程上并不是特别方便，人们便开始研究索伯列夫空间。\n\n## 范数\n\n$$\n||f||_{k,p} = (\\sum_{i=0}^k{||f^{(i)}||_p^p})^{1/p} \\\\\n=||f^{(i)}||_p +||f||_p\n$$\n\n赋予了范数的$W^{k,p}$是一个完备空间。\n\n## $H^k$\n\n一些记号：\n$$\nH^k = W^{k,2} \\\\\nH^1(\\Omega) := \\{ v\\in L^2(\\Omega) : \\nabla v \\in (L^2(\\Omega))^d \\} \\\\ \\\\\n(.,.)_{H^1} : (u,v) \\in H^1 \\times H^1 \\mapsto (u,v)_{L^2} +(\\nabla u,\\nabla v)_{L^2} \\\\\n\\qquad \\qquad \\qquad = \\int_\\Omega {uv} + \\sum_{i=1}^d{\\int_ \\Omega {\\partial_{x_i}{u} \\partial_{x_i}{v}}}\n$$\n\n范数和半范数：\n$$\n||.||_{H^1}: v \\mapsto \\sqrt{||v||_{L^2}^2+||\\nabla v||_{L^2}^2} \\\\\n|.|_{H^1(\\Omega)} := ||\\nabla .||_{L^2(\\Omega)} \\\\\n||.||_{H_0^1} := |.|_{H^1}\n$$\n","slug":"Sobolev-space","published":1,"updated":"2017-02-11T16:48:47.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckw4qufpe001jgwtl4fjl5y7q","content":"<h1 id=\"索伯列夫空间-Sobolev-space\"><a href=\"#索伯列夫空间-Sobolev-space\" class=\"headerlink\" title=\"索伯列夫空间 Sobolev space\"></a>索伯列夫空间 Sobolev space</h1><h2 id=\"感性认识\"><a href=\"#感性认识\" class=\"headerlink\" title=\"感性认识\"></a>感性认识</h2><p>摘自wiki：</p>\n<blockquote>\n<p>数学上，一个索博列夫空间是一个由函数组成的赋范向量空间，对于某个给定的p ≥ 1，它对一个函数f和它的直到某个k阶导数加上有限Lp范数的这个条件。</p>\n</blockquote>\n<p>我们通常在C1(函数1阶可导并且1阶导数连续)上研究微分方程的解，但是在偏微分方程上并不是特别方便，人们便开始研究索伯列夫空间。</p>\n<h2 id=\"范数\"><a href=\"#范数\" class=\"headerlink\" title=\"范数\"></a>范数</h2><p>$$<br>||f||<em>{k,p} = (\\sum</em>{i=0}^k{||f^{(i)}||_p^p})^{1/p} \\<br>=||f^{(i)}||_p +||f||_p<br>$$</p>\n<p>赋予了范数的$W^{k,p}$是一个完备空间。</p>\n<h2 id=\"H-k\"><a href=\"#H-k\" class=\"headerlink\" title=\"$H^k$\"></a>$H^k$</h2><p>一些记号：<br>$$<br>H^k = W^{k,2} \\<br>H^1(\\Omega) := { v\\in L^2(\\Omega) : \\nabla v \\in (L^2(\\Omega))^d } \\ \\<br>(.,.)<em>{H^1} : (u,v) \\in H^1 \\times H^1 \\mapsto (u,v)</em>{L^2} +(\\nabla u,\\nabla v)<em>{L^2} \\<br>\\qquad \\qquad \\qquad = \\int_\\Omega {uv} + \\sum</em>{i=1}^d{\\int_ \\Omega {\\partial_{x_i}{u} \\partial_{x_i}{v}}}<br>$$</p>\n<p>范数和半范数：<br>$$<br>||.||<em>{H^1}: v \\mapsto \\sqrt{||v||</em>{L^2}^2+||\\nabla v||<em>{L^2}^2} \\<br>|.|</em>{H^1(\\Omega)} := ||\\nabla .||<em>{L^2(\\Omega)} \\<br>||.||</em>{H_0^1} := |.|_{H^1}<br>$$</p>\n","site":{"data":{"recommended_posts":{"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"}],"https://LorrinWWW.github.io/posts/complexity/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/hexo-with-latex/":[{"title":"区块链时代的最正确的打赏姿势——在Hexo博客配置Coinhive网页挖矿","permalink":"http://www.davidfnck.com/blockchain/mine-xmr-in-hexo-by-coinhive.html/"}],"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/":[{"title":"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-statistic/":[{"title":"贝叶斯估计 Bayes estimation","permalink":"https://LorrinWWW.github.io/posts/Bayes-estimation/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"}],"https://LorrinWWW.github.io/posts/[2018.2.5]Nested-LSTMs/":[{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"}],"https://LorrinWWW.github.io/posts/datamining-pretreatment/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Several models for knowledge graph representing and completing 几个知识图谱模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/"}],"https://LorrinWWW.github.io/posts/compression/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/Sobolev-space/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/proba-ch5/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch3/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/proba-ch1/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/ML-CNN/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/EDP-basic-models/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/machine-learning/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/datamining-class-pred/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction/":[],"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/":[{"title":"OS notes","permalink":"https://LorrinWWW.github.io/posts/OS-notes/"}],"https://LorrinWWW.github.io/posts/projet-enjeu-plugin-chrome-101/":[{"title":"Chrome油猴脚本","permalink":"http://gubangzhong.cn/2017/06/13/Chrome油猴脚本/"}],"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/":[{"title":"MongoDB, Docker and Python","permalink":"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/":[{"title":"Hands on Scrapy","permalink":"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/OS-notes/":[{"title":"learning OS and building LorriOS","permalink":"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/"}],"https://LorrinWWW.github.io/posts/Bayes-estimation/":[{"title":"Note of statistic","permalink":"https://LorrinWWW.github.io/posts/Note-of-statistic/"}],"https://LorrinWWW.github.io/posts/management-of-the-firm/":[],"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"}],"https://LorrinWWW.github.io/posts/datamining-qualitative-induction/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-finite-element-method/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/QuadTree/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/proba-ch4/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch2/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/":[{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/Note-of-NLP/":[{"title":"Generative Adversarial Network","permalink":"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/"},{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"}],"https://LorrinWWW.github.io/posts/Note-of-datamining/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"数据挖掘-分类与预测","permalink":"https://LorrinWWW.github.io/posts/datamining-class-pred/"},{"title":"数据挖掘-预处理","permalink":"https://LorrinWWW.github.io/posts/datamining-pretreatment/"}],"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/Note-of-probability/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/participe-present-et-gerondif/":[],"https://LorrinWWW.github.io/posts/Hilbert-space/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/proba-ch6/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}]}}},"excerpt":"","more":"<h1 id=\"索伯列夫空间-Sobolev-space\"><a href=\"#索伯列夫空间-Sobolev-space\" class=\"headerlink\" title=\"索伯列夫空间 Sobolev space\"></a>索伯列夫空间 Sobolev space</h1><h2 id=\"感性认识\"><a href=\"#感性认识\" class=\"headerlink\" title=\"感性认识\"></a>感性认识</h2><p>摘自wiki：</p>\n<blockquote>\n<p>数学上，一个索博列夫空间是一个由函数组成的赋范向量空间，对于某个给定的p ≥ 1，它对一个函数f和它的直到某个k阶导数加上有限Lp范数的这个条件。</p>\n</blockquote>\n<p>我们通常在C1(函数1阶可导并且1阶导数连续)上研究微分方程的解，但是在偏微分方程上并不是特别方便，人们便开始研究索伯列夫空间。</p>\n<h2 id=\"范数\"><a href=\"#范数\" class=\"headerlink\" title=\"范数\"></a>范数</h2><p>$$<br>||f||<em>{k,p} = (\\sum</em>{i=0}^k{||f^{(i)}||_p^p})^{1/p} \\<br>=||f^{(i)}||_p +||f||_p<br>$$</p>\n<p>赋予了范数的$W^{k,p}$是一个完备空间。</p>\n<h2 id=\"H-k\"><a href=\"#H-k\" class=\"headerlink\" title=\"$H^k$\"></a>$H^k$</h2><p>一些记号：<br>$$<br>H^k = W^{k,2} \\<br>H^1(\\Omega) := { v\\in L^2(\\Omega) : \\nabla v \\in (L^2(\\Omega))^d } \\ \\<br>(.,.)<em>{H^1} : (u,v) \\in H^1 \\times H^1 \\mapsto (u,v)</em>{L^2} +(\\nabla u,\\nabla v)<em>{L^2} \\<br>\\qquad \\qquad \\qquad = \\int_\\Omega {uv} + \\sum</em>{i=1}^d{\\int_ \\Omega {\\partial_{x_i}{u} \\partial_{x_i}{v}}}<br>$$</p>\n<p>范数和半范数：<br>$$<br>||.||<em>{H^1}: v \\mapsto \\sqrt{||v||</em>{L^2}^2+||\\nabla v||<em>{L^2}^2} \\<br>|.|</em>{H^1(\\Omega)} := ||\\nabla .||<em>{L^2(\\Omega)} \\<br>||.||</em>{H_0^1} := |.|_{H^1}<br>$$</p>\n"},{"title":"实体解析 Entity resolution","date":"2017-12-10T00:00:00.000Z","_content":"\n## 1. Entity resolution\n\n###1.1 Sequence labeling\n\n我们通常在ML中把Named Entity Recognition任务认为是一个Sequence labeling任务，事实上很多nlp任务都可以被转化为sequence labeling。暑假实习的时候也在这方面看了一些文献。目前业内比较主流的解决方案是RNN-CRF模型，一般来说分为：\n\n- Embedding layer\n- Bi-directional RNN (usually LSTM) layer\n- Tanh hidden layer\n- CRF layer\n\n从结果上来看，该模型对大多数sequence labeling任务有较好的效果，如named entity recognition等。但是对于一些更灵活的标注任务（如暑假实习时，我曾试图将event recognition转化为seq labeling任务），尤其是在训练集不足的情况下，往往效果还是不能令人满意。\n\n#### 1.1.1 应用Attention \n\n> [1]在 RNN-CRF 模型结构基础上，重点改进了词向量与字符向量的拼接。使用 attention 机制将原始的字符向量和词向量拼接改进为了权重求和，使用两层传统神经网络隐层来学习 attention 的权值，这样就使得模型可以动态地利用词向量和字符向量信息。实验结果表明比原始的拼接方法效果更好。\n>\n> [2]在原始 BiLSTM-CRF 模型上，加入了音韵特征，并在字符向量上使用 attention 机制来学习关注更有效的字符。\n>\n> ​                      — from paperweekly\n\n#### 1.1.2 使用少量标注数据 \n\n深度学习方法一般需要大量标注数据，但是在一些领域很难有海量的标注数据。所以在基于神经网络结构方法中如何使用少量标注数据也是一个重点。\n\n- [Deep Active Learning for Named Entity Recognition](https://openreview.net/forum?id=ry018WZAZ)[7]\n\n  ICLR 2018看到的paper。这片文章把active learning应用到了CNN-CNN-LSTM模型，用于处理NER问题，也就是seq labeling问题。它能够仅使用25%的数据，达到state-of-the-art的水平。\n\n  这篇paper总结了很多做seq labeling的方法，本身的思路也深入简出。decoder使用了LSTM而不是常用的CRF，发现LSTM比CRF有一些的优势。同时该文也证明了active learning能提高seq labeling的表现。\n\n- Semi-supervised sequence tagging with bidirectional language models[4]\n\n  该论文使用海量无标注语料库训练了一个双向神经网络语言模型，然后使用这个训练好的语言模型来获取当前要标注词的语言模型向量（LM embedding），然后将该向量作为特征加入到原始的双向 RNN-CRF 模型中。\n\n  实验结果表明，在少量标注数据上，加入这个语言模型向量能够大幅度提高 NER 效果，即使在大量的标注训练数据上，加入这个语言模型向量仍能提供原始 RNN-CRF 模型的效果。\n\n### 1.2 Relation extraction\n\n实体的关系的抽取方法可以简单分为两类：一类是pipeline抽取方法。另一类是并行或联合抽取方法。\n\npipeline方法需要先识别entity，然后采用关系抽取模型得到实体对之间的关系。缺点是实体识别的结果会进一步影响关系抽取的结果，导致误差累积，也降低信息使用率，分开抽取也造成了信息冗余。\n\n[9]提出了一种联合实体检测参数共享的关系抽取模型，模型中有两个双向的LSTM-RNN，一个是基于word sequence（bidirectional sequential LSTM-RNNs），主要用于实体检测；一个基于Tree Structures （bidirectional tree- structured LSTM-RNNs），主要用于关系抽取；后者堆在前者上，前者的输出和隐含层作为后者输入的一部分。下图为整个模型的结构图：\n\n![LSTM-RNNs](https://pic3.zhimg.com/v2-8a44b362fb60fff951dbfaa2bc4469f3_r.jpg)\n\n该paper用了参数共享，实体的识别过程和关系的判断过程并没有交互的过程，还无法称其为真正意义上的joint。\n\n[7]提出了一种端到端的基于序列标注的的方法进行关系抽取，它将实体发现任务和关系抽取任务转化为一个标注任务。在 encoder-decoder 框架下，采用主流的 bi-lstm 为 encoder，lstm 为 decoder。对每个词标注上 BIEM+关系类型+实体的序号。目前这种思路有人测试下来发现，总的来说，联合抽取比pipeline的方法好，序列标注联合抽取要比其他联合抽取方法好，然而目前实体关系抽取任务的 F1 值仍然不到 0.5。因此虽然效果还可以，但是就实际使用还有一段距离。\n\n此外，该模型还无法处理一个句子有多个关系三元组，和一个实体在多个关系中出现的一对多的问题。一个改进方向是把最后的softmax改成多分类器以实现多标签，这样就能实现一个实体的多关系抽取。其次，该方法是非开放域的关系抽取，关系词是从预定义的关系集里抽取的。\n\n## 2. Others\n\n这里主要是有相关性不强但挺有意思，或泛用性很强的一些文章。\n\n1. Ngram2vec[5]\n\n   一个词向量生成的方法，基于经典的 word2vec 的思想，在其之上加入了 ngram 的共现信息，取得了更好的结果。代码实现：[https://github.com/zhezhaoa/ngram2vec/](http://link.zhihu.com/?target=https%3A//github.com/zhezhaoa/ngram2vec/)\n\n2. [AutoML](https://research.googleblog.com/2017/05/using-machine-learning-to-explore.html)\n\n   google在五月份发布的模型，主要思想是将reinforcement learning应用在神经网络的构建、参数确定上。我们对网络进行测试，将反馈的结果返回到控制器中，以此来帮助提升下一次循环中的训练设定。生成新的架构、测试、把反馈传送给控制器以吸取经验。以此往复以得到更优的结构。\n\n3. Introspection:Accelerating Neural Network Training By Learning Weight Evolution[6]\n\n   这个本质上是meta learning的问题。他们训练了一个网络，网络的输入是某个时间点之前随机选取的4个旧参数的值，输出就是新的参数。因此可以将训练其他模型时得到的这个网络，用于加速其他模型。他们训练了mnist的两层conv net，用该任务的参数更新历史训练网络。他们最后将pretrained好的这个网络用于更新大网络，结果都能更好。\n\n## Reference\n\n[1] Rei, M., Crichton, G. K., & Pyysalo, S. (2016). Attending to Characters in Neural Sequence Labeling Models. *arXiv preprint arXiv:1611.04361*.\n\n[2] Mortensen, A. B. D., & Carbonell, C. D. J. G. (2016). Phonologically aware neural model for named entity recognition in low resource transfer settings.\n\n[3] Yang, Z., Salakhutdinov, R., & Cohen, W. W. (2017). Transfer learning for sequence tagging with hierarchical recurrent networks. *arXiv preprint arXiv:1703.06345*.\n\n[4] Peters, M. E., Ammar, W., Bhagavatula, C., & Power, R. (2017). Semi-supervised sequence tagging with bidirectional language models. *arXiv preprint arXiv:1705.00108*.\n\n[5] Zhao, Z., Liu, T., Li, S., Li, B., & Du, X. (2017). Ngram2vec: Learning Improved Word Representations from Ngram Co-occurrence Statistics. In *Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing* (pp. 244-253).\n\n[6] Sinha, A., Sarkar, M., Mukherjee, A., & Krishnamurthy, B. (2017). Introspection: Accelerating Neural Network Training By Learning Weight Evolution. *arXiv preprint arXiv:1704.04959*.\n\n[7] Shen, Yanyao, Yun, Hyokun, Lipton, Zachary C, Kronrod, Yakov, & Anandkumar, Animashree. (2017). Deep active learning for named entity recognition.\n\n[8] Zheng, S., Wang, F., Bao, H., Hao, Y., Zhou, P., & Xu, B. (2017). Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme. *arXiv preprint arXiv:1706.05075*.\n\n[9] Miwa, M., & Bansal, M. (2016). End-to-end relation extraction using lstms on sequences and tree structures. *arXiv preprint arXiv:1601.00770*.","source":"_posts/[2017.12.10]Entity-resolution.md","raw":"---\ntitle: 实体解析 Entity resolution\ndate: 2017-12-10 08:00:00\ncategories: [research]\ntags: [entity-resolution, sequence-labeling, relation-extraction, LSTM, RNN]\n---\n\n## 1. Entity resolution\n\n###1.1 Sequence labeling\n\n我们通常在ML中把Named Entity Recognition任务认为是一个Sequence labeling任务，事实上很多nlp任务都可以被转化为sequence labeling。暑假实习的时候也在这方面看了一些文献。目前业内比较主流的解决方案是RNN-CRF模型，一般来说分为：\n\n- Embedding layer\n- Bi-directional RNN (usually LSTM) layer\n- Tanh hidden layer\n- CRF layer\n\n从结果上来看，该模型对大多数sequence labeling任务有较好的效果，如named entity recognition等。但是对于一些更灵活的标注任务（如暑假实习时，我曾试图将event recognition转化为seq labeling任务），尤其是在训练集不足的情况下，往往效果还是不能令人满意。\n\n#### 1.1.1 应用Attention \n\n> [1]在 RNN-CRF 模型结构基础上，重点改进了词向量与字符向量的拼接。使用 attention 机制将原始的字符向量和词向量拼接改进为了权重求和，使用两层传统神经网络隐层来学习 attention 的权值，这样就使得模型可以动态地利用词向量和字符向量信息。实验结果表明比原始的拼接方法效果更好。\n>\n> [2]在原始 BiLSTM-CRF 模型上，加入了音韵特征，并在字符向量上使用 attention 机制来学习关注更有效的字符。\n>\n> ​                      — from paperweekly\n\n#### 1.1.2 使用少量标注数据 \n\n深度学习方法一般需要大量标注数据，但是在一些领域很难有海量的标注数据。所以在基于神经网络结构方法中如何使用少量标注数据也是一个重点。\n\n- [Deep Active Learning for Named Entity Recognition](https://openreview.net/forum?id=ry018WZAZ)[7]\n\n  ICLR 2018看到的paper。这片文章把active learning应用到了CNN-CNN-LSTM模型，用于处理NER问题，也就是seq labeling问题。它能够仅使用25%的数据，达到state-of-the-art的水平。\n\n  这篇paper总结了很多做seq labeling的方法，本身的思路也深入简出。decoder使用了LSTM而不是常用的CRF，发现LSTM比CRF有一些的优势。同时该文也证明了active learning能提高seq labeling的表现。\n\n- Semi-supervised sequence tagging with bidirectional language models[4]\n\n  该论文使用海量无标注语料库训练了一个双向神经网络语言模型，然后使用这个训练好的语言模型来获取当前要标注词的语言模型向量（LM embedding），然后将该向量作为特征加入到原始的双向 RNN-CRF 模型中。\n\n  实验结果表明，在少量标注数据上，加入这个语言模型向量能够大幅度提高 NER 效果，即使在大量的标注训练数据上，加入这个语言模型向量仍能提供原始 RNN-CRF 模型的效果。\n\n### 1.2 Relation extraction\n\n实体的关系的抽取方法可以简单分为两类：一类是pipeline抽取方法。另一类是并行或联合抽取方法。\n\npipeline方法需要先识别entity，然后采用关系抽取模型得到实体对之间的关系。缺点是实体识别的结果会进一步影响关系抽取的结果，导致误差累积，也降低信息使用率，分开抽取也造成了信息冗余。\n\n[9]提出了一种联合实体检测参数共享的关系抽取模型，模型中有两个双向的LSTM-RNN，一个是基于word sequence（bidirectional sequential LSTM-RNNs），主要用于实体检测；一个基于Tree Structures （bidirectional tree- structured LSTM-RNNs），主要用于关系抽取；后者堆在前者上，前者的输出和隐含层作为后者输入的一部分。下图为整个模型的结构图：\n\n![LSTM-RNNs](https://pic3.zhimg.com/v2-8a44b362fb60fff951dbfaa2bc4469f3_r.jpg)\n\n该paper用了参数共享，实体的识别过程和关系的判断过程并没有交互的过程，还无法称其为真正意义上的joint。\n\n[7]提出了一种端到端的基于序列标注的的方法进行关系抽取，它将实体发现任务和关系抽取任务转化为一个标注任务。在 encoder-decoder 框架下，采用主流的 bi-lstm 为 encoder，lstm 为 decoder。对每个词标注上 BIEM+关系类型+实体的序号。目前这种思路有人测试下来发现，总的来说，联合抽取比pipeline的方法好，序列标注联合抽取要比其他联合抽取方法好，然而目前实体关系抽取任务的 F1 值仍然不到 0.5。因此虽然效果还可以，但是就实际使用还有一段距离。\n\n此外，该模型还无法处理一个句子有多个关系三元组，和一个实体在多个关系中出现的一对多的问题。一个改进方向是把最后的softmax改成多分类器以实现多标签，这样就能实现一个实体的多关系抽取。其次，该方法是非开放域的关系抽取，关系词是从预定义的关系集里抽取的。\n\n## 2. Others\n\n这里主要是有相关性不强但挺有意思，或泛用性很强的一些文章。\n\n1. Ngram2vec[5]\n\n   一个词向量生成的方法，基于经典的 word2vec 的思想，在其之上加入了 ngram 的共现信息，取得了更好的结果。代码实现：[https://github.com/zhezhaoa/ngram2vec/](http://link.zhihu.com/?target=https%3A//github.com/zhezhaoa/ngram2vec/)\n\n2. [AutoML](https://research.googleblog.com/2017/05/using-machine-learning-to-explore.html)\n\n   google在五月份发布的模型，主要思想是将reinforcement learning应用在神经网络的构建、参数确定上。我们对网络进行测试，将反馈的结果返回到控制器中，以此来帮助提升下一次循环中的训练设定。生成新的架构、测试、把反馈传送给控制器以吸取经验。以此往复以得到更优的结构。\n\n3. Introspection:Accelerating Neural Network Training By Learning Weight Evolution[6]\n\n   这个本质上是meta learning的问题。他们训练了一个网络，网络的输入是某个时间点之前随机选取的4个旧参数的值，输出就是新的参数。因此可以将训练其他模型时得到的这个网络，用于加速其他模型。他们训练了mnist的两层conv net，用该任务的参数更新历史训练网络。他们最后将pretrained好的这个网络用于更新大网络，结果都能更好。\n\n## Reference\n\n[1] Rei, M., Crichton, G. K., & Pyysalo, S. (2016). Attending to Characters in Neural Sequence Labeling Models. *arXiv preprint arXiv:1611.04361*.\n\n[2] Mortensen, A. B. D., & Carbonell, C. D. J. G. (2016). Phonologically aware neural model for named entity recognition in low resource transfer settings.\n\n[3] Yang, Z., Salakhutdinov, R., & Cohen, W. W. (2017). Transfer learning for sequence tagging with hierarchical recurrent networks. *arXiv preprint arXiv:1703.06345*.\n\n[4] Peters, M. E., Ammar, W., Bhagavatula, C., & Power, R. (2017). Semi-supervised sequence tagging with bidirectional language models. *arXiv preprint arXiv:1705.00108*.\n\n[5] Zhao, Z., Liu, T., Li, S., Li, B., & Du, X. (2017). Ngram2vec: Learning Improved Word Representations from Ngram Co-occurrence Statistics. In *Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing* (pp. 244-253).\n\n[6] Sinha, A., Sarkar, M., Mukherjee, A., & Krishnamurthy, B. (2017). Introspection: Accelerating Neural Network Training By Learning Weight Evolution. *arXiv preprint arXiv:1704.04959*.\n\n[7] Shen, Yanyao, Yun, Hyokun, Lipton, Zachary C, Kronrod, Yakov, & Anandkumar, Animashree. (2017). Deep active learning for named entity recognition.\n\n[8] Zheng, S., Wang, F., Bao, H., Hao, Y., Zhou, P., & Xu, B. (2017). Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme. *arXiv preprint arXiv:1706.05075*.\n\n[9] Miwa, M., & Bansal, M. (2016). End-to-end relation extraction using lstms on sequences and tree structures. *arXiv preprint arXiv:1601.00770*.","slug":"[2017.12.10]Entity-resolution","published":1,"updated":"2018-01-27T11:07:30.140Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckw4qufpf001ngwtlek9x1j7n","content":"<h2 id=\"1-Entity-resolution\"><a href=\"#1-Entity-resolution\" class=\"headerlink\" title=\"1. Entity resolution\"></a>1. Entity resolution</h2><p>###1.1 Sequence labeling</p>\n<p>我们通常在ML中把Named Entity Recognition任务认为是一个Sequence labeling任务，事实上很多nlp任务都可以被转化为sequence labeling。暑假实习的时候也在这方面看了一些文献。目前业内比较主流的解决方案是RNN-CRF模型，一般来说分为：</p>\n<ul>\n<li>Embedding layer</li>\n<li>Bi-directional RNN (usually LSTM) layer</li>\n<li>Tanh hidden layer</li>\n<li>CRF layer</li>\n</ul>\n<p>从结果上来看，该模型对大多数sequence labeling任务有较好的效果，如named entity recognition等。但是对于一些更灵活的标注任务（如暑假实习时，我曾试图将event recognition转化为seq labeling任务），尤其是在训练集不足的情况下，往往效果还是不能令人满意。</p>\n<h4 id=\"1-1-1-应用Attention\"><a href=\"#1-1-1-应用Attention\" class=\"headerlink\" title=\"1.1.1 应用Attention\"></a>1.1.1 应用Attention</h4><blockquote>\n<p>[1]在 RNN-CRF 模型结构基础上，重点改进了词向量与字符向量的拼接。使用 attention 机制将原始的字符向量和词向量拼接改进为了权重求和，使用两层传统神经网络隐层来学习 attention 的权值，这样就使得模型可以动态地利用词向量和字符向量信息。实验结果表明比原始的拼接方法效果更好。</p>\n<p>[2]在原始 BiLSTM-CRF 模型上，加入了音韵特征，并在字符向量上使用 attention 机制来学习关注更有效的字符。</p>\n<p>​                      — from paperweekly</p>\n</blockquote>\n<h4 id=\"1-1-2-使用少量标注数据\"><a href=\"#1-1-2-使用少量标注数据\" class=\"headerlink\" title=\"1.1.2 使用少量标注数据\"></a>1.1.2 使用少量标注数据</h4><p>深度学习方法一般需要大量标注数据，但是在一些领域很难有海量的标注数据。所以在基于神经网络结构方法中如何使用少量标注数据也是一个重点。</p>\n<ul>\n<li><p><a href=\"https://openreview.net/forum?id=ry018WZAZ\">Deep Active Learning for Named Entity Recognition</a>[7]</p>\n<p>ICLR 2018看到的paper。这片文章把active learning应用到了CNN-CNN-LSTM模型，用于处理NER问题，也就是seq labeling问题。它能够仅使用25%的数据，达到state-of-the-art的水平。</p>\n<p>这篇paper总结了很多做seq labeling的方法，本身的思路也深入简出。decoder使用了LSTM而不是常用的CRF，发现LSTM比CRF有一些的优势。同时该文也证明了active learning能提高seq labeling的表现。</p>\n</li>\n<li><p>Semi-supervised sequence tagging with bidirectional language models[4]</p>\n<p>该论文使用海量无标注语料库训练了一个双向神经网络语言模型，然后使用这个训练好的语言模型来获取当前要标注词的语言模型向量（LM embedding），然后将该向量作为特征加入到原始的双向 RNN-CRF 模型中。</p>\n<p>实验结果表明，在少量标注数据上，加入这个语言模型向量能够大幅度提高 NER 效果，即使在大量的标注训练数据上，加入这个语言模型向量仍能提供原始 RNN-CRF 模型的效果。</p>\n</li>\n</ul>\n<h3 id=\"1-2-Relation-extraction\"><a href=\"#1-2-Relation-extraction\" class=\"headerlink\" title=\"1.2 Relation extraction\"></a>1.2 Relation extraction</h3><p>实体的关系的抽取方法可以简单分为两类：一类是pipeline抽取方法。另一类是并行或联合抽取方法。</p>\n<p>pipeline方法需要先识别entity，然后采用关系抽取模型得到实体对之间的关系。缺点是实体识别的结果会进一步影响关系抽取的结果，导致误差累积，也降低信息使用率，分开抽取也造成了信息冗余。</p>\n<p>[9]提出了一种联合实体检测参数共享的关系抽取模型，模型中有两个双向的LSTM-RNN，一个是基于word sequence（bidirectional sequential LSTM-RNNs），主要用于实体检测；一个基于Tree Structures （bidirectional tree- structured LSTM-RNNs），主要用于关系抽取；后者堆在前者上，前者的输出和隐含层作为后者输入的一部分。下图为整个模型的结构图：</p>\n<p><img src=\"https://pic3.zhimg.com/v2-8a44b362fb60fff951dbfaa2bc4469f3_r.jpg\" alt=\"LSTM-RNNs\"></p>\n<p>该paper用了参数共享，实体的识别过程和关系的判断过程并没有交互的过程，还无法称其为真正意义上的joint。</p>\n<p>[7]提出了一种端到端的基于序列标注的的方法进行关系抽取，它将实体发现任务和关系抽取任务转化为一个标注任务。在 encoder-decoder 框架下，采用主流的 bi-lstm 为 encoder，lstm 为 decoder。对每个词标注上 BIEM+关系类型+实体的序号。目前这种思路有人测试下来发现，总的来说，联合抽取比pipeline的方法好，序列标注联合抽取要比其他联合抽取方法好，然而目前实体关系抽取任务的 F1 值仍然不到 0.5。因此虽然效果还可以，但是就实际使用还有一段距离。</p>\n<p>此外，该模型还无法处理一个句子有多个关系三元组，和一个实体在多个关系中出现的一对多的问题。一个改进方向是把最后的softmax改成多分类器以实现多标签，这样就能实现一个实体的多关系抽取。其次，该方法是非开放域的关系抽取，关系词是从预定义的关系集里抽取的。</p>\n<h2 id=\"2-Others\"><a href=\"#2-Others\" class=\"headerlink\" title=\"2. Others\"></a>2. Others</h2><p>这里主要是有相关性不强但挺有意思，或泛用性很强的一些文章。</p>\n<ol>\n<li><p>Ngram2vec[5]</p>\n<p>一个词向量生成的方法，基于经典的 word2vec 的思想，在其之上加入了 ngram 的共现信息，取得了更好的结果。代码实现：<a href=\"http://link.zhihu.com/?target=https://github.com/zhezhaoa/ngram2vec/\">https://github.com/zhezhaoa/ngram2vec/</a></p>\n</li>\n<li><p><a href=\"https://research.googleblog.com/2017/05/using-machine-learning-to-explore.html\">AutoML</a></p>\n<p>google在五月份发布的模型，主要思想是将reinforcement learning应用在神经网络的构建、参数确定上。我们对网络进行测试，将反馈的结果返回到控制器中，以此来帮助提升下一次循环中的训练设定。生成新的架构、测试、把反馈传送给控制器以吸取经验。以此往复以得到更优的结构。</p>\n</li>\n<li><p>Introspection:Accelerating Neural Network Training By Learning Weight Evolution[6]</p>\n<p>这个本质上是meta learning的问题。他们训练了一个网络，网络的输入是某个时间点之前随机选取的4个旧参数的值，输出就是新的参数。因此可以将训练其他模型时得到的这个网络，用于加速其他模型。他们训练了mnist的两层conv net，用该任务的参数更新历史训练网络。他们最后将pretrained好的这个网络用于更新大网络，结果都能更好。</p>\n</li>\n</ol>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><p>[1] Rei, M., Crichton, G. K., &amp; Pyysalo, S. (2016). Attending to Characters in Neural Sequence Labeling Models. <em>arXiv preprint arXiv:1611.04361</em>.</p>\n<p>[2] Mortensen, A. B. D., &amp; Carbonell, C. D. J. G. (2016). Phonologically aware neural model for named entity recognition in low resource transfer settings.</p>\n<p>[3] Yang, Z., Salakhutdinov, R., &amp; Cohen, W. W. (2017). Transfer learning for sequence tagging with hierarchical recurrent networks. <em>arXiv preprint arXiv:1703.06345</em>.</p>\n<p>[4] Peters, M. E., Ammar, W., Bhagavatula, C., &amp; Power, R. (2017). Semi-supervised sequence tagging with bidirectional language models. <em>arXiv preprint arXiv:1705.00108</em>.</p>\n<p>[5] Zhao, Z., Liu, T., Li, S., Li, B., &amp; Du, X. (2017). Ngram2vec: Learning Improved Word Representations from Ngram Co-occurrence Statistics. In <em>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</em> (pp. 244-253).</p>\n<p>[6] Sinha, A., Sarkar, M., Mukherjee, A., &amp; Krishnamurthy, B. (2017). Introspection: Accelerating Neural Network Training By Learning Weight Evolution. <em>arXiv preprint arXiv:1704.04959</em>.</p>\n<p>[7] Shen, Yanyao, Yun, Hyokun, Lipton, Zachary C, Kronrod, Yakov, &amp; Anandkumar, Animashree. (2017). Deep active learning for named entity recognition.</p>\n<p>[8] Zheng, S., Wang, F., Bao, H., Hao, Y., Zhou, P., &amp; Xu, B. (2017). Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme. <em>arXiv preprint arXiv:1706.05075</em>.</p>\n<p>[9] Miwa, M., &amp; Bansal, M. (2016). End-to-end relation extraction using lstms on sequences and tree structures. <em>arXiv preprint arXiv:1601.00770</em>.</p>\n","site":{"data":{"recommended_posts":{"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"}],"https://LorrinWWW.github.io/posts/complexity/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/hexo-with-latex/":[{"title":"区块链时代的最正确的打赏姿势——在Hexo博客配置Coinhive网页挖矿","permalink":"http://www.davidfnck.com/blockchain/mine-xmr-in-hexo-by-coinhive.html/"}],"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/":[{"title":"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-statistic/":[{"title":"贝叶斯估计 Bayes estimation","permalink":"https://LorrinWWW.github.io/posts/Bayes-estimation/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"}],"https://LorrinWWW.github.io/posts/[2018.2.5]Nested-LSTMs/":[{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"}],"https://LorrinWWW.github.io/posts/datamining-pretreatment/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Several models for knowledge graph representing and completing 几个知识图谱模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/"}],"https://LorrinWWW.github.io/posts/compression/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/Sobolev-space/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/proba-ch5/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch3/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/proba-ch1/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/ML-CNN/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/EDP-basic-models/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/machine-learning/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/datamining-class-pred/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction/":[],"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/":[{"title":"OS notes","permalink":"https://LorrinWWW.github.io/posts/OS-notes/"}],"https://LorrinWWW.github.io/posts/projet-enjeu-plugin-chrome-101/":[{"title":"Chrome油猴脚本","permalink":"http://gubangzhong.cn/2017/06/13/Chrome油猴脚本/"}],"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/":[{"title":"MongoDB, Docker and Python","permalink":"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/":[{"title":"Hands on Scrapy","permalink":"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/OS-notes/":[{"title":"learning OS and building LorriOS","permalink":"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/"}],"https://LorrinWWW.github.io/posts/Bayes-estimation/":[{"title":"Note of statistic","permalink":"https://LorrinWWW.github.io/posts/Note-of-statistic/"}],"https://LorrinWWW.github.io/posts/management-of-the-firm/":[],"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"}],"https://LorrinWWW.github.io/posts/datamining-qualitative-induction/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-finite-element-method/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/QuadTree/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/proba-ch4/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch2/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/":[{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/Note-of-NLP/":[{"title":"Generative Adversarial Network","permalink":"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/"},{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"}],"https://LorrinWWW.github.io/posts/Note-of-datamining/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"数据挖掘-分类与预测","permalink":"https://LorrinWWW.github.io/posts/datamining-class-pred/"},{"title":"数据挖掘-预处理","permalink":"https://LorrinWWW.github.io/posts/datamining-pretreatment/"}],"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/Note-of-probability/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/participe-present-et-gerondif/":[],"https://LorrinWWW.github.io/posts/Hilbert-space/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/proba-ch6/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}]}}},"excerpt":"","more":"<h2 id=\"1-Entity-resolution\"><a href=\"#1-Entity-resolution\" class=\"headerlink\" title=\"1. Entity resolution\"></a>1. Entity resolution</h2><p>###1.1 Sequence labeling</p>\n<p>我们通常在ML中把Named Entity Recognition任务认为是一个Sequence labeling任务，事实上很多nlp任务都可以被转化为sequence labeling。暑假实习的时候也在这方面看了一些文献。目前业内比较主流的解决方案是RNN-CRF模型，一般来说分为：</p>\n<ul>\n<li>Embedding layer</li>\n<li>Bi-directional RNN (usually LSTM) layer</li>\n<li>Tanh hidden layer</li>\n<li>CRF layer</li>\n</ul>\n<p>从结果上来看，该模型对大多数sequence labeling任务有较好的效果，如named entity recognition等。但是对于一些更灵活的标注任务（如暑假实习时，我曾试图将event recognition转化为seq labeling任务），尤其是在训练集不足的情况下，往往效果还是不能令人满意。</p>\n<h4 id=\"1-1-1-应用Attention\"><a href=\"#1-1-1-应用Attention\" class=\"headerlink\" title=\"1.1.1 应用Attention\"></a>1.1.1 应用Attention</h4><blockquote>\n<p>[1]在 RNN-CRF 模型结构基础上，重点改进了词向量与字符向量的拼接。使用 attention 机制将原始的字符向量和词向量拼接改进为了权重求和，使用两层传统神经网络隐层来学习 attention 的权值，这样就使得模型可以动态地利用词向量和字符向量信息。实验结果表明比原始的拼接方法效果更好。</p>\n<p>[2]在原始 BiLSTM-CRF 模型上，加入了音韵特征，并在字符向量上使用 attention 机制来学习关注更有效的字符。</p>\n<p>​                      — from paperweekly</p>\n</blockquote>\n<h4 id=\"1-1-2-使用少量标注数据\"><a href=\"#1-1-2-使用少量标注数据\" class=\"headerlink\" title=\"1.1.2 使用少量标注数据\"></a>1.1.2 使用少量标注数据</h4><p>深度学习方法一般需要大量标注数据，但是在一些领域很难有海量的标注数据。所以在基于神经网络结构方法中如何使用少量标注数据也是一个重点。</p>\n<ul>\n<li><p><a href=\"https://openreview.net/forum?id=ry018WZAZ\">Deep Active Learning for Named Entity Recognition</a>[7]</p>\n<p>ICLR 2018看到的paper。这片文章把active learning应用到了CNN-CNN-LSTM模型，用于处理NER问题，也就是seq labeling问题。它能够仅使用25%的数据，达到state-of-the-art的水平。</p>\n<p>这篇paper总结了很多做seq labeling的方法，本身的思路也深入简出。decoder使用了LSTM而不是常用的CRF，发现LSTM比CRF有一些的优势。同时该文也证明了active learning能提高seq labeling的表现。</p>\n</li>\n<li><p>Semi-supervised sequence tagging with bidirectional language models[4]</p>\n<p>该论文使用海量无标注语料库训练了一个双向神经网络语言模型，然后使用这个训练好的语言模型来获取当前要标注词的语言模型向量（LM embedding），然后将该向量作为特征加入到原始的双向 RNN-CRF 模型中。</p>\n<p>实验结果表明，在少量标注数据上，加入这个语言模型向量能够大幅度提高 NER 效果，即使在大量的标注训练数据上，加入这个语言模型向量仍能提供原始 RNN-CRF 模型的效果。</p>\n</li>\n</ul>\n<h3 id=\"1-2-Relation-extraction\"><a href=\"#1-2-Relation-extraction\" class=\"headerlink\" title=\"1.2 Relation extraction\"></a>1.2 Relation extraction</h3><p>实体的关系的抽取方法可以简单分为两类：一类是pipeline抽取方法。另一类是并行或联合抽取方法。</p>\n<p>pipeline方法需要先识别entity，然后采用关系抽取模型得到实体对之间的关系。缺点是实体识别的结果会进一步影响关系抽取的结果，导致误差累积，也降低信息使用率，分开抽取也造成了信息冗余。</p>\n<p>[9]提出了一种联合实体检测参数共享的关系抽取模型，模型中有两个双向的LSTM-RNN，一个是基于word sequence（bidirectional sequential LSTM-RNNs），主要用于实体检测；一个基于Tree Structures （bidirectional tree- structured LSTM-RNNs），主要用于关系抽取；后者堆在前者上，前者的输出和隐含层作为后者输入的一部分。下图为整个模型的结构图：</p>\n<p><img src=\"https://pic3.zhimg.com/v2-8a44b362fb60fff951dbfaa2bc4469f3_r.jpg\" alt=\"LSTM-RNNs\"></p>\n<p>该paper用了参数共享，实体的识别过程和关系的判断过程并没有交互的过程，还无法称其为真正意义上的joint。</p>\n<p>[7]提出了一种端到端的基于序列标注的的方法进行关系抽取，它将实体发现任务和关系抽取任务转化为一个标注任务。在 encoder-decoder 框架下，采用主流的 bi-lstm 为 encoder，lstm 为 decoder。对每个词标注上 BIEM+关系类型+实体的序号。目前这种思路有人测试下来发现，总的来说，联合抽取比pipeline的方法好，序列标注联合抽取要比其他联合抽取方法好，然而目前实体关系抽取任务的 F1 值仍然不到 0.5。因此虽然效果还可以，但是就实际使用还有一段距离。</p>\n<p>此外，该模型还无法处理一个句子有多个关系三元组，和一个实体在多个关系中出现的一对多的问题。一个改进方向是把最后的softmax改成多分类器以实现多标签，这样就能实现一个实体的多关系抽取。其次，该方法是非开放域的关系抽取，关系词是从预定义的关系集里抽取的。</p>\n<h2 id=\"2-Others\"><a href=\"#2-Others\" class=\"headerlink\" title=\"2. Others\"></a>2. Others</h2><p>这里主要是有相关性不强但挺有意思，或泛用性很强的一些文章。</p>\n<ol>\n<li><p>Ngram2vec[5]</p>\n<p>一个词向量生成的方法，基于经典的 word2vec 的思想，在其之上加入了 ngram 的共现信息，取得了更好的结果。代码实现：<a href=\"http://link.zhihu.com/?target=https://github.com/zhezhaoa/ngram2vec/\">https://github.com/zhezhaoa/ngram2vec/</a></p>\n</li>\n<li><p><a href=\"https://research.googleblog.com/2017/05/using-machine-learning-to-explore.html\">AutoML</a></p>\n<p>google在五月份发布的模型，主要思想是将reinforcement learning应用在神经网络的构建、参数确定上。我们对网络进行测试，将反馈的结果返回到控制器中，以此来帮助提升下一次循环中的训练设定。生成新的架构、测试、把反馈传送给控制器以吸取经验。以此往复以得到更优的结构。</p>\n</li>\n<li><p>Introspection:Accelerating Neural Network Training By Learning Weight Evolution[6]</p>\n<p>这个本质上是meta learning的问题。他们训练了一个网络，网络的输入是某个时间点之前随机选取的4个旧参数的值，输出就是新的参数。因此可以将训练其他模型时得到的这个网络，用于加速其他模型。他们训练了mnist的两层conv net，用该任务的参数更新历史训练网络。他们最后将pretrained好的这个网络用于更新大网络，结果都能更好。</p>\n</li>\n</ol>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><p>[1] Rei, M., Crichton, G. K., &amp; Pyysalo, S. (2016). Attending to Characters in Neural Sequence Labeling Models. <em>arXiv preprint arXiv:1611.04361</em>.</p>\n<p>[2] Mortensen, A. B. D., &amp; Carbonell, C. D. J. G. (2016). Phonologically aware neural model for named entity recognition in low resource transfer settings.</p>\n<p>[3] Yang, Z., Salakhutdinov, R., &amp; Cohen, W. W. (2017). Transfer learning for sequence tagging with hierarchical recurrent networks. <em>arXiv preprint arXiv:1703.06345</em>.</p>\n<p>[4] Peters, M. E., Ammar, W., Bhagavatula, C., &amp; Power, R. (2017). Semi-supervised sequence tagging with bidirectional language models. <em>arXiv preprint arXiv:1705.00108</em>.</p>\n<p>[5] Zhao, Z., Liu, T., Li, S., Li, B., &amp; Du, X. (2017). Ngram2vec: Learning Improved Word Representations from Ngram Co-occurrence Statistics. In <em>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</em> (pp. 244-253).</p>\n<p>[6] Sinha, A., Sarkar, M., Mukherjee, A., &amp; Krishnamurthy, B. (2017). Introspection: Accelerating Neural Network Training By Learning Weight Evolution. <em>arXiv preprint arXiv:1704.04959</em>.</p>\n<p>[7] Shen, Yanyao, Yun, Hyokun, Lipton, Zachary C, Kronrod, Yakov, &amp; Anandkumar, Animashree. (2017). Deep active learning for named entity recognition.</p>\n<p>[8] Zheng, S., Wang, F., Bao, H., Hao, Y., Zhou, P., &amp; Xu, B. (2017). Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme. <em>arXiv preprint arXiv:1706.05075</em>.</p>\n<p>[9] Miwa, M., &amp; Bansal, M. (2016). End-to-end relation extraction using lstms on sequences and tree structures. <em>arXiv preprint arXiv:1601.00770</em>.</p>\n"},{"title":"几个 relation extraction 远程监督模型","date":"2018-01-14T00:00:00.000Z","_content":"\n## 几个 relation extraction 远程监督模型\n\n**摘要：**远程监督（Distant supervision）显著地减少了建立用于分类任务的训练集所需要的人工。但是这一项技术也会带来很大的噪音，并可能因此而大大地影响了模型的性能表现。这里，我们以 relation extraction 这项任务为例，深入讨论分析该噪声的分布。文献[1]提出了 dynamic-transition matrix，并证明了它能很好地代表了由 distant supervision 所带来的噪声。通过该矩阵，我们能够大大提高 relation extraction 的效果。文献[2]则是一种经典的方法，通过定义规则，定义否定模式（negative pattern）过滤掉一些噪音数据，可以很大程度提高性能。缺点是规则依赖人工定义，但是方法本身简单有效。文献[3]将 relation extraction 定义为一个 Multi-instance Multi-label 学习问题，一定程度上解决了错误标签的问题。\n\n### 1. Problem of distant supervision\n\nDistant supervision 是一种生成关系抽取训练集的常用方法。它把现有知识库中的三元组 \\<e1, r, e2\\> （或写成\\<subj, r, obj\\>）作为种子，匹配同时含有 e1 和 e2 的文本，得到的文本用作关系 r 的标注数据。这样可以省去大量人工标记的工作。\n\n但是，相比于人工标注方法，这种匹配方式会产生很多噪音：比如三元组\\<DonaldTrump, born-in, New York\\>，可能对齐到“Donald Trump was born in New York”，也可能对齐到“DonaldTrump worked in New York”。其中前一句是我们想要的标注数据，后一句则是噪音数据，它并不表示born-in关系。如何去除这些噪音数据，是一个重要的研究课题。\n\n### 2. Approaches to this problems\n\n-  拟合噪音\n  - dynamic-transition matrix [1]\n-  去除噪音\n  - 通过定义规则过滤掉一些噪音数据[2]，缺点是依赖人工定义，并且被关系种类所限制。\n  - Multi-instance learning[3], 把训练语句分包学习，包内取平均值，或者用 attention 加权，可以中和掉包内的噪音数据。缺点是受限于 at-least-one-assumption：每个包内至少有一个正确的数据。\n\n下面我们简单介绍这几个模型。\n\n#### 2.1 Learning with dynamic-transition matrix [1]\n\n文献[1] 提出了 dynamic-transition matrix，用于表达 Distant supervision 所产生的噪声。dynamic-transition matrix 可以通过基于 curriculum learning 的方法训练得到。通过该矩阵，我们能够大大提高 relation extraction 的效果，能够达到目前该领域的 state-of-the-art。\n\n![overview](https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202153.jpg)\n\nTransition matrix 是一个转移矩阵，记为T，大小为 n*n，n是关系种类的数目。T 的元素，$T_{ij}$的值是 p( j| i )，即该句子代表关系为 i，但被误判为 j 的概率。\n\n这样我们就可以得到：𝑃𝑟𝑒𝑑𝑖𝑐𝑡𝑒𝑑 𝑑𝑖𝑠𝑡𝑟𝑖𝑏𝑢𝑡𝑖𝑜𝑛 × 𝑇𝑟𝑎𝑠𝑖𝑡𝑖𝑜𝑛 𝑚𝑎𝑡𝑟𝑖𝑥=𝑂𝑏𝑠𝑒𝑟𝑣𝑒𝑑 𝑑𝑖𝑠𝑡𝑟𝑖𝑏𝑢𝑡𝑖𝑜𝑛\n\n其中，predicted 是我们想要的真实分布，observed 是我们观测到的噪音分布，这样就可以用噪音数据进行联合训练了。作者在 timeRE 和 entityRE(NYT) 上均进行了训练，取得了降噪的 state-of-art。具体分析结果可以参照论文。\n\n#### 2.2 Reducing Wrong Labels [2] \n\n在关系提取方面，远程监督试图通过使用知识库（如Freebase）作为监督来源，从文本中提取实体之间的关系。 当一个句子和一个知识库引用同一个实体对时，这种方法试图用知识库中的对应关系来启发式地标注句子。 然而，这种启发式可能会导致一些句子被错误地标记。 这种嘈杂的标记数据导致较差的抽取性能。 在本文中，我们提出了一种减少错误标签数量的方法。 我们提出了一个新的生成模型，直接模拟远程监督的启发式标签过程。 该模型通过其隐藏变量来预测分配的标签是正确的还是错误的。在实验中，我们也发现错误的标签减少提高了关系抽取的性能。\n\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202150.jpg\" width=\"70%\">\n\nNegPat(r)即为事先定义的对于r的否定模式（negative pattern）。在我们的方法中，我们按如下所示去除错误标签：（i）给定一个已标注的语料库，我们首先验证其中的模式是否表达一种relation，然后（ii）使用否定模式列表（NegPat）去除错误的标签， 即该模式被定义为不表示relation的模式。 第一步，我们引入新的生成模型，直接模拟DS的标注过程并进行预测。 第二步在算法1中描述，见上图。对于关系提取，我们使用上述得到的标注数据来训练分类器（给定实体对，该分类器预测所属关系）。\n\n####2.3 Multi-instance Multi-label Learning [3]\n\n很多的共现 entities 都没有什么关系，仅仅是出现在同一个句子中；而有的 entities 之间的关系其实并不仅仅只有一种，可能有多种，比如奥巴马和美国的关系，可能是 born in，也可能是 is the president of 的关系。\n\n因此训练集会产生大量的错误标记，比如两个实体有多种关系或者根本在这句话中没有任何关系，这样的训练数据会对关系抽取器产生影响。正因为如此，传统的监督式学习，假设每个实例明确地映射到一个标签，是不合适的。\n\n对于这个问题，我们将关系抽取定义为一个 Multi-instance Multi-label 学习问题，它使用带有潜在变量的图模型，对文本中一对实体的所有实例以及它们的所有标签进行联合建模。 该模型在 relation extraction 领域表现出色。\n\n### 3. Conclusion \n\n上面提到的几个模型都有其新颖的地方，其中[1]这种拟合噪音的思想很有创新点，实际的效果也很理想；而后两个模型主要都是在数据预处理阶段进行，因此可以和其他 relation extraction 模型很好的结合。\n\n## References\n\n\\*笔记部分参考[论文浅尝 | Learning with Noise: Supervised Relation Extraction](https://mp.weixin.qq.com/s/O9JaalDhoX97DMoUBFxmtg)\n\n[1] Luo, Bingfeng, et al. \"Learning with noise: enhance distantly supervised relation extraction with dynamic transition matrix.\" *arXiv preprint arXiv:1705.03995* (2017).\n\n[2] Takamatsu, Shingo, Issei Sato, and Hiroshi Nakagawa. \"Reducing wrong labels in distant supervision for relation extraction.\" *Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1*. Association for Computational Linguistics, 2012.\n\n[3] Surdeanu, Mihai, et al. \"Multi-instance multi-label learning for relation extraction.\" *Proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning*. Association for Computational Linguistics, 2012.","source":"_posts/[2018.1.14]Models-for-relation-extraction.md","raw":"---\ntitle: 几个 relation extraction 远程监督模型\ndate: 2018-01-14 08:00:00\ncategories: [research]\ntags: [relation-extraction, distant-supervision]\n---\n\n## 几个 relation extraction 远程监督模型\n\n**摘要：**远程监督（Distant supervision）显著地减少了建立用于分类任务的训练集所需要的人工。但是这一项技术也会带来很大的噪音，并可能因此而大大地影响了模型的性能表现。这里，我们以 relation extraction 这项任务为例，深入讨论分析该噪声的分布。文献[1]提出了 dynamic-transition matrix，并证明了它能很好地代表了由 distant supervision 所带来的噪声。通过该矩阵，我们能够大大提高 relation extraction 的效果。文献[2]则是一种经典的方法，通过定义规则，定义否定模式（negative pattern）过滤掉一些噪音数据，可以很大程度提高性能。缺点是规则依赖人工定义，但是方法本身简单有效。文献[3]将 relation extraction 定义为一个 Multi-instance Multi-label 学习问题，一定程度上解决了错误标签的问题。\n\n### 1. Problem of distant supervision\n\nDistant supervision 是一种生成关系抽取训练集的常用方法。它把现有知识库中的三元组 \\<e1, r, e2\\> （或写成\\<subj, r, obj\\>）作为种子，匹配同时含有 e1 和 e2 的文本，得到的文本用作关系 r 的标注数据。这样可以省去大量人工标记的工作。\n\n但是，相比于人工标注方法，这种匹配方式会产生很多噪音：比如三元组\\<DonaldTrump, born-in, New York\\>，可能对齐到“Donald Trump was born in New York”，也可能对齐到“DonaldTrump worked in New York”。其中前一句是我们想要的标注数据，后一句则是噪音数据，它并不表示born-in关系。如何去除这些噪音数据，是一个重要的研究课题。\n\n### 2. Approaches to this problems\n\n-  拟合噪音\n  - dynamic-transition matrix [1]\n-  去除噪音\n  - 通过定义规则过滤掉一些噪音数据[2]，缺点是依赖人工定义，并且被关系种类所限制。\n  - Multi-instance learning[3], 把训练语句分包学习，包内取平均值，或者用 attention 加权，可以中和掉包内的噪音数据。缺点是受限于 at-least-one-assumption：每个包内至少有一个正确的数据。\n\n下面我们简单介绍这几个模型。\n\n#### 2.1 Learning with dynamic-transition matrix [1]\n\n文献[1] 提出了 dynamic-transition matrix，用于表达 Distant supervision 所产生的噪声。dynamic-transition matrix 可以通过基于 curriculum learning 的方法训练得到。通过该矩阵，我们能够大大提高 relation extraction 的效果，能够达到目前该领域的 state-of-the-art。\n\n![overview](https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202153.jpg)\n\nTransition matrix 是一个转移矩阵，记为T，大小为 n*n，n是关系种类的数目。T 的元素，$T_{ij}$的值是 p( j| i )，即该句子代表关系为 i，但被误判为 j 的概率。\n\n这样我们就可以得到：𝑃𝑟𝑒𝑑𝑖𝑐𝑡𝑒𝑑 𝑑𝑖𝑠𝑡𝑟𝑖𝑏𝑢𝑡𝑖𝑜𝑛 × 𝑇𝑟𝑎𝑠𝑖𝑡𝑖𝑜𝑛 𝑚𝑎𝑡𝑟𝑖𝑥=𝑂𝑏𝑠𝑒𝑟𝑣𝑒𝑑 𝑑𝑖𝑠𝑡𝑟𝑖𝑏𝑢𝑡𝑖𝑜𝑛\n\n其中，predicted 是我们想要的真实分布，observed 是我们观测到的噪音分布，这样就可以用噪音数据进行联合训练了。作者在 timeRE 和 entityRE(NYT) 上均进行了训练，取得了降噪的 state-of-art。具体分析结果可以参照论文。\n\n#### 2.2 Reducing Wrong Labels [2] \n\n在关系提取方面，远程监督试图通过使用知识库（如Freebase）作为监督来源，从文本中提取实体之间的关系。 当一个句子和一个知识库引用同一个实体对时，这种方法试图用知识库中的对应关系来启发式地标注句子。 然而，这种启发式可能会导致一些句子被错误地标记。 这种嘈杂的标记数据导致较差的抽取性能。 在本文中，我们提出了一种减少错误标签数量的方法。 我们提出了一个新的生成模型，直接模拟远程监督的启发式标签过程。 该模型通过其隐藏变量来预测分配的标签是正确的还是错误的。在实验中，我们也发现错误的标签减少提高了关系抽取的性能。\n\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202150.jpg\" width=\"70%\">\n\nNegPat(r)即为事先定义的对于r的否定模式（negative pattern）。在我们的方法中，我们按如下所示去除错误标签：（i）给定一个已标注的语料库，我们首先验证其中的模式是否表达一种relation，然后（ii）使用否定模式列表（NegPat）去除错误的标签， 即该模式被定义为不表示relation的模式。 第一步，我们引入新的生成模型，直接模拟DS的标注过程并进行预测。 第二步在算法1中描述，见上图。对于关系提取，我们使用上述得到的标注数据来训练分类器（给定实体对，该分类器预测所属关系）。\n\n####2.3 Multi-instance Multi-label Learning [3]\n\n很多的共现 entities 都没有什么关系，仅仅是出现在同一个句子中；而有的 entities 之间的关系其实并不仅仅只有一种，可能有多种，比如奥巴马和美国的关系，可能是 born in，也可能是 is the president of 的关系。\n\n因此训练集会产生大量的错误标记，比如两个实体有多种关系或者根本在这句话中没有任何关系，这样的训练数据会对关系抽取器产生影响。正因为如此，传统的监督式学习，假设每个实例明确地映射到一个标签，是不合适的。\n\n对于这个问题，我们将关系抽取定义为一个 Multi-instance Multi-label 学习问题，它使用带有潜在变量的图模型，对文本中一对实体的所有实例以及它们的所有标签进行联合建模。 该模型在 relation extraction 领域表现出色。\n\n### 3. Conclusion \n\n上面提到的几个模型都有其新颖的地方，其中[1]这种拟合噪音的思想很有创新点，实际的效果也很理想；而后两个模型主要都是在数据预处理阶段进行，因此可以和其他 relation extraction 模型很好的结合。\n\n## References\n\n\\*笔记部分参考[论文浅尝 | Learning with Noise: Supervised Relation Extraction](https://mp.weixin.qq.com/s/O9JaalDhoX97DMoUBFxmtg)\n\n[1] Luo, Bingfeng, et al. \"Learning with noise: enhance distantly supervised relation extraction with dynamic transition matrix.\" *arXiv preprint arXiv:1705.03995* (2017).\n\n[2] Takamatsu, Shingo, Issei Sato, and Hiroshi Nakagawa. \"Reducing wrong labels in distant supervision for relation extraction.\" *Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1*. Association for Computational Linguistics, 2012.\n\n[3] Surdeanu, Mihai, et al. \"Multi-instance multi-label learning for relation extraction.\" *Proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning*. Association for Computational Linguistics, 2012.","slug":"[2018.1.14]Models-for-relation-extraction","published":1,"updated":"2020-11-03T03:26:13.796Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckw4qufph001qgwtldfrq166l","content":"<h2 id=\"几个-relation-extraction-远程监督模型\"><a href=\"#几个-relation-extraction-远程监督模型\" class=\"headerlink\" title=\"几个 relation extraction 远程监督模型\"></a>几个 relation extraction 远程监督模型</h2><p><strong>摘要：</strong>远程监督（Distant supervision）显著地减少了建立用于分类任务的训练集所需要的人工。但是这一项技术也会带来很大的噪音，并可能因此而大大地影响了模型的性能表现。这里，我们以 relation extraction 这项任务为例，深入讨论分析该噪声的分布。文献[1]提出了 dynamic-transition matrix，并证明了它能很好地代表了由 distant supervision 所带来的噪声。通过该矩阵，我们能够大大提高 relation extraction 的效果。文献[2]则是一种经典的方法，通过定义规则，定义否定模式（negative pattern）过滤掉一些噪音数据，可以很大程度提高性能。缺点是规则依赖人工定义，但是方法本身简单有效。文献[3]将 relation extraction 定义为一个 Multi-instance Multi-label 学习问题，一定程度上解决了错误标签的问题。</p>\n<h3 id=\"1-Problem-of-distant-supervision\"><a href=\"#1-Problem-of-distant-supervision\" class=\"headerlink\" title=\"1. Problem of distant supervision\"></a>1. Problem of distant supervision</h3><p>Distant supervision 是一种生成关系抽取训练集的常用方法。它把现有知识库中的三元组 &lt;e1, r, e2&gt; （或写成&lt;subj, r, obj&gt;）作为种子，匹配同时含有 e1 和 e2 的文本，得到的文本用作关系 r 的标注数据。这样可以省去大量人工标记的工作。</p>\n<p>但是，相比于人工标注方法，这种匹配方式会产生很多噪音：比如三元组&lt;DonaldTrump, born-in, New York&gt;，可能对齐到“Donald Trump was born in New York”，也可能对齐到“DonaldTrump worked in New York”。其中前一句是我们想要的标注数据，后一句则是噪音数据，它并不表示born-in关系。如何去除这些噪音数据，是一个重要的研究课题。</p>\n<h3 id=\"2-Approaches-to-this-problems\"><a href=\"#2-Approaches-to-this-problems\" class=\"headerlink\" title=\"2. Approaches to this problems\"></a>2. Approaches to this problems</h3><ul>\n<li> 拟合噪音</li>\n<li>dynamic-transition matrix [1]</li>\n<li> 去除噪音</li>\n<li>通过定义规则过滤掉一些噪音数据[2]，缺点是依赖人工定义，并且被关系种类所限制。</li>\n<li>Multi-instance learning[3], 把训练语句分包学习，包内取平均值，或者用 attention 加权，可以中和掉包内的噪音数据。缺点是受限于 at-least-one-assumption：每个包内至少有一个正确的数据。</li>\n</ul>\n<p>下面我们简单介绍这几个模型。</p>\n<h4 id=\"2-1-Learning-with-dynamic-transition-matrix-1\"><a href=\"#2-1-Learning-with-dynamic-transition-matrix-1\" class=\"headerlink\" title=\"2.1 Learning with dynamic-transition matrix [1]\"></a>2.1 Learning with dynamic-transition matrix [1]</h4><p>文献[1] 提出了 dynamic-transition matrix，用于表达 Distant supervision 所产生的噪声。dynamic-transition matrix 可以通过基于 curriculum learning 的方法训练得到。通过该矩阵，我们能够大大提高 relation extraction 的效果，能够达到目前该领域的 state-of-the-art。</p>\n<p><img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202153.jpg\" alt=\"overview\"></p>\n<p>Transition matrix 是一个转移矩阵，记为T，大小为 n*n，n是关系种类的数目。T 的元素，$T_{ij}$的值是 p( j| i )，即该句子代表关系为 i，但被误判为 j 的概率。</p>\n<p>这样我们就可以得到：𝑃𝑟𝑒𝑑𝑖𝑐𝑡𝑒𝑑 𝑑𝑖𝑠𝑡𝑟𝑖𝑏𝑢𝑡𝑖𝑜𝑛 × 𝑇𝑟𝑎𝑠𝑖𝑡𝑖𝑜𝑛 𝑚𝑎𝑡𝑟𝑖𝑥=𝑂𝑏𝑠𝑒𝑟𝑣𝑒𝑑 𝑑𝑖𝑠𝑡𝑟𝑖𝑏𝑢𝑡𝑖𝑜𝑛</p>\n<p>其中，predicted 是我们想要的真实分布，observed 是我们观测到的噪音分布，这样就可以用噪音数据进行联合训练了。作者在 timeRE 和 entityRE(NYT) 上均进行了训练，取得了降噪的 state-of-art。具体分析结果可以参照论文。</p>\n<h4 id=\"2-2-Reducing-Wrong-Labels-2\"><a href=\"#2-2-Reducing-Wrong-Labels-2\" class=\"headerlink\" title=\"2.2 Reducing Wrong Labels [2]\"></a>2.2 Reducing Wrong Labels [2]</h4><p>在关系提取方面，远程监督试图通过使用知识库（如Freebase）作为监督来源，从文本中提取实体之间的关系。 当一个句子和一个知识库引用同一个实体对时，这种方法试图用知识库中的对应关系来启发式地标注句子。 然而，这种启发式可能会导致一些句子被错误地标记。 这种嘈杂的标记数据导致较差的抽取性能。 在本文中，我们提出了一种减少错误标签数量的方法。 我们提出了一个新的生成模型，直接模拟远程监督的启发式标签过程。 该模型通过其隐藏变量来预测分配的标签是正确的还是错误的。在实验中，我们也发现错误的标签减少提高了关系抽取的性能。</p>\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202150.jpg\" width=\"70%\">\n\n<p>NegPat(r)即为事先定义的对于r的否定模式（negative pattern）。在我们的方法中，我们按如下所示去除错误标签：（i）给定一个已标注的语料库，我们首先验证其中的模式是否表达一种relation，然后（ii）使用否定模式列表（NegPat）去除错误的标签， 即该模式被定义为不表示relation的模式。 第一步，我们引入新的生成模型，直接模拟DS的标注过程并进行预测。 第二步在算法1中描述，见上图。对于关系提取，我们使用上述得到的标注数据来训练分类器（给定实体对，该分类器预测所属关系）。</p>\n<p>####2.3 Multi-instance Multi-label Learning [3]</p>\n<p>很多的共现 entities 都没有什么关系，仅仅是出现在同一个句子中；而有的 entities 之间的关系其实并不仅仅只有一种，可能有多种，比如奥巴马和美国的关系，可能是 born in，也可能是 is the president of 的关系。</p>\n<p>因此训练集会产生大量的错误标记，比如两个实体有多种关系或者根本在这句话中没有任何关系，这样的训练数据会对关系抽取器产生影响。正因为如此，传统的监督式学习，假设每个实例明确地映射到一个标签，是不合适的。</p>\n<p>对于这个问题，我们将关系抽取定义为一个 Multi-instance Multi-label 学习问题，它使用带有潜在变量的图模型，对文本中一对实体的所有实例以及它们的所有标签进行联合建模。 该模型在 relation extraction 领域表现出色。</p>\n<h3 id=\"3-Conclusion\"><a href=\"#3-Conclusion\" class=\"headerlink\" title=\"3. Conclusion\"></a>3. Conclusion</h3><p>上面提到的几个模型都有其新颖的地方，其中[1]这种拟合噪音的思想很有创新点，实际的效果也很理想；而后两个模型主要都是在数据预处理阶段进行，因此可以和其他 relation extraction 模型很好的结合。</p>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><p>*笔记部分参考<a href=\"https://mp.weixin.qq.com/s/O9JaalDhoX97DMoUBFxmtg\">论文浅尝 | Learning with Noise: Supervised Relation Extraction</a></p>\n<p>[1] Luo, Bingfeng, et al. “Learning with noise: enhance distantly supervised relation extraction with dynamic transition matrix.” <em>arXiv preprint arXiv:1705.03995</em> (2017).</p>\n<p>[2] Takamatsu, Shingo, Issei Sato, and Hiroshi Nakagawa. “Reducing wrong labels in distant supervision for relation extraction.” <em>Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1</em>. Association for Computational Linguistics, 2012.</p>\n<p>[3] Surdeanu, Mihai, et al. “Multi-instance multi-label learning for relation extraction.” <em>Proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning</em>. Association for Computational Linguistics, 2012.</p>\n","site":{"data":{"recommended_posts":{"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"}],"https://LorrinWWW.github.io/posts/complexity/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/hexo-with-latex/":[{"title":"区块链时代的最正确的打赏姿势——在Hexo博客配置Coinhive网页挖矿","permalink":"http://www.davidfnck.com/blockchain/mine-xmr-in-hexo-by-coinhive.html/"}],"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/":[{"title":"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-statistic/":[{"title":"贝叶斯估计 Bayes estimation","permalink":"https://LorrinWWW.github.io/posts/Bayes-estimation/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"}],"https://LorrinWWW.github.io/posts/[2018.2.5]Nested-LSTMs/":[{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"}],"https://LorrinWWW.github.io/posts/datamining-pretreatment/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Several models for knowledge graph representing and completing 几个知识图谱模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/"}],"https://LorrinWWW.github.io/posts/compression/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/Sobolev-space/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/proba-ch5/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch3/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/proba-ch1/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/ML-CNN/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/EDP-basic-models/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/machine-learning/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/datamining-class-pred/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction/":[],"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/":[{"title":"OS notes","permalink":"https://LorrinWWW.github.io/posts/OS-notes/"}],"https://LorrinWWW.github.io/posts/projet-enjeu-plugin-chrome-101/":[{"title":"Chrome油猴脚本","permalink":"http://gubangzhong.cn/2017/06/13/Chrome油猴脚本/"}],"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/":[{"title":"MongoDB, Docker and Python","permalink":"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/":[{"title":"Hands on Scrapy","permalink":"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/OS-notes/":[{"title":"learning OS and building LorriOS","permalink":"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/"}],"https://LorrinWWW.github.io/posts/Bayes-estimation/":[{"title":"Note of statistic","permalink":"https://LorrinWWW.github.io/posts/Note-of-statistic/"}],"https://LorrinWWW.github.io/posts/management-of-the-firm/":[],"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"}],"https://LorrinWWW.github.io/posts/datamining-qualitative-induction/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-finite-element-method/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/QuadTree/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/proba-ch4/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch2/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/":[{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/Note-of-NLP/":[{"title":"Generative Adversarial Network","permalink":"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/"},{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"}],"https://LorrinWWW.github.io/posts/Note-of-datamining/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"数据挖掘-分类与预测","permalink":"https://LorrinWWW.github.io/posts/datamining-class-pred/"},{"title":"数据挖掘-预处理","permalink":"https://LorrinWWW.github.io/posts/datamining-pretreatment/"}],"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/Note-of-probability/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/participe-present-et-gerondif/":[],"https://LorrinWWW.github.io/posts/Hilbert-space/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/proba-ch6/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}]}}},"excerpt":"","more":"<h2 id=\"几个-relation-extraction-远程监督模型\"><a href=\"#几个-relation-extraction-远程监督模型\" class=\"headerlink\" title=\"几个 relation extraction 远程监督模型\"></a>几个 relation extraction 远程监督模型</h2><p><strong>摘要：</strong>远程监督（Distant supervision）显著地减少了建立用于分类任务的训练集所需要的人工。但是这一项技术也会带来很大的噪音，并可能因此而大大地影响了模型的性能表现。这里，我们以 relation extraction 这项任务为例，深入讨论分析该噪声的分布。文献[1]提出了 dynamic-transition matrix，并证明了它能很好地代表了由 distant supervision 所带来的噪声。通过该矩阵，我们能够大大提高 relation extraction 的效果。文献[2]则是一种经典的方法，通过定义规则，定义否定模式（negative pattern）过滤掉一些噪音数据，可以很大程度提高性能。缺点是规则依赖人工定义，但是方法本身简单有效。文献[3]将 relation extraction 定义为一个 Multi-instance Multi-label 学习问题，一定程度上解决了错误标签的问题。</p>\n<h3 id=\"1-Problem-of-distant-supervision\"><a href=\"#1-Problem-of-distant-supervision\" class=\"headerlink\" title=\"1. Problem of distant supervision\"></a>1. Problem of distant supervision</h3><p>Distant supervision 是一种生成关系抽取训练集的常用方法。它把现有知识库中的三元组 &lt;e1, r, e2&gt; （或写成&lt;subj, r, obj&gt;）作为种子，匹配同时含有 e1 和 e2 的文本，得到的文本用作关系 r 的标注数据。这样可以省去大量人工标记的工作。</p>\n<p>但是，相比于人工标注方法，这种匹配方式会产生很多噪音：比如三元组&lt;DonaldTrump, born-in, New York&gt;，可能对齐到“Donald Trump was born in New York”，也可能对齐到“DonaldTrump worked in New York”。其中前一句是我们想要的标注数据，后一句则是噪音数据，它并不表示born-in关系。如何去除这些噪音数据，是一个重要的研究课题。</p>\n<h3 id=\"2-Approaches-to-this-problems\"><a href=\"#2-Approaches-to-this-problems\" class=\"headerlink\" title=\"2. Approaches to this problems\"></a>2. Approaches to this problems</h3><ul>\n<li> 拟合噪音</li>\n<li>dynamic-transition matrix [1]</li>\n<li> 去除噪音</li>\n<li>通过定义规则过滤掉一些噪音数据[2]，缺点是依赖人工定义，并且被关系种类所限制。</li>\n<li>Multi-instance learning[3], 把训练语句分包学习，包内取平均值，或者用 attention 加权，可以中和掉包内的噪音数据。缺点是受限于 at-least-one-assumption：每个包内至少有一个正确的数据。</li>\n</ul>\n<p>下面我们简单介绍这几个模型。</p>\n<h4 id=\"2-1-Learning-with-dynamic-transition-matrix-1\"><a href=\"#2-1-Learning-with-dynamic-transition-matrix-1\" class=\"headerlink\" title=\"2.1 Learning with dynamic-transition matrix [1]\"></a>2.1 Learning with dynamic-transition matrix [1]</h4><p>文献[1] 提出了 dynamic-transition matrix，用于表达 Distant supervision 所产生的噪声。dynamic-transition matrix 可以通过基于 curriculum learning 的方法训练得到。通过该矩阵，我们能够大大提高 relation extraction 的效果，能够达到目前该领域的 state-of-the-art。</p>\n<p><img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202153.jpg\" alt=\"overview\"></p>\n<p>Transition matrix 是一个转移矩阵，记为T，大小为 n*n，n是关系种类的数目。T 的元素，$T_{ij}$的值是 p( j| i )，即该句子代表关系为 i，但被误判为 j 的概率。</p>\n<p>这样我们就可以得到：𝑃𝑟𝑒𝑑𝑖𝑐𝑡𝑒𝑑 𝑑𝑖𝑠𝑡𝑟𝑖𝑏𝑢𝑡𝑖𝑜𝑛 × 𝑇𝑟𝑎𝑠𝑖𝑡𝑖𝑜𝑛 𝑚𝑎𝑡𝑟𝑖𝑥=𝑂𝑏𝑠𝑒𝑟𝑣𝑒𝑑 𝑑𝑖𝑠𝑡𝑟𝑖𝑏𝑢𝑡𝑖𝑜𝑛</p>\n<p>其中，predicted 是我们想要的真实分布，observed 是我们观测到的噪音分布，这样就可以用噪音数据进行联合训练了。作者在 timeRE 和 entityRE(NYT) 上均进行了训练，取得了降噪的 state-of-art。具体分析结果可以参照论文。</p>\n<h4 id=\"2-2-Reducing-Wrong-Labels-2\"><a href=\"#2-2-Reducing-Wrong-Labels-2\" class=\"headerlink\" title=\"2.2 Reducing Wrong Labels [2]\"></a>2.2 Reducing Wrong Labels [2]</h4><p>在关系提取方面，远程监督试图通过使用知识库（如Freebase）作为监督来源，从文本中提取实体之间的关系。 当一个句子和一个知识库引用同一个实体对时，这种方法试图用知识库中的对应关系来启发式地标注句子。 然而，这种启发式可能会导致一些句子被错误地标记。 这种嘈杂的标记数据导致较差的抽取性能。 在本文中，我们提出了一种减少错误标签数量的方法。 我们提出了一个新的生成模型，直接模拟远程监督的启发式标签过程。 该模型通过其隐藏变量来预测分配的标签是正确的还是错误的。在实验中，我们也发现错误的标签减少提高了关系抽取的性能。</p>\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202150.jpg\" width=\"70%\">\n\n<p>NegPat(r)即为事先定义的对于r的否定模式（negative pattern）。在我们的方法中，我们按如下所示去除错误标签：（i）给定一个已标注的语料库，我们首先验证其中的模式是否表达一种relation，然后（ii）使用否定模式列表（NegPat）去除错误的标签， 即该模式被定义为不表示relation的模式。 第一步，我们引入新的生成模型，直接模拟DS的标注过程并进行预测。 第二步在算法1中描述，见上图。对于关系提取，我们使用上述得到的标注数据来训练分类器（给定实体对，该分类器预测所属关系）。</p>\n<p>####2.3 Multi-instance Multi-label Learning [3]</p>\n<p>很多的共现 entities 都没有什么关系，仅仅是出现在同一个句子中；而有的 entities 之间的关系其实并不仅仅只有一种，可能有多种，比如奥巴马和美国的关系，可能是 born in，也可能是 is the president of 的关系。</p>\n<p>因此训练集会产生大量的错误标记，比如两个实体有多种关系或者根本在这句话中没有任何关系，这样的训练数据会对关系抽取器产生影响。正因为如此，传统的监督式学习，假设每个实例明确地映射到一个标签，是不合适的。</p>\n<p>对于这个问题，我们将关系抽取定义为一个 Multi-instance Multi-label 学习问题，它使用带有潜在变量的图模型，对文本中一对实体的所有实例以及它们的所有标签进行联合建模。 该模型在 relation extraction 领域表现出色。</p>\n<h3 id=\"3-Conclusion\"><a href=\"#3-Conclusion\" class=\"headerlink\" title=\"3. Conclusion\"></a>3. Conclusion</h3><p>上面提到的几个模型都有其新颖的地方，其中[1]这种拟合噪音的思想很有创新点，实际的效果也很理想；而后两个模型主要都是在数据预处理阶段进行，因此可以和其他 relation extraction 模型很好的结合。</p>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><p>*笔记部分参考<a href=\"https://mp.weixin.qq.com/s/O9JaalDhoX97DMoUBFxmtg\">论文浅尝 | Learning with Noise: Supervised Relation Extraction</a></p>\n<p>[1] Luo, Bingfeng, et al. “Learning with noise: enhance distantly supervised relation extraction with dynamic transition matrix.” <em>arXiv preprint arXiv:1705.03995</em> (2017).</p>\n<p>[2] Takamatsu, Shingo, Issei Sato, and Hiroshi Nakagawa. “Reducing wrong labels in distant supervision for relation extraction.” <em>Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1</em>. Association for Computational Linguistics, 2012.</p>\n<p>[3] Surdeanu, Mihai, et al. “Multi-instance multi-label learning for relation extraction.” <em>Proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning</em>. Association for Computational Linguistics, 2012.</p>\n"},{"title":"Relation Classification via Attention Model 笔记","date":"2017-12-17T00:00:00.000Z","_content":"\n## Relation Classification via Attention Model\n\n这个笔记主要是阅读论文[1]，它的工作重点是在神经网络构成的端到端学习的关系抽取任务中加入Attention机制。作者主要通过自动学习关系句中注意力较高的部分，而引入attention机制，对反映实体关系更加重要的词语给予更大的attention，较好地提高了关系抽取的效果。\n\n<img src=\"https://github.com/lawlietAi/relation-classification-via-attention-model/raw/master/acnn_structure.png\" width=\"50%\">\n\n### 1. Attention\n\n#### 1.1 概述\n\nAttention机制最早是在视觉图像领域被提出来的。在NLP任务上，Bahdanau[2]等人使用类似attention的机制在机器翻译任务上将翻译和对齐同时进行。接着类似的基于attention机制的深度学习模型开始广泛应用到各种NLP任务中。\n\n#### 1.2 Recurrent Models of Visual Attention \n\n人们在进行观察图像的时候，其实并不是一次就把整幅图像的每个位置像素都看过，大多是根据需求将注意力集中到图像的特定部分。由此，在传统的RNN上加入了attention机制，每次当前状态，都会根据前一个状态学习得到的要关注的位置和当前输入的图像，去处理注意力部分像素。可以看到应用Attention机制后，任务的复杂度被降低了很多。\n\n#### 1.3 Attention-based RNN in NLP\n\n[1]的成果是在机器翻译任务，一般机器翻译工作由一个Encoder和一个Decoder构成，一个典型的Seq2seq任务。Encoder将源句子进行编码，再利用Decoder将编码后的向量解码成目标语言。\n\n我们在求注意力分配概率分布的时候，对于输入句子中任意一个单词都给出个概率，从而得到一个概率分布，再对输入句子所有单词的概率进行加权求和，得到Decoder的注意力分配。如下图。\n\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-201155.jpg\" width=\"30%\">\n\n另一个扩展性更好的论文是[3]，他们的工作告诉了大家attention在RNN中可以如何进行扩展。\n\n#### 1.4 Attention-based CNN in NLP\n\n[4]这篇论文研究的是两个CNN网络，分别处理两个句子，最后输入到分类器中处理。但是这样的模型在输入分类器前句对间是没有相互联系的，作者就想通过设计attention机制将不同cnn通道的句对联系起来。于是提出了3中在CNN中使用attention的方法。\n\n- ABCNN-1: 在卷积前进行attention，通过attention矩阵计算出相应句对的attention feature map，然后连同原来的feature map一起输入到卷积层。\n- ABCNN-2: 在池化时进行attention，通过attention对卷积后的表达重新加权，然后再进行池化.\n- ABCNN-3: ABCNN-1 + ABCNN-2\n\n### 2. Relation Classification \n\n<img src=\"https://github.com/lawlietAi/relation-classification-via-attention-model/raw/master/acnn_structure.png\" width=\"50%\">\n\n#### \t2.1 Classification Objective\n\n作者提出了一种距离函数，即正则化向量差的L2范数：\n$$\n\\delta_{\\theta}(S,y) = ||\\frac{w^O}{|w^O|} - W_y^L||_{L^2} \\\\\nS:\\text{Sentence}, y:\\text{Output relation}, w^O: \\text{Network output}, W^L:\\text{Relation embedding}\n$$\n基于此，作者定义了目标函数：\n$$\n\\mathcal{L} = [\\delta_\\theta(S,y) + (1-\\delta_\\theta(S, \\hat{y}^-))] + \\beta||\\theta||^2 \\\\\n\\hat{y}^- : \\text{A selected incorrect relation label chosen as the one with the highest score among all i.e.} \\\\\n\\hat{y}^- = argmax_{y'\\in \\mathcal{Y},y'\\ne y}(\\delta(S, y'))\n$$\n目标中的两个距离分别为网络输出向量与正例和与某负例的距离，该负例是所有错误类别中与该输出最接近的。最后加上一个正则项，通过使该目标函数最小化来训练网络中的各参数，$\\beta$用于控制其比重。\n\n#### 2.2 Input Representation \n\n现有句子，以及两个已知的实体e1,e2：\n$$\nS = (w_1,w_2,...,w_n) \\\\\ne_1 := w_p, e_2 := w_t . p,t\\in [1,n], p\\ne t\n$$\n为了得到它们的关系，我们把所有词转为词向量；并且根据每个词与实体的相对位置，也转为word position embeddings，每个词与两个实体有两个相对位置，所以得到第i个词的Embedding：\n$$\nw_i^M = [(w_i^d)^T, (w_{i,2}^p)^T,(w_{i,2}^p)^T]^T\n$$\n为了充分得到上下文的信息，再考虑大小为k的滑窗，得到最终的input representation\n$$\nz_i = [(w_{i - (k-1)/2}^M)^T,...,(w_{i + (k-1)/2}^M)^T]^T\n$$\n\n#### 2.3 Input Attention Mechanism\n\n<img src=\"https://pic3.zhimg.com/50/v2-2399a406ad0960c422702728b6418fa3_hd.jpg\" width=\"70%\">\n\n输入级的attention机制是设计两个关于实体对上下文相关的对角矩阵，该矩阵中各元素反映该词语与给定实体间联系的强弱，如$A_{i,i}^j=f(e_j,w_i)$反映了wi和ej之间的联系强弱，这里作者给的 f 就是内积。我们定义：\n$$\n\\alpha_i^j = \\frac{exp(A_{i,i}^j)}{\\sum_{i'=1}^{n}{exp(A_{i',i}^j)}}\n$$\n对于j=1,2 两个相关因子，作者提出了三种处理方式:\n\n- 平均\n  $$\n  r_i = z_i \\frac{\\alpha_i^1 + \\alpha_i^2}{2}\n  $$\n\n- 串联\n  $$\n  r_i = [(z_i \\alpha_i^1)^T, (z_i \\alpha_i^2)^T]^T\n  $$\n\n- 距离\n  $$\n  r_i = z_i \\frac{\\alpha_i^1 - \\alpha_i^2}{2}\n  $$\n\n\n\n\n\n\n\n\n\n最终得到$R = [r_1, r_2,…,r_n]$\n\n#### 2.4 Convolutional Max-Pooling with Secondary Attention\n\n将前面得到的矩阵R送入卷积核大小为dc的卷积层，卷积操作可形式化表示为:\n$$\nR^\\star = tanh(W_fR+B_f), \\text{where the siaze of Wf is } d^c \\times k(d^w+2d^p)\n$$\n然后构建一个相关性矩阵来捕获卷积层输出R*与实体关系WL之间的联系\n$$\nG = R^{\\star T}UW^L, \\\\U :\\text{weighting matrix learnt by the network}\n$$\n\n再用softmax函数来处理相关性矩阵G，获得attention pooling matrix Ap:\n$$\nA_{i,j}^p = \\frac{exp(G_{i,j})}{\\sum_{i'=1}^n{exp(G_{i',j})}}\n$$\n最后用Ap与卷积层输出R*相乘，也就是加入混合中的attention，然后取出每一维度的最大值，得到网络的输出\n$$\nw_i^O = max_j(R^\\star A^p)_{i,j}\n$$\n\n### 3. 总结\n\n从[1]中提到的结果上看，attention的表现确实是在重要的词上有更好的权重，在Sem-Eval-2010 Task 8数据集上取得了显著的效果提升。对于关系抽取来说无疑是非常大的一个进步。\n\n但是还是有一些不足：\n\n- 它要求实体已知，因此需要其他工作来完成实体的识别，使得一些信息的丢失以及错误累加。此时并行模型或端到端模型，同时完成实体识别可能效果会更好；\n- 关系是事先定义的集合，因此更多的是对关系的分类，若能启发式地抽取关系可能会有更广的应用空间；\n- 对于一些上下文没有明显帮助的隐式关系或是使用了比喻之类的修辞，较为容易出错。\n\n这次选择读这篇文章也是想更具体地了解Attention机制，同时了解一些关系抽取的方案，它也有一个pytorch版本的[实现](https://github.com/lawlietAi/relation-classification-via-attention-model)，可以辅以参考。\n\n## Reference\n\n笔记部分参考https://zhuanlan.zhihu.com/p/22867750\n\n[1] Wang, L., Cao, Z., Melo, G. D., & Liu, Z. (2016). Relation Classification via Multi-Level Attention CNNs. *Meeting of the Association for Computational Linguistics* (pp.1298-1307).\n\n[2] Bahdanau, D., Cho, K., & Bengio, Y. (2014). Neural machine translation by jointly learning to align and translate. *Computer Science*.\n\n[3] Luong, M. T., Pham, H., & Manning, C. D. (2015). Effective approaches to attention-based neural machine translation. *Computer Science*.\n\n[4] Yin, W., Schütze, H., Xiang, B., & Zhou, B. (2015). Abcnn: attention-based convolutional neural network for modeling sentence pairs. *Computer Science*.","source":"_posts/[2017.12.17]Relation-Classification-via-Attention-Model.md","raw":"---\ntitle: Relation Classification via Attention Model 笔记\ndate: 2017-12-17 08:00:00\ncategories: [research]\ntags: [relation-classification, attention, relation-extraction]\n---\n\n## Relation Classification via Attention Model\n\n这个笔记主要是阅读论文[1]，它的工作重点是在神经网络构成的端到端学习的关系抽取任务中加入Attention机制。作者主要通过自动学习关系句中注意力较高的部分，而引入attention机制，对反映实体关系更加重要的词语给予更大的attention，较好地提高了关系抽取的效果。\n\n<img src=\"https://github.com/lawlietAi/relation-classification-via-attention-model/raw/master/acnn_structure.png\" width=\"50%\">\n\n### 1. Attention\n\n#### 1.1 概述\n\nAttention机制最早是在视觉图像领域被提出来的。在NLP任务上，Bahdanau[2]等人使用类似attention的机制在机器翻译任务上将翻译和对齐同时进行。接着类似的基于attention机制的深度学习模型开始广泛应用到各种NLP任务中。\n\n#### 1.2 Recurrent Models of Visual Attention \n\n人们在进行观察图像的时候，其实并不是一次就把整幅图像的每个位置像素都看过，大多是根据需求将注意力集中到图像的特定部分。由此，在传统的RNN上加入了attention机制，每次当前状态，都会根据前一个状态学习得到的要关注的位置和当前输入的图像，去处理注意力部分像素。可以看到应用Attention机制后，任务的复杂度被降低了很多。\n\n#### 1.3 Attention-based RNN in NLP\n\n[1]的成果是在机器翻译任务，一般机器翻译工作由一个Encoder和一个Decoder构成，一个典型的Seq2seq任务。Encoder将源句子进行编码，再利用Decoder将编码后的向量解码成目标语言。\n\n我们在求注意力分配概率分布的时候，对于输入句子中任意一个单词都给出个概率，从而得到一个概率分布，再对输入句子所有单词的概率进行加权求和，得到Decoder的注意力分配。如下图。\n\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-201155.jpg\" width=\"30%\">\n\n另一个扩展性更好的论文是[3]，他们的工作告诉了大家attention在RNN中可以如何进行扩展。\n\n#### 1.4 Attention-based CNN in NLP\n\n[4]这篇论文研究的是两个CNN网络，分别处理两个句子，最后输入到分类器中处理。但是这样的模型在输入分类器前句对间是没有相互联系的，作者就想通过设计attention机制将不同cnn通道的句对联系起来。于是提出了3中在CNN中使用attention的方法。\n\n- ABCNN-1: 在卷积前进行attention，通过attention矩阵计算出相应句对的attention feature map，然后连同原来的feature map一起输入到卷积层。\n- ABCNN-2: 在池化时进行attention，通过attention对卷积后的表达重新加权，然后再进行池化.\n- ABCNN-3: ABCNN-1 + ABCNN-2\n\n### 2. Relation Classification \n\n<img src=\"https://github.com/lawlietAi/relation-classification-via-attention-model/raw/master/acnn_structure.png\" width=\"50%\">\n\n#### \t2.1 Classification Objective\n\n作者提出了一种距离函数，即正则化向量差的L2范数：\n$$\n\\delta_{\\theta}(S,y) = ||\\frac{w^O}{|w^O|} - W_y^L||_{L^2} \\\\\nS:\\text{Sentence}, y:\\text{Output relation}, w^O: \\text{Network output}, W^L:\\text{Relation embedding}\n$$\n基于此，作者定义了目标函数：\n$$\n\\mathcal{L} = [\\delta_\\theta(S,y) + (1-\\delta_\\theta(S, \\hat{y}^-))] + \\beta||\\theta||^2 \\\\\n\\hat{y}^- : \\text{A selected incorrect relation label chosen as the one with the highest score among all i.e.} \\\\\n\\hat{y}^- = argmax_{y'\\in \\mathcal{Y},y'\\ne y}(\\delta(S, y'))\n$$\n目标中的两个距离分别为网络输出向量与正例和与某负例的距离，该负例是所有错误类别中与该输出最接近的。最后加上一个正则项，通过使该目标函数最小化来训练网络中的各参数，$\\beta$用于控制其比重。\n\n#### 2.2 Input Representation \n\n现有句子，以及两个已知的实体e1,e2：\n$$\nS = (w_1,w_2,...,w_n) \\\\\ne_1 := w_p, e_2 := w_t . p,t\\in [1,n], p\\ne t\n$$\n为了得到它们的关系，我们把所有词转为词向量；并且根据每个词与实体的相对位置，也转为word position embeddings，每个词与两个实体有两个相对位置，所以得到第i个词的Embedding：\n$$\nw_i^M = [(w_i^d)^T, (w_{i,2}^p)^T,(w_{i,2}^p)^T]^T\n$$\n为了充分得到上下文的信息，再考虑大小为k的滑窗，得到最终的input representation\n$$\nz_i = [(w_{i - (k-1)/2}^M)^T,...,(w_{i + (k-1)/2}^M)^T]^T\n$$\n\n#### 2.3 Input Attention Mechanism\n\n<img src=\"https://pic3.zhimg.com/50/v2-2399a406ad0960c422702728b6418fa3_hd.jpg\" width=\"70%\">\n\n输入级的attention机制是设计两个关于实体对上下文相关的对角矩阵，该矩阵中各元素反映该词语与给定实体间联系的强弱，如$A_{i,i}^j=f(e_j,w_i)$反映了wi和ej之间的联系强弱，这里作者给的 f 就是内积。我们定义：\n$$\n\\alpha_i^j = \\frac{exp(A_{i,i}^j)}{\\sum_{i'=1}^{n}{exp(A_{i',i}^j)}}\n$$\n对于j=1,2 两个相关因子，作者提出了三种处理方式:\n\n- 平均\n  $$\n  r_i = z_i \\frac{\\alpha_i^1 + \\alpha_i^2}{2}\n  $$\n\n- 串联\n  $$\n  r_i = [(z_i \\alpha_i^1)^T, (z_i \\alpha_i^2)^T]^T\n  $$\n\n- 距离\n  $$\n  r_i = z_i \\frac{\\alpha_i^1 - \\alpha_i^2}{2}\n  $$\n\n\n\n\n\n\n\n\n\n最终得到$R = [r_1, r_2,…,r_n]$\n\n#### 2.4 Convolutional Max-Pooling with Secondary Attention\n\n将前面得到的矩阵R送入卷积核大小为dc的卷积层，卷积操作可形式化表示为:\n$$\nR^\\star = tanh(W_fR+B_f), \\text{where the siaze of Wf is } d^c \\times k(d^w+2d^p)\n$$\n然后构建一个相关性矩阵来捕获卷积层输出R*与实体关系WL之间的联系\n$$\nG = R^{\\star T}UW^L, \\\\U :\\text{weighting matrix learnt by the network}\n$$\n\n再用softmax函数来处理相关性矩阵G，获得attention pooling matrix Ap:\n$$\nA_{i,j}^p = \\frac{exp(G_{i,j})}{\\sum_{i'=1}^n{exp(G_{i',j})}}\n$$\n最后用Ap与卷积层输出R*相乘，也就是加入混合中的attention，然后取出每一维度的最大值，得到网络的输出\n$$\nw_i^O = max_j(R^\\star A^p)_{i,j}\n$$\n\n### 3. 总结\n\n从[1]中提到的结果上看，attention的表现确实是在重要的词上有更好的权重，在Sem-Eval-2010 Task 8数据集上取得了显著的效果提升。对于关系抽取来说无疑是非常大的一个进步。\n\n但是还是有一些不足：\n\n- 它要求实体已知，因此需要其他工作来完成实体的识别，使得一些信息的丢失以及错误累加。此时并行模型或端到端模型，同时完成实体识别可能效果会更好；\n- 关系是事先定义的集合，因此更多的是对关系的分类，若能启发式地抽取关系可能会有更广的应用空间；\n- 对于一些上下文没有明显帮助的隐式关系或是使用了比喻之类的修辞，较为容易出错。\n\n这次选择读这篇文章也是想更具体地了解Attention机制，同时了解一些关系抽取的方案，它也有一个pytorch版本的[实现](https://github.com/lawlietAi/relation-classification-via-attention-model)，可以辅以参考。\n\n## Reference\n\n笔记部分参考https://zhuanlan.zhihu.com/p/22867750\n\n[1] Wang, L., Cao, Z., Melo, G. D., & Liu, Z. (2016). Relation Classification via Multi-Level Attention CNNs. *Meeting of the Association for Computational Linguistics* (pp.1298-1307).\n\n[2] Bahdanau, D., Cho, K., & Bengio, Y. (2014). Neural machine translation by jointly learning to align and translate. *Computer Science*.\n\n[3] Luong, M. T., Pham, H., & Manning, C. D. (2015). Effective approaches to attention-based neural machine translation. *Computer Science*.\n\n[4] Yin, W., Schütze, H., Xiang, B., & Zhou, B. (2015). Abcnn: attention-based convolutional neural network for modeling sentence pairs. *Computer Science*.","slug":"[2017.12.17]Relation-Classification-via-Attention-Model","published":1,"updated":"2020-11-03T03:26:14.244Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckw4qufpi001ugwtlfxsu6m4s","content":"<h2 id=\"Relation-Classification-via-Attention-Model\"><a href=\"#Relation-Classification-via-Attention-Model\" class=\"headerlink\" title=\"Relation Classification via Attention Model\"></a>Relation Classification via Attention Model</h2><p>这个笔记主要是阅读论文[1]，它的工作重点是在神经网络构成的端到端学习的关系抽取任务中加入Attention机制。作者主要通过自动学习关系句中注意力较高的部分，而引入attention机制，对反映实体关系更加重要的词语给予更大的attention，较好地提高了关系抽取的效果。</p>\n<img src=\"https://github.com/lawlietAi/relation-classification-via-attention-model/raw/master/acnn_structure.png\" width=\"50%\">\n\n<h3 id=\"1-Attention\"><a href=\"#1-Attention\" class=\"headerlink\" title=\"1. Attention\"></a>1. Attention</h3><h4 id=\"1-1-概述\"><a href=\"#1-1-概述\" class=\"headerlink\" title=\"1.1 概述\"></a>1.1 概述</h4><p>Attention机制最早是在视觉图像领域被提出来的。在NLP任务上，Bahdanau[2]等人使用类似attention的机制在机器翻译任务上将翻译和对齐同时进行。接着类似的基于attention机制的深度学习模型开始广泛应用到各种NLP任务中。</p>\n<h4 id=\"1-2-Recurrent-Models-of-Visual-Attention\"><a href=\"#1-2-Recurrent-Models-of-Visual-Attention\" class=\"headerlink\" title=\"1.2 Recurrent Models of Visual Attention\"></a>1.2 Recurrent Models of Visual Attention</h4><p>人们在进行观察图像的时候，其实并不是一次就把整幅图像的每个位置像素都看过，大多是根据需求将注意力集中到图像的特定部分。由此，在传统的RNN上加入了attention机制，每次当前状态，都会根据前一个状态学习得到的要关注的位置和当前输入的图像，去处理注意力部分像素。可以看到应用Attention机制后，任务的复杂度被降低了很多。</p>\n<h4 id=\"1-3-Attention-based-RNN-in-NLP\"><a href=\"#1-3-Attention-based-RNN-in-NLP\" class=\"headerlink\" title=\"1.3 Attention-based RNN in NLP\"></a>1.3 Attention-based RNN in NLP</h4><p>[1]的成果是在机器翻译任务，一般机器翻译工作由一个Encoder和一个Decoder构成，一个典型的Seq2seq任务。Encoder将源句子进行编码，再利用Decoder将编码后的向量解码成目标语言。</p>\n<p>我们在求注意力分配概率分布的时候，对于输入句子中任意一个单词都给出个概率，从而得到一个概率分布，再对输入句子所有单词的概率进行加权求和，得到Decoder的注意力分配。如下图。</p>\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-201155.jpg\" width=\"30%\">\n\n<p>另一个扩展性更好的论文是[3]，他们的工作告诉了大家attention在RNN中可以如何进行扩展。</p>\n<h4 id=\"1-4-Attention-based-CNN-in-NLP\"><a href=\"#1-4-Attention-based-CNN-in-NLP\" class=\"headerlink\" title=\"1.4 Attention-based CNN in NLP\"></a>1.4 Attention-based CNN in NLP</h4><p>[4]这篇论文研究的是两个CNN网络，分别处理两个句子，最后输入到分类器中处理。但是这样的模型在输入分类器前句对间是没有相互联系的，作者就想通过设计attention机制将不同cnn通道的句对联系起来。于是提出了3中在CNN中使用attention的方法。</p>\n<ul>\n<li>ABCNN-1: 在卷积前进行attention，通过attention矩阵计算出相应句对的attention feature map，然后连同原来的feature map一起输入到卷积层。</li>\n<li>ABCNN-2: 在池化时进行attention，通过attention对卷积后的表达重新加权，然后再进行池化.</li>\n<li>ABCNN-3: ABCNN-1 + ABCNN-2</li>\n</ul>\n<h3 id=\"2-Relation-Classification\"><a href=\"#2-Relation-Classification\" class=\"headerlink\" title=\"2. Relation Classification\"></a>2. Relation Classification</h3><img src=\"https://github.com/lawlietAi/relation-classification-via-attention-model/raw/master/acnn_structure.png\" width=\"50%\">\n\n<h4 id=\"2-1-Classification-Objective\"><a href=\"#2-1-Classification-Objective\" class=\"headerlink\" title=\"2.1 Classification Objective\"></a>2.1 Classification Objective</h4><p>作者提出了一种距离函数，即正则化向量差的L2范数：<br>$$<br>\\delta_{\\theta}(S,y) = ||\\frac{w^O}{|w^O|} - W_y^L||<em>{L^2} \\<br>S:\\text{Sentence}, y:\\text{Output relation}, w^O: \\text{Network output}, W^L:\\text{Relation embedding}<br>$$<br>基于此，作者定义了目标函数：<br>$$<br>\\mathcal{L} = [\\delta_\\theta(S,y) + (1-\\delta_\\theta(S, \\hat{y}^-))] + \\beta||\\theta||^2 \\<br>\\hat{y}^- : \\text{A selected incorrect relation label chosen as the one with the highest score among all i.e.} \\<br>\\hat{y}^- = argmax</em>{y’\\in \\mathcal{Y},y’\\ne y}(\\delta(S, y’))<br>$$<br>目标中的两个距离分别为网络输出向量与正例和与某负例的距离，该负例是所有错误类别中与该输出最接近的。最后加上一个正则项，通过使该目标函数最小化来训练网络中的各参数，$\\beta$用于控制其比重。</p>\n<h4 id=\"2-2-Input-Representation\"><a href=\"#2-2-Input-Representation\" class=\"headerlink\" title=\"2.2 Input Representation\"></a>2.2 Input Representation</h4><p>现有句子，以及两个已知的实体e1,e2：<br>$$<br>S = (w_1,w_2,…,w_n) \\<br>e_1 := w_p, e_2 := w_t . p,t\\in [1,n], p\\ne t<br>$$<br>为了得到它们的关系，我们把所有词转为词向量；并且根据每个词与实体的相对位置，也转为word position embeddings，每个词与两个实体有两个相对位置，所以得到第i个词的Embedding：<br>$$<br>w_i^M = [(w_i^d)^T, (w_{i,2}^p)^T,(w_{i,2}^p)^T]^T<br>$$<br>为了充分得到上下文的信息，再考虑大小为k的滑窗，得到最终的input representation<br>$$<br>z_i = [(w_{i - (k-1)/2}^M)^T,…,(w_{i + (k-1)/2}^M)^T]^T<br>$$</p>\n<h4 id=\"2-3-Input-Attention-Mechanism\"><a href=\"#2-3-Input-Attention-Mechanism\" class=\"headerlink\" title=\"2.3 Input Attention Mechanism\"></a>2.3 Input Attention Mechanism</h4><img src=\"https://pic3.zhimg.com/50/v2-2399a406ad0960c422702728b6418fa3_hd.jpg\" width=\"70%\">\n\n<p>输入级的attention机制是设计两个关于实体对上下文相关的对角矩阵，该矩阵中各元素反映该词语与给定实体间联系的强弱，如$A_{i,i}^j=f(e_j,w_i)$反映了wi和ej之间的联系强弱，这里作者给的 f 就是内积。我们定义：<br>$$<br>\\alpha_i^j = \\frac{exp(A_{i,i}^j)}{\\sum_{i’=1}^{n}{exp(A_{i’,i}^j)}}<br>$$<br>对于j=1,2 两个相关因子，作者提出了三种处理方式:</p>\n<ul>\n<li><p>平均<br>$$<br>r_i = z_i \\frac{\\alpha_i^1 + \\alpha_i^2}{2}<br>$$</p>\n</li>\n<li><p>串联<br>$$<br>r_i = [(z_i \\alpha_i^1)^T, (z_i \\alpha_i^2)^T]^T<br>$$</p>\n</li>\n<li><p>距离<br>$$<br>r_i = z_i \\frac{\\alpha_i^1 - \\alpha_i^2}{2}<br>$$</p>\n</li>\n</ul>\n<p>最终得到$R = [r_1, r_2,…,r_n]$</p>\n<h4 id=\"2-4-Convolutional-Max-Pooling-with-Secondary-Attention\"><a href=\"#2-4-Convolutional-Max-Pooling-with-Secondary-Attention\" class=\"headerlink\" title=\"2.4 Convolutional Max-Pooling with Secondary Attention\"></a>2.4 Convolutional Max-Pooling with Secondary Attention</h4><p>将前面得到的矩阵R送入卷积核大小为dc的卷积层，卷积操作可形式化表示为:<br>$$<br>R^\\star = tanh(W_fR+B_f), \\text{where the siaze of Wf is } d^c \\times k(d^w+2d^p)<br>$$<br>然后构建一个相关性矩阵来捕获卷积层输出R*与实体关系WL之间的联系<br>$$<br>G = R^{\\star T}UW^L, \\U :\\text{weighting matrix learnt by the network}<br>$$</p>\n<p>再用softmax函数来处理相关性矩阵G，获得attention pooling matrix Ap:<br>$$<br>A_{i,j}^p = \\frac{exp(G_{i,j})}{\\sum_{i’=1}^n{exp(G_{i’,j})}}<br>$$<br>最后用Ap与卷积层输出R*相乘，也就是加入混合中的attention，然后取出每一维度的最大值，得到网络的输出<br>$$<br>w_i^O = max_j(R^\\star A^p)_{i,j}<br>$$</p>\n<h3 id=\"3-总结\"><a href=\"#3-总结\" class=\"headerlink\" title=\"3. 总结\"></a>3. 总结</h3><p>从[1]中提到的结果上看，attention的表现确实是在重要的词上有更好的权重，在Sem-Eval-2010 Task 8数据集上取得了显著的效果提升。对于关系抽取来说无疑是非常大的一个进步。</p>\n<p>但是还是有一些不足：</p>\n<ul>\n<li>它要求实体已知，因此需要其他工作来完成实体的识别，使得一些信息的丢失以及错误累加。此时并行模型或端到端模型，同时完成实体识别可能效果会更好；</li>\n<li>关系是事先定义的集合，因此更多的是对关系的分类，若能启发式地抽取关系可能会有更广的应用空间；</li>\n<li>对于一些上下文没有明显帮助的隐式关系或是使用了比喻之类的修辞，较为容易出错。</li>\n</ul>\n<p>这次选择读这篇文章也是想更具体地了解Attention机制，同时了解一些关系抽取的方案，它也有一个pytorch版本的<a href=\"https://github.com/lawlietAi/relation-classification-via-attention-model\">实现</a>，可以辅以参考。</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><p>笔记部分参考<a href=\"https://zhuanlan.zhihu.com/p/22867750\">https://zhuanlan.zhihu.com/p/22867750</a></p>\n<p>[1] Wang, L., Cao, Z., Melo, G. D., &amp; Liu, Z. (2016). Relation Classification via Multi-Level Attention CNNs. <em>Meeting of the Association for Computational Linguistics</em> (pp.1298-1307).</p>\n<p>[2] Bahdanau, D., Cho, K., &amp; Bengio, Y. (2014). Neural machine translation by jointly learning to align and translate. <em>Computer Science</em>.</p>\n<p>[3] Luong, M. T., Pham, H., &amp; Manning, C. D. (2015). Effective approaches to attention-based neural machine translation. <em>Computer Science</em>.</p>\n<p>[4] Yin, W., Schütze, H., Xiang, B., &amp; Zhou, B. (2015). Abcnn: attention-based convolutional neural network for modeling sentence pairs. <em>Computer Science</em>.</p>\n","site":{"data":{"recommended_posts":{"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"}],"https://LorrinWWW.github.io/posts/complexity/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/hexo-with-latex/":[{"title":"区块链时代的最正确的打赏姿势——在Hexo博客配置Coinhive网页挖矿","permalink":"http://www.davidfnck.com/blockchain/mine-xmr-in-hexo-by-coinhive.html/"}],"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/":[{"title":"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-statistic/":[{"title":"贝叶斯估计 Bayes estimation","permalink":"https://LorrinWWW.github.io/posts/Bayes-estimation/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"}],"https://LorrinWWW.github.io/posts/[2018.2.5]Nested-LSTMs/":[{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"}],"https://LorrinWWW.github.io/posts/datamining-pretreatment/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Several models for knowledge graph representing and completing 几个知识图谱模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/"}],"https://LorrinWWW.github.io/posts/compression/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/Sobolev-space/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/proba-ch5/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch3/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/proba-ch1/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/ML-CNN/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/EDP-basic-models/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/machine-learning/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/datamining-class-pred/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction/":[],"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/":[{"title":"OS notes","permalink":"https://LorrinWWW.github.io/posts/OS-notes/"}],"https://LorrinWWW.github.io/posts/projet-enjeu-plugin-chrome-101/":[{"title":"Chrome油猴脚本","permalink":"http://gubangzhong.cn/2017/06/13/Chrome油猴脚本/"}],"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/":[{"title":"MongoDB, Docker and Python","permalink":"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/":[{"title":"Hands on Scrapy","permalink":"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/OS-notes/":[{"title":"learning OS and building LorriOS","permalink":"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/"}],"https://LorrinWWW.github.io/posts/Bayes-estimation/":[{"title":"Note of statistic","permalink":"https://LorrinWWW.github.io/posts/Note-of-statistic/"}],"https://LorrinWWW.github.io/posts/management-of-the-firm/":[],"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"}],"https://LorrinWWW.github.io/posts/datamining-qualitative-induction/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-finite-element-method/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/QuadTree/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/proba-ch4/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch2/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/":[{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/Note-of-NLP/":[{"title":"Generative Adversarial Network","permalink":"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/"},{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"}],"https://LorrinWWW.github.io/posts/Note-of-datamining/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"数据挖掘-分类与预测","permalink":"https://LorrinWWW.github.io/posts/datamining-class-pred/"},{"title":"数据挖掘-预处理","permalink":"https://LorrinWWW.github.io/posts/datamining-pretreatment/"}],"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/Note-of-probability/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/participe-present-et-gerondif/":[],"https://LorrinWWW.github.io/posts/Hilbert-space/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/proba-ch6/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}]}}},"excerpt":"","more":"<h2 id=\"Relation-Classification-via-Attention-Model\"><a href=\"#Relation-Classification-via-Attention-Model\" class=\"headerlink\" title=\"Relation Classification via Attention Model\"></a>Relation Classification via Attention Model</h2><p>这个笔记主要是阅读论文[1]，它的工作重点是在神经网络构成的端到端学习的关系抽取任务中加入Attention机制。作者主要通过自动学习关系句中注意力较高的部分，而引入attention机制，对反映实体关系更加重要的词语给予更大的attention，较好地提高了关系抽取的效果。</p>\n<img src=\"https://github.com/lawlietAi/relation-classification-via-attention-model/raw/master/acnn_structure.png\" width=\"50%\">\n\n<h3 id=\"1-Attention\"><a href=\"#1-Attention\" class=\"headerlink\" title=\"1. Attention\"></a>1. Attention</h3><h4 id=\"1-1-概述\"><a href=\"#1-1-概述\" class=\"headerlink\" title=\"1.1 概述\"></a>1.1 概述</h4><p>Attention机制最早是在视觉图像领域被提出来的。在NLP任务上，Bahdanau[2]等人使用类似attention的机制在机器翻译任务上将翻译和对齐同时进行。接着类似的基于attention机制的深度学习模型开始广泛应用到各种NLP任务中。</p>\n<h4 id=\"1-2-Recurrent-Models-of-Visual-Attention\"><a href=\"#1-2-Recurrent-Models-of-Visual-Attention\" class=\"headerlink\" title=\"1.2 Recurrent Models of Visual Attention\"></a>1.2 Recurrent Models of Visual Attention</h4><p>人们在进行观察图像的时候，其实并不是一次就把整幅图像的每个位置像素都看过，大多是根据需求将注意力集中到图像的特定部分。由此，在传统的RNN上加入了attention机制，每次当前状态，都会根据前一个状态学习得到的要关注的位置和当前输入的图像，去处理注意力部分像素。可以看到应用Attention机制后，任务的复杂度被降低了很多。</p>\n<h4 id=\"1-3-Attention-based-RNN-in-NLP\"><a href=\"#1-3-Attention-based-RNN-in-NLP\" class=\"headerlink\" title=\"1.3 Attention-based RNN in NLP\"></a>1.3 Attention-based RNN in NLP</h4><p>[1]的成果是在机器翻译任务，一般机器翻译工作由一个Encoder和一个Decoder构成，一个典型的Seq2seq任务。Encoder将源句子进行编码，再利用Decoder将编码后的向量解码成目标语言。</p>\n<p>我们在求注意力分配概率分布的时候，对于输入句子中任意一个单词都给出个概率，从而得到一个概率分布，再对输入句子所有单词的概率进行加权求和，得到Decoder的注意力分配。如下图。</p>\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-201155.jpg\" width=\"30%\">\n\n<p>另一个扩展性更好的论文是[3]，他们的工作告诉了大家attention在RNN中可以如何进行扩展。</p>\n<h4 id=\"1-4-Attention-based-CNN-in-NLP\"><a href=\"#1-4-Attention-based-CNN-in-NLP\" class=\"headerlink\" title=\"1.4 Attention-based CNN in NLP\"></a>1.4 Attention-based CNN in NLP</h4><p>[4]这篇论文研究的是两个CNN网络，分别处理两个句子，最后输入到分类器中处理。但是这样的模型在输入分类器前句对间是没有相互联系的，作者就想通过设计attention机制将不同cnn通道的句对联系起来。于是提出了3中在CNN中使用attention的方法。</p>\n<ul>\n<li>ABCNN-1: 在卷积前进行attention，通过attention矩阵计算出相应句对的attention feature map，然后连同原来的feature map一起输入到卷积层。</li>\n<li>ABCNN-2: 在池化时进行attention，通过attention对卷积后的表达重新加权，然后再进行池化.</li>\n<li>ABCNN-3: ABCNN-1 + ABCNN-2</li>\n</ul>\n<h3 id=\"2-Relation-Classification\"><a href=\"#2-Relation-Classification\" class=\"headerlink\" title=\"2. Relation Classification\"></a>2. Relation Classification</h3><img src=\"https://github.com/lawlietAi/relation-classification-via-attention-model/raw/master/acnn_structure.png\" width=\"50%\">\n\n<h4 id=\"2-1-Classification-Objective\"><a href=\"#2-1-Classification-Objective\" class=\"headerlink\" title=\"2.1 Classification Objective\"></a>2.1 Classification Objective</h4><p>作者提出了一种距离函数，即正则化向量差的L2范数：<br>$$<br>\\delta_{\\theta}(S,y) = ||\\frac{w^O}{|w^O|} - W_y^L||<em>{L^2} \\<br>S:\\text{Sentence}, y:\\text{Output relation}, w^O: \\text{Network output}, W^L:\\text{Relation embedding}<br>$$<br>基于此，作者定义了目标函数：<br>$$<br>\\mathcal{L} = [\\delta_\\theta(S,y) + (1-\\delta_\\theta(S, \\hat{y}^-))] + \\beta||\\theta||^2 \\<br>\\hat{y}^- : \\text{A selected incorrect relation label chosen as the one with the highest score among all i.e.} \\<br>\\hat{y}^- = argmax</em>{y’\\in \\mathcal{Y},y’\\ne y}(\\delta(S, y’))<br>$$<br>目标中的两个距离分别为网络输出向量与正例和与某负例的距离，该负例是所有错误类别中与该输出最接近的。最后加上一个正则项，通过使该目标函数最小化来训练网络中的各参数，$\\beta$用于控制其比重。</p>\n<h4 id=\"2-2-Input-Representation\"><a href=\"#2-2-Input-Representation\" class=\"headerlink\" title=\"2.2 Input Representation\"></a>2.2 Input Representation</h4><p>现有句子，以及两个已知的实体e1,e2：<br>$$<br>S = (w_1,w_2,…,w_n) \\<br>e_1 := w_p, e_2 := w_t . p,t\\in [1,n], p\\ne t<br>$$<br>为了得到它们的关系，我们把所有词转为词向量；并且根据每个词与实体的相对位置，也转为word position embeddings，每个词与两个实体有两个相对位置，所以得到第i个词的Embedding：<br>$$<br>w_i^M = [(w_i^d)^T, (w_{i,2}^p)^T,(w_{i,2}^p)^T]^T<br>$$<br>为了充分得到上下文的信息，再考虑大小为k的滑窗，得到最终的input representation<br>$$<br>z_i = [(w_{i - (k-1)/2}^M)^T,…,(w_{i + (k-1)/2}^M)^T]^T<br>$$</p>\n<h4 id=\"2-3-Input-Attention-Mechanism\"><a href=\"#2-3-Input-Attention-Mechanism\" class=\"headerlink\" title=\"2.3 Input Attention Mechanism\"></a>2.3 Input Attention Mechanism</h4><img src=\"https://pic3.zhimg.com/50/v2-2399a406ad0960c422702728b6418fa3_hd.jpg\" width=\"70%\">\n\n<p>输入级的attention机制是设计两个关于实体对上下文相关的对角矩阵，该矩阵中各元素反映该词语与给定实体间联系的强弱，如$A_{i,i}^j=f(e_j,w_i)$反映了wi和ej之间的联系强弱，这里作者给的 f 就是内积。我们定义：<br>$$<br>\\alpha_i^j = \\frac{exp(A_{i,i}^j)}{\\sum_{i’=1}^{n}{exp(A_{i’,i}^j)}}<br>$$<br>对于j=1,2 两个相关因子，作者提出了三种处理方式:</p>\n<ul>\n<li><p>平均<br>$$<br>r_i = z_i \\frac{\\alpha_i^1 + \\alpha_i^2}{2}<br>$$</p>\n</li>\n<li><p>串联<br>$$<br>r_i = [(z_i \\alpha_i^1)^T, (z_i \\alpha_i^2)^T]^T<br>$$</p>\n</li>\n<li><p>距离<br>$$<br>r_i = z_i \\frac{\\alpha_i^1 - \\alpha_i^2}{2}<br>$$</p>\n</li>\n</ul>\n<p>最终得到$R = [r_1, r_2,…,r_n]$</p>\n<h4 id=\"2-4-Convolutional-Max-Pooling-with-Secondary-Attention\"><a href=\"#2-4-Convolutional-Max-Pooling-with-Secondary-Attention\" class=\"headerlink\" title=\"2.4 Convolutional Max-Pooling with Secondary Attention\"></a>2.4 Convolutional Max-Pooling with Secondary Attention</h4><p>将前面得到的矩阵R送入卷积核大小为dc的卷积层，卷积操作可形式化表示为:<br>$$<br>R^\\star = tanh(W_fR+B_f), \\text{where the siaze of Wf is } d^c \\times k(d^w+2d^p)<br>$$<br>然后构建一个相关性矩阵来捕获卷积层输出R*与实体关系WL之间的联系<br>$$<br>G = R^{\\star T}UW^L, \\U :\\text{weighting matrix learnt by the network}<br>$$</p>\n<p>再用softmax函数来处理相关性矩阵G，获得attention pooling matrix Ap:<br>$$<br>A_{i,j}^p = \\frac{exp(G_{i,j})}{\\sum_{i’=1}^n{exp(G_{i’,j})}}<br>$$<br>最后用Ap与卷积层输出R*相乘，也就是加入混合中的attention，然后取出每一维度的最大值，得到网络的输出<br>$$<br>w_i^O = max_j(R^\\star A^p)_{i,j}<br>$$</p>\n<h3 id=\"3-总结\"><a href=\"#3-总结\" class=\"headerlink\" title=\"3. 总结\"></a>3. 总结</h3><p>从[1]中提到的结果上看，attention的表现确实是在重要的词上有更好的权重，在Sem-Eval-2010 Task 8数据集上取得了显著的效果提升。对于关系抽取来说无疑是非常大的一个进步。</p>\n<p>但是还是有一些不足：</p>\n<ul>\n<li>它要求实体已知，因此需要其他工作来完成实体的识别，使得一些信息的丢失以及错误累加。此时并行模型或端到端模型，同时完成实体识别可能效果会更好；</li>\n<li>关系是事先定义的集合，因此更多的是对关系的分类，若能启发式地抽取关系可能会有更广的应用空间；</li>\n<li>对于一些上下文没有明显帮助的隐式关系或是使用了比喻之类的修辞，较为容易出错。</li>\n</ul>\n<p>这次选择读这篇文章也是想更具体地了解Attention机制，同时了解一些关系抽取的方案，它也有一个pytorch版本的<a href=\"https://github.com/lawlietAi/relation-classification-via-attention-model\">实现</a>，可以辅以参考。</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><p>笔记部分参考<a href=\"https://zhuanlan.zhihu.com/p/22867750\">https://zhuanlan.zhihu.com/p/22867750</a></p>\n<p>[1] Wang, L., Cao, Z., Melo, G. D., &amp; Liu, Z. (2016). Relation Classification via Multi-Level Attention CNNs. <em>Meeting of the Association for Computational Linguistics</em> (pp.1298-1307).</p>\n<p>[2] Bahdanau, D., Cho, K., &amp; Bengio, Y. (2014). Neural machine translation by jointly learning to align and translate. <em>Computer Science</em>.</p>\n<p>[3] Luong, M. T., Pham, H., &amp; Manning, C. D. (2015). Effective approaches to attention-based neural machine translation. <em>Computer Science</em>.</p>\n<p>[4] Yin, W., Schütze, H., Xiang, B., &amp; Zhou, B. (2015). Abcnn: attention-based convolutional neural network for modeling sentence pairs. <em>Computer Science</em>.</p>\n"},{"title":"Event detection and co-reference with minimal supervision 笔记","date":"2018-01-21T00:00:00.000Z","_content":"\n## Event detection and co-reference with minimal supervision [1]\n\n**摘要：**该论文使用了一种弱监督的算法解决了事件检测与共指问题。事件共指问题可以看作是一种事件之间的相似度计算问题，而在该文中，事件检测问题也被看作是一种相似度检测问题。对于ACE或rich ERE划分的所有事件类型，使用每个类型中的几个实例作为该类型事件的向量，然后计算新事件向量与每个类型事件向量之间的相似度，根据这一相似度对事件进行判断。该文的另一个特点在于事件特征的选择，在将事件表示为向量的过程中，使用了Freebase作为特征来对事件进行表示。\n\n### 1. Introduction\n\n<img src='https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202215.jpg' width='70%'>\n\n上图是论文提出的MSEP（Minimally Supervised Event Pipeline）框架。这里 Event examples 是唯一的监督来源，用于产生 Example vectors。在MSEP框架中不需要训练。\n\n这篇论文主要是针对两个问题：\n\n- Event detection 指的是对一段文本内容，检测是否存在符合要求的事件。\n- Co-reference problem. 为了更好的理解和利用事件的信息，我们需要从文本中提取出时间、地点、人物、行为等信息。此外，我们还需要了解两个事件的关系，例如，判断两个事件是否表示同一个事件，这就是Co-reference problem。\n\n在本文中，我们提出了一种更加可行且更加可测的方法来描述事件。对于一个事件e，event detection 所要做的就是判断是否存在一个事件集合，事件e在语义上是否有关联，以至于可以被划分到该集合内；而 co-reference problem 则是判断两个事件e1、e2是否在语义上表述足够接近，以至于我们认为它们所表示的实际上是同一个事件。可以看到两个任务实际上都需要判断相似性，我们可以把它们转化为语义相似性问题。\n\n现在主要问题有：1. 如何表示一个事件；2. 如何表达相似性。前者我们采用了semantic role labeling  representation（SRL），来结构化地描述一个事件；对于后者，我们将对事件做一个embedding，通过计算其余弦距离来表达相似性。\n\n我们提出了一个通用事件检测和指代消解框架，它基本上不需要标记数据。在实践中，为了将一个事件提法（event mention）和一个事件本体（event ontology）相联系起来，我们只需要一些事件示例。这种定义类型的方式是非常合理的，因为给出例子是定义事件类型的最简单的方法。我们的方法比标准的无监督方法要求更少假设，在我们的模型中，给定事件类型的定义（以事件例子的形式），我们可以将单个事件分类到已知本体，并确定两个事件是否是 co-reference 的。\n\n\n\n### 2. The MSEP System\n\n#### 2.1 Structured Vector Representation \n\n事件结构和句子结构之间有一个平行关系。我们发现一般来说，事件的触发词往往是谓语，所以可以针对谓语对其做一些改进：\n\n![basic](https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202214.jpg)\n\n**Basic event vector representation**。基本事件向量由它的各个组成部分组成。\n\n![basic](https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202217.jpg)\n\n**Augmented event vector representation**。在这里，“+” 表示我们首先将文本片段放在一起，然后将组合的文本片段转换成ESA向量。\n\n#### 2.2 Event Mention Detection\n\n我们定义 Event type representation 为该类别下的事件向量的平均值。\n\n我们定义定义相似度如下\n$$\nS(e_1, e_2) = \\frac{vec(e_1) · vec(e_2)}{||vec(e_1)||·||vec(e_2)||} \\\\\n= \\frac{\\sum_a{vec(a_1) · vec(a_2)}}{\\sqrt{\\sum_a{||vec(a_1)||^2} · \\sum_a{||vec(a_2)||^2}}}\n$$\n其中 e1 是待处理事件，e2 是事件的类别。a 就是事件里的各个组件。若遇到 a 缺失的情况（如地点、时间等），我们用非缺失的部分的平均值来代替它。具体的操作方法参见原文。\n\n#### 2.3 Event co-reference\n\n这里如上一节的内容所说，通过余弦距离$S(e_1, e_2)$来计算两个事件的相似度。\n\n对于每一个事件，我们分别比较$agnet_{sub}, agnet_{obj}$，若都不相同，我们认为它们是独立的；如果有缺失，我们认为它和任意值匹配。这样，我们可以得到一个不重复的事件集合，$Set_{conflict}$。\n\n接下来遍历所有事件，对于事件k+1，\n$$\ne_p = argmax_{e\\in \\{e_1,...,e_k\\} e \\notin Set_{conflit}} {S(e_p, e_{k+1})}\n$$\n如果$S(e_p, e_{k+1})$的值大于我们设定的阈值，我们就认为它是同一个事件；否则，我们把他分为一个新的类。\n\n### 3. Vector Representation\n\n我们可以看到，其实文章之前的内容都不依赖于 embedding 的具体选择，事实上，作者也测试了很多的方法，可以根据实际情况来选择。\n\n- Explicit Semantic Analysis\n- Brown Cluster\n- Word2Vec\n- Dependency-Based Embedding\n\n### 4. Semantic Role Labeling\n\n上面工作建立在已经完成了 Semantic Role Labeling 的情况下，这里我们在讨论一下如何进行 Semantic Role Labeling。\n\n对于标注任务来说大同小异，现在往往使用神经网络模型来进行标注，例如[2]，缺点是需要大量标注数据。目前业内比较主流的解决方案是RNN-CRF模型，一般来说分为：\n\n- Embedding layer\n- Bi-directional RNN (usually LSTM) layer\n- Tanh hidden layer\n- CRF layer\n\n在实际应用上，可能还会增加Attention机制等来进一步提高它的效果。\n\n目前已有的系统如哈工大的语言技术平台LTP，能够用于 Semantic Role Labeling 等。\n\n### 5. Conclusion\n\n这一篇文章提出了一种新颖的事件检测和指代消解方法。其最重要的部分就是提出了一种结构化的向量，能够更好地表示event，用以进行事件分类、指代消解等工作。这个方法在一些关键指标上甚至能优于最新的监督方法，并且能够更好地适应新的领域。\n\n\n\n## Bibliography\n\n[1] Peng, H., Song, Y., & Roth, D. (2016). Event Detection and Co-reference with Minimal Supervision. In *EMNLP* (pp. 392-402).\n\n[2] Zhou, J., & Xu, W. (2015). End-to-end learning of semantic role labeling using recurrent neural networks. In *ACL (1)* (pp. 1127-1137).","source":"_posts/[2018.1.21]Event-detection-and-co-referentce.md","raw":"---\ntitle: Event detection and co-reference with minimal supervision 笔记\ndate: 2018-01-21 08:00:00\ncategories: [research]\ntags: [event-detection, co-reference]\n---\n\n## Event detection and co-reference with minimal supervision [1]\n\n**摘要：**该论文使用了一种弱监督的算法解决了事件检测与共指问题。事件共指问题可以看作是一种事件之间的相似度计算问题，而在该文中，事件检测问题也被看作是一种相似度检测问题。对于ACE或rich ERE划分的所有事件类型，使用每个类型中的几个实例作为该类型事件的向量，然后计算新事件向量与每个类型事件向量之间的相似度，根据这一相似度对事件进行判断。该文的另一个特点在于事件特征的选择，在将事件表示为向量的过程中，使用了Freebase作为特征来对事件进行表示。\n\n### 1. Introduction\n\n<img src='https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202215.jpg' width='70%'>\n\n上图是论文提出的MSEP（Minimally Supervised Event Pipeline）框架。这里 Event examples 是唯一的监督来源，用于产生 Example vectors。在MSEP框架中不需要训练。\n\n这篇论文主要是针对两个问题：\n\n- Event detection 指的是对一段文本内容，检测是否存在符合要求的事件。\n- Co-reference problem. 为了更好的理解和利用事件的信息，我们需要从文本中提取出时间、地点、人物、行为等信息。此外，我们还需要了解两个事件的关系，例如，判断两个事件是否表示同一个事件，这就是Co-reference problem。\n\n在本文中，我们提出了一种更加可行且更加可测的方法来描述事件。对于一个事件e，event detection 所要做的就是判断是否存在一个事件集合，事件e在语义上是否有关联，以至于可以被划分到该集合内；而 co-reference problem 则是判断两个事件e1、e2是否在语义上表述足够接近，以至于我们认为它们所表示的实际上是同一个事件。可以看到两个任务实际上都需要判断相似性，我们可以把它们转化为语义相似性问题。\n\n现在主要问题有：1. 如何表示一个事件；2. 如何表达相似性。前者我们采用了semantic role labeling  representation（SRL），来结构化地描述一个事件；对于后者，我们将对事件做一个embedding，通过计算其余弦距离来表达相似性。\n\n我们提出了一个通用事件检测和指代消解框架，它基本上不需要标记数据。在实践中，为了将一个事件提法（event mention）和一个事件本体（event ontology）相联系起来，我们只需要一些事件示例。这种定义类型的方式是非常合理的，因为给出例子是定义事件类型的最简单的方法。我们的方法比标准的无监督方法要求更少假设，在我们的模型中，给定事件类型的定义（以事件例子的形式），我们可以将单个事件分类到已知本体，并确定两个事件是否是 co-reference 的。\n\n\n\n### 2. The MSEP System\n\n#### 2.1 Structured Vector Representation \n\n事件结构和句子结构之间有一个平行关系。我们发现一般来说，事件的触发词往往是谓语，所以可以针对谓语对其做一些改进：\n\n![basic](https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202214.jpg)\n\n**Basic event vector representation**。基本事件向量由它的各个组成部分组成。\n\n![basic](https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202217.jpg)\n\n**Augmented event vector representation**。在这里，“+” 表示我们首先将文本片段放在一起，然后将组合的文本片段转换成ESA向量。\n\n#### 2.2 Event Mention Detection\n\n我们定义 Event type representation 为该类别下的事件向量的平均值。\n\n我们定义定义相似度如下\n$$\nS(e_1, e_2) = \\frac{vec(e_1) · vec(e_2)}{||vec(e_1)||·||vec(e_2)||} \\\\\n= \\frac{\\sum_a{vec(a_1) · vec(a_2)}}{\\sqrt{\\sum_a{||vec(a_1)||^2} · \\sum_a{||vec(a_2)||^2}}}\n$$\n其中 e1 是待处理事件，e2 是事件的类别。a 就是事件里的各个组件。若遇到 a 缺失的情况（如地点、时间等），我们用非缺失的部分的平均值来代替它。具体的操作方法参见原文。\n\n#### 2.3 Event co-reference\n\n这里如上一节的内容所说，通过余弦距离$S(e_1, e_2)$来计算两个事件的相似度。\n\n对于每一个事件，我们分别比较$agnet_{sub}, agnet_{obj}$，若都不相同，我们认为它们是独立的；如果有缺失，我们认为它和任意值匹配。这样，我们可以得到一个不重复的事件集合，$Set_{conflict}$。\n\n接下来遍历所有事件，对于事件k+1，\n$$\ne_p = argmax_{e\\in \\{e_1,...,e_k\\} e \\notin Set_{conflit}} {S(e_p, e_{k+1})}\n$$\n如果$S(e_p, e_{k+1})$的值大于我们设定的阈值，我们就认为它是同一个事件；否则，我们把他分为一个新的类。\n\n### 3. Vector Representation\n\n我们可以看到，其实文章之前的内容都不依赖于 embedding 的具体选择，事实上，作者也测试了很多的方法，可以根据实际情况来选择。\n\n- Explicit Semantic Analysis\n- Brown Cluster\n- Word2Vec\n- Dependency-Based Embedding\n\n### 4. Semantic Role Labeling\n\n上面工作建立在已经完成了 Semantic Role Labeling 的情况下，这里我们在讨论一下如何进行 Semantic Role Labeling。\n\n对于标注任务来说大同小异，现在往往使用神经网络模型来进行标注，例如[2]，缺点是需要大量标注数据。目前业内比较主流的解决方案是RNN-CRF模型，一般来说分为：\n\n- Embedding layer\n- Bi-directional RNN (usually LSTM) layer\n- Tanh hidden layer\n- CRF layer\n\n在实际应用上，可能还会增加Attention机制等来进一步提高它的效果。\n\n目前已有的系统如哈工大的语言技术平台LTP，能够用于 Semantic Role Labeling 等。\n\n### 5. Conclusion\n\n这一篇文章提出了一种新颖的事件检测和指代消解方法。其最重要的部分就是提出了一种结构化的向量，能够更好地表示event，用以进行事件分类、指代消解等工作。这个方法在一些关键指标上甚至能优于最新的监督方法，并且能够更好地适应新的领域。\n\n\n\n## Bibliography\n\n[1] Peng, H., Song, Y., & Roth, D. (2016). Event Detection and Co-reference with Minimal Supervision. In *EMNLP* (pp. 392-402).\n\n[2] Zhou, J., & Xu, W. (2015). End-to-end learning of semantic role labeling using recurrent neural networks. In *ACL (1)* (pp. 1127-1137).","slug":"[2018.1.21]Event-detection-and-co-referentce","published":1,"updated":"2020-11-03T03:26:13.403Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckw4qufpm001ygwtl9ur9czfz","content":"<h2 id=\"Event-detection-and-co-reference-with-minimal-supervision-1\"><a href=\"#Event-detection-and-co-reference-with-minimal-supervision-1\" class=\"headerlink\" title=\"Event detection and co-reference with minimal supervision [1]\"></a>Event detection and co-reference with minimal supervision [1]</h2><p><strong>摘要：</strong>该论文使用了一种弱监督的算法解决了事件检测与共指问题。事件共指问题可以看作是一种事件之间的相似度计算问题，而在该文中，事件检测问题也被看作是一种相似度检测问题。对于ACE或rich ERE划分的所有事件类型，使用每个类型中的几个实例作为该类型事件的向量，然后计算新事件向量与每个类型事件向量之间的相似度，根据这一相似度对事件进行判断。该文的另一个特点在于事件特征的选择，在将事件表示为向量的过程中，使用了Freebase作为特征来对事件进行表示。</p>\n<h3 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1. Introduction\"></a>1. Introduction</h3><img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202215.jpg\" width=\"70%\">\n\n<p>上图是论文提出的MSEP（Minimally Supervised Event Pipeline）框架。这里 Event examples 是唯一的监督来源，用于产生 Example vectors。在MSEP框架中不需要训练。</p>\n<p>这篇论文主要是针对两个问题：</p>\n<ul>\n<li>Event detection 指的是对一段文本内容，检测是否存在符合要求的事件。</li>\n<li>Co-reference problem. 为了更好的理解和利用事件的信息，我们需要从文本中提取出时间、地点、人物、行为等信息。此外，我们还需要了解两个事件的关系，例如，判断两个事件是否表示同一个事件，这就是Co-reference problem。</li>\n</ul>\n<p>在本文中，我们提出了一种更加可行且更加可测的方法来描述事件。对于一个事件e，event detection 所要做的就是判断是否存在一个事件集合，事件e在语义上是否有关联，以至于可以被划分到该集合内；而 co-reference problem 则是判断两个事件e1、e2是否在语义上表述足够接近，以至于我们认为它们所表示的实际上是同一个事件。可以看到两个任务实际上都需要判断相似性，我们可以把它们转化为语义相似性问题。</p>\n<p>现在主要问题有：1. 如何表示一个事件；2. 如何表达相似性。前者我们采用了semantic role labeling  representation（SRL），来结构化地描述一个事件；对于后者，我们将对事件做一个embedding，通过计算其余弦距离来表达相似性。</p>\n<p>我们提出了一个通用事件检测和指代消解框架，它基本上不需要标记数据。在实践中，为了将一个事件提法（event mention）和一个事件本体（event ontology）相联系起来，我们只需要一些事件示例。这种定义类型的方式是非常合理的，因为给出例子是定义事件类型的最简单的方法。我们的方法比标准的无监督方法要求更少假设，在我们的模型中，给定事件类型的定义（以事件例子的形式），我们可以将单个事件分类到已知本体，并确定两个事件是否是 co-reference 的。</p>\n<h3 id=\"2-The-MSEP-System\"><a href=\"#2-The-MSEP-System\" class=\"headerlink\" title=\"2. The MSEP System\"></a>2. The MSEP System</h3><h4 id=\"2-1-Structured-Vector-Representation\"><a href=\"#2-1-Structured-Vector-Representation\" class=\"headerlink\" title=\"2.1 Structured Vector Representation\"></a>2.1 Structured Vector Representation</h4><p>事件结构和句子结构之间有一个平行关系。我们发现一般来说，事件的触发词往往是谓语，所以可以针对谓语对其做一些改进：</p>\n<p><img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202214.jpg\" alt=\"basic\"></p>\n<p><strong>Basic event vector representation</strong>。基本事件向量由它的各个组成部分组成。</p>\n<p><img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202217.jpg\" alt=\"basic\"></p>\n<p><strong>Augmented event vector representation</strong>。在这里，“+” 表示我们首先将文本片段放在一起，然后将组合的文本片段转换成ESA向量。</p>\n<h4 id=\"2-2-Event-Mention-Detection\"><a href=\"#2-2-Event-Mention-Detection\" class=\"headerlink\" title=\"2.2 Event Mention Detection\"></a>2.2 Event Mention Detection</h4><p>我们定义 Event type representation 为该类别下的事件向量的平均值。</p>\n<p>我们定义定义相似度如下<br>$$<br>S(e_1, e_2) = \\frac{vec(e_1) · vec(e_2)}{||vec(e_1)||·||vec(e_2)||} \\<br>= \\frac{\\sum_a{vec(a_1) · vec(a_2)}}{\\sqrt{\\sum_a{||vec(a_1)||^2} · \\sum_a{||vec(a_2)||^2}}}<br>$$<br>其中 e1 是待处理事件，e2 是事件的类别。a 就是事件里的各个组件。若遇到 a 缺失的情况（如地点、时间等），我们用非缺失的部分的平均值来代替它。具体的操作方法参见原文。</p>\n<h4 id=\"2-3-Event-co-reference\"><a href=\"#2-3-Event-co-reference\" class=\"headerlink\" title=\"2.3 Event co-reference\"></a>2.3 Event co-reference</h4><p>这里如上一节的内容所说，通过余弦距离$S(e_1, e_2)$来计算两个事件的相似度。</p>\n<p>对于每一个事件，我们分别比较$agnet_{sub}, agnet_{obj}$，若都不相同，我们认为它们是独立的；如果有缺失，我们认为它和任意值匹配。这样，我们可以得到一个不重复的事件集合，$Set_{conflict}$。</p>\n<p>接下来遍历所有事件，对于事件k+1，<br>$$<br>e_p = argmax_{e\\in {e_1,…,e_k} e \\notin Set_{conflit}} {S(e_p, e_{k+1})}<br>$$<br>如果$S(e_p, e_{k+1})$的值大于我们设定的阈值，我们就认为它是同一个事件；否则，我们把他分为一个新的类。</p>\n<h3 id=\"3-Vector-Representation\"><a href=\"#3-Vector-Representation\" class=\"headerlink\" title=\"3. Vector Representation\"></a>3. Vector Representation</h3><p>我们可以看到，其实文章之前的内容都不依赖于 embedding 的具体选择，事实上，作者也测试了很多的方法，可以根据实际情况来选择。</p>\n<ul>\n<li>Explicit Semantic Analysis</li>\n<li>Brown Cluster</li>\n<li>Word2Vec</li>\n<li>Dependency-Based Embedding</li>\n</ul>\n<h3 id=\"4-Semantic-Role-Labeling\"><a href=\"#4-Semantic-Role-Labeling\" class=\"headerlink\" title=\"4. Semantic Role Labeling\"></a>4. Semantic Role Labeling</h3><p>上面工作建立在已经完成了 Semantic Role Labeling 的情况下，这里我们在讨论一下如何进行 Semantic Role Labeling。</p>\n<p>对于标注任务来说大同小异，现在往往使用神经网络模型来进行标注，例如[2]，缺点是需要大量标注数据。目前业内比较主流的解决方案是RNN-CRF模型，一般来说分为：</p>\n<ul>\n<li>Embedding layer</li>\n<li>Bi-directional RNN (usually LSTM) layer</li>\n<li>Tanh hidden layer</li>\n<li>CRF layer</li>\n</ul>\n<p>在实际应用上，可能还会增加Attention机制等来进一步提高它的效果。</p>\n<p>目前已有的系统如哈工大的语言技术平台LTP，能够用于 Semantic Role Labeling 等。</p>\n<h3 id=\"5-Conclusion\"><a href=\"#5-Conclusion\" class=\"headerlink\" title=\"5. Conclusion\"></a>5. Conclusion</h3><p>这一篇文章提出了一种新颖的事件检测和指代消解方法。其最重要的部分就是提出了一种结构化的向量，能够更好地表示event，用以进行事件分类、指代消解等工作。这个方法在一些关键指标上甚至能优于最新的监督方法，并且能够更好地适应新的领域。</p>\n<h2 id=\"Bibliography\"><a href=\"#Bibliography\" class=\"headerlink\" title=\"Bibliography\"></a>Bibliography</h2><p>[1] Peng, H., Song, Y., &amp; Roth, D. (2016). Event Detection and Co-reference with Minimal Supervision. In <em>EMNLP</em> (pp. 392-402).</p>\n<p>[2] Zhou, J., &amp; Xu, W. (2015). End-to-end learning of semantic role labeling using recurrent neural networks. In <em>ACL (1)</em> (pp. 1127-1137).</p>\n","site":{"data":{"recommended_posts":{"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"}],"https://LorrinWWW.github.io/posts/complexity/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/hexo-with-latex/":[{"title":"区块链时代的最正确的打赏姿势——在Hexo博客配置Coinhive网页挖矿","permalink":"http://www.davidfnck.com/blockchain/mine-xmr-in-hexo-by-coinhive.html/"}],"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/":[{"title":"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-statistic/":[{"title":"贝叶斯估计 Bayes estimation","permalink":"https://LorrinWWW.github.io/posts/Bayes-estimation/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"}],"https://LorrinWWW.github.io/posts/[2018.2.5]Nested-LSTMs/":[{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"}],"https://LorrinWWW.github.io/posts/datamining-pretreatment/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Several models for knowledge graph representing and completing 几个知识图谱模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/"}],"https://LorrinWWW.github.io/posts/compression/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/Sobolev-space/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/proba-ch5/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch3/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/proba-ch1/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/ML-CNN/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/EDP-basic-models/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/machine-learning/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/datamining-class-pred/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction/":[],"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/":[{"title":"OS notes","permalink":"https://LorrinWWW.github.io/posts/OS-notes/"}],"https://LorrinWWW.github.io/posts/projet-enjeu-plugin-chrome-101/":[{"title":"Chrome油猴脚本","permalink":"http://gubangzhong.cn/2017/06/13/Chrome油猴脚本/"}],"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/":[{"title":"MongoDB, Docker and Python","permalink":"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/":[{"title":"Hands on Scrapy","permalink":"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/OS-notes/":[{"title":"learning OS and building LorriOS","permalink":"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/"}],"https://LorrinWWW.github.io/posts/Bayes-estimation/":[{"title":"Note of statistic","permalink":"https://LorrinWWW.github.io/posts/Note-of-statistic/"}],"https://LorrinWWW.github.io/posts/management-of-the-firm/":[],"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"}],"https://LorrinWWW.github.io/posts/datamining-qualitative-induction/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-finite-element-method/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/QuadTree/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/proba-ch4/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch2/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/":[{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/Note-of-NLP/":[{"title":"Generative Adversarial Network","permalink":"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/"},{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"}],"https://LorrinWWW.github.io/posts/Note-of-datamining/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"数据挖掘-分类与预测","permalink":"https://LorrinWWW.github.io/posts/datamining-class-pred/"},{"title":"数据挖掘-预处理","permalink":"https://LorrinWWW.github.io/posts/datamining-pretreatment/"}],"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/Note-of-probability/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/participe-present-et-gerondif/":[],"https://LorrinWWW.github.io/posts/Hilbert-space/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/proba-ch6/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}]}}},"excerpt":"","more":"<h2 id=\"Event-detection-and-co-reference-with-minimal-supervision-1\"><a href=\"#Event-detection-and-co-reference-with-minimal-supervision-1\" class=\"headerlink\" title=\"Event detection and co-reference with minimal supervision [1]\"></a>Event detection and co-reference with minimal supervision [1]</h2><p><strong>摘要：</strong>该论文使用了一种弱监督的算法解决了事件检测与共指问题。事件共指问题可以看作是一种事件之间的相似度计算问题，而在该文中，事件检测问题也被看作是一种相似度检测问题。对于ACE或rich ERE划分的所有事件类型，使用每个类型中的几个实例作为该类型事件的向量，然后计算新事件向量与每个类型事件向量之间的相似度，根据这一相似度对事件进行判断。该文的另一个特点在于事件特征的选择，在将事件表示为向量的过程中，使用了Freebase作为特征来对事件进行表示。</p>\n<h3 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1. Introduction\"></a>1. Introduction</h3><img src='https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202215.jpg' width='70%'>\n\n<p>上图是论文提出的MSEP（Minimally Supervised Event Pipeline）框架。这里 Event examples 是唯一的监督来源，用于产生 Example vectors。在MSEP框架中不需要训练。</p>\n<p>这篇论文主要是针对两个问题：</p>\n<ul>\n<li>Event detection 指的是对一段文本内容，检测是否存在符合要求的事件。</li>\n<li>Co-reference problem. 为了更好的理解和利用事件的信息，我们需要从文本中提取出时间、地点、人物、行为等信息。此外，我们还需要了解两个事件的关系，例如，判断两个事件是否表示同一个事件，这就是Co-reference problem。</li>\n</ul>\n<p>在本文中，我们提出了一种更加可行且更加可测的方法来描述事件。对于一个事件e，event detection 所要做的就是判断是否存在一个事件集合，事件e在语义上是否有关联，以至于可以被划分到该集合内；而 co-reference problem 则是判断两个事件e1、e2是否在语义上表述足够接近，以至于我们认为它们所表示的实际上是同一个事件。可以看到两个任务实际上都需要判断相似性，我们可以把它们转化为语义相似性问题。</p>\n<p>现在主要问题有：1. 如何表示一个事件；2. 如何表达相似性。前者我们采用了semantic role labeling  representation（SRL），来结构化地描述一个事件；对于后者，我们将对事件做一个embedding，通过计算其余弦距离来表达相似性。</p>\n<p>我们提出了一个通用事件检测和指代消解框架，它基本上不需要标记数据。在实践中，为了将一个事件提法（event mention）和一个事件本体（event ontology）相联系起来，我们只需要一些事件示例。这种定义类型的方式是非常合理的，因为给出例子是定义事件类型的最简单的方法。我们的方法比标准的无监督方法要求更少假设，在我们的模型中，给定事件类型的定义（以事件例子的形式），我们可以将单个事件分类到已知本体，并确定两个事件是否是 co-reference 的。</p>\n<h3 id=\"2-The-MSEP-System\"><a href=\"#2-The-MSEP-System\" class=\"headerlink\" title=\"2. The MSEP System\"></a>2. The MSEP System</h3><h4 id=\"2-1-Structured-Vector-Representation\"><a href=\"#2-1-Structured-Vector-Representation\" class=\"headerlink\" title=\"2.1 Structured Vector Representation\"></a>2.1 Structured Vector Representation</h4><p>事件结构和句子结构之间有一个平行关系。我们发现一般来说，事件的触发词往往是谓语，所以可以针对谓语对其做一些改进：</p>\n<p><img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202214.jpg\" alt=\"basic\"></p>\n<p><strong>Basic event vector representation</strong>。基本事件向量由它的各个组成部分组成。</p>\n<p><img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202217.jpg\" alt=\"basic\"></p>\n<p><strong>Augmented event vector representation</strong>。在这里，“+” 表示我们首先将文本片段放在一起，然后将组合的文本片段转换成ESA向量。</p>\n<h4 id=\"2-2-Event-Mention-Detection\"><a href=\"#2-2-Event-Mention-Detection\" class=\"headerlink\" title=\"2.2 Event Mention Detection\"></a>2.2 Event Mention Detection</h4><p>我们定义 Event type representation 为该类别下的事件向量的平均值。</p>\n<p>我们定义定义相似度如下<br>$$<br>S(e_1, e_2) = \\frac{vec(e_1) · vec(e_2)}{||vec(e_1)||·||vec(e_2)||} \\<br>= \\frac{\\sum_a{vec(a_1) · vec(a_2)}}{\\sqrt{\\sum_a{||vec(a_1)||^2} · \\sum_a{||vec(a_2)||^2}}}<br>$$<br>其中 e1 是待处理事件，e2 是事件的类别。a 就是事件里的各个组件。若遇到 a 缺失的情况（如地点、时间等），我们用非缺失的部分的平均值来代替它。具体的操作方法参见原文。</p>\n<h4 id=\"2-3-Event-co-reference\"><a href=\"#2-3-Event-co-reference\" class=\"headerlink\" title=\"2.3 Event co-reference\"></a>2.3 Event co-reference</h4><p>这里如上一节的内容所说，通过余弦距离$S(e_1, e_2)$来计算两个事件的相似度。</p>\n<p>对于每一个事件，我们分别比较$agnet_{sub}, agnet_{obj}$，若都不相同，我们认为它们是独立的；如果有缺失，我们认为它和任意值匹配。这样，我们可以得到一个不重复的事件集合，$Set_{conflict}$。</p>\n<p>接下来遍历所有事件，对于事件k+1，<br>$$<br>e_p = argmax_{e\\in {e_1,…,e_k} e \\notin Set_{conflit}} {S(e_p, e_{k+1})}<br>$$<br>如果$S(e_p, e_{k+1})$的值大于我们设定的阈值，我们就认为它是同一个事件；否则，我们把他分为一个新的类。</p>\n<h3 id=\"3-Vector-Representation\"><a href=\"#3-Vector-Representation\" class=\"headerlink\" title=\"3. Vector Representation\"></a>3. Vector Representation</h3><p>我们可以看到，其实文章之前的内容都不依赖于 embedding 的具体选择，事实上，作者也测试了很多的方法，可以根据实际情况来选择。</p>\n<ul>\n<li>Explicit Semantic Analysis</li>\n<li>Brown Cluster</li>\n<li>Word2Vec</li>\n<li>Dependency-Based Embedding</li>\n</ul>\n<h3 id=\"4-Semantic-Role-Labeling\"><a href=\"#4-Semantic-Role-Labeling\" class=\"headerlink\" title=\"4. Semantic Role Labeling\"></a>4. Semantic Role Labeling</h3><p>上面工作建立在已经完成了 Semantic Role Labeling 的情况下，这里我们在讨论一下如何进行 Semantic Role Labeling。</p>\n<p>对于标注任务来说大同小异，现在往往使用神经网络模型来进行标注，例如[2]，缺点是需要大量标注数据。目前业内比较主流的解决方案是RNN-CRF模型，一般来说分为：</p>\n<ul>\n<li>Embedding layer</li>\n<li>Bi-directional RNN (usually LSTM) layer</li>\n<li>Tanh hidden layer</li>\n<li>CRF layer</li>\n</ul>\n<p>在实际应用上，可能还会增加Attention机制等来进一步提高它的效果。</p>\n<p>目前已有的系统如哈工大的语言技术平台LTP，能够用于 Semantic Role Labeling 等。</p>\n<h3 id=\"5-Conclusion\"><a href=\"#5-Conclusion\" class=\"headerlink\" title=\"5. Conclusion\"></a>5. Conclusion</h3><p>这一篇文章提出了一种新颖的事件检测和指代消解方法。其最重要的部分就是提出了一种结构化的向量，能够更好地表示event，用以进行事件分类、指代消解等工作。这个方法在一些关键指标上甚至能优于最新的监督方法，并且能够更好地适应新的领域。</p>\n<h2 id=\"Bibliography\"><a href=\"#Bibliography\" class=\"headerlink\" title=\"Bibliography\"></a>Bibliography</h2><p>[1] Peng, H., Song, Y., &amp; Roth, D. (2016). Event Detection and Co-reference with Minimal Supervision. In <em>EMNLP</em> (pp. 392-402).</p>\n<p>[2] Zhou, J., &amp; Xu, W. (2015). End-to-end learning of semantic role labeling using recurrent neural networks. In <em>ACL (1)</em> (pp. 1127-1137).</p>\n"},{"title":"A convolution BiLSTM neural network model for chinese event extraction 笔记","date":"2018-01-29T00:00:00.000Z","_content":"\n## A convolution BiLSTM neural network model for chinese event extraction\n\n**摘要：**中文事件提取是信息抽取中的一项具有挑战性的任务，以前的方法高度依赖于复杂的特征工程和复杂的自然语言处理（NLP）工具。 在文献\\[1\\]中，提出了一种结合LSTM和CNN的卷积双向LSTM神经网络来捕获句级和词汇信息。最终的测试中达到相当不错的水平。\n\n### 1. Introduction\n\n在事件提取中，我们需要提取事件类别、参与者和其他属性（时间、地点等）。根据Automatic Content Extraction（ACE）定义的事件抽取任务，我们定义：\n\n- 触发词：最主要的、用于表达一个事件的词，通常是句子的谓语。\n- 事件属性：实体、短语或数值。在一个事件中扮演特定作用。\n\n因此，我们把事件抽取分为两步，即**触发词标注**和**事件属性标注**。例如：\n\nS1：Intel在中国**成立**了研究中心。\n\n其中，“成立”表明该句子表达了一个商业事件；Intel、中国、研究中心则是事件的属性，属性将被标注为参与者、地点、时间等。\n\n目前的 state-of-the-art [2-4] 通常很依赖于特征的选择。这些特征通常可以被划分为**语义特征**和**结构特征**。再给两个包含”成立“的例子，但它在其中并不表达一个商业事件。\n\nS2：它**成立**于1994年，现在是一支深受欢迎的摇滚乐队。\n\nS3：医院已**成立**救援中心。\n\n从结构特征上来看，S2可以被缩写为“它是乐队”，因此“成立”在这个句子中不是一个触发词，这个句子不是一个事件。\n\n从语义特征上来看，S3中的“救援中心”的语义上看，这个事件不是一个商业行为，因此“成立”不表达一个商业事件。\n\n传统的方法[2, 3]通常依赖于大量的NLP工具，对于语义特征而言，有词性标注、命名实体识别等；对于结构特征而言，有依存关系分析。尽管最终效果很好，但是这需要大量的人工特征，并且需要忍受传递误差。\n\nChen et al. [5] 提出了一个用于完成事件抽取的卷积神经网络。受此激发，本文提出一个卷积双向LSTM神经网络，用来同时捕获语义特征和结构特征。我们首先使用双向LSTM将整个句子中的单词的语义编码成句子级别的特征。 然后，我们可以利用卷积神经网络来捕获突出的局部词汇特征，以便在没有任何POS标签或NER帮助的情况下进行触发词消歧。\n\n### 2. Trigger Labeling \n\n#### 2.1 Language Specific Issues\n\n由于中文的特殊性，触发词可以被分为两类：\n\n- 多词触发词：任何拆开后就无法被人为是触发词的，我们把它组合起来认为是触发词。例如“犯罪嫌疑人都落入法网”，其中“落入法网”被认为是触发词。\n- 单词触发词：往往是谓语，但也可以是组合词中的一部分。例如“警察击毙了一名歹徒”中的“击毙”，“这是一件预谋的凶杀案”中的“凶杀”\n\n为了解决这个问题，我们将事件检测视为序列标记任务而不是分类任务。 采用BIO方案，其中标记B是事件触发词的开始，I型是在触发词内，否则标记为O。我们利用卷积双向LSTM神经网络来完成这个任务。\n\n![trigger-labeling](https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202231.jpg)\n\n我们基于单词模型的主要架构。 （a）中的每个词wt的局部上下文特征ct（灰色矩形）由CNN计算（b）所示。 我们的卷积神经网络学习了关于中心词“落入”的本地上下文信息的表示。 这里的上下文大小是7（中心词的左右各3个词），我们使用一个大小为4的内核与两个特征映射。 （b）句子中的符号P表示填充词。\n\n#### 2.2 Word-Based Method\n\n**LSTM Network**  在nlp任务中LSTM相对常用，特别的，双向LSTM能够联系历史和未来的信息，能够重复利用句子信息，有利于我们进行判断。因为之前的报告已经叙述过，故这里略写。\n\n**CNN**  卷积神经网络最一开始用于图像领域，近年也在nlp领域大放光彩。这里，我们采用卷积神经网络来提取句子中每个单词的局部上下文信息。\n\n给定一个包含n个单词{w1, w2, ... , wn}的句子和当前中心词wt，卷积运算包含一个内核，将其应用于wt周围的单词以生成特征映射。 我们可以利用不同宽度的多个内核来提取不同粒度的局部特征。 然后在每个map上执行最大汇集，以便仅记录每个特征地图的最大数量。 池的一个特性是它产生一个固定大小的输出向量，这使我们能够应用不同的大小内核。 而通过执行最大操作，我们保持最显着的信息。 最后，将固定长度的输出向量cwt作为关于中心词wt的本地上下文信息的表示。\n\n在我们的实现中，滑动窗口大小为7（中心词的左右各3个词），并且我们使用不同的内核来捕获各种粒度的上下文信息。\n\n**Output Layer**  我们将BiLSTM的隐藏状态与CNN在每个时间步t提取的上下文特征cwt连接起来。 然后\\[ht; cwt\\]被送入softmax层以产生wt的每个标记的对数概率。\n然而，基于单词的方法仍然不能解决内部词触发引起的一致性问题，即无法识别长词内部的触发词。\n\n#### 2.3 Character-Based Method\n\n为了解决一致性问题，我们可以采用Character-embedding，唯一的区别就在input layer。\n\n![character](https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202234.jpg)\n\n### 3. Argument Labeling\n\n上面介绍的触发词标注模型依然可以被沿用，我们将介绍用于触发词标注和事件属性标注的模型之间的主要区别。\n\n#### 3.1 Input Layer\n\n作为一个pipeline系统，除了word embeddings之外，还可以使用从上面触发词标记任务中提取的信息。 因此，我们提出了另外四种类型的特征embedding来形成BiLSTM和CNN的输入层。\n\n- 触发位置特征：一个单词是否属于触发词的一部分\n- 触发类型特征：单词触发类型，NONE类型对于非触发词\n- 实体位置特征：一个单词是否属于实体的一部分\n- 实体类型特征：单词的实体类型，NONE类型对于非实体。 ACE数据集提供了实体识别的结果，无需使用外部NLP工具。（*思考*：若数据集不提供实体信息，两种解决方法：1. 不embed实体特征；2. 借助外部工具）\n  然后，我们通过查表将这些特征转换成矢量，并将它们与原始单词嵌入级联，作为BiLSTM和CNN的最终输入层。\n\n#### 3.2 Output Layer\n\n值得一提的是，事件属性标注不再是一个序列标注任务，而是一个分类任务。 ACE数据集提供了实体识别的结果，它保证了事件属性只能出现在这些实体。 因此，我们只需要预测标记实体的角色，而不是整个句子中的每个单词。 例如，S4中有三个触发器（粗体字）和三个实体（斜体字），它们共同组成九对要分类的触发词和事件属性候选。\n\nS7：六起**谋杀案**发生在*法国*，包括*Bob*的**暗杀**和*Joe*的**杀害**。\n\n我们修改CNN和BiLSTM网络的输出层以适应新的任务。\n\n对于BiLSTM，我们仍然试图利用其记忆长序列的能力，所以我们把最后一个单词hN的隐藏状态视为句子信息。\n\n对于CNN，我们把整个句子的所有单词作为上下文，而不是每个中心单词的浅窗口。 最后，我们将来自两个网络的输出向量的串联输入到softmax分类器中，就像处理之前的触发词标注任务一样。\n\n### 4. Conclusion\n\n论文[1]主要提出了卷积双向LSTM神经网络，用以完成中文事件抽取任务，在ACE 2005数据集上获得了不错的结果。我在暑假时，将事件抽取认为为一个序列标注任务，使用BiLSTM+CRF；相比而言，论文[1]的模型考虑更全面，并充分利用已知的实体信息。不过对于现实问题而言，标注实体信息的成本也很高，故在没有实体标注的情况下保持性能也是一个难点。\n\n## Bibliography\n\n\\[1\\] Zeng, Y., Yang, H., Feng, Y., Wang, Z., & Zhao, D. (2016). A convolution BiLSTM neural network model for Chinese event extraction. In *Natural Language Understanding and Intelligent Applications* (pp. 275-287). Springer, Cham.\n\n\\[2\\] Chen, C., Ng, V.: Joint modeling for Chinese event extraction with rich linguistic features. In: COLING, pp. 529–544. Citeseer (2012)\n\n\\[3\\] Chen, Y., Xu, L., Liu, K., Zeng, D., Zhao, J.: Event extraction via dynamic multipooling convolutional neural networks. In: Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, vol. 1, pp. 167–176 (2015)\n\n[4] Li, Q., Ji, H., Huang, L.: Joint event extraction via structured prediction with global features. In: ACL (1), pp. 73–82 (2013)\n\n[5] Chen, Y., Xu, L., Liu, K., Zeng, D., Zhao, J.: Event extraction via dynamic multipooling convolutional neural networks. In: Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, vol. 1, pp. 167–176 (2015)","source":"_posts/[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction.md","raw":"---\ntitle: A convolution BiLSTM neural network model for chinese event extraction 笔记\ndate: 2018-01-29 08:00:00\ncategories: [research]\ntags: [convolution, BiLSTM, event-extraction]\n---\n\n## A convolution BiLSTM neural network model for chinese event extraction\n\n**摘要：**中文事件提取是信息抽取中的一项具有挑战性的任务，以前的方法高度依赖于复杂的特征工程和复杂的自然语言处理（NLP）工具。 在文献\\[1\\]中，提出了一种结合LSTM和CNN的卷积双向LSTM神经网络来捕获句级和词汇信息。最终的测试中达到相当不错的水平。\n\n### 1. Introduction\n\n在事件提取中，我们需要提取事件类别、参与者和其他属性（时间、地点等）。根据Automatic Content Extraction（ACE）定义的事件抽取任务，我们定义：\n\n- 触发词：最主要的、用于表达一个事件的词，通常是句子的谓语。\n- 事件属性：实体、短语或数值。在一个事件中扮演特定作用。\n\n因此，我们把事件抽取分为两步，即**触发词标注**和**事件属性标注**。例如：\n\nS1：Intel在中国**成立**了研究中心。\n\n其中，“成立”表明该句子表达了一个商业事件；Intel、中国、研究中心则是事件的属性，属性将被标注为参与者、地点、时间等。\n\n目前的 state-of-the-art [2-4] 通常很依赖于特征的选择。这些特征通常可以被划分为**语义特征**和**结构特征**。再给两个包含”成立“的例子，但它在其中并不表达一个商业事件。\n\nS2：它**成立**于1994年，现在是一支深受欢迎的摇滚乐队。\n\nS3：医院已**成立**救援中心。\n\n从结构特征上来看，S2可以被缩写为“它是乐队”，因此“成立”在这个句子中不是一个触发词，这个句子不是一个事件。\n\n从语义特征上来看，S3中的“救援中心”的语义上看，这个事件不是一个商业行为，因此“成立”不表达一个商业事件。\n\n传统的方法[2, 3]通常依赖于大量的NLP工具，对于语义特征而言，有词性标注、命名实体识别等；对于结构特征而言，有依存关系分析。尽管最终效果很好，但是这需要大量的人工特征，并且需要忍受传递误差。\n\nChen et al. [5] 提出了一个用于完成事件抽取的卷积神经网络。受此激发，本文提出一个卷积双向LSTM神经网络，用来同时捕获语义特征和结构特征。我们首先使用双向LSTM将整个句子中的单词的语义编码成句子级别的特征。 然后，我们可以利用卷积神经网络来捕获突出的局部词汇特征，以便在没有任何POS标签或NER帮助的情况下进行触发词消歧。\n\n### 2. Trigger Labeling \n\n#### 2.1 Language Specific Issues\n\n由于中文的特殊性，触发词可以被分为两类：\n\n- 多词触发词：任何拆开后就无法被人为是触发词的，我们把它组合起来认为是触发词。例如“犯罪嫌疑人都落入法网”，其中“落入法网”被认为是触发词。\n- 单词触发词：往往是谓语，但也可以是组合词中的一部分。例如“警察击毙了一名歹徒”中的“击毙”，“这是一件预谋的凶杀案”中的“凶杀”\n\n为了解决这个问题，我们将事件检测视为序列标记任务而不是分类任务。 采用BIO方案，其中标记B是事件触发词的开始，I型是在触发词内，否则标记为O。我们利用卷积双向LSTM神经网络来完成这个任务。\n\n![trigger-labeling](https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202231.jpg)\n\n我们基于单词模型的主要架构。 （a）中的每个词wt的局部上下文特征ct（灰色矩形）由CNN计算（b）所示。 我们的卷积神经网络学习了关于中心词“落入”的本地上下文信息的表示。 这里的上下文大小是7（中心词的左右各3个词），我们使用一个大小为4的内核与两个特征映射。 （b）句子中的符号P表示填充词。\n\n#### 2.2 Word-Based Method\n\n**LSTM Network**  在nlp任务中LSTM相对常用，特别的，双向LSTM能够联系历史和未来的信息，能够重复利用句子信息，有利于我们进行判断。因为之前的报告已经叙述过，故这里略写。\n\n**CNN**  卷积神经网络最一开始用于图像领域，近年也在nlp领域大放光彩。这里，我们采用卷积神经网络来提取句子中每个单词的局部上下文信息。\n\n给定一个包含n个单词{w1, w2, ... , wn}的句子和当前中心词wt，卷积运算包含一个内核，将其应用于wt周围的单词以生成特征映射。 我们可以利用不同宽度的多个内核来提取不同粒度的局部特征。 然后在每个map上执行最大汇集，以便仅记录每个特征地图的最大数量。 池的一个特性是它产生一个固定大小的输出向量，这使我们能够应用不同的大小内核。 而通过执行最大操作，我们保持最显着的信息。 最后，将固定长度的输出向量cwt作为关于中心词wt的本地上下文信息的表示。\n\n在我们的实现中，滑动窗口大小为7（中心词的左右各3个词），并且我们使用不同的内核来捕获各种粒度的上下文信息。\n\n**Output Layer**  我们将BiLSTM的隐藏状态与CNN在每个时间步t提取的上下文特征cwt连接起来。 然后\\[ht; cwt\\]被送入softmax层以产生wt的每个标记的对数概率。\n然而，基于单词的方法仍然不能解决内部词触发引起的一致性问题，即无法识别长词内部的触发词。\n\n#### 2.3 Character-Based Method\n\n为了解决一致性问题，我们可以采用Character-embedding，唯一的区别就在input layer。\n\n![character](https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202234.jpg)\n\n### 3. Argument Labeling\n\n上面介绍的触发词标注模型依然可以被沿用，我们将介绍用于触发词标注和事件属性标注的模型之间的主要区别。\n\n#### 3.1 Input Layer\n\n作为一个pipeline系统，除了word embeddings之外，还可以使用从上面触发词标记任务中提取的信息。 因此，我们提出了另外四种类型的特征embedding来形成BiLSTM和CNN的输入层。\n\n- 触发位置特征：一个单词是否属于触发词的一部分\n- 触发类型特征：单词触发类型，NONE类型对于非触发词\n- 实体位置特征：一个单词是否属于实体的一部分\n- 实体类型特征：单词的实体类型，NONE类型对于非实体。 ACE数据集提供了实体识别的结果，无需使用外部NLP工具。（*思考*：若数据集不提供实体信息，两种解决方法：1. 不embed实体特征；2. 借助外部工具）\n  然后，我们通过查表将这些特征转换成矢量，并将它们与原始单词嵌入级联，作为BiLSTM和CNN的最终输入层。\n\n#### 3.2 Output Layer\n\n值得一提的是，事件属性标注不再是一个序列标注任务，而是一个分类任务。 ACE数据集提供了实体识别的结果，它保证了事件属性只能出现在这些实体。 因此，我们只需要预测标记实体的角色，而不是整个句子中的每个单词。 例如，S4中有三个触发器（粗体字）和三个实体（斜体字），它们共同组成九对要分类的触发词和事件属性候选。\n\nS7：六起**谋杀案**发生在*法国*，包括*Bob*的**暗杀**和*Joe*的**杀害**。\n\n我们修改CNN和BiLSTM网络的输出层以适应新的任务。\n\n对于BiLSTM，我们仍然试图利用其记忆长序列的能力，所以我们把最后一个单词hN的隐藏状态视为句子信息。\n\n对于CNN，我们把整个句子的所有单词作为上下文，而不是每个中心单词的浅窗口。 最后，我们将来自两个网络的输出向量的串联输入到softmax分类器中，就像处理之前的触发词标注任务一样。\n\n### 4. Conclusion\n\n论文[1]主要提出了卷积双向LSTM神经网络，用以完成中文事件抽取任务，在ACE 2005数据集上获得了不错的结果。我在暑假时，将事件抽取认为为一个序列标注任务，使用BiLSTM+CRF；相比而言，论文[1]的模型考虑更全面，并充分利用已知的实体信息。不过对于现实问题而言，标注实体信息的成本也很高，故在没有实体标注的情况下保持性能也是一个难点。\n\n## Bibliography\n\n\\[1\\] Zeng, Y., Yang, H., Feng, Y., Wang, Z., & Zhao, D. (2016). A convolution BiLSTM neural network model for Chinese event extraction. In *Natural Language Understanding and Intelligent Applications* (pp. 275-287). Springer, Cham.\n\n\\[2\\] Chen, C., Ng, V.: Joint modeling for Chinese event extraction with rich linguistic features. In: COLING, pp. 529–544. Citeseer (2012)\n\n\\[3\\] Chen, Y., Xu, L., Liu, K., Zeng, D., Zhao, J.: Event extraction via dynamic multipooling convolutional neural networks. In: Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, vol. 1, pp. 167–176 (2015)\n\n[4] Li, Q., Ji, H., Huang, L.: Joint event extraction via structured prediction with global features. In: ACL (1), pp. 73–82 (2013)\n\n[5] Chen, Y., Xu, L., Liu, K., Zeng, D., Zhao, J.: Event extraction via dynamic multipooling convolutional neural networks. In: Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, vol. 1, pp. 167–176 (2015)","slug":"[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction","published":1,"updated":"2020-11-03T03:26:12.997Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckw4qufpn0021gwtl3zwebh66","content":"<h2 id=\"A-convolution-BiLSTM-neural-network-model-for-chinese-event-extraction\"><a href=\"#A-convolution-BiLSTM-neural-network-model-for-chinese-event-extraction\" class=\"headerlink\" title=\"A convolution BiLSTM neural network model for chinese event extraction\"></a>A convolution BiLSTM neural network model for chinese event extraction</h2><p><strong>摘要：</strong>中文事件提取是信息抽取中的一项具有挑战性的任务，以前的方法高度依赖于复杂的特征工程和复杂的自然语言处理（NLP）工具。 在文献[1]中，提出了一种结合LSTM和CNN的卷积双向LSTM神经网络来捕获句级和词汇信息。最终的测试中达到相当不错的水平。</p>\n<h3 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1. Introduction\"></a>1. Introduction</h3><p>在事件提取中，我们需要提取事件类别、参与者和其他属性（时间、地点等）。根据Automatic Content Extraction（ACE）定义的事件抽取任务，我们定义：</p>\n<ul>\n<li>触发词：最主要的、用于表达一个事件的词，通常是句子的谓语。</li>\n<li>事件属性：实体、短语或数值。在一个事件中扮演特定作用。</li>\n</ul>\n<p>因此，我们把事件抽取分为两步，即<strong>触发词标注</strong>和<strong>事件属性标注</strong>。例如：</p>\n<p>S1：Intel在中国<strong>成立</strong>了研究中心。</p>\n<p>其中，“成立”表明该句子表达了一个商业事件；Intel、中国、研究中心则是事件的属性，属性将被标注为参与者、地点、时间等。</p>\n<p>目前的 state-of-the-art [2-4] 通常很依赖于特征的选择。这些特征通常可以被划分为<strong>语义特征</strong>和<strong>结构特征</strong>。再给两个包含”成立“的例子，但它在其中并不表达一个商业事件。</p>\n<p>S2：它<strong>成立</strong>于1994年，现在是一支深受欢迎的摇滚乐队。</p>\n<p>S3：医院已<strong>成立</strong>救援中心。</p>\n<p>从结构特征上来看，S2可以被缩写为“它是乐队”，因此“成立”在这个句子中不是一个触发词，这个句子不是一个事件。</p>\n<p>从语义特征上来看，S3中的“救援中心”的语义上看，这个事件不是一个商业行为，因此“成立”不表达一个商业事件。</p>\n<p>传统的方法[2, 3]通常依赖于大量的NLP工具，对于语义特征而言，有词性标注、命名实体识别等；对于结构特征而言，有依存关系分析。尽管最终效果很好，但是这需要大量的人工特征，并且需要忍受传递误差。</p>\n<p>Chen et al. [5] 提出了一个用于完成事件抽取的卷积神经网络。受此激发，本文提出一个卷积双向LSTM神经网络，用来同时捕获语义特征和结构特征。我们首先使用双向LSTM将整个句子中的单词的语义编码成句子级别的特征。 然后，我们可以利用卷积神经网络来捕获突出的局部词汇特征，以便在没有任何POS标签或NER帮助的情况下进行触发词消歧。</p>\n<h3 id=\"2-Trigger-Labeling\"><a href=\"#2-Trigger-Labeling\" class=\"headerlink\" title=\"2. Trigger Labeling\"></a>2. Trigger Labeling</h3><h4 id=\"2-1-Language-Specific-Issues\"><a href=\"#2-1-Language-Specific-Issues\" class=\"headerlink\" title=\"2.1 Language Specific Issues\"></a>2.1 Language Specific Issues</h4><p>由于中文的特殊性，触发词可以被分为两类：</p>\n<ul>\n<li>多词触发词：任何拆开后就无法被人为是触发词的，我们把它组合起来认为是触发词。例如“犯罪嫌疑人都落入法网”，其中“落入法网”被认为是触发词。</li>\n<li>单词触发词：往往是谓语，但也可以是组合词中的一部分。例如“警察击毙了一名歹徒”中的“击毙”，“这是一件预谋的凶杀案”中的“凶杀”</li>\n</ul>\n<p>为了解决这个问题，我们将事件检测视为序列标记任务而不是分类任务。 采用BIO方案，其中标记B是事件触发词的开始，I型是在触发词内，否则标记为O。我们利用卷积双向LSTM神经网络来完成这个任务。</p>\n<p><img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202231.jpg\" alt=\"trigger-labeling\"></p>\n<p>我们基于单词模型的主要架构。 （a）中的每个词wt的局部上下文特征ct（灰色矩形）由CNN计算（b）所示。 我们的卷积神经网络学习了关于中心词“落入”的本地上下文信息的表示。 这里的上下文大小是7（中心词的左右各3个词），我们使用一个大小为4的内核与两个特征映射。 （b）句子中的符号P表示填充词。</p>\n<h4 id=\"2-2-Word-Based-Method\"><a href=\"#2-2-Word-Based-Method\" class=\"headerlink\" title=\"2.2 Word-Based Method\"></a>2.2 Word-Based Method</h4><p><strong>LSTM Network</strong>  在nlp任务中LSTM相对常用，特别的，双向LSTM能够联系历史和未来的信息，能够重复利用句子信息，有利于我们进行判断。因为之前的报告已经叙述过，故这里略写。</p>\n<p><strong>CNN</strong>  卷积神经网络最一开始用于图像领域，近年也在nlp领域大放光彩。这里，我们采用卷积神经网络来提取句子中每个单词的局部上下文信息。</p>\n<p>给定一个包含n个单词{w1, w2, … , wn}的句子和当前中心词wt，卷积运算包含一个内核，将其应用于wt周围的单词以生成特征映射。 我们可以利用不同宽度的多个内核来提取不同粒度的局部特征。 然后在每个map上执行最大汇集，以便仅记录每个特征地图的最大数量。 池的一个特性是它产生一个固定大小的输出向量，这使我们能够应用不同的大小内核。 而通过执行最大操作，我们保持最显着的信息。 最后，将固定长度的输出向量cwt作为关于中心词wt的本地上下文信息的表示。</p>\n<p>在我们的实现中，滑动窗口大小为7（中心词的左右各3个词），并且我们使用不同的内核来捕获各种粒度的上下文信息。</p>\n<p><strong>Output Layer</strong>  我们将BiLSTM的隐藏状态与CNN在每个时间步t提取的上下文特征cwt连接起来。 然后[ht; cwt]被送入softmax层以产生wt的每个标记的对数概率。<br>然而，基于单词的方法仍然不能解决内部词触发引起的一致性问题，即无法识别长词内部的触发词。</p>\n<h4 id=\"2-3-Character-Based-Method\"><a href=\"#2-3-Character-Based-Method\" class=\"headerlink\" title=\"2.3 Character-Based Method\"></a>2.3 Character-Based Method</h4><p>为了解决一致性问题，我们可以采用Character-embedding，唯一的区别就在input layer。</p>\n<p><img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202234.jpg\" alt=\"character\"></p>\n<h3 id=\"3-Argument-Labeling\"><a href=\"#3-Argument-Labeling\" class=\"headerlink\" title=\"3. Argument Labeling\"></a>3. Argument Labeling</h3><p>上面介绍的触发词标注模型依然可以被沿用，我们将介绍用于触发词标注和事件属性标注的模型之间的主要区别。</p>\n<h4 id=\"3-1-Input-Layer\"><a href=\"#3-1-Input-Layer\" class=\"headerlink\" title=\"3.1 Input Layer\"></a>3.1 Input Layer</h4><p>作为一个pipeline系统，除了word embeddings之外，还可以使用从上面触发词标记任务中提取的信息。 因此，我们提出了另外四种类型的特征embedding来形成BiLSTM和CNN的输入层。</p>\n<ul>\n<li>触发位置特征：一个单词是否属于触发词的一部分</li>\n<li>触发类型特征：单词触发类型，NONE类型对于非触发词</li>\n<li>实体位置特征：一个单词是否属于实体的一部分</li>\n<li>实体类型特征：单词的实体类型，NONE类型对于非实体。 ACE数据集提供了实体识别的结果，无需使用外部NLP工具。（<em>思考</em>：若数据集不提供实体信息，两种解决方法：1. 不embed实体特征；2. 借助外部工具）<br>然后，我们通过查表将这些特征转换成矢量，并将它们与原始单词嵌入级联，作为BiLSTM和CNN的最终输入层。</li>\n</ul>\n<h4 id=\"3-2-Output-Layer\"><a href=\"#3-2-Output-Layer\" class=\"headerlink\" title=\"3.2 Output Layer\"></a>3.2 Output Layer</h4><p>值得一提的是，事件属性标注不再是一个序列标注任务，而是一个分类任务。 ACE数据集提供了实体识别的结果，它保证了事件属性只能出现在这些实体。 因此，我们只需要预测标记实体的角色，而不是整个句子中的每个单词。 例如，S4中有三个触发器（粗体字）和三个实体（斜体字），它们共同组成九对要分类的触发词和事件属性候选。</p>\n<p>S7：六起<strong>谋杀案</strong>发生在<em>法国</em>，包括<em>Bob</em>的<strong>暗杀</strong>和<em>Joe</em>的<strong>杀害</strong>。</p>\n<p>我们修改CNN和BiLSTM网络的输出层以适应新的任务。</p>\n<p>对于BiLSTM，我们仍然试图利用其记忆长序列的能力，所以我们把最后一个单词hN的隐藏状态视为句子信息。</p>\n<p>对于CNN，我们把整个句子的所有单词作为上下文，而不是每个中心单词的浅窗口。 最后，我们将来自两个网络的输出向量的串联输入到softmax分类器中，就像处理之前的触发词标注任务一样。</p>\n<h3 id=\"4-Conclusion\"><a href=\"#4-Conclusion\" class=\"headerlink\" title=\"4. Conclusion\"></a>4. Conclusion</h3><p>论文[1]主要提出了卷积双向LSTM神经网络，用以完成中文事件抽取任务，在ACE 2005数据集上获得了不错的结果。我在暑假时，将事件抽取认为为一个序列标注任务，使用BiLSTM+CRF；相比而言，论文[1]的模型考虑更全面，并充分利用已知的实体信息。不过对于现实问题而言，标注实体信息的成本也很高，故在没有实体标注的情况下保持性能也是一个难点。</p>\n<h2 id=\"Bibliography\"><a href=\"#Bibliography\" class=\"headerlink\" title=\"Bibliography\"></a>Bibliography</h2><p>[1] Zeng, Y., Yang, H., Feng, Y., Wang, Z., &amp; Zhao, D. (2016). A convolution BiLSTM neural network model for Chinese event extraction. In <em>Natural Language Understanding and Intelligent Applications</em> (pp. 275-287). Springer, Cham.</p>\n<p>[2] Chen, C., Ng, V.: Joint modeling for Chinese event extraction with rich linguistic features. In: COLING, pp. 529–544. Citeseer (2012)</p>\n<p>[3] Chen, Y., Xu, L., Liu, K., Zeng, D., Zhao, J.: Event extraction via dynamic multipooling convolutional neural networks. In: Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, vol. 1, pp. 167–176 (2015)</p>\n<p>[4] Li, Q., Ji, H., Huang, L.: Joint event extraction via structured prediction with global features. In: ACL (1), pp. 73–82 (2013)</p>\n<p>[5] Chen, Y., Xu, L., Liu, K., Zeng, D., Zhao, J.: Event extraction via dynamic multipooling convolutional neural networks. In: Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, vol. 1, pp. 167–176 (2015)</p>\n","site":{"data":{"recommended_posts":{"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"}],"https://LorrinWWW.github.io/posts/complexity/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/hexo-with-latex/":[{"title":"区块链时代的最正确的打赏姿势——在Hexo博客配置Coinhive网页挖矿","permalink":"http://www.davidfnck.com/blockchain/mine-xmr-in-hexo-by-coinhive.html/"}],"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/":[{"title":"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-statistic/":[{"title":"贝叶斯估计 Bayes estimation","permalink":"https://LorrinWWW.github.io/posts/Bayes-estimation/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"}],"https://LorrinWWW.github.io/posts/[2018.2.5]Nested-LSTMs/":[{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"}],"https://LorrinWWW.github.io/posts/datamining-pretreatment/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Several models for knowledge graph representing and completing 几个知识图谱模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/"}],"https://LorrinWWW.github.io/posts/compression/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/Sobolev-space/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/proba-ch5/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch3/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/proba-ch1/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/ML-CNN/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/EDP-basic-models/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/machine-learning/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/datamining-class-pred/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction/":[],"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/":[{"title":"OS notes","permalink":"https://LorrinWWW.github.io/posts/OS-notes/"}],"https://LorrinWWW.github.io/posts/projet-enjeu-plugin-chrome-101/":[{"title":"Chrome油猴脚本","permalink":"http://gubangzhong.cn/2017/06/13/Chrome油猴脚本/"}],"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/":[{"title":"MongoDB, Docker and Python","permalink":"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/":[{"title":"Hands on Scrapy","permalink":"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/OS-notes/":[{"title":"learning OS and building LorriOS","permalink":"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/"}],"https://LorrinWWW.github.io/posts/Bayes-estimation/":[{"title":"Note of statistic","permalink":"https://LorrinWWW.github.io/posts/Note-of-statistic/"}],"https://LorrinWWW.github.io/posts/management-of-the-firm/":[],"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"}],"https://LorrinWWW.github.io/posts/datamining-qualitative-induction/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-finite-element-method/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/QuadTree/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/proba-ch4/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch2/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/":[{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/Note-of-NLP/":[{"title":"Generative Adversarial Network","permalink":"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/"},{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"}],"https://LorrinWWW.github.io/posts/Note-of-datamining/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"数据挖掘-分类与预测","permalink":"https://LorrinWWW.github.io/posts/datamining-class-pred/"},{"title":"数据挖掘-预处理","permalink":"https://LorrinWWW.github.io/posts/datamining-pretreatment/"}],"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/Note-of-probability/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/participe-present-et-gerondif/":[],"https://LorrinWWW.github.io/posts/Hilbert-space/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/proba-ch6/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}]}}},"excerpt":"","more":"<h2 id=\"A-convolution-BiLSTM-neural-network-model-for-chinese-event-extraction\"><a href=\"#A-convolution-BiLSTM-neural-network-model-for-chinese-event-extraction\" class=\"headerlink\" title=\"A convolution BiLSTM neural network model for chinese event extraction\"></a>A convolution BiLSTM neural network model for chinese event extraction</h2><p><strong>摘要：</strong>中文事件提取是信息抽取中的一项具有挑战性的任务，以前的方法高度依赖于复杂的特征工程和复杂的自然语言处理（NLP）工具。 在文献[1]中，提出了一种结合LSTM和CNN的卷积双向LSTM神经网络来捕获句级和词汇信息。最终的测试中达到相当不错的水平。</p>\n<h3 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1. Introduction\"></a>1. Introduction</h3><p>在事件提取中，我们需要提取事件类别、参与者和其他属性（时间、地点等）。根据Automatic Content Extraction（ACE）定义的事件抽取任务，我们定义：</p>\n<ul>\n<li>触发词：最主要的、用于表达一个事件的词，通常是句子的谓语。</li>\n<li>事件属性：实体、短语或数值。在一个事件中扮演特定作用。</li>\n</ul>\n<p>因此，我们把事件抽取分为两步，即<strong>触发词标注</strong>和<strong>事件属性标注</strong>。例如：</p>\n<p>S1：Intel在中国<strong>成立</strong>了研究中心。</p>\n<p>其中，“成立”表明该句子表达了一个商业事件；Intel、中国、研究中心则是事件的属性，属性将被标注为参与者、地点、时间等。</p>\n<p>目前的 state-of-the-art [2-4] 通常很依赖于特征的选择。这些特征通常可以被划分为<strong>语义特征</strong>和<strong>结构特征</strong>。再给两个包含”成立“的例子，但它在其中并不表达一个商业事件。</p>\n<p>S2：它<strong>成立</strong>于1994年，现在是一支深受欢迎的摇滚乐队。</p>\n<p>S3：医院已<strong>成立</strong>救援中心。</p>\n<p>从结构特征上来看，S2可以被缩写为“它是乐队”，因此“成立”在这个句子中不是一个触发词，这个句子不是一个事件。</p>\n<p>从语义特征上来看，S3中的“救援中心”的语义上看，这个事件不是一个商业行为，因此“成立”不表达一个商业事件。</p>\n<p>传统的方法[2, 3]通常依赖于大量的NLP工具，对于语义特征而言，有词性标注、命名实体识别等；对于结构特征而言，有依存关系分析。尽管最终效果很好，但是这需要大量的人工特征，并且需要忍受传递误差。</p>\n<p>Chen et al. [5] 提出了一个用于完成事件抽取的卷积神经网络。受此激发，本文提出一个卷积双向LSTM神经网络，用来同时捕获语义特征和结构特征。我们首先使用双向LSTM将整个句子中的单词的语义编码成句子级别的特征。 然后，我们可以利用卷积神经网络来捕获突出的局部词汇特征，以便在没有任何POS标签或NER帮助的情况下进行触发词消歧。</p>\n<h3 id=\"2-Trigger-Labeling\"><a href=\"#2-Trigger-Labeling\" class=\"headerlink\" title=\"2. Trigger Labeling\"></a>2. Trigger Labeling</h3><h4 id=\"2-1-Language-Specific-Issues\"><a href=\"#2-1-Language-Specific-Issues\" class=\"headerlink\" title=\"2.1 Language Specific Issues\"></a>2.1 Language Specific Issues</h4><p>由于中文的特殊性，触发词可以被分为两类：</p>\n<ul>\n<li>多词触发词：任何拆开后就无法被人为是触发词的，我们把它组合起来认为是触发词。例如“犯罪嫌疑人都落入法网”，其中“落入法网”被认为是触发词。</li>\n<li>单词触发词：往往是谓语，但也可以是组合词中的一部分。例如“警察击毙了一名歹徒”中的“击毙”，“这是一件预谋的凶杀案”中的“凶杀”</li>\n</ul>\n<p>为了解决这个问题，我们将事件检测视为序列标记任务而不是分类任务。 采用BIO方案，其中标记B是事件触发词的开始，I型是在触发词内，否则标记为O。我们利用卷积双向LSTM神经网络来完成这个任务。</p>\n<p><img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202231.jpg\" alt=\"trigger-labeling\"></p>\n<p>我们基于单词模型的主要架构。 （a）中的每个词wt的局部上下文特征ct（灰色矩形）由CNN计算（b）所示。 我们的卷积神经网络学习了关于中心词“落入”的本地上下文信息的表示。 这里的上下文大小是7（中心词的左右各3个词），我们使用一个大小为4的内核与两个特征映射。 （b）句子中的符号P表示填充词。</p>\n<h4 id=\"2-2-Word-Based-Method\"><a href=\"#2-2-Word-Based-Method\" class=\"headerlink\" title=\"2.2 Word-Based Method\"></a>2.2 Word-Based Method</h4><p><strong>LSTM Network</strong>  在nlp任务中LSTM相对常用，特别的，双向LSTM能够联系历史和未来的信息，能够重复利用句子信息，有利于我们进行判断。因为之前的报告已经叙述过，故这里略写。</p>\n<p><strong>CNN</strong>  卷积神经网络最一开始用于图像领域，近年也在nlp领域大放光彩。这里，我们采用卷积神经网络来提取句子中每个单词的局部上下文信息。</p>\n<p>给定一个包含n个单词{w1, w2, … , wn}的句子和当前中心词wt，卷积运算包含一个内核，将其应用于wt周围的单词以生成特征映射。 我们可以利用不同宽度的多个内核来提取不同粒度的局部特征。 然后在每个map上执行最大汇集，以便仅记录每个特征地图的最大数量。 池的一个特性是它产生一个固定大小的输出向量，这使我们能够应用不同的大小内核。 而通过执行最大操作，我们保持最显着的信息。 最后，将固定长度的输出向量cwt作为关于中心词wt的本地上下文信息的表示。</p>\n<p>在我们的实现中，滑动窗口大小为7（中心词的左右各3个词），并且我们使用不同的内核来捕获各种粒度的上下文信息。</p>\n<p><strong>Output Layer</strong>  我们将BiLSTM的隐藏状态与CNN在每个时间步t提取的上下文特征cwt连接起来。 然后[ht; cwt]被送入softmax层以产生wt的每个标记的对数概率。<br>然而，基于单词的方法仍然不能解决内部词触发引起的一致性问题，即无法识别长词内部的触发词。</p>\n<h4 id=\"2-3-Character-Based-Method\"><a href=\"#2-3-Character-Based-Method\" class=\"headerlink\" title=\"2.3 Character-Based Method\"></a>2.3 Character-Based Method</h4><p>为了解决一致性问题，我们可以采用Character-embedding，唯一的区别就在input layer。</p>\n<p><img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202234.jpg\" alt=\"character\"></p>\n<h3 id=\"3-Argument-Labeling\"><a href=\"#3-Argument-Labeling\" class=\"headerlink\" title=\"3. Argument Labeling\"></a>3. Argument Labeling</h3><p>上面介绍的触发词标注模型依然可以被沿用，我们将介绍用于触发词标注和事件属性标注的模型之间的主要区别。</p>\n<h4 id=\"3-1-Input-Layer\"><a href=\"#3-1-Input-Layer\" class=\"headerlink\" title=\"3.1 Input Layer\"></a>3.1 Input Layer</h4><p>作为一个pipeline系统，除了word embeddings之外，还可以使用从上面触发词标记任务中提取的信息。 因此，我们提出了另外四种类型的特征embedding来形成BiLSTM和CNN的输入层。</p>\n<ul>\n<li>触发位置特征：一个单词是否属于触发词的一部分</li>\n<li>触发类型特征：单词触发类型，NONE类型对于非触发词</li>\n<li>实体位置特征：一个单词是否属于实体的一部分</li>\n<li>实体类型特征：单词的实体类型，NONE类型对于非实体。 ACE数据集提供了实体识别的结果，无需使用外部NLP工具。（<em>思考</em>：若数据集不提供实体信息，两种解决方法：1. 不embed实体特征；2. 借助外部工具）<br>然后，我们通过查表将这些特征转换成矢量，并将它们与原始单词嵌入级联，作为BiLSTM和CNN的最终输入层。</li>\n</ul>\n<h4 id=\"3-2-Output-Layer\"><a href=\"#3-2-Output-Layer\" class=\"headerlink\" title=\"3.2 Output Layer\"></a>3.2 Output Layer</h4><p>值得一提的是，事件属性标注不再是一个序列标注任务，而是一个分类任务。 ACE数据集提供了实体识别的结果，它保证了事件属性只能出现在这些实体。 因此，我们只需要预测标记实体的角色，而不是整个句子中的每个单词。 例如，S4中有三个触发器（粗体字）和三个实体（斜体字），它们共同组成九对要分类的触发词和事件属性候选。</p>\n<p>S7：六起<strong>谋杀案</strong>发生在<em>法国</em>，包括<em>Bob</em>的<strong>暗杀</strong>和<em>Joe</em>的<strong>杀害</strong>。</p>\n<p>我们修改CNN和BiLSTM网络的输出层以适应新的任务。</p>\n<p>对于BiLSTM，我们仍然试图利用其记忆长序列的能力，所以我们把最后一个单词hN的隐藏状态视为句子信息。</p>\n<p>对于CNN，我们把整个句子的所有单词作为上下文，而不是每个中心单词的浅窗口。 最后，我们将来自两个网络的输出向量的串联输入到softmax分类器中，就像处理之前的触发词标注任务一样。</p>\n<h3 id=\"4-Conclusion\"><a href=\"#4-Conclusion\" class=\"headerlink\" title=\"4. Conclusion\"></a>4. Conclusion</h3><p>论文[1]主要提出了卷积双向LSTM神经网络，用以完成中文事件抽取任务，在ACE 2005数据集上获得了不错的结果。我在暑假时，将事件抽取认为为一个序列标注任务，使用BiLSTM+CRF；相比而言，论文[1]的模型考虑更全面，并充分利用已知的实体信息。不过对于现实问题而言，标注实体信息的成本也很高，故在没有实体标注的情况下保持性能也是一个难点。</p>\n<h2 id=\"Bibliography\"><a href=\"#Bibliography\" class=\"headerlink\" title=\"Bibliography\"></a>Bibliography</h2><p>[1] Zeng, Y., Yang, H., Feng, Y., Wang, Z., &amp; Zhao, D. (2016). A convolution BiLSTM neural network model for Chinese event extraction. In <em>Natural Language Understanding and Intelligent Applications</em> (pp. 275-287). Springer, Cham.</p>\n<p>[2] Chen, C., Ng, V.: Joint modeling for Chinese event extraction with rich linguistic features. In: COLING, pp. 529–544. Citeseer (2012)</p>\n<p>[3] Chen, Y., Xu, L., Liu, K., Zeng, D., Zhao, J.: Event extraction via dynamic multipooling convolutional neural networks. In: Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, vol. 1, pp. 167–176 (2015)</p>\n<p>[4] Li, Q., Ji, H., Huang, L.: Joint event extraction via structured prediction with global features. In: ACL (1), pp. 73–82 (2013)</p>\n<p>[5] Chen, Y., Xu, L., Liu, K., Zeng, D., Zhao, J.: Event extraction via dynamic multipooling convolutional neural networks. In: Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, vol. 1, pp. 167–176 (2015)</p>\n"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","date":"2018-01-04T00:00:00.000Z","_content":"\n这次主要阅读的论文是《Overcoming Limited Supervision in Relation Extraction: A Pattern-enhanced Distributional Representation Approach》[1]。该文主要针对了现有模型对标注数据的依赖，提出一种比较有意思的思路。基于分布的方法（distributional approach）利用两个实体共同出现的统计频率来预测他们的关系，需要大量标注数据，而基于模式的方法（pattern-based approach）一般使用神经网络建模，但这种方法需要更多的标注数据。本文同时建立两个模型，互相为对方提供监督。以分布模型作为判别模型，模式模型作为生成模型。训练过程中不断迭代，从而提升两个模型的性能。\n\n![illustration](https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-201441.jpg)\n\n### 1. Introduction\n\n#### 1.1 Weakly Supervised Learning\n\n弱监督学习介于监督学习和无监督学习之间，它提供的标注数据带有较大的噪音，或标注的相对粗糙，标注结果可能出错。对于关系抽取而言，就是将一些关系实例作为seed，用它们从大型语料库中去除冗余信息并提取更多的实例。\n\n弱监督学习的基本思路：\n\n1. 用容易获得的标注替代较难获得的标注\n2. 选择最需要做精细标注的样例\n3. 模型训练和自动标注交替进行\n\n#### 1.2 Co-training strategy\n\n以往的工作主要是单个模型，该文采用了co-training策略[2]，将两个模型互相协作，取得了比较好的效果。\n\nco-training策略是一种半监督方法，核心就是利用少量已标记样本，通过两个（或多个）模型去学习，对未标记样本进行标记，挑选置信度最高的样本加入已标记样本阵营。\n\n#### 1.3 REPEL (Relation Extraction with pattern-enhanced Embedding)\n\nREPEL是本文提出的一个模型。基于模式的模型学习用于关系抽取文本的模式，基于分布的模型作为分类器，两者互补，互相提供监督。前者相当于一个生成器，基于模式生成候选实例；而后者作为判别器，从中选择最优实例，并将选择结果反馈给前者。训练完成相当于得到了两个关系抽取模型。\n\n### 2. Problem definition\n\n实体识别：使用现成的工具标注。\n\n关系识别：实体对 $(e_h, e_t)$，三元组$(e_h, e_t, r)$\n\n给定语料库D，关系集合R。给定少量seed实例$ \\{(e_h^{r(k)}, e_t^{r(k)}, r)\\} _{k=1}^{N_r} $，提取尽可能多的$ \\{(e_h^{r(i)}, e_t^{r(i)}, r)\\} _{i=1}^M $；换言之，对于每个$ r \\in R $，我们要提取尽可能多的$ \\{(e_h^{r(i)}, e_t^{r(i)})\\} _{i=1}^{M_r} $。\n\n### 3. REPEL Framework\n\n模式模型：找到文本中的模式集合\n\n分布模型：学习实体表示，以及打分函数\n\n目标函数：\n$$\nmax_{P,D}O = max_{P,D}\\{O_p + O_d + \\lambda O_i\\}\n$$\n上面公式中，P表示模式模型的参数，给定关系的全部模式集合。D表示分布模型的参数，实体表示和打分函数。Op和Od分别表示两个目标函数，Oi表示两个模型交互的目标。\n\n注意这里只考虑关系抽取，实体识别使用现有的工具或模型。\n\n#### 3.1 Pattern Module\n\n对于一个指定的关系r，我们的目标是找到K个最可靠的模式，然后进一步使用它们来发现更多的关系实例。\n\n基于模式关系抽取主要分为两种：path-based pattern、meta pattern。对于一句话中的实体对，前者定义为两个实体通过依存信息跳转的最短路径；后者则是两个实体附近的文字序列。利用这两种模式从语料库中寻找匹配的实体对。这样就得到了很多候选模式，每个模式又能分别找到许多匹配的实体对。\n\n对于一个模式$\\pi$，我们通过以下式子计算它的置信度：\n$$\nR(\\pi)=\\frac{|G(\\pi)\\cap S_{pair}|}{|G(\\pi)|}\n$$\n$G(\\pi)$表示被模式$\\pi$所匹配的所有实体对，$S_{pair}$表示seed实体对。可以看到，R实际表示的是，在满足$\\pi$模式的实体对中，seed实体对所占的比例。显然，该比值越高，该模式越符合seed的分布。由此，我们定义：\n$$\nO_p = \\sum_{\\pi \\in P}R(\\pi)\n$$\n下面说明一下整个进行的过程：\n\n- 给定seed实体对，我们通过模式关系抽取的方法获得一系列候选模式。\n- 计算每个候选模式的R值，取最高的K个\n\n#### 3.2 Distributional Module\n\n该模块学习语料中的实体全局分布信息。我们利用给定的关系实例作为打分函数。\n\n对于一个实体e，和一个词w\n$$\nP(w|e) =\\frac{exp(x_e*c_w)}{Z}\n$$\n$x_e$表示需要训练的实体表示向量， $c_w$是预训练的word embedding，Z是归一化项。\n$$\nO_{text} = \\sum_{w,e}n_{w,e}log(P(w|e))\n$$\n$n_{w,e}$是字与实体之间边的权重，也就是实体和这个字同时出现的统计频率。我们希望分布概率能够拟合经验分布概率。\n\n定义打分函数：\n$$\nL_D(f|r)=1-||x_{e_h} + y_r- x_{e_t} ||^2_2\n$$\n实体向量$(x_{e_h} - x_{e_t})$和$y_r$（关系r的表示，也是要学习的参数）越接近，$L_D$就越接近1；反之则会非常小。\n$$\nO_{seed} = \\sum_{f\\in S_{pair}} \\sum_{f'\\in(e'_h,e'_t)} {min\\{1, L_D(f|r) - L_D(f'|r)\\}}\n$$\n$(e'_h,e'_t)$是随机选取的实体对。最小值函数是为了防止两个分数差距太多，因为往往$L_D(f'|r)$会是一个很小的负数。\n\n最后有总目标函数中的Od：\n$$\nO_d = O_{text} + \\eta O_{seed}\n$$\n$\\eta$用于调整两部分的比值。\n\n#### 3.3 Modeling the Module Interaction\n\n$$\nO_i = E_{f\\in G(P)}[L_D(f|r)]\n$$\n\n这里E指的是期望。\n\n我们给模式模型生成的实体对也打分。Oi作为目标函数，为了最大化它，模式集合P应该尽可能包含那些可靠有效的模式。也就是说，模式模型生成的实体对应该得到的打分越大越好。这样一来分布模型就能为模式模型提供监督（打分）。并且，对于分布模型来说，最大化该目标函数能够给实体对分配更高的打分（也就是说，要令Oi最大化，G(P)和LD都要合适）。通过这种方式两个模型能够互相提供监督。\n\n### 4. The Joint Optimization Problem\n\n![algo](https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-201444.jpg)\n\n具体算法如上图原文，为了优化总目标函数，采用协梯度下降算法。\n\n先固定模式模型，将seed实体对$S_{pair}$和模式模型生成的实体对$G(P)$训练分布模型。图中的Eqn.11就是下式：\n$$\nmax_D \\{ O_d + \\lambda O_i \\} = max_D \\{ O_d + \\lambda E_{f \\in G(P)}[L_D(f|r)] \\}\n$$\n然后再固定分布模型，对实体对筛选后得到的$S_{pair}$训练模式模型。图中的Eqn.12就是下式：\n$$\nmax_P \\{ O_p + \\lambda O_i \\} = max_P \\{ \\sum_{\\pi \\in P}(R(\\pi) + \\lambda E_{f \\in G(\\pi)}[L_D(f|r)]) \\}\n$$\n往复迭代。\n\n### 5. Conclusion\n\n利用两个模型进行互补的思路很新颖，从论文的测试结果上来看，本文提出的模型并不逊色于神经网络，可见两个模型互补的效果是相当不错的。但是这种弱监督学习需要的人工标注数据非常少，降低了对标注数据的依赖性。\n\n## Reference\n\n\\*笔记部分参考https://zhuanlan.zhihu.com/p/32364723\n\n[1] Qu, M., Ren, X., Zhang, Y., & Han, J. (2017). Overcoming Limited Supervision in Relation Extraction: A Pattern-enhanced Distributional Representation Approach. *arXiv preprint arXiv:1711.03226*.\n\n[2] Blum, Avrim, and Tom Mitchell. \"Combining labeled and unlabeled data with co-training.\" *Proceedings of the eleventh annual conference on Computational learning theory*. ACM, 1998.\n\n","source":"_posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction.md","raw":"---\ntitle: Overcoming Limited Supervision in Relation Extraction 笔记\ndate: 2018-01-04 08:00:00\ncategories: [research]\ntags: [relation-extraction, limited-supervision, weak-supervision]\n---\n\n这次主要阅读的论文是《Overcoming Limited Supervision in Relation Extraction: A Pattern-enhanced Distributional Representation Approach》[1]。该文主要针对了现有模型对标注数据的依赖，提出一种比较有意思的思路。基于分布的方法（distributional approach）利用两个实体共同出现的统计频率来预测他们的关系，需要大量标注数据，而基于模式的方法（pattern-based approach）一般使用神经网络建模，但这种方法需要更多的标注数据。本文同时建立两个模型，互相为对方提供监督。以分布模型作为判别模型，模式模型作为生成模型。训练过程中不断迭代，从而提升两个模型的性能。\n\n![illustration](https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-201441.jpg)\n\n### 1. Introduction\n\n#### 1.1 Weakly Supervised Learning\n\n弱监督学习介于监督学习和无监督学习之间，它提供的标注数据带有较大的噪音，或标注的相对粗糙，标注结果可能出错。对于关系抽取而言，就是将一些关系实例作为seed，用它们从大型语料库中去除冗余信息并提取更多的实例。\n\n弱监督学习的基本思路：\n\n1. 用容易获得的标注替代较难获得的标注\n2. 选择最需要做精细标注的样例\n3. 模型训练和自动标注交替进行\n\n#### 1.2 Co-training strategy\n\n以往的工作主要是单个模型，该文采用了co-training策略[2]，将两个模型互相协作，取得了比较好的效果。\n\nco-training策略是一种半监督方法，核心就是利用少量已标记样本，通过两个（或多个）模型去学习，对未标记样本进行标记，挑选置信度最高的样本加入已标记样本阵营。\n\n#### 1.3 REPEL (Relation Extraction with pattern-enhanced Embedding)\n\nREPEL是本文提出的一个模型。基于模式的模型学习用于关系抽取文本的模式，基于分布的模型作为分类器，两者互补，互相提供监督。前者相当于一个生成器，基于模式生成候选实例；而后者作为判别器，从中选择最优实例，并将选择结果反馈给前者。训练完成相当于得到了两个关系抽取模型。\n\n### 2. Problem definition\n\n实体识别：使用现成的工具标注。\n\n关系识别：实体对 $(e_h, e_t)$，三元组$(e_h, e_t, r)$\n\n给定语料库D，关系集合R。给定少量seed实例$ \\{(e_h^{r(k)}, e_t^{r(k)}, r)\\} _{k=1}^{N_r} $，提取尽可能多的$ \\{(e_h^{r(i)}, e_t^{r(i)}, r)\\} _{i=1}^M $；换言之，对于每个$ r \\in R $，我们要提取尽可能多的$ \\{(e_h^{r(i)}, e_t^{r(i)})\\} _{i=1}^{M_r} $。\n\n### 3. REPEL Framework\n\n模式模型：找到文本中的模式集合\n\n分布模型：学习实体表示，以及打分函数\n\n目标函数：\n$$\nmax_{P,D}O = max_{P,D}\\{O_p + O_d + \\lambda O_i\\}\n$$\n上面公式中，P表示模式模型的参数，给定关系的全部模式集合。D表示分布模型的参数，实体表示和打分函数。Op和Od分别表示两个目标函数，Oi表示两个模型交互的目标。\n\n注意这里只考虑关系抽取，实体识别使用现有的工具或模型。\n\n#### 3.1 Pattern Module\n\n对于一个指定的关系r，我们的目标是找到K个最可靠的模式，然后进一步使用它们来发现更多的关系实例。\n\n基于模式关系抽取主要分为两种：path-based pattern、meta pattern。对于一句话中的实体对，前者定义为两个实体通过依存信息跳转的最短路径；后者则是两个实体附近的文字序列。利用这两种模式从语料库中寻找匹配的实体对。这样就得到了很多候选模式，每个模式又能分别找到许多匹配的实体对。\n\n对于一个模式$\\pi$，我们通过以下式子计算它的置信度：\n$$\nR(\\pi)=\\frac{|G(\\pi)\\cap S_{pair}|}{|G(\\pi)|}\n$$\n$G(\\pi)$表示被模式$\\pi$所匹配的所有实体对，$S_{pair}$表示seed实体对。可以看到，R实际表示的是，在满足$\\pi$模式的实体对中，seed实体对所占的比例。显然，该比值越高，该模式越符合seed的分布。由此，我们定义：\n$$\nO_p = \\sum_{\\pi \\in P}R(\\pi)\n$$\n下面说明一下整个进行的过程：\n\n- 给定seed实体对，我们通过模式关系抽取的方法获得一系列候选模式。\n- 计算每个候选模式的R值，取最高的K个\n\n#### 3.2 Distributional Module\n\n该模块学习语料中的实体全局分布信息。我们利用给定的关系实例作为打分函数。\n\n对于一个实体e，和一个词w\n$$\nP(w|e) =\\frac{exp(x_e*c_w)}{Z}\n$$\n$x_e$表示需要训练的实体表示向量， $c_w$是预训练的word embedding，Z是归一化项。\n$$\nO_{text} = \\sum_{w,e}n_{w,e}log(P(w|e))\n$$\n$n_{w,e}$是字与实体之间边的权重，也就是实体和这个字同时出现的统计频率。我们希望分布概率能够拟合经验分布概率。\n\n定义打分函数：\n$$\nL_D(f|r)=1-||x_{e_h} + y_r- x_{e_t} ||^2_2\n$$\n实体向量$(x_{e_h} - x_{e_t})$和$y_r$（关系r的表示，也是要学习的参数）越接近，$L_D$就越接近1；反之则会非常小。\n$$\nO_{seed} = \\sum_{f\\in S_{pair}} \\sum_{f'\\in(e'_h,e'_t)} {min\\{1, L_D(f|r) - L_D(f'|r)\\}}\n$$\n$(e'_h,e'_t)$是随机选取的实体对。最小值函数是为了防止两个分数差距太多，因为往往$L_D(f'|r)$会是一个很小的负数。\n\n最后有总目标函数中的Od：\n$$\nO_d = O_{text} + \\eta O_{seed}\n$$\n$\\eta$用于调整两部分的比值。\n\n#### 3.3 Modeling the Module Interaction\n\n$$\nO_i = E_{f\\in G(P)}[L_D(f|r)]\n$$\n\n这里E指的是期望。\n\n我们给模式模型生成的实体对也打分。Oi作为目标函数，为了最大化它，模式集合P应该尽可能包含那些可靠有效的模式。也就是说，模式模型生成的实体对应该得到的打分越大越好。这样一来分布模型就能为模式模型提供监督（打分）。并且，对于分布模型来说，最大化该目标函数能够给实体对分配更高的打分（也就是说，要令Oi最大化，G(P)和LD都要合适）。通过这种方式两个模型能够互相提供监督。\n\n### 4. The Joint Optimization Problem\n\n![algo](https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-201444.jpg)\n\n具体算法如上图原文，为了优化总目标函数，采用协梯度下降算法。\n\n先固定模式模型，将seed实体对$S_{pair}$和模式模型生成的实体对$G(P)$训练分布模型。图中的Eqn.11就是下式：\n$$\nmax_D \\{ O_d + \\lambda O_i \\} = max_D \\{ O_d + \\lambda E_{f \\in G(P)}[L_D(f|r)] \\}\n$$\n然后再固定分布模型，对实体对筛选后得到的$S_{pair}$训练模式模型。图中的Eqn.12就是下式：\n$$\nmax_P \\{ O_p + \\lambda O_i \\} = max_P \\{ \\sum_{\\pi \\in P}(R(\\pi) + \\lambda E_{f \\in G(\\pi)}[L_D(f|r)]) \\}\n$$\n往复迭代。\n\n### 5. Conclusion\n\n利用两个模型进行互补的思路很新颖，从论文的测试结果上来看，本文提出的模型并不逊色于神经网络，可见两个模型互补的效果是相当不错的。但是这种弱监督学习需要的人工标注数据非常少，降低了对标注数据的依赖性。\n\n## Reference\n\n\\*笔记部分参考https://zhuanlan.zhihu.com/p/32364723\n\n[1] Qu, M., Ren, X., Zhang, Y., & Han, J. (2017). Overcoming Limited Supervision in Relation Extraction: A Pattern-enhanced Distributional Representation Approach. *arXiv preprint arXiv:1711.03226*.\n\n[2] Blum, Avrim, and Tom Mitchell. \"Combining labeled and unlabeled data with co-training.\" *Proceedings of the eleventh annual conference on Computational learning theory*. ACM, 1998.\n\n","slug":"[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction","published":1,"updated":"2020-11-03T03:26:12.569Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckw4qufpo0024gwtlezik90yy","content":"<p>这次主要阅读的论文是《Overcoming Limited Supervision in Relation Extraction: A Pattern-enhanced Distributional Representation Approach》[1]。该文主要针对了现有模型对标注数据的依赖，提出一种比较有意思的思路。基于分布的方法（distributional approach）利用两个实体共同出现的统计频率来预测他们的关系，需要大量标注数据，而基于模式的方法（pattern-based approach）一般使用神经网络建模，但这种方法需要更多的标注数据。本文同时建立两个模型，互相为对方提供监督。以分布模型作为判别模型，模式模型作为生成模型。训练过程中不断迭代，从而提升两个模型的性能。</p>\n<p><img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-201441.jpg\" alt=\"illustration\"></p>\n<h3 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1. Introduction\"></a>1. Introduction</h3><h4 id=\"1-1-Weakly-Supervised-Learning\"><a href=\"#1-1-Weakly-Supervised-Learning\" class=\"headerlink\" title=\"1.1 Weakly Supervised Learning\"></a>1.1 Weakly Supervised Learning</h4><p>弱监督学习介于监督学习和无监督学习之间，它提供的标注数据带有较大的噪音，或标注的相对粗糙，标注结果可能出错。对于关系抽取而言，就是将一些关系实例作为seed，用它们从大型语料库中去除冗余信息并提取更多的实例。</p>\n<p>弱监督学习的基本思路：</p>\n<ol>\n<li>用容易获得的标注替代较难获得的标注</li>\n<li>选择最需要做精细标注的样例</li>\n<li>模型训练和自动标注交替进行</li>\n</ol>\n<h4 id=\"1-2-Co-training-strategy\"><a href=\"#1-2-Co-training-strategy\" class=\"headerlink\" title=\"1.2 Co-training strategy\"></a>1.2 Co-training strategy</h4><p>以往的工作主要是单个模型，该文采用了co-training策略[2]，将两个模型互相协作，取得了比较好的效果。</p>\n<p>co-training策略是一种半监督方法，核心就是利用少量已标记样本，通过两个（或多个）模型去学习，对未标记样本进行标记，挑选置信度最高的样本加入已标记样本阵营。</p>\n<h4 id=\"1-3-REPEL-Relation-Extraction-with-pattern-enhanced-Embedding\"><a href=\"#1-3-REPEL-Relation-Extraction-with-pattern-enhanced-Embedding\" class=\"headerlink\" title=\"1.3 REPEL (Relation Extraction with pattern-enhanced Embedding)\"></a>1.3 REPEL (Relation Extraction with pattern-enhanced Embedding)</h4><p>REPEL是本文提出的一个模型。基于模式的模型学习用于关系抽取文本的模式，基于分布的模型作为分类器，两者互补，互相提供监督。前者相当于一个生成器，基于模式生成候选实例；而后者作为判别器，从中选择最优实例，并将选择结果反馈给前者。训练完成相当于得到了两个关系抽取模型。</p>\n<h3 id=\"2-Problem-definition\"><a href=\"#2-Problem-definition\" class=\"headerlink\" title=\"2. Problem definition\"></a>2. Problem definition</h3><p>实体识别：使用现成的工具标注。</p>\n<p>关系识别：实体对 $(e_h, e_t)$，三元组$(e_h, e_t, r)$</p>\n<p>给定语料库D，关系集合R。给定少量seed实例$ {(e_h^{r(k)}, e_t^{r(k)}, r)} _{k=1}^{N_r} $，提取尽可能多的$ {(e_h^{r(i)}, e_t^{r(i)}, r)} _{i=1}^M $；换言之，对于每个$ r \\in R $，我们要提取尽可能多的$ {(e_h^{r(i)}, e_t^{r(i)})} _{i=1}^{M_r} $。</p>\n<h3 id=\"3-REPEL-Framework\"><a href=\"#3-REPEL-Framework\" class=\"headerlink\" title=\"3. REPEL Framework\"></a>3. REPEL Framework</h3><p>模式模型：找到文本中的模式集合</p>\n<p>分布模型：学习实体表示，以及打分函数</p>\n<p>目标函数：<br>$$<br>max_{P,D}O = max_{P,D}{O_p + O_d + \\lambda O_i}<br>$$<br>上面公式中，P表示模式模型的参数，给定关系的全部模式集合。D表示分布模型的参数，实体表示和打分函数。Op和Od分别表示两个目标函数，Oi表示两个模型交互的目标。</p>\n<p>注意这里只考虑关系抽取，实体识别使用现有的工具或模型。</p>\n<h4 id=\"3-1-Pattern-Module\"><a href=\"#3-1-Pattern-Module\" class=\"headerlink\" title=\"3.1 Pattern Module\"></a>3.1 Pattern Module</h4><p>对于一个指定的关系r，我们的目标是找到K个最可靠的模式，然后进一步使用它们来发现更多的关系实例。</p>\n<p>基于模式关系抽取主要分为两种：path-based pattern、meta pattern。对于一句话中的实体对，前者定义为两个实体通过依存信息跳转的最短路径；后者则是两个实体附近的文字序列。利用这两种模式从语料库中寻找匹配的实体对。这样就得到了很多候选模式，每个模式又能分别找到许多匹配的实体对。</p>\n<p>对于一个模式$\\pi$，我们通过以下式子计算它的置信度：<br>$$<br>R(\\pi)=\\frac{|G(\\pi)\\cap S_{pair}|}{|G(\\pi)|}<br>$$<br>$G(\\pi)$表示被模式$\\pi$所匹配的所有实体对，$S_{pair}$表示seed实体对。可以看到，R实际表示的是，在满足$\\pi$模式的实体对中，seed实体对所占的比例。显然，该比值越高，该模式越符合seed的分布。由此，我们定义：<br>$$<br>O_p = \\sum_{\\pi \\in P}R(\\pi)<br>$$<br>下面说明一下整个进行的过程：</p>\n<ul>\n<li>给定seed实体对，我们通过模式关系抽取的方法获得一系列候选模式。</li>\n<li>计算每个候选模式的R值，取最高的K个</li>\n</ul>\n<h4 id=\"3-2-Distributional-Module\"><a href=\"#3-2-Distributional-Module\" class=\"headerlink\" title=\"3.2 Distributional Module\"></a>3.2 Distributional Module</h4><p>该模块学习语料中的实体全局分布信息。我们利用给定的关系实例作为打分函数。</p>\n<p>对于一个实体e，和一个词w<br>$$<br>P(w|e) =\\frac{exp(x_e*c_w)}{Z}<br>$$<br>$x_e$表示需要训练的实体表示向量， $c_w$是预训练的word embedding，Z是归一化项。<br>$$<br>O_{text} = \\sum_{w,e}n_{w,e}log(P(w|e))<br>$$<br>$n_{w,e}$是字与实体之间边的权重，也就是实体和这个字同时出现的统计频率。我们希望分布概率能够拟合经验分布概率。</p>\n<p>定义打分函数：<br>$$<br>L_D(f|r)=1-||x_{e_h} + y_r- x_{e_t} ||^2_2<br>$$<br>实体向量$(x_{e_h} - x_{e_t})$和$y_r$（关系r的表示，也是要学习的参数）越接近，$L_D$就越接近1；反之则会非常小。<br>$$<br>O_{seed} = \\sum_{f\\in S_{pair}} \\sum_{f’\\in(e’_h,e’_t)} {min{1, L_D(f|r) - L_D(f’|r)}}<br>$$<br>$(e’_h,e’_t)$是随机选取的实体对。最小值函数是为了防止两个分数差距太多，因为往往$L_D(f’|r)$会是一个很小的负数。</p>\n<p>最后有总目标函数中的Od：<br>$$<br>O_d = O_{text} + \\eta O_{seed}<br>$$<br>$\\eta$用于调整两部分的比值。</p>\n<h4 id=\"3-3-Modeling-the-Module-Interaction\"><a href=\"#3-3-Modeling-the-Module-Interaction\" class=\"headerlink\" title=\"3.3 Modeling the Module Interaction\"></a>3.3 Modeling the Module Interaction</h4><p>$$<br>O_i = E_{f\\in G(P)}[L_D(f|r)]<br>$$</p>\n<p>这里E指的是期望。</p>\n<p>我们给模式模型生成的实体对也打分。Oi作为目标函数，为了最大化它，模式集合P应该尽可能包含那些可靠有效的模式。也就是说，模式模型生成的实体对应该得到的打分越大越好。这样一来分布模型就能为模式模型提供监督（打分）。并且，对于分布模型来说，最大化该目标函数能够给实体对分配更高的打分（也就是说，要令Oi最大化，G(P)和LD都要合适）。通过这种方式两个模型能够互相提供监督。</p>\n<h3 id=\"4-The-Joint-Optimization-Problem\"><a href=\"#4-The-Joint-Optimization-Problem\" class=\"headerlink\" title=\"4. The Joint Optimization Problem\"></a>4. The Joint Optimization Problem</h3><p><img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-201444.jpg\" alt=\"algo\"></p>\n<p>具体算法如上图原文，为了优化总目标函数，采用协梯度下降算法。</p>\n<p>先固定模式模型，将seed实体对$S_{pair}$和模式模型生成的实体对$G(P)$训练分布模型。图中的Eqn.11就是下式：<br>$$<br>max_D { O_d + \\lambda O_i } = max_D { O_d + \\lambda E_{f \\in G(P)}[L_D(f|r)] }<br>$$<br>然后再固定分布模型，对实体对筛选后得到的$S_{pair}$训练模式模型。图中的Eqn.12就是下式：<br>$$<br>max_P { O_p + \\lambda O_i } = max_P { \\sum_{\\pi \\in P}(R(\\pi) + \\lambda E_{f \\in G(\\pi)}[L_D(f|r)]) }<br>$$<br>往复迭代。</p>\n<h3 id=\"5-Conclusion\"><a href=\"#5-Conclusion\" class=\"headerlink\" title=\"5. Conclusion\"></a>5. Conclusion</h3><p>利用两个模型进行互补的思路很新颖，从论文的测试结果上来看，本文提出的模型并不逊色于神经网络，可见两个模型互补的效果是相当不错的。但是这种弱监督学习需要的人工标注数据非常少，降低了对标注数据的依赖性。</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><p>*笔记部分参考<a href=\"https://zhuanlan.zhihu.com/p/32364723\">https://zhuanlan.zhihu.com/p/32364723</a></p>\n<p>[1] Qu, M., Ren, X., Zhang, Y., &amp; Han, J. (2017). Overcoming Limited Supervision in Relation Extraction: A Pattern-enhanced Distributional Representation Approach. <em>arXiv preprint arXiv:1711.03226</em>.</p>\n<p>[2] Blum, Avrim, and Tom Mitchell. “Combining labeled and unlabeled data with co-training.” <em>Proceedings of the eleventh annual conference on Computational learning theory</em>. ACM, 1998.</p>\n","site":{"data":{"recommended_posts":{"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"}],"https://LorrinWWW.github.io/posts/complexity/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/hexo-with-latex/":[{"title":"区块链时代的最正确的打赏姿势——在Hexo博客配置Coinhive网页挖矿","permalink":"http://www.davidfnck.com/blockchain/mine-xmr-in-hexo-by-coinhive.html/"}],"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/":[{"title":"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-statistic/":[{"title":"贝叶斯估计 Bayes estimation","permalink":"https://LorrinWWW.github.io/posts/Bayes-estimation/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"}],"https://LorrinWWW.github.io/posts/[2018.2.5]Nested-LSTMs/":[{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"}],"https://LorrinWWW.github.io/posts/datamining-pretreatment/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Several models for knowledge graph representing and completing 几个知识图谱模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/"}],"https://LorrinWWW.github.io/posts/compression/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/Sobolev-space/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/proba-ch5/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch3/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/proba-ch1/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/ML-CNN/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/EDP-basic-models/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/machine-learning/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/datamining-class-pred/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction/":[],"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/":[{"title":"OS notes","permalink":"https://LorrinWWW.github.io/posts/OS-notes/"}],"https://LorrinWWW.github.io/posts/projet-enjeu-plugin-chrome-101/":[{"title":"Chrome油猴脚本","permalink":"http://gubangzhong.cn/2017/06/13/Chrome油猴脚本/"}],"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/":[{"title":"MongoDB, Docker and Python","permalink":"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/":[{"title":"Hands on Scrapy","permalink":"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/OS-notes/":[{"title":"learning OS and building LorriOS","permalink":"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/"}],"https://LorrinWWW.github.io/posts/Bayes-estimation/":[{"title":"Note of statistic","permalink":"https://LorrinWWW.github.io/posts/Note-of-statistic/"}],"https://LorrinWWW.github.io/posts/management-of-the-firm/":[],"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"}],"https://LorrinWWW.github.io/posts/datamining-qualitative-induction/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-finite-element-method/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/QuadTree/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/proba-ch4/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch2/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/":[{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/Note-of-NLP/":[{"title":"Generative Adversarial Network","permalink":"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/"},{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"}],"https://LorrinWWW.github.io/posts/Note-of-datamining/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"数据挖掘-分类与预测","permalink":"https://LorrinWWW.github.io/posts/datamining-class-pred/"},{"title":"数据挖掘-预处理","permalink":"https://LorrinWWW.github.io/posts/datamining-pretreatment/"}],"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/Note-of-probability/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/participe-present-et-gerondif/":[],"https://LorrinWWW.github.io/posts/Hilbert-space/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/proba-ch6/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}]}}},"excerpt":"","more":"<p>这次主要阅读的论文是《Overcoming Limited Supervision in Relation Extraction: A Pattern-enhanced Distributional Representation Approach》[1]。该文主要针对了现有模型对标注数据的依赖，提出一种比较有意思的思路。基于分布的方法（distributional approach）利用两个实体共同出现的统计频率来预测他们的关系，需要大量标注数据，而基于模式的方法（pattern-based approach）一般使用神经网络建模，但这种方法需要更多的标注数据。本文同时建立两个模型，互相为对方提供监督。以分布模型作为判别模型，模式模型作为生成模型。训练过程中不断迭代，从而提升两个模型的性能。</p>\n<p><img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-201441.jpg\" alt=\"illustration\"></p>\n<h3 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1. Introduction\"></a>1. Introduction</h3><h4 id=\"1-1-Weakly-Supervised-Learning\"><a href=\"#1-1-Weakly-Supervised-Learning\" class=\"headerlink\" title=\"1.1 Weakly Supervised Learning\"></a>1.1 Weakly Supervised Learning</h4><p>弱监督学习介于监督学习和无监督学习之间，它提供的标注数据带有较大的噪音，或标注的相对粗糙，标注结果可能出错。对于关系抽取而言，就是将一些关系实例作为seed，用它们从大型语料库中去除冗余信息并提取更多的实例。</p>\n<p>弱监督学习的基本思路：</p>\n<ol>\n<li>用容易获得的标注替代较难获得的标注</li>\n<li>选择最需要做精细标注的样例</li>\n<li>模型训练和自动标注交替进行</li>\n</ol>\n<h4 id=\"1-2-Co-training-strategy\"><a href=\"#1-2-Co-training-strategy\" class=\"headerlink\" title=\"1.2 Co-training strategy\"></a>1.2 Co-training strategy</h4><p>以往的工作主要是单个模型，该文采用了co-training策略[2]，将两个模型互相协作，取得了比较好的效果。</p>\n<p>co-training策略是一种半监督方法，核心就是利用少量已标记样本，通过两个（或多个）模型去学习，对未标记样本进行标记，挑选置信度最高的样本加入已标记样本阵营。</p>\n<h4 id=\"1-3-REPEL-Relation-Extraction-with-pattern-enhanced-Embedding\"><a href=\"#1-3-REPEL-Relation-Extraction-with-pattern-enhanced-Embedding\" class=\"headerlink\" title=\"1.3 REPEL (Relation Extraction with pattern-enhanced Embedding)\"></a>1.3 REPEL (Relation Extraction with pattern-enhanced Embedding)</h4><p>REPEL是本文提出的一个模型。基于模式的模型学习用于关系抽取文本的模式，基于分布的模型作为分类器，两者互补，互相提供监督。前者相当于一个生成器，基于模式生成候选实例；而后者作为判别器，从中选择最优实例，并将选择结果反馈给前者。训练完成相当于得到了两个关系抽取模型。</p>\n<h3 id=\"2-Problem-definition\"><a href=\"#2-Problem-definition\" class=\"headerlink\" title=\"2. Problem definition\"></a>2. Problem definition</h3><p>实体识别：使用现成的工具标注。</p>\n<p>关系识别：实体对 $(e_h, e_t)$，三元组$(e_h, e_t, r)$</p>\n<p>给定语料库D，关系集合R。给定少量seed实例$ {(e_h^{r(k)}, e_t^{r(k)}, r)} _{k=1}^{N_r} $，提取尽可能多的$ {(e_h^{r(i)}, e_t^{r(i)}, r)} _{i=1}^M $；换言之，对于每个$ r \\in R $，我们要提取尽可能多的$ {(e_h^{r(i)}, e_t^{r(i)})} _{i=1}^{M_r} $。</p>\n<h3 id=\"3-REPEL-Framework\"><a href=\"#3-REPEL-Framework\" class=\"headerlink\" title=\"3. REPEL Framework\"></a>3. REPEL Framework</h3><p>模式模型：找到文本中的模式集合</p>\n<p>分布模型：学习实体表示，以及打分函数</p>\n<p>目标函数：<br>$$<br>max_{P,D}O = max_{P,D}{O_p + O_d + \\lambda O_i}<br>$$<br>上面公式中，P表示模式模型的参数，给定关系的全部模式集合。D表示分布模型的参数，实体表示和打分函数。Op和Od分别表示两个目标函数，Oi表示两个模型交互的目标。</p>\n<p>注意这里只考虑关系抽取，实体识别使用现有的工具或模型。</p>\n<h4 id=\"3-1-Pattern-Module\"><a href=\"#3-1-Pattern-Module\" class=\"headerlink\" title=\"3.1 Pattern Module\"></a>3.1 Pattern Module</h4><p>对于一个指定的关系r，我们的目标是找到K个最可靠的模式，然后进一步使用它们来发现更多的关系实例。</p>\n<p>基于模式关系抽取主要分为两种：path-based pattern、meta pattern。对于一句话中的实体对，前者定义为两个实体通过依存信息跳转的最短路径；后者则是两个实体附近的文字序列。利用这两种模式从语料库中寻找匹配的实体对。这样就得到了很多候选模式，每个模式又能分别找到许多匹配的实体对。</p>\n<p>对于一个模式$\\pi$，我们通过以下式子计算它的置信度：<br>$$<br>R(\\pi)=\\frac{|G(\\pi)\\cap S_{pair}|}{|G(\\pi)|}<br>$$<br>$G(\\pi)$表示被模式$\\pi$所匹配的所有实体对，$S_{pair}$表示seed实体对。可以看到，R实际表示的是，在满足$\\pi$模式的实体对中，seed实体对所占的比例。显然，该比值越高，该模式越符合seed的分布。由此，我们定义：<br>$$<br>O_p = \\sum_{\\pi \\in P}R(\\pi)<br>$$<br>下面说明一下整个进行的过程：</p>\n<ul>\n<li>给定seed实体对，我们通过模式关系抽取的方法获得一系列候选模式。</li>\n<li>计算每个候选模式的R值，取最高的K个</li>\n</ul>\n<h4 id=\"3-2-Distributional-Module\"><a href=\"#3-2-Distributional-Module\" class=\"headerlink\" title=\"3.2 Distributional Module\"></a>3.2 Distributional Module</h4><p>该模块学习语料中的实体全局分布信息。我们利用给定的关系实例作为打分函数。</p>\n<p>对于一个实体e，和一个词w<br>$$<br>P(w|e) =\\frac{exp(x_e*c_w)}{Z}<br>$$<br>$x_e$表示需要训练的实体表示向量， $c_w$是预训练的word embedding，Z是归一化项。<br>$$<br>O_{text} = \\sum_{w,e}n_{w,e}log(P(w|e))<br>$$<br>$n_{w,e}$是字与实体之间边的权重，也就是实体和这个字同时出现的统计频率。我们希望分布概率能够拟合经验分布概率。</p>\n<p>定义打分函数：<br>$$<br>L_D(f|r)=1-||x_{e_h} + y_r- x_{e_t} ||^2_2<br>$$<br>实体向量$(x_{e_h} - x_{e_t})$和$y_r$（关系r的表示，也是要学习的参数）越接近，$L_D$就越接近1；反之则会非常小。<br>$$<br>O_{seed} = \\sum_{f\\in S_{pair}} \\sum_{f’\\in(e’_h,e’_t)} {min{1, L_D(f|r) - L_D(f’|r)}}<br>$$<br>$(e’_h,e’_t)$是随机选取的实体对。最小值函数是为了防止两个分数差距太多，因为往往$L_D(f’|r)$会是一个很小的负数。</p>\n<p>最后有总目标函数中的Od：<br>$$<br>O_d = O_{text} + \\eta O_{seed}<br>$$<br>$\\eta$用于调整两部分的比值。</p>\n<h4 id=\"3-3-Modeling-the-Module-Interaction\"><a href=\"#3-3-Modeling-the-Module-Interaction\" class=\"headerlink\" title=\"3.3 Modeling the Module Interaction\"></a>3.3 Modeling the Module Interaction</h4><p>$$<br>O_i = E_{f\\in G(P)}[L_D(f|r)]<br>$$</p>\n<p>这里E指的是期望。</p>\n<p>我们给模式模型生成的实体对也打分。Oi作为目标函数，为了最大化它，模式集合P应该尽可能包含那些可靠有效的模式。也就是说，模式模型生成的实体对应该得到的打分越大越好。这样一来分布模型就能为模式模型提供监督（打分）。并且，对于分布模型来说，最大化该目标函数能够给实体对分配更高的打分（也就是说，要令Oi最大化，G(P)和LD都要合适）。通过这种方式两个模型能够互相提供监督。</p>\n<h3 id=\"4-The-Joint-Optimization-Problem\"><a href=\"#4-The-Joint-Optimization-Problem\" class=\"headerlink\" title=\"4. The Joint Optimization Problem\"></a>4. The Joint Optimization Problem</h3><p><img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-201444.jpg\" alt=\"algo\"></p>\n<p>具体算法如上图原文，为了优化总目标函数，采用协梯度下降算法。</p>\n<p>先固定模式模型，将seed实体对$S_{pair}$和模式模型生成的实体对$G(P)$训练分布模型。图中的Eqn.11就是下式：<br>$$<br>max_D { O_d + \\lambda O_i } = max_D { O_d + \\lambda E_{f \\in G(P)}[L_D(f|r)] }<br>$$<br>然后再固定分布模型，对实体对筛选后得到的$S_{pair}$训练模式模型。图中的Eqn.12就是下式：<br>$$<br>max_P { O_p + \\lambda O_i } = max_P { \\sum_{\\pi \\in P}(R(\\pi) + \\lambda E_{f \\in G(\\pi)}[L_D(f|r)]) }<br>$$<br>往复迭代。</p>\n<h3 id=\"5-Conclusion\"><a href=\"#5-Conclusion\" class=\"headerlink\" title=\"5. Conclusion\"></a>5. Conclusion</h3><p>利用两个模型进行互补的思路很新颖，从论文的测试结果上来看，本文提出的模型并不逊色于神经网络，可见两个模型互补的效果是相当不错的。但是这种弱监督学习需要的人工标注数据非常少，降低了对标注数据的依赖性。</p>\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><p>*笔记部分参考<a href=\"https://zhuanlan.zhihu.com/p/32364723\">https://zhuanlan.zhihu.com/p/32364723</a></p>\n<p>[1] Qu, M., Ren, X., Zhang, Y., &amp; Han, J. (2017). Overcoming Limited Supervision in Relation Extraction: A Pattern-enhanced Distributional Representation Approach. <em>arXiv preprint arXiv:1711.03226</em>.</p>\n<p>[2] Blum, Avrim, and Tom Mitchell. “Combining labeled and unlabeled data with co-training.” <em>Proceedings of the eleventh annual conference on Computational learning theory</em>. ACM, 1998.</p>\n"},{"title":"Open-World Knowledge Graph Completion 笔记","date":"2018-02-26T00:00:00.000Z","_content":"\n## Open-World Knowledge Graph Completion\n\n**摘要**：\\[1\\]文首先讨论了Closed-World KGC，它无法处理从 KG 外部加入的新实体，并严重依赖已有KG连接的，不能对弱连接有好的预测。为此定义了 Open-World KGC，可以接收 新的实体并链接到 KG；并依此提出了ConMask模型，在给定关系和实体名、实体描述的前提下，利用attention机制通过关系定位实体描述中最相关的词，再以这些词和实体得到要链接的实体。\n\n### 1. Introduction\n\n知识图谱（KG）是一种信息网络，它用三元组 $(h,r,t)$ 来表示知识（h: head entity, t: tail entity, r: relation），目前比较出名的KG有 DBPedia，ConceptNet 等，目前的大多数KG都有噪音且不完整，比如基于Wikipedia的DBPedia有460万个实体，但其中一半实体拥有少于5个的关系。\n\n这说明了大部分的知识图谱仍然是非常不完善的，我们必须从一开始就要考虑系统的修改、补充完善的可能性。这项任务被定义为Knowledge Graph Completion (KGC)。\n\n#### Closed-World KGC\n\n给定一个不完整的KG $G=(E,R,T)$ 其中 $E,R,T$ 分别表示实体集，关系集以及三元组集，Closed-World KGC的任务就是通过找到一系列丢失的三元组 $ T' = \\{ \\langle h,r,t \\rangle|h \\in E, r \\in R, t \\in E, \\langle h,r,t \\rangle \\notin T \\}$ 来补充现有的 $G$.\n\n一个很重要的地方在于，Closed-World KGC 假定了新的实体、关系都被原有的 $G$ 包含，对于不在 $G$ 中的实体则一筹莫展。\n\n目前的Closed-World KGC方法很多往往使用TranE或者低维特征表示模型，前者的核心思想就是 $h+r=t$ ，后者则指 Embedding 等。\n\n该方法仅对固定的或者缓慢更新的KG有效，对于快速变更的KG则效果一般。\n\n#### Open-World KGC\n\n给定一个不完整的KG $G=(E,R,T)$ 其中 $E,R,T$ 分别表示实体集，关系集以及三元组集，Open-World KGC 的任务就是找到 $G$ 中没有的三元组集，$T' =\\{<h,r,t>|h\\in E^i,r\\in R, t\\in E^i,<h,r,t>\\notin T\\}$ 其中 $E^i$ 是G的实体超集。\n\nClosed-World方法就是根据知识图谱的拓扑结构更新一个随机的向量作为实体和关系的embedding，但对于不在网络中的实体，这个方法就失效了，这个时候就需要用别的特征来代替这个用网络拓扑结构得到的特征。\n\n一般直觉就是用实体的描述（entity description），根据实体的描述来得到特征，但从非结构化文本中学习向量表示比在网络的拓扑结构中要难得多，原因如下：\n\n1. 在Closed-world KGC模型中，每个实体都有一个embedding (从与它相连的实体上学得的)，但Open-World KGC模型则需要从实体描述的word embedding中得到entity embedding。而无论实体之间的联系情况是什么，word embedding的更新都会导致有相同词的entities的更新。\n2. 因为使用了非结构化文本，所以Open-World KGC模型可能会引入噪音或者冗余信息。\n\n### 2. Closed-World KGC \n\n在 Closed-World KGC 中，最为常用也最为基础的方法是一种给予强化学习(RL)的模型，被称为TransE \\[2\\]. 它有一个简单实用的假设：\n$$\nh+r = t\n$$\n其中h是head entity的向量，t是tail entity的向量，r是关系向量。\n\nTransE定义了loss function：\n$$\n\\mathcal{L(T)} = \\sum_{<h,r,t>\\in T} [\\gamma + E(\\langle h,r,t \\rangle) - E(\\langle h',r',t' \\rangle)]_+\n$$\n其中 $T$ 代表一个三元组的集合；$E(\\langle h,r,t \\rangle) = ||h+r-t||_{L_n}$是energy function；$\\langle h,r,t \\rangle$是G中的一个三元组；$h',\\langle r',t' \\rangle$代表一个不存在于 $T$ 的三元组，通过随机替换一部分$\\langle h,r,t \\rangle$来得到。\n\n这里还略去了很多TransE的变体等其他模型，但它们都是基于Closed-World KGC来做的。\n\n### 3. ConMask for Open-World KGC\n\n首先通过一个例子来说明：\n\n**任务：**填补三元组 $\\langle \\text{Ameen Sayani, residence, ?}\\rangle$，其中KG中并没有Ameen Sayani这个实体。\n\n**描述：**\"... **Ameen Sayani** was introduced to All India Radio, **Bombay**, by his brother Hamid Sayani. Ameen participated in English programmes there for ten years ...\" ，\n\n**目标预测实体：**Bombay (or Mumbai)\n\n为了找到Ameen Sayani的住址，在处理这个任务的过程中，我们不会从头看到尾，而是找到相关的关键词比如家庭或工作相关的词。这里，我们发现Ameen Sayani的工作地点All India Radio在Bombay，因此我们推测Ameen Sayani也住在Bombay（Bombay就是现在的Mumbai）。\n\n这个过程也可以被归纳为：\n\n1. 定位与该任务相关的信息。\n2. 根据上下文和相关文本推断。\n3. 根据相关文本推出正确目标实体。\n\n仿照这个过程，ConMask的工作方式被设计为：\n\n1. **Relationship-dependent content masking** -- 标记那些与任务相关的词语。\n2. **Target fusion** -- 从相关文本中抽取目标实体的embedding。\n3. **Target entity resolution** -- 通过计算KG中的候选目标实体，2中抽取出的实体embedding以及其它文本特征之间的相似度来选定目标实体。\n\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202310.jpg\" width=\"60%\">\n\nConMask模型总体结构如上，ConMask通过选择与给定关系相关的词来避免引入不相关的和有噪音的词。对于相关的文本，ConMask通过全连接卷积神经网络（FCN）来提取word-embedding。最后它将提取的embedding于KG中存在的实体进行比较，从而获得一系列目标实体。\n\n#### 3.1 Relationship-dependent content masking \n\nConMask根据给定的关系预处理输入文本，来选择一些相关的小片段，从而屏蔽掉无关文本。content-masking这一灵感来源于基于attention机制的RNN网络\\[3\\]，关于attention之前的笔记也有学习过。\n\n基于相似度得到选择最相关的词，具体公式如下：\n$$\n\\tau(\\phi(e), \\psi(r)) = W_{\\phi(e)} \\circ f_w(W_{\\phi(e)}, W_{\\psi(r)})\n$$\n其中 $e$ 是一个实体，$r$ 是某个关系, $\\phi$ 是description function并返回一个向量用于表示对一个实体或关系的描述，$\\psi$ 是name mapping function并返回一个向量用于表示一个实体或关系的名字， $ W_{\\phi{(e)}} \\in \\mathbb{R}^{|\\phi(r)|\\times k} $ 是一个描述矩阵每一行表示一个k维的描述中的word-embedding， $W_{\\phi{(e)}} \\in \\mathbb{R}^{|\\phi(r)|\\times k} $ 是一个名字矩阵每一行表示一个k维的实体名字word-embedding，$\\circ$ 是row-wise product，$f_w$ 用于计算的每一行的屏蔽比重。\n\n作者给了一个简单的$f_w$ ，Maximal Word-Relationship Weights(MWRW)，就是计算实体描述中每个词向量与关系名称的每个词向量的最大cos相似度:\n$$\nf_w^{MWRW}(W_{\\phi(e)}, W_{\\psi(r)})_{[i]} =  max_j(\\frac{\\sum_m^k{W_{\\phi(e)[i,m]} W_{\\psi(r)[j,m]}}}{\\sqrt{\\sum_m^k{W^2_{\\phi(e)[i,m]}}}\\sqrt{\\sum_m^k{W^2_{\\psi(e)[j,m]}}}})\n$$\n这个公式会给与给定关系无关的词更小的权重，与关系语义接近的词更大的权重，但权重最高的词一般不是目标实体，如下图所示，给定关系spouse，得到最大权重的是married，虽然married与spouse在语义上接近，但它并不是目标实体，因此作者称这种有着最大MWRW权重的词为指示词（indicator word），因为正确的词一般就在该词附近，在下图例子中可以发现目标实体barack obama就在married后面。\n\n为了给目标实体word正确的权重，作者改进了这个公式，具体公式如下，这个公式就是每个词的权重不会小于之前 $k_m$ 称为 Maximal Context-Relationship Weights (MCRW)：\n$$\nf_w^{MCRW}(W_{\\phi(e)}, W_{\\psi(r)})_{[i]} =  max(f_w^{MWRW}(W_{\\phi(e)}, W_{\\psi(r)})_{[i-k_m:i]})\n$$\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202313.jpg\">\n\n#### 3.2 Target Fusion\n\n这一步骤用于输出基于词的实体embedding，这个过程记为$\\xi$，使用Conetent Masking $\\tau$ 的输出。它使用全连接卷积网络，其结构如下：\n\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202312.jpg\">\n\n**Semantic Averaging**\n\n我们可以对所有实体进行embedding，但是这会产生大量的参数，使计算变得非常复杂。事实上，因为Target fusion函数用于抽取，所以对不需要抽取的实体名字使用target fusion就会显得很奇怪也很没有必要。\n\n这里作者提出了一个简单的语义平均法来计算这些实体的embedding：$\\eta(W) = \\frac{1}{k_l}\\sum_i^{k_i}W_i$\n\n#### 3.3 Loss function\n\n为了加速训练，我们参考 list-wise ranking loss function (Shi and Weninger 2017)，并设计 partial list-wise ranking loss function，拥有正负目标采样。正样本就是训练集的标注内容，记为$E^+$；负样本就是替换正样本的head entity或tail entity所得到的，记为$E^-$ 。\n$$\n\\mathcal{L}(h, r, t) =  \\begin{cases}\n\\sum_{h_+\\in E^+}{-\\frac{log(S(h_+,r,t,E^+\\cup E^-))}{|E^+|}}, & \\text{if }p_c > 0.5; \\\\\n\\sum_{h_+\\in E^+}{-\\frac{log(S(h,r,t_+,E^+\\cup E^-))}{|E^+|}}, & \\text{if }p_c \\le 0.5; .\n\\end{cases}\n$$\n$p_c$ 服从$[0,1]$的均匀分布，大于0.5时，把输入实体作为tail entity，小于0.5的时候就是作为head entity，表示替换head entity和tail entity的概率各为50%。另有$S$, 即 softmax normalized output of ConMask：\n$$\nS(h,r,t,E^+) = \\begin{cases}\n\\sum_{e \\in E^\\pm}^{exp(ConMask(h,r,t))}{exp(ConMask(e,r,t))} & \\text{if } p_c > 0.5 \\\\\n\\sum_{e \\in E^\\pm}^{exp(ConMask(e,r,t))}{exp(ConMask(h,r,t))} & \\text{if } p_c \\le 0.5 \\\\\n\\end{cases}\n$$\n\n### 4. Results\n\n从结果上看，对比其他模型，在开放领域，ConMask获得了最佳的效果；在Closed-World中，尽管ConMask不是为此设计的，但是对比TransE和TransR依然不逊色，结果相仿。\n\n目前而言，ConMask模型只能预测在实体描述中表达的关系，将来还应考虑扩展它，使其能够发现新的或隐含的关系。\n\n## Bibliographies\n\n笔记参考：https://zhuanlan.zhihu.com/p/33026043，http://blog.csdn.net/TgqDT3gGaMdkHasLZv/article/details/79224178\n\n代码实现：https://github.com/bxshi/ConMask\n\n\\[1\\] Shi, Baoxu, and Tim Weninger. \"Open-World Knowledge Graph Completion.\" *arXiv preprint arXiv:1711.03438* (2017).\n\n\\[2\\] Bordes, A., Usunier, N., Garcia-Duran, A., Weston, J., & Yakhnenko, O. (2013). Translating embeddings for modeling multi-relational data. In *Advances in neural information processing systems* (pp. 2787-2795).\n\n\\[3\\] Chorowski, J. K., Bahdanau, D., Serdyuk, D., Cho, K., & Bengio, Y. (2015). Attention-based models for speech recognition. In *Advances in neural information processing systems* (pp. 577-585).","source":"_posts/[2018.2.26]Open-World-Knowledge-Graph-Completion.md","raw":"---\ntitle: Open-World Knowledge Graph Completion 笔记\ndate: 2018-02-26 08:00:00\ncategories: [research]\ntags: [KGC, CNN, knowledge-graph]\n---\n\n## Open-World Knowledge Graph Completion\n\n**摘要**：\\[1\\]文首先讨论了Closed-World KGC，它无法处理从 KG 外部加入的新实体，并严重依赖已有KG连接的，不能对弱连接有好的预测。为此定义了 Open-World KGC，可以接收 新的实体并链接到 KG；并依此提出了ConMask模型，在给定关系和实体名、实体描述的前提下，利用attention机制通过关系定位实体描述中最相关的词，再以这些词和实体得到要链接的实体。\n\n### 1. Introduction\n\n知识图谱（KG）是一种信息网络，它用三元组 $(h,r,t)$ 来表示知识（h: head entity, t: tail entity, r: relation），目前比较出名的KG有 DBPedia，ConceptNet 等，目前的大多数KG都有噪音且不完整，比如基于Wikipedia的DBPedia有460万个实体，但其中一半实体拥有少于5个的关系。\n\n这说明了大部分的知识图谱仍然是非常不完善的，我们必须从一开始就要考虑系统的修改、补充完善的可能性。这项任务被定义为Knowledge Graph Completion (KGC)。\n\n#### Closed-World KGC\n\n给定一个不完整的KG $G=(E,R,T)$ 其中 $E,R,T$ 分别表示实体集，关系集以及三元组集，Closed-World KGC的任务就是通过找到一系列丢失的三元组 $ T' = \\{ \\langle h,r,t \\rangle|h \\in E, r \\in R, t \\in E, \\langle h,r,t \\rangle \\notin T \\}$ 来补充现有的 $G$.\n\n一个很重要的地方在于，Closed-World KGC 假定了新的实体、关系都被原有的 $G$ 包含，对于不在 $G$ 中的实体则一筹莫展。\n\n目前的Closed-World KGC方法很多往往使用TranE或者低维特征表示模型，前者的核心思想就是 $h+r=t$ ，后者则指 Embedding 等。\n\n该方法仅对固定的或者缓慢更新的KG有效，对于快速变更的KG则效果一般。\n\n#### Open-World KGC\n\n给定一个不完整的KG $G=(E,R,T)$ 其中 $E,R,T$ 分别表示实体集，关系集以及三元组集，Open-World KGC 的任务就是找到 $G$ 中没有的三元组集，$T' =\\{<h,r,t>|h\\in E^i,r\\in R, t\\in E^i,<h,r,t>\\notin T\\}$ 其中 $E^i$ 是G的实体超集。\n\nClosed-World方法就是根据知识图谱的拓扑结构更新一个随机的向量作为实体和关系的embedding，但对于不在网络中的实体，这个方法就失效了，这个时候就需要用别的特征来代替这个用网络拓扑结构得到的特征。\n\n一般直觉就是用实体的描述（entity description），根据实体的描述来得到特征，但从非结构化文本中学习向量表示比在网络的拓扑结构中要难得多，原因如下：\n\n1. 在Closed-world KGC模型中，每个实体都有一个embedding (从与它相连的实体上学得的)，但Open-World KGC模型则需要从实体描述的word embedding中得到entity embedding。而无论实体之间的联系情况是什么，word embedding的更新都会导致有相同词的entities的更新。\n2. 因为使用了非结构化文本，所以Open-World KGC模型可能会引入噪音或者冗余信息。\n\n### 2. Closed-World KGC \n\n在 Closed-World KGC 中，最为常用也最为基础的方法是一种给予强化学习(RL)的模型，被称为TransE \\[2\\]. 它有一个简单实用的假设：\n$$\nh+r = t\n$$\n其中h是head entity的向量，t是tail entity的向量，r是关系向量。\n\nTransE定义了loss function：\n$$\n\\mathcal{L(T)} = \\sum_{<h,r,t>\\in T} [\\gamma + E(\\langle h,r,t \\rangle) - E(\\langle h',r',t' \\rangle)]_+\n$$\n其中 $T$ 代表一个三元组的集合；$E(\\langle h,r,t \\rangle) = ||h+r-t||_{L_n}$是energy function；$\\langle h,r,t \\rangle$是G中的一个三元组；$h',\\langle r',t' \\rangle$代表一个不存在于 $T$ 的三元组，通过随机替换一部分$\\langle h,r,t \\rangle$来得到。\n\n这里还略去了很多TransE的变体等其他模型，但它们都是基于Closed-World KGC来做的。\n\n### 3. ConMask for Open-World KGC\n\n首先通过一个例子来说明：\n\n**任务：**填补三元组 $\\langle \\text{Ameen Sayani, residence, ?}\\rangle$，其中KG中并没有Ameen Sayani这个实体。\n\n**描述：**\"... **Ameen Sayani** was introduced to All India Radio, **Bombay**, by his brother Hamid Sayani. Ameen participated in English programmes there for ten years ...\" ，\n\n**目标预测实体：**Bombay (or Mumbai)\n\n为了找到Ameen Sayani的住址，在处理这个任务的过程中，我们不会从头看到尾，而是找到相关的关键词比如家庭或工作相关的词。这里，我们发现Ameen Sayani的工作地点All India Radio在Bombay，因此我们推测Ameen Sayani也住在Bombay（Bombay就是现在的Mumbai）。\n\n这个过程也可以被归纳为：\n\n1. 定位与该任务相关的信息。\n2. 根据上下文和相关文本推断。\n3. 根据相关文本推出正确目标实体。\n\n仿照这个过程，ConMask的工作方式被设计为：\n\n1. **Relationship-dependent content masking** -- 标记那些与任务相关的词语。\n2. **Target fusion** -- 从相关文本中抽取目标实体的embedding。\n3. **Target entity resolution** -- 通过计算KG中的候选目标实体，2中抽取出的实体embedding以及其它文本特征之间的相似度来选定目标实体。\n\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202310.jpg\" width=\"60%\">\n\nConMask模型总体结构如上，ConMask通过选择与给定关系相关的词来避免引入不相关的和有噪音的词。对于相关的文本，ConMask通过全连接卷积神经网络（FCN）来提取word-embedding。最后它将提取的embedding于KG中存在的实体进行比较，从而获得一系列目标实体。\n\n#### 3.1 Relationship-dependent content masking \n\nConMask根据给定的关系预处理输入文本，来选择一些相关的小片段，从而屏蔽掉无关文本。content-masking这一灵感来源于基于attention机制的RNN网络\\[3\\]，关于attention之前的笔记也有学习过。\n\n基于相似度得到选择最相关的词，具体公式如下：\n$$\n\\tau(\\phi(e), \\psi(r)) = W_{\\phi(e)} \\circ f_w(W_{\\phi(e)}, W_{\\psi(r)})\n$$\n其中 $e$ 是一个实体，$r$ 是某个关系, $\\phi$ 是description function并返回一个向量用于表示对一个实体或关系的描述，$\\psi$ 是name mapping function并返回一个向量用于表示一个实体或关系的名字， $ W_{\\phi{(e)}} \\in \\mathbb{R}^{|\\phi(r)|\\times k} $ 是一个描述矩阵每一行表示一个k维的描述中的word-embedding， $W_{\\phi{(e)}} \\in \\mathbb{R}^{|\\phi(r)|\\times k} $ 是一个名字矩阵每一行表示一个k维的实体名字word-embedding，$\\circ$ 是row-wise product，$f_w$ 用于计算的每一行的屏蔽比重。\n\n作者给了一个简单的$f_w$ ，Maximal Word-Relationship Weights(MWRW)，就是计算实体描述中每个词向量与关系名称的每个词向量的最大cos相似度:\n$$\nf_w^{MWRW}(W_{\\phi(e)}, W_{\\psi(r)})_{[i]} =  max_j(\\frac{\\sum_m^k{W_{\\phi(e)[i,m]} W_{\\psi(r)[j,m]}}}{\\sqrt{\\sum_m^k{W^2_{\\phi(e)[i,m]}}}\\sqrt{\\sum_m^k{W^2_{\\psi(e)[j,m]}}}})\n$$\n这个公式会给与给定关系无关的词更小的权重，与关系语义接近的词更大的权重，但权重最高的词一般不是目标实体，如下图所示，给定关系spouse，得到最大权重的是married，虽然married与spouse在语义上接近，但它并不是目标实体，因此作者称这种有着最大MWRW权重的词为指示词（indicator word），因为正确的词一般就在该词附近，在下图例子中可以发现目标实体barack obama就在married后面。\n\n为了给目标实体word正确的权重，作者改进了这个公式，具体公式如下，这个公式就是每个词的权重不会小于之前 $k_m$ 称为 Maximal Context-Relationship Weights (MCRW)：\n$$\nf_w^{MCRW}(W_{\\phi(e)}, W_{\\psi(r)})_{[i]} =  max(f_w^{MWRW}(W_{\\phi(e)}, W_{\\psi(r)})_{[i-k_m:i]})\n$$\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202313.jpg\">\n\n#### 3.2 Target Fusion\n\n这一步骤用于输出基于词的实体embedding，这个过程记为$\\xi$，使用Conetent Masking $\\tau$ 的输出。它使用全连接卷积网络，其结构如下：\n\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202312.jpg\">\n\n**Semantic Averaging**\n\n我们可以对所有实体进行embedding，但是这会产生大量的参数，使计算变得非常复杂。事实上，因为Target fusion函数用于抽取，所以对不需要抽取的实体名字使用target fusion就会显得很奇怪也很没有必要。\n\n这里作者提出了一个简单的语义平均法来计算这些实体的embedding：$\\eta(W) = \\frac{1}{k_l}\\sum_i^{k_i}W_i$\n\n#### 3.3 Loss function\n\n为了加速训练，我们参考 list-wise ranking loss function (Shi and Weninger 2017)，并设计 partial list-wise ranking loss function，拥有正负目标采样。正样本就是训练集的标注内容，记为$E^+$；负样本就是替换正样本的head entity或tail entity所得到的，记为$E^-$ 。\n$$\n\\mathcal{L}(h, r, t) =  \\begin{cases}\n\\sum_{h_+\\in E^+}{-\\frac{log(S(h_+,r,t,E^+\\cup E^-))}{|E^+|}}, & \\text{if }p_c > 0.5; \\\\\n\\sum_{h_+\\in E^+}{-\\frac{log(S(h,r,t_+,E^+\\cup E^-))}{|E^+|}}, & \\text{if }p_c \\le 0.5; .\n\\end{cases}\n$$\n$p_c$ 服从$[0,1]$的均匀分布，大于0.5时，把输入实体作为tail entity，小于0.5的时候就是作为head entity，表示替换head entity和tail entity的概率各为50%。另有$S$, 即 softmax normalized output of ConMask：\n$$\nS(h,r,t,E^+) = \\begin{cases}\n\\sum_{e \\in E^\\pm}^{exp(ConMask(h,r,t))}{exp(ConMask(e,r,t))} & \\text{if } p_c > 0.5 \\\\\n\\sum_{e \\in E^\\pm}^{exp(ConMask(e,r,t))}{exp(ConMask(h,r,t))} & \\text{if } p_c \\le 0.5 \\\\\n\\end{cases}\n$$\n\n### 4. Results\n\n从结果上看，对比其他模型，在开放领域，ConMask获得了最佳的效果；在Closed-World中，尽管ConMask不是为此设计的，但是对比TransE和TransR依然不逊色，结果相仿。\n\n目前而言，ConMask模型只能预测在实体描述中表达的关系，将来还应考虑扩展它，使其能够发现新的或隐含的关系。\n\n## Bibliographies\n\n笔记参考：https://zhuanlan.zhihu.com/p/33026043，http://blog.csdn.net/TgqDT3gGaMdkHasLZv/article/details/79224178\n\n代码实现：https://github.com/bxshi/ConMask\n\n\\[1\\] Shi, Baoxu, and Tim Weninger. \"Open-World Knowledge Graph Completion.\" *arXiv preprint arXiv:1711.03438* (2017).\n\n\\[2\\] Bordes, A., Usunier, N., Garcia-Duran, A., Weston, J., & Yakhnenko, O. (2013). Translating embeddings for modeling multi-relational data. In *Advances in neural information processing systems* (pp. 2787-2795).\n\n\\[3\\] Chorowski, J. K., Bahdanau, D., Serdyuk, D., Cho, K., & Bengio, Y. (2015). Attention-based models for speech recognition. In *Advances in neural information processing systems* (pp. 577-585).","slug":"[2018.2.26]Open-World-Knowledge-Graph-Completion","published":1,"updated":"2020-11-03T03:26:12.121Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckw4qufpp0027gwtl4fief2tn","content":"<h2 id=\"Open-World-Knowledge-Graph-Completion\"><a href=\"#Open-World-Knowledge-Graph-Completion\" class=\"headerlink\" title=\"Open-World Knowledge Graph Completion\"></a>Open-World Knowledge Graph Completion</h2><p><strong>摘要</strong>：[1]文首先讨论了Closed-World KGC，它无法处理从 KG 外部加入的新实体，并严重依赖已有KG连接的，不能对弱连接有好的预测。为此定义了 Open-World KGC，可以接收 新的实体并链接到 KG；并依此提出了ConMask模型，在给定关系和实体名、实体描述的前提下，利用attention机制通过关系定位实体描述中最相关的词，再以这些词和实体得到要链接的实体。</p>\n<h3 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1. Introduction\"></a>1. Introduction</h3><p>知识图谱（KG）是一种信息网络，它用三元组 $(h,r,t)$ 来表示知识（h: head entity, t: tail entity, r: relation），目前比较出名的KG有 DBPedia，ConceptNet 等，目前的大多数KG都有噪音且不完整，比如基于Wikipedia的DBPedia有460万个实体，但其中一半实体拥有少于5个的关系。</p>\n<p>这说明了大部分的知识图谱仍然是非常不完善的，我们必须从一开始就要考虑系统的修改、补充完善的可能性。这项任务被定义为Knowledge Graph Completion (KGC)。</p>\n<h4 id=\"Closed-World-KGC\"><a href=\"#Closed-World-KGC\" class=\"headerlink\" title=\"Closed-World KGC\"></a>Closed-World KGC</h4><p>给定一个不完整的KG $G=(E,R,T)$ 其中 $E,R,T$ 分别表示实体集，关系集以及三元组集，Closed-World KGC的任务就是通过找到一系列丢失的三元组 $ T’ = { \\langle h,r,t \\rangle|h \\in E, r \\in R, t \\in E, \\langle h,r,t \\rangle \\notin T }$ 来补充现有的 $G$.</p>\n<p>一个很重要的地方在于，Closed-World KGC 假定了新的实体、关系都被原有的 $G$ 包含，对于不在 $G$ 中的实体则一筹莫展。</p>\n<p>目前的Closed-World KGC方法很多往往使用TranE或者低维特征表示模型，前者的核心思想就是 $h+r=t$ ，后者则指 Embedding 等。</p>\n<p>该方法仅对固定的或者缓慢更新的KG有效，对于快速变更的KG则效果一般。</p>\n<h4 id=\"Open-World-KGC\"><a href=\"#Open-World-KGC\" class=\"headerlink\" title=\"Open-World KGC\"></a>Open-World KGC</h4><p>给定一个不完整的KG $G=(E,R,T)$ 其中 $E,R,T$ 分别表示实体集，关系集以及三元组集，Open-World KGC 的任务就是找到 $G$ 中没有的三元组集，$T’ ={&lt;h,r,t&gt;|h\\in E^i,r\\in R, t\\in E^i,&lt;h,r,t&gt;\\notin T}$ 其中 $E^i$ 是G的实体超集。</p>\n<p>Closed-World方法就是根据知识图谱的拓扑结构更新一个随机的向量作为实体和关系的embedding，但对于不在网络中的实体，这个方法就失效了，这个时候就需要用别的特征来代替这个用网络拓扑结构得到的特征。</p>\n<p>一般直觉就是用实体的描述（entity description），根据实体的描述来得到特征，但从非结构化文本中学习向量表示比在网络的拓扑结构中要难得多，原因如下：</p>\n<ol>\n<li>在Closed-world KGC模型中，每个实体都有一个embedding (从与它相连的实体上学得的)，但Open-World KGC模型则需要从实体描述的word embedding中得到entity embedding。而无论实体之间的联系情况是什么，word embedding的更新都会导致有相同词的entities的更新。</li>\n<li>因为使用了非结构化文本，所以Open-World KGC模型可能会引入噪音或者冗余信息。</li>\n</ol>\n<h3 id=\"2-Closed-World-KGC\"><a href=\"#2-Closed-World-KGC\" class=\"headerlink\" title=\"2. Closed-World KGC\"></a>2. Closed-World KGC</h3><p>在 Closed-World KGC 中，最为常用也最为基础的方法是一种给予强化学习(RL)的模型，被称为TransE [2]. 它有一个简单实用的假设：<br>$$<br>h+r = t<br>$$<br>其中h是head entity的向量，t是tail entity的向量，r是关系向量。</p>\n<p>TransE定义了loss function：<br>$$<br>\\mathcal{L(T)} = \\sum_{&lt;h,r,t&gt;\\in T} [\\gamma + E(\\langle h,r,t \\rangle) - E(\\langle h’,r’,t’ \\rangle)]<em>+<br>$$<br>其中 $T$ 代表一个三元组的集合；$E(\\langle h,r,t \\rangle) = ||h+r-t||</em>{L_n}$是energy function；$\\langle h,r,t \\rangle$是G中的一个三元组；$h’,\\langle r’,t’ \\rangle$代表一个不存在于 $T$ 的三元组，通过随机替换一部分$\\langle h,r,t \\rangle$来得到。</p>\n<p>这里还略去了很多TransE的变体等其他模型，但它们都是基于Closed-World KGC来做的。</p>\n<h3 id=\"3-ConMask-for-Open-World-KGC\"><a href=\"#3-ConMask-for-Open-World-KGC\" class=\"headerlink\" title=\"3. ConMask for Open-World KGC\"></a>3. ConMask for Open-World KGC</h3><p>首先通过一个例子来说明：</p>\n<p><strong>任务：</strong>填补三元组 $\\langle \\text{Ameen Sayani, residence, ?}\\rangle$，其中KG中并没有Ameen Sayani这个实体。</p>\n<p><strong>描述：</strong>“… <strong>Ameen Sayani</strong> was introduced to All India Radio, <strong>Bombay</strong>, by his brother Hamid Sayani. Ameen participated in English programmes there for ten years …” ，</p>\n<p><strong>目标预测实体：</strong>Bombay (or Mumbai)</p>\n<p>为了找到Ameen Sayani的住址，在处理这个任务的过程中，我们不会从头看到尾，而是找到相关的关键词比如家庭或工作相关的词。这里，我们发现Ameen Sayani的工作地点All India Radio在Bombay，因此我们推测Ameen Sayani也住在Bombay（Bombay就是现在的Mumbai）。</p>\n<p>这个过程也可以被归纳为：</p>\n<ol>\n<li>定位与该任务相关的信息。</li>\n<li>根据上下文和相关文本推断。</li>\n<li>根据相关文本推出正确目标实体。</li>\n</ol>\n<p>仿照这个过程，ConMask的工作方式被设计为：</p>\n<ol>\n<li><strong>Relationship-dependent content masking</strong> – 标记那些与任务相关的词语。</li>\n<li><strong>Target fusion</strong> – 从相关文本中抽取目标实体的embedding。</li>\n<li><strong>Target entity resolution</strong> – 通过计算KG中的候选目标实体，2中抽取出的实体embedding以及其它文本特征之间的相似度来选定目标实体。</li>\n</ol>\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202310.jpg\" width=\"60%\">\n\n<p>ConMask模型总体结构如上，ConMask通过选择与给定关系相关的词来避免引入不相关的和有噪音的词。对于相关的文本，ConMask通过全连接卷积神经网络（FCN）来提取word-embedding。最后它将提取的embedding于KG中存在的实体进行比较，从而获得一系列目标实体。</p>\n<h4 id=\"3-1-Relationship-dependent-content-masking\"><a href=\"#3-1-Relationship-dependent-content-masking\" class=\"headerlink\" title=\"3.1 Relationship-dependent content masking\"></a>3.1 Relationship-dependent content masking</h4><p>ConMask根据给定的关系预处理输入文本，来选择一些相关的小片段，从而屏蔽掉无关文本。content-masking这一灵感来源于基于attention机制的RNN网络[3]，关于attention之前的笔记也有学习过。</p>\n<p>基于相似度得到选择最相关的词，具体公式如下：<br>$$<br>\\tau(\\phi(e), \\psi(r)) = W_{\\phi(e)} \\circ f_w(W_{\\phi(e)}, W_{\\psi(r)})<br>$$<br>其中 $e$ 是一个实体，$r$ 是某个关系, $\\phi$ 是description function并返回一个向量用于表示对一个实体或关系的描述，$\\psi$ 是name mapping function并返回一个向量用于表示一个实体或关系的名字， $ W_{\\phi{(e)}} \\in \\mathbb{R}^{|\\phi(r)|\\times k} $ 是一个描述矩阵每一行表示一个k维的描述中的word-embedding， $W_{\\phi{(e)}} \\in \\mathbb{R}^{|\\phi(r)|\\times k} $ 是一个名字矩阵每一行表示一个k维的实体名字word-embedding，$\\circ$ 是row-wise product，$f_w$ 用于计算的每一行的屏蔽比重。</p>\n<p>作者给了一个简单的$f_w$ ，Maximal Word-Relationship Weights(MWRW)，就是计算实体描述中每个词向量与关系名称的每个词向量的最大cos相似度:<br>$$<br>f_w^{MWRW}(W_{\\phi(e)}, W_{\\psi(r)})<em>{[i]} =  max_j(\\frac{\\sum_m^k{W</em>{\\phi(e)[i,m]} W_{\\psi(r)[j,m]}}}{\\sqrt{\\sum_m^k{W^2_{\\phi(e)[i,m]}}}\\sqrt{\\sum_m^k{W^2_{\\psi(e)[j,m]}}}})<br>$$<br>这个公式会给与给定关系无关的词更小的权重，与关系语义接近的词更大的权重，但权重最高的词一般不是目标实体，如下图所示，给定关系spouse，得到最大权重的是married，虽然married与spouse在语义上接近，但它并不是目标实体，因此作者称这种有着最大MWRW权重的词为指示词（indicator word），因为正确的词一般就在该词附近，在下图例子中可以发现目标实体barack obama就在married后面。</p>\n<p>为了给目标实体word正确的权重，作者改进了这个公式，具体公式如下，这个公式就是每个词的权重不会小于之前 $k_m$ 称为 Maximal Context-Relationship Weights (MCRW)：<br>$$<br>f_w^{MCRW}(W_{\\phi(e)}, W_{\\psi(r)})<em>{[i]} =  max(f_w^{MWRW}(W</em>{\\phi(e)}, W_{\\psi(r)})_{[i-k_m:i]})<br>$$<br><img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202313.jpg\"></p>\n<h4 id=\"3-2-Target-Fusion\"><a href=\"#3-2-Target-Fusion\" class=\"headerlink\" title=\"3.2 Target Fusion\"></a>3.2 Target Fusion</h4><p>这一步骤用于输出基于词的实体embedding，这个过程记为$\\xi$，使用Conetent Masking $\\tau$ 的输出。它使用全连接卷积网络，其结构如下：</p>\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202312.jpg\">\n\n<p><strong>Semantic Averaging</strong></p>\n<p>我们可以对所有实体进行embedding，但是这会产生大量的参数，使计算变得非常复杂。事实上，因为Target fusion函数用于抽取，所以对不需要抽取的实体名字使用target fusion就会显得很奇怪也很没有必要。</p>\n<p>这里作者提出了一个简单的语义平均法来计算这些实体的embedding：$\\eta(W) = \\frac{1}{k_l}\\sum_i^{k_i}W_i$</p>\n<h4 id=\"3-3-Loss-function\"><a href=\"#3-3-Loss-function\" class=\"headerlink\" title=\"3.3 Loss function\"></a>3.3 Loss function</h4><p>为了加速训练，我们参考 list-wise ranking loss function (Shi and Weninger 2017)，并设计 partial list-wise ranking loss function，拥有正负目标采样。正样本就是训练集的标注内容，记为$E^+$；负样本就是替换正样本的head entity或tail entity所得到的，记为$E^-$ 。<br>$$<br>\\mathcal{L}(h, r, t) =  \\begin{cases}<br>\\sum_{h_+\\in E^+}{-\\frac{log(S(h_+,r,t,E^+\\cup E^-))}{|E^+|}}, &amp; \\text{if }p_c &gt; 0.5; \\<br>\\sum_{h_+\\in E^+}{-\\frac{log(S(h,r,t_+,E^+\\cup E^-))}{|E^+|}}, &amp; \\text{if }p_c \\le 0.5; .<br>\\end{cases}<br>$$<br>$p_c$ 服从$[0,1]$的均匀分布，大于0.5时，把输入实体作为tail entity，小于0.5的时候就是作为head entity，表示替换head entity和tail entity的概率各为50%。另有$S$, 即 softmax normalized output of ConMask：<br>$$<br>S(h,r,t,E^+) = \\begin{cases}<br>\\sum_{e \\in E^\\pm}^{exp(ConMask(h,r,t))}{exp(ConMask(e,r,t))} &amp; \\text{if } p_c &gt; 0.5 \\<br>\\sum_{e \\in E^\\pm}^{exp(ConMask(e,r,t))}{exp(ConMask(h,r,t))} &amp; \\text{if } p_c \\le 0.5 \\<br>\\end{cases}<br>$$</p>\n<h3 id=\"4-Results\"><a href=\"#4-Results\" class=\"headerlink\" title=\"4. Results\"></a>4. Results</h3><p>从结果上看，对比其他模型，在开放领域，ConMask获得了最佳的效果；在Closed-World中，尽管ConMask不是为此设计的，但是对比TransE和TransR依然不逊色，结果相仿。</p>\n<p>目前而言，ConMask模型只能预测在实体描述中表达的关系，将来还应考虑扩展它，使其能够发现新的或隐含的关系。</p>\n<h2 id=\"Bibliographies\"><a href=\"#Bibliographies\" class=\"headerlink\" title=\"Bibliographies\"></a>Bibliographies</h2><p>笔记参考：<a href=\"https://zhuanlan.zhihu.com/p/33026043%EF%BC%8Chttp://blog.csdn.net/TgqDT3gGaMdkHasLZv/article/details/79224178\">https://zhuanlan.zhihu.com/p/33026043，http://blog.csdn.net/TgqDT3gGaMdkHasLZv/article/details/79224178</a></p>\n<p>代码实现：<a href=\"https://github.com/bxshi/ConMask\">https://github.com/bxshi/ConMask</a></p>\n<p>[1] Shi, Baoxu, and Tim Weninger. “Open-World Knowledge Graph Completion.” <em>arXiv preprint arXiv:1711.03438</em> (2017).</p>\n<p>[2] Bordes, A., Usunier, N., Garcia-Duran, A., Weston, J., &amp; Yakhnenko, O. (2013). Translating embeddings for modeling multi-relational data. In <em>Advances in neural information processing systems</em> (pp. 2787-2795).</p>\n<p>[3] Chorowski, J. K., Bahdanau, D., Serdyuk, D., Cho, K., &amp; Bengio, Y. (2015). Attention-based models for speech recognition. In <em>Advances in neural information processing systems</em> (pp. 577-585).</p>\n","site":{"data":{"recommended_posts":{"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"}],"https://LorrinWWW.github.io/posts/complexity/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/hexo-with-latex/":[{"title":"区块链时代的最正确的打赏姿势——在Hexo博客配置Coinhive网页挖矿","permalink":"http://www.davidfnck.com/blockchain/mine-xmr-in-hexo-by-coinhive.html/"}],"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/":[{"title":"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-statistic/":[{"title":"贝叶斯估计 Bayes estimation","permalink":"https://LorrinWWW.github.io/posts/Bayes-estimation/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"}],"https://LorrinWWW.github.io/posts/[2018.2.5]Nested-LSTMs/":[{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"}],"https://LorrinWWW.github.io/posts/datamining-pretreatment/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Several models for knowledge graph representing and completing 几个知识图谱模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/"}],"https://LorrinWWW.github.io/posts/compression/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/Sobolev-space/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/proba-ch5/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch3/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/proba-ch1/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/ML-CNN/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/EDP-basic-models/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/machine-learning/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/datamining-class-pred/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction/":[],"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/":[{"title":"OS notes","permalink":"https://LorrinWWW.github.io/posts/OS-notes/"}],"https://LorrinWWW.github.io/posts/projet-enjeu-plugin-chrome-101/":[{"title":"Chrome油猴脚本","permalink":"http://gubangzhong.cn/2017/06/13/Chrome油猴脚本/"}],"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/":[{"title":"MongoDB, Docker and Python","permalink":"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/":[{"title":"Hands on Scrapy","permalink":"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/OS-notes/":[{"title":"learning OS and building LorriOS","permalink":"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/"}],"https://LorrinWWW.github.io/posts/Bayes-estimation/":[{"title":"Note of statistic","permalink":"https://LorrinWWW.github.io/posts/Note-of-statistic/"}],"https://LorrinWWW.github.io/posts/management-of-the-firm/":[],"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"}],"https://LorrinWWW.github.io/posts/datamining-qualitative-induction/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-finite-element-method/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/QuadTree/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/proba-ch4/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch2/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/":[{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/Note-of-NLP/":[{"title":"Generative Adversarial Network","permalink":"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/"},{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"}],"https://LorrinWWW.github.io/posts/Note-of-datamining/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"数据挖掘-分类与预测","permalink":"https://LorrinWWW.github.io/posts/datamining-class-pred/"},{"title":"数据挖掘-预处理","permalink":"https://LorrinWWW.github.io/posts/datamining-pretreatment/"}],"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/Note-of-probability/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/participe-present-et-gerondif/":[],"https://LorrinWWW.github.io/posts/Hilbert-space/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/proba-ch6/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}]}}},"excerpt":"","more":"<h2 id=\"Open-World-Knowledge-Graph-Completion\"><a href=\"#Open-World-Knowledge-Graph-Completion\" class=\"headerlink\" title=\"Open-World Knowledge Graph Completion\"></a>Open-World Knowledge Graph Completion</h2><p><strong>摘要</strong>：[1]文首先讨论了Closed-World KGC，它无法处理从 KG 外部加入的新实体，并严重依赖已有KG连接的，不能对弱连接有好的预测。为此定义了 Open-World KGC，可以接收 新的实体并链接到 KG；并依此提出了ConMask模型，在给定关系和实体名、实体描述的前提下，利用attention机制通过关系定位实体描述中最相关的词，再以这些词和实体得到要链接的实体。</p>\n<h3 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1. Introduction\"></a>1. Introduction</h3><p>知识图谱（KG）是一种信息网络，它用三元组 $(h,r,t)$ 来表示知识（h: head entity, t: tail entity, r: relation），目前比较出名的KG有 DBPedia，ConceptNet 等，目前的大多数KG都有噪音且不完整，比如基于Wikipedia的DBPedia有460万个实体，但其中一半实体拥有少于5个的关系。</p>\n<p>这说明了大部分的知识图谱仍然是非常不完善的，我们必须从一开始就要考虑系统的修改、补充完善的可能性。这项任务被定义为Knowledge Graph Completion (KGC)。</p>\n<h4 id=\"Closed-World-KGC\"><a href=\"#Closed-World-KGC\" class=\"headerlink\" title=\"Closed-World KGC\"></a>Closed-World KGC</h4><p>给定一个不完整的KG $G=(E,R,T)$ 其中 $E,R,T$ 分别表示实体集，关系集以及三元组集，Closed-World KGC的任务就是通过找到一系列丢失的三元组 $ T’ = { \\langle h,r,t \\rangle|h \\in E, r \\in R, t \\in E, \\langle h,r,t \\rangle \\notin T }$ 来补充现有的 $G$.</p>\n<p>一个很重要的地方在于，Closed-World KGC 假定了新的实体、关系都被原有的 $G$ 包含，对于不在 $G$ 中的实体则一筹莫展。</p>\n<p>目前的Closed-World KGC方法很多往往使用TranE或者低维特征表示模型，前者的核心思想就是 $h+r=t$ ，后者则指 Embedding 等。</p>\n<p>该方法仅对固定的或者缓慢更新的KG有效，对于快速变更的KG则效果一般。</p>\n<h4 id=\"Open-World-KGC\"><a href=\"#Open-World-KGC\" class=\"headerlink\" title=\"Open-World KGC\"></a>Open-World KGC</h4><p>给定一个不完整的KG $G=(E,R,T)$ 其中 $E,R,T$ 分别表示实体集，关系集以及三元组集，Open-World KGC 的任务就是找到 $G$ 中没有的三元组集，$T’ ={&lt;h,r,t&gt;|h\\in E^i,r\\in R, t\\in E^i,&lt;h,r,t&gt;\\notin T}$ 其中 $E^i$ 是G的实体超集。</p>\n<p>Closed-World方法就是根据知识图谱的拓扑结构更新一个随机的向量作为实体和关系的embedding，但对于不在网络中的实体，这个方法就失效了，这个时候就需要用别的特征来代替这个用网络拓扑结构得到的特征。</p>\n<p>一般直觉就是用实体的描述（entity description），根据实体的描述来得到特征，但从非结构化文本中学习向量表示比在网络的拓扑结构中要难得多，原因如下：</p>\n<ol>\n<li>在Closed-world KGC模型中，每个实体都有一个embedding (从与它相连的实体上学得的)，但Open-World KGC模型则需要从实体描述的word embedding中得到entity embedding。而无论实体之间的联系情况是什么，word embedding的更新都会导致有相同词的entities的更新。</li>\n<li>因为使用了非结构化文本，所以Open-World KGC模型可能会引入噪音或者冗余信息。</li>\n</ol>\n<h3 id=\"2-Closed-World-KGC\"><a href=\"#2-Closed-World-KGC\" class=\"headerlink\" title=\"2. Closed-World KGC\"></a>2. Closed-World KGC</h3><p>在 Closed-World KGC 中，最为常用也最为基础的方法是一种给予强化学习(RL)的模型，被称为TransE [2]. 它有一个简单实用的假设：<br>$$<br>h+r = t<br>$$<br>其中h是head entity的向量，t是tail entity的向量，r是关系向量。</p>\n<p>TransE定义了loss function：<br>$$<br>\\mathcal{L(T)} = \\sum_{&lt;h,r,t&gt;\\in T} [\\gamma + E(\\langle h,r,t \\rangle) - E(\\langle h’,r’,t’ \\rangle)]<em>+<br>$$<br>其中 $T$ 代表一个三元组的集合；$E(\\langle h,r,t \\rangle) = ||h+r-t||</em>{L_n}$是energy function；$\\langle h,r,t \\rangle$是G中的一个三元组；$h’,\\langle r’,t’ \\rangle$代表一个不存在于 $T$ 的三元组，通过随机替换一部分$\\langle h,r,t \\rangle$来得到。</p>\n<p>这里还略去了很多TransE的变体等其他模型，但它们都是基于Closed-World KGC来做的。</p>\n<h3 id=\"3-ConMask-for-Open-World-KGC\"><a href=\"#3-ConMask-for-Open-World-KGC\" class=\"headerlink\" title=\"3. ConMask for Open-World KGC\"></a>3. ConMask for Open-World KGC</h3><p>首先通过一个例子来说明：</p>\n<p><strong>任务：</strong>填补三元组 $\\langle \\text{Ameen Sayani, residence, ?}\\rangle$，其中KG中并没有Ameen Sayani这个实体。</p>\n<p><strong>描述：</strong>“… <strong>Ameen Sayani</strong> was introduced to All India Radio, <strong>Bombay</strong>, by his brother Hamid Sayani. Ameen participated in English programmes there for ten years …” ，</p>\n<p><strong>目标预测实体：</strong>Bombay (or Mumbai)</p>\n<p>为了找到Ameen Sayani的住址，在处理这个任务的过程中，我们不会从头看到尾，而是找到相关的关键词比如家庭或工作相关的词。这里，我们发现Ameen Sayani的工作地点All India Radio在Bombay，因此我们推测Ameen Sayani也住在Bombay（Bombay就是现在的Mumbai）。</p>\n<p>这个过程也可以被归纳为：</p>\n<ol>\n<li>定位与该任务相关的信息。</li>\n<li>根据上下文和相关文本推断。</li>\n<li>根据相关文本推出正确目标实体。</li>\n</ol>\n<p>仿照这个过程，ConMask的工作方式被设计为：</p>\n<ol>\n<li><strong>Relationship-dependent content masking</strong> – 标记那些与任务相关的词语。</li>\n<li><strong>Target fusion</strong> – 从相关文本中抽取目标实体的embedding。</li>\n<li><strong>Target entity resolution</strong> – 通过计算KG中的候选目标实体，2中抽取出的实体embedding以及其它文本特征之间的相似度来选定目标实体。</li>\n</ol>\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202310.jpg\" width=\"60%\">\n\n<p>ConMask模型总体结构如上，ConMask通过选择与给定关系相关的词来避免引入不相关的和有噪音的词。对于相关的文本，ConMask通过全连接卷积神经网络（FCN）来提取word-embedding。最后它将提取的embedding于KG中存在的实体进行比较，从而获得一系列目标实体。</p>\n<h4 id=\"3-1-Relationship-dependent-content-masking\"><a href=\"#3-1-Relationship-dependent-content-masking\" class=\"headerlink\" title=\"3.1 Relationship-dependent content masking\"></a>3.1 Relationship-dependent content masking</h4><p>ConMask根据给定的关系预处理输入文本，来选择一些相关的小片段，从而屏蔽掉无关文本。content-masking这一灵感来源于基于attention机制的RNN网络[3]，关于attention之前的笔记也有学习过。</p>\n<p>基于相似度得到选择最相关的词，具体公式如下：<br>$$<br>\\tau(\\phi(e), \\psi(r)) = W_{\\phi(e)} \\circ f_w(W_{\\phi(e)}, W_{\\psi(r)})<br>$$<br>其中 $e$ 是一个实体，$r$ 是某个关系, $\\phi$ 是description function并返回一个向量用于表示对一个实体或关系的描述，$\\psi$ 是name mapping function并返回一个向量用于表示一个实体或关系的名字， $ W_{\\phi{(e)}} \\in \\mathbb{R}^{|\\phi(r)|\\times k} $ 是一个描述矩阵每一行表示一个k维的描述中的word-embedding， $W_{\\phi{(e)}} \\in \\mathbb{R}^{|\\phi(r)|\\times k} $ 是一个名字矩阵每一行表示一个k维的实体名字word-embedding，$\\circ$ 是row-wise product，$f_w$ 用于计算的每一行的屏蔽比重。</p>\n<p>作者给了一个简单的$f_w$ ，Maximal Word-Relationship Weights(MWRW)，就是计算实体描述中每个词向量与关系名称的每个词向量的最大cos相似度:<br>$$<br>f_w^{MWRW}(W_{\\phi(e)}, W_{\\psi(r)})<em>{[i]} =  max_j(\\frac{\\sum_m^k{W</em>{\\phi(e)[i,m]} W_{\\psi(r)[j,m]}}}{\\sqrt{\\sum_m^k{W^2_{\\phi(e)[i,m]}}}\\sqrt{\\sum_m^k{W^2_{\\psi(e)[j,m]}}}})<br>$$<br>这个公式会给与给定关系无关的词更小的权重，与关系语义接近的词更大的权重，但权重最高的词一般不是目标实体，如下图所示，给定关系spouse，得到最大权重的是married，虽然married与spouse在语义上接近，但它并不是目标实体，因此作者称这种有着最大MWRW权重的词为指示词（indicator word），因为正确的词一般就在该词附近，在下图例子中可以发现目标实体barack obama就在married后面。</p>\n<p>为了给目标实体word正确的权重，作者改进了这个公式，具体公式如下，这个公式就是每个词的权重不会小于之前 $k_m$ 称为 Maximal Context-Relationship Weights (MCRW)：<br>$$<br>f_w^{MCRW}(W_{\\phi(e)}, W_{\\psi(r)})<em>{[i]} =  max(f_w^{MWRW}(W</em>{\\phi(e)}, W_{\\psi(r)})_{[i-k_m:i]})<br>$$<br><img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202313.jpg\"></p>\n<h4 id=\"3-2-Target-Fusion\"><a href=\"#3-2-Target-Fusion\" class=\"headerlink\" title=\"3.2 Target Fusion\"></a>3.2 Target Fusion</h4><p>这一步骤用于输出基于词的实体embedding，这个过程记为$\\xi$，使用Conetent Masking $\\tau$ 的输出。它使用全连接卷积网络，其结构如下：</p>\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202312.jpg\">\n\n<p><strong>Semantic Averaging</strong></p>\n<p>我们可以对所有实体进行embedding，但是这会产生大量的参数，使计算变得非常复杂。事实上，因为Target fusion函数用于抽取，所以对不需要抽取的实体名字使用target fusion就会显得很奇怪也很没有必要。</p>\n<p>这里作者提出了一个简单的语义平均法来计算这些实体的embedding：$\\eta(W) = \\frac{1}{k_l}\\sum_i^{k_i}W_i$</p>\n<h4 id=\"3-3-Loss-function\"><a href=\"#3-3-Loss-function\" class=\"headerlink\" title=\"3.3 Loss function\"></a>3.3 Loss function</h4><p>为了加速训练，我们参考 list-wise ranking loss function (Shi and Weninger 2017)，并设计 partial list-wise ranking loss function，拥有正负目标采样。正样本就是训练集的标注内容，记为$E^+$；负样本就是替换正样本的head entity或tail entity所得到的，记为$E^-$ 。<br>$$<br>\\mathcal{L}(h, r, t) =  \\begin{cases}<br>\\sum_{h_+\\in E^+}{-\\frac{log(S(h_+,r,t,E^+\\cup E^-))}{|E^+|}}, &amp; \\text{if }p_c &gt; 0.5; \\<br>\\sum_{h_+\\in E^+}{-\\frac{log(S(h,r,t_+,E^+\\cup E^-))}{|E^+|}}, &amp; \\text{if }p_c \\le 0.5; .<br>\\end{cases}<br>$$<br>$p_c$ 服从$[0,1]$的均匀分布，大于0.5时，把输入实体作为tail entity，小于0.5的时候就是作为head entity，表示替换head entity和tail entity的概率各为50%。另有$S$, 即 softmax normalized output of ConMask：<br>$$<br>S(h,r,t,E^+) = \\begin{cases}<br>\\sum_{e \\in E^\\pm}^{exp(ConMask(h,r,t))}{exp(ConMask(e,r,t))} &amp; \\text{if } p_c &gt; 0.5 \\<br>\\sum_{e \\in E^\\pm}^{exp(ConMask(e,r,t))}{exp(ConMask(h,r,t))} &amp; \\text{if } p_c \\le 0.5 \\<br>\\end{cases}<br>$$</p>\n<h3 id=\"4-Results\"><a href=\"#4-Results\" class=\"headerlink\" title=\"4. Results\"></a>4. Results</h3><p>从结果上看，对比其他模型，在开放领域，ConMask获得了最佳的效果；在Closed-World中，尽管ConMask不是为此设计的，但是对比TransE和TransR依然不逊色，结果相仿。</p>\n<p>目前而言，ConMask模型只能预测在实体描述中表达的关系，将来还应考虑扩展它，使其能够发现新的或隐含的关系。</p>\n<h2 id=\"Bibliographies\"><a href=\"#Bibliographies\" class=\"headerlink\" title=\"Bibliographies\"></a>Bibliographies</h2><p>笔记参考：<a href=\"https://zhuanlan.zhihu.com/p/33026043%EF%BC%8Chttp://blog.csdn.net/TgqDT3gGaMdkHasLZv/article/details/79224178\">https://zhuanlan.zhihu.com/p/33026043，http://blog.csdn.net/TgqDT3gGaMdkHasLZv/article/details/79224178</a></p>\n<p>代码实现：<a href=\"https://github.com/bxshi/ConMask\">https://github.com/bxshi/ConMask</a></p>\n<p>[1] Shi, Baoxu, and Tim Weninger. “Open-World Knowledge Graph Completion.” <em>arXiv preprint arXiv:1711.03438</em> (2017).</p>\n<p>[2] Bordes, A., Usunier, N., Garcia-Duran, A., Weston, J., &amp; Yakhnenko, O. (2013). Translating embeddings for modeling multi-relational data. In <em>Advances in neural information processing systems</em> (pp. 2787-2795).</p>\n<p>[3] Chorowski, J. K., Bahdanau, D., Serdyuk, D., Cho, K., &amp; Bengio, Y. (2015). Attention-based models for speech recognition. In <em>Advances in neural information processing systems</em> (pp. 577-585).</p>\n"},{"title":"Nested LSTMs 笔记","date":"2018-02-05T00:00:00.000Z","_content":"\n## Nested LSTMs\n\n**摘要**：最近，一种新的 Nested LSTMs 网络被提出。在通常的LSTM网络中，我们通过将LSTM单元堆叠，从而形成深度RNN网络，提高其效果；Nested LSTM则通过嵌套而不是堆栈来增添LSTM的深度。在NLSTM中，记忆单元的值是由LSTM单元计算的，其中，LSTM单元具有自身内在的记忆单元。具体而言，NLSTM记忆单元并不是按照等式：$c_t^{outer} = f_t \\odot c_{t-1} + i_t \\odot g_t$ 对（外部）记忆单元的值进行计算，而是使用级联：$(f_t \\odot c_{t-1}, i_t \\odot g_t)$ 将其作为内部LSTM（或NLSTM）记忆单元的输入，并设定 $c_t^{outer} = h_t^{inner}$。在访问内部记忆时，Nested LSTM 相比传统的堆栈 LSTM 有更高的自由度，从而能处理更长时间规模的内部记忆；实验也表明，在参数数量相似的情况下，NLSTM 在多种任务上都超越了堆栈 LSTM。作者认为Nested LSTM 有潜力直接取代堆栈 LSTM。\n\n### 1. Introduction\n\n学习长期的依赖关系是当前人工智能领域中，尤其是在nlp领域，机器学习方法的关键性挑战。基于循环神经网络的体系结构已经在使得机器能够模仿这种能力方面取得了显著进展。\n\n#### single-layer LSTM\n\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202251.jpg\" width=\"90%\">\n\nRNN的输入是以当前的状态为依据，适合学习时间上的抽象特征。在实践中，许多专家已经证明，更为复杂的体系结构是解决许多任务的关键。其中一个原因是梯度消失问题（Hochreiter于1991年、Bengio等人于1994年提出），它使得简单的RNN难以学习长期依赖关系。Hochreiter和Schmidhuber于1997年提出了LSTM，包含能够改善梯度消失问题的记忆机制。单层LSTM如上图，图中的三个单元实际上是同一个单元，它循环地将内部的参数传递给自己。\n\n#### Stacked LSTMs\n\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202254.jpg\" width=\"60%\">\n\n堆栈 LSTM 架构使用一系列 LSTM 一层层地堆叠在一起来处理数据，一层的输出成为下一层的输入。上图为一个两层的LSTM网络。\n\n引入多层的结构，即将多个LSTM单元堆叠，每一层的输出成为下一层的输入。 每层处理我们希望解决的任务的一部分，并将其传递给下一层。额外的隐藏层可以添加到多层感知器神经网络，使其有更深入的“理解”。 额外的隐藏层被认为重新组合了来自先前层的学习表示，并在高度抽象层次上找到新的表示。 例如，从线条到形状到对象。\n\n#### Nested LSTMs\n\n在 NLSTM 中，LSTM 的记忆单元可以访问内部记忆。相比于传统的堆栈 LSTM，这一关键特征使得该模型能实现更有效的时间层级。在 NLSTM 中，外部记忆单元可自由选择读取、编写的相关长期信息到内部单元。相比之下，在Stacked LSTM 中，高层级的激活（类似内部记忆）直接生成输出，因此必须包含所有的与当前预测相关的短期信息。换言之，Stacked LSTM 与Nested LSTM 之间的主要不同在于，NLSTM 可以选择性地访问内部记忆。这使得，即使这些事件与当前事件不相关，内部记忆也能够记住、处理更长时间规模上的事件。我们在后面一章更详细地介绍它。\n\n### 2. Model of Nested LSTMs\n\nLSTM 中的输出门会编码可能与当前的时间步骤不相关，但是仍然值得记忆的信息。Nested LSTM 根据这一直观理解来创造一种记忆的时间层级。以同样的方式被gate控访问内部记忆，因此长期信息只有在情景相关的条件下才能选择性地访问。\n\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202253.jpg\" width=\"80%\">\n\n#### The architecture\n\n在 LSTM 网络中，单元状态的更新公式和门控机制可以表示为以下方程式：\n$$\ni_t = \\sigma_i (x_t W_{xi} + h_{t-1} W_{hi} + b_i) \\\\\nf_t = \\sigma_t (x_t W_{xf} + h_{t-1} W_{hf} + b_i) \\\\\nc_t = f_t \\odot c_{c-1} + \\sigma_c (x_t W_{xc} + h_{t-1} W_{hc} + b_c) \\\\\no_t = \\sigma_o (x_t W_{xo} + h_{t-1} W_{ho} + b_o) \\\\\nh_t = o_t \\odot \\sigma_h(c_t)\n$$\nNested LSTM 使用已学习的状态函数 $c_t = m_t(f_t\\odot c_{t−1}, i_t \\odot g_t)​$ 来替代 LSTM 中计算 $c_t​$ 的加运算。我们将函数的状态表示为 m 在时间 t 的内部记忆（inner memory），调用该函数以计算 $c_t​$ 和 $m_{t+1}​$。我们可以使用另一个 LSTM 单元来实现该记忆函数，就生成了 Nested LSTM。同样，该记忆函数能够由另一个 Nested LSTM 单元替换，因此就能构建任意深的嵌套网络。\n\n因此，我们得到NLSTM 中记忆函数的输入和隐藏状态：\n$$\n\\tilde{h}_{t-1} = f_t \\odot c_{t-1} \\\\\n\\tilde{x}_t = i_t \\odot \\sigma_c (x_t W_{xc} + h_{t-1} W_{hc} + b_c)\n$$\n注意如果记忆函数是加性的，那么$c_t = f_t \\odot c_{c-1} + \\sigma_c (x_t W_{xc} + h_{t-1} W_{hc} + b_c) =  \\tilde{h}_{t-1} + \\tilde{x}_t $，整个系统将退化到经典的 LSTM。\n\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202250.jpg\">\n\n*LSTM、Stacked LSTM 和 Nested LSTM 的计算图形。隐藏的状态、外部记忆单元和内部记忆单元分别由h、c和d进行表示。虽然当前的隐藏状态可以直接影响下一个内部记忆单元的内容，但内部记忆只有通过外部记忆才能够影响隐藏状态。*\n$$\n\\widetilde{i}_t = \\widetilde{\\sigma}_i (\\widetilde{x}_t \\widetilde{W}_{xi} + \\widetilde{h}_{t-1} \\widetilde{W}_{hi} + \\widetilde{b}_i) \\\\\n\\widetilde{f}_t = \\widetilde{\\sigma}_t (\\widetilde{x}_t \\widetilde{W}_{xf} + \\widetilde{h}_{t-1} \\widetilde{W}_{hf} + \\widetilde{b}_i) \\\\\n\\widetilde{c}_t = \\widetilde{f}_t \\odot \\widetilde{c}_{c-1} + \\widetilde{\\sigma}_c (\\widetilde{x}_t \\widetilde{W}_{xc} + \\widetilde{h}_{t-1} \\widetilde{W}_{hc} + \\widetilde{b}_c) \\\\\n\\widetilde{o_t} = \\widetilde{\\sigma}_o (\\widetilde{x}_t \\widetilde{W}_{xo} + \\widetilde{h}_{t-1} \\widetilde{W}_{ho} + \\widetilde{b}_o) \\\\\n\\widetilde{h}_t = \\widetilde{o}_t \\odot \\widetilde{\\sigma}_h(\\widetilde{c}_t)\n$$\n现在，外部 LSTM 的单元状态更新方式为 $ c_t = \\tilde{h}_{t} $ 。\n\n### 3. Experiments\n\n见附件论文[1]\n\n### 4. Conclusion\n\nNested LSTM（NLSTM）是LSTM模型的简单扩展，通过嵌套来增加深度，而不是通过堆叠。 NLSTM的内部存储器单元形成内部存储器，其仅通过外部存储器单元被其他计算元件访问，实现了时间层级的形式。\n\n论文[1]的实验表明，在相似的参数设置下，Nested LSTM 在多种字符级语言建模任务中的表现都超越了Stacked LSTM和single-layer LSTM，并且和Stacked LSTM 的高层级单元相比，LSTM 的内部记忆可以学习更长期的依赖关系。\n\n[NLSTM的Tensorflow实现](https://github.com/hannw/nlstm)\n\n[NLSTM的Keras实现](https://github.com/titu1994/Nested-LSTM)\n\n\n\n## Bibliographies\n\n笔记参考：http://www.sohu.com/a/220745456_390227，http://posts.careerengine.us/p/5a768ab3381fe136215b3de5?from=latest-posts-panel&type=title\n\n[1] Moniz, Joel Ruben Antony, and David Krueger. \"Nested LSTMs.\" *Asian Conference on Machine Learning*. 2017.\n\n[2] Hochreiter, Sepp, and Jürgen Schmidhuber. \"Long short-term memory.\" *Neural computation* 9.8 (1997): 1735-1780.\n\n","source":"_posts/[2018.2.5]Nested-LSTMs.md","raw":"---\ntitle: Nested LSTMs 笔记\ndate: 2018-02-05 08:00:00\ncategories: [research]\ntags: [LSTM, RNN]\n---\n\n## Nested LSTMs\n\n**摘要**：最近，一种新的 Nested LSTMs 网络被提出。在通常的LSTM网络中，我们通过将LSTM单元堆叠，从而形成深度RNN网络，提高其效果；Nested LSTM则通过嵌套而不是堆栈来增添LSTM的深度。在NLSTM中，记忆单元的值是由LSTM单元计算的，其中，LSTM单元具有自身内在的记忆单元。具体而言，NLSTM记忆单元并不是按照等式：$c_t^{outer} = f_t \\odot c_{t-1} + i_t \\odot g_t$ 对（外部）记忆单元的值进行计算，而是使用级联：$(f_t \\odot c_{t-1}, i_t \\odot g_t)$ 将其作为内部LSTM（或NLSTM）记忆单元的输入，并设定 $c_t^{outer} = h_t^{inner}$。在访问内部记忆时，Nested LSTM 相比传统的堆栈 LSTM 有更高的自由度，从而能处理更长时间规模的内部记忆；实验也表明，在参数数量相似的情况下，NLSTM 在多种任务上都超越了堆栈 LSTM。作者认为Nested LSTM 有潜力直接取代堆栈 LSTM。\n\n### 1. Introduction\n\n学习长期的依赖关系是当前人工智能领域中，尤其是在nlp领域，机器学习方法的关键性挑战。基于循环神经网络的体系结构已经在使得机器能够模仿这种能力方面取得了显著进展。\n\n#### single-layer LSTM\n\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202251.jpg\" width=\"90%\">\n\nRNN的输入是以当前的状态为依据，适合学习时间上的抽象特征。在实践中，许多专家已经证明，更为复杂的体系结构是解决许多任务的关键。其中一个原因是梯度消失问题（Hochreiter于1991年、Bengio等人于1994年提出），它使得简单的RNN难以学习长期依赖关系。Hochreiter和Schmidhuber于1997年提出了LSTM，包含能够改善梯度消失问题的记忆机制。单层LSTM如上图，图中的三个单元实际上是同一个单元，它循环地将内部的参数传递给自己。\n\n#### Stacked LSTMs\n\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202254.jpg\" width=\"60%\">\n\n堆栈 LSTM 架构使用一系列 LSTM 一层层地堆叠在一起来处理数据，一层的输出成为下一层的输入。上图为一个两层的LSTM网络。\n\n引入多层的结构，即将多个LSTM单元堆叠，每一层的输出成为下一层的输入。 每层处理我们希望解决的任务的一部分，并将其传递给下一层。额外的隐藏层可以添加到多层感知器神经网络，使其有更深入的“理解”。 额外的隐藏层被认为重新组合了来自先前层的学习表示，并在高度抽象层次上找到新的表示。 例如，从线条到形状到对象。\n\n#### Nested LSTMs\n\n在 NLSTM 中，LSTM 的记忆单元可以访问内部记忆。相比于传统的堆栈 LSTM，这一关键特征使得该模型能实现更有效的时间层级。在 NLSTM 中，外部记忆单元可自由选择读取、编写的相关长期信息到内部单元。相比之下，在Stacked LSTM 中，高层级的激活（类似内部记忆）直接生成输出，因此必须包含所有的与当前预测相关的短期信息。换言之，Stacked LSTM 与Nested LSTM 之间的主要不同在于，NLSTM 可以选择性地访问内部记忆。这使得，即使这些事件与当前事件不相关，内部记忆也能够记住、处理更长时间规模上的事件。我们在后面一章更详细地介绍它。\n\n### 2. Model of Nested LSTMs\n\nLSTM 中的输出门会编码可能与当前的时间步骤不相关，但是仍然值得记忆的信息。Nested LSTM 根据这一直观理解来创造一种记忆的时间层级。以同样的方式被gate控访问内部记忆，因此长期信息只有在情景相关的条件下才能选择性地访问。\n\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202253.jpg\" width=\"80%\">\n\n#### The architecture\n\n在 LSTM 网络中，单元状态的更新公式和门控机制可以表示为以下方程式：\n$$\ni_t = \\sigma_i (x_t W_{xi} + h_{t-1} W_{hi} + b_i) \\\\\nf_t = \\sigma_t (x_t W_{xf} + h_{t-1} W_{hf} + b_i) \\\\\nc_t = f_t \\odot c_{c-1} + \\sigma_c (x_t W_{xc} + h_{t-1} W_{hc} + b_c) \\\\\no_t = \\sigma_o (x_t W_{xo} + h_{t-1} W_{ho} + b_o) \\\\\nh_t = o_t \\odot \\sigma_h(c_t)\n$$\nNested LSTM 使用已学习的状态函数 $c_t = m_t(f_t\\odot c_{t−1}, i_t \\odot g_t)​$ 来替代 LSTM 中计算 $c_t​$ 的加运算。我们将函数的状态表示为 m 在时间 t 的内部记忆（inner memory），调用该函数以计算 $c_t​$ 和 $m_{t+1}​$。我们可以使用另一个 LSTM 单元来实现该记忆函数，就生成了 Nested LSTM。同样，该记忆函数能够由另一个 Nested LSTM 单元替换，因此就能构建任意深的嵌套网络。\n\n因此，我们得到NLSTM 中记忆函数的输入和隐藏状态：\n$$\n\\tilde{h}_{t-1} = f_t \\odot c_{t-1} \\\\\n\\tilde{x}_t = i_t \\odot \\sigma_c (x_t W_{xc} + h_{t-1} W_{hc} + b_c)\n$$\n注意如果记忆函数是加性的，那么$c_t = f_t \\odot c_{c-1} + \\sigma_c (x_t W_{xc} + h_{t-1} W_{hc} + b_c) =  \\tilde{h}_{t-1} + \\tilde{x}_t $，整个系统将退化到经典的 LSTM。\n\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202250.jpg\">\n\n*LSTM、Stacked LSTM 和 Nested LSTM 的计算图形。隐藏的状态、外部记忆单元和内部记忆单元分别由h、c和d进行表示。虽然当前的隐藏状态可以直接影响下一个内部记忆单元的内容，但内部记忆只有通过外部记忆才能够影响隐藏状态。*\n$$\n\\widetilde{i}_t = \\widetilde{\\sigma}_i (\\widetilde{x}_t \\widetilde{W}_{xi} + \\widetilde{h}_{t-1} \\widetilde{W}_{hi} + \\widetilde{b}_i) \\\\\n\\widetilde{f}_t = \\widetilde{\\sigma}_t (\\widetilde{x}_t \\widetilde{W}_{xf} + \\widetilde{h}_{t-1} \\widetilde{W}_{hf} + \\widetilde{b}_i) \\\\\n\\widetilde{c}_t = \\widetilde{f}_t \\odot \\widetilde{c}_{c-1} + \\widetilde{\\sigma}_c (\\widetilde{x}_t \\widetilde{W}_{xc} + \\widetilde{h}_{t-1} \\widetilde{W}_{hc} + \\widetilde{b}_c) \\\\\n\\widetilde{o_t} = \\widetilde{\\sigma}_o (\\widetilde{x}_t \\widetilde{W}_{xo} + \\widetilde{h}_{t-1} \\widetilde{W}_{ho} + \\widetilde{b}_o) \\\\\n\\widetilde{h}_t = \\widetilde{o}_t \\odot \\widetilde{\\sigma}_h(\\widetilde{c}_t)\n$$\n现在，外部 LSTM 的单元状态更新方式为 $ c_t = \\tilde{h}_{t} $ 。\n\n### 3. Experiments\n\n见附件论文[1]\n\n### 4. Conclusion\n\nNested LSTM（NLSTM）是LSTM模型的简单扩展，通过嵌套来增加深度，而不是通过堆叠。 NLSTM的内部存储器单元形成内部存储器，其仅通过外部存储器单元被其他计算元件访问，实现了时间层级的形式。\n\n论文[1]的实验表明，在相似的参数设置下，Nested LSTM 在多种字符级语言建模任务中的表现都超越了Stacked LSTM和single-layer LSTM，并且和Stacked LSTM 的高层级单元相比，LSTM 的内部记忆可以学习更长期的依赖关系。\n\n[NLSTM的Tensorflow实现](https://github.com/hannw/nlstm)\n\n[NLSTM的Keras实现](https://github.com/titu1994/Nested-LSTM)\n\n\n\n## Bibliographies\n\n笔记参考：http://www.sohu.com/a/220745456_390227，http://posts.careerengine.us/p/5a768ab3381fe136215b3de5?from=latest-posts-panel&type=title\n\n[1] Moniz, Joel Ruben Antony, and David Krueger. \"Nested LSTMs.\" *Asian Conference on Machine Learning*. 2017.\n\n[2] Hochreiter, Sepp, and Jürgen Schmidhuber. \"Long short-term memory.\" *Neural computation* 9.8 (1997): 1735-1780.\n\n","slug":"[2018.2.5]Nested-LSTMs","published":1,"updated":"2020-11-03T03:26:11.654Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckw4qufpq0029gwtlb95sfef3","content":"<h2 id=\"Nested-LSTMs\"><a href=\"#Nested-LSTMs\" class=\"headerlink\" title=\"Nested LSTMs\"></a>Nested LSTMs</h2><p><strong>摘要</strong>：最近，一种新的 Nested LSTMs 网络被提出。在通常的LSTM网络中，我们通过将LSTM单元堆叠，从而形成深度RNN网络，提高其效果；Nested LSTM则通过嵌套而不是堆栈来增添LSTM的深度。在NLSTM中，记忆单元的值是由LSTM单元计算的，其中，LSTM单元具有自身内在的记忆单元。具体而言，NLSTM记忆单元并不是按照等式：$c_t^{outer} = f_t \\odot c_{t-1} + i_t \\odot g_t$ 对（外部）记忆单元的值进行计算，而是使用级联：$(f_t \\odot c_{t-1}, i_t \\odot g_t)$ 将其作为内部LSTM（或NLSTM）记忆单元的输入，并设定 $c_t^{outer} = h_t^{inner}$。在访问内部记忆时，Nested LSTM 相比传统的堆栈 LSTM 有更高的自由度，从而能处理更长时间规模的内部记忆；实验也表明，在参数数量相似的情况下，NLSTM 在多种任务上都超越了堆栈 LSTM。作者认为Nested LSTM 有潜力直接取代堆栈 LSTM。</p>\n<h3 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1. Introduction\"></a>1. Introduction</h3><p>学习长期的依赖关系是当前人工智能领域中，尤其是在nlp领域，机器学习方法的关键性挑战。基于循环神经网络的体系结构已经在使得机器能够模仿这种能力方面取得了显著进展。</p>\n<h4 id=\"single-layer-LSTM\"><a href=\"#single-layer-LSTM\" class=\"headerlink\" title=\"single-layer LSTM\"></a>single-layer LSTM</h4><img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202251.jpg\" width=\"90%\">\n\n<p>RNN的输入是以当前的状态为依据，适合学习时间上的抽象特征。在实践中，许多专家已经证明，更为复杂的体系结构是解决许多任务的关键。其中一个原因是梯度消失问题（Hochreiter于1991年、Bengio等人于1994年提出），它使得简单的RNN难以学习长期依赖关系。Hochreiter和Schmidhuber于1997年提出了LSTM，包含能够改善梯度消失问题的记忆机制。单层LSTM如上图，图中的三个单元实际上是同一个单元，它循环地将内部的参数传递给自己。</p>\n<h4 id=\"Stacked-LSTMs\"><a href=\"#Stacked-LSTMs\" class=\"headerlink\" title=\"Stacked LSTMs\"></a>Stacked LSTMs</h4><img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202254.jpg\" width=\"60%\">\n\n<p>堆栈 LSTM 架构使用一系列 LSTM 一层层地堆叠在一起来处理数据，一层的输出成为下一层的输入。上图为一个两层的LSTM网络。</p>\n<p>引入多层的结构，即将多个LSTM单元堆叠，每一层的输出成为下一层的输入。 每层处理我们希望解决的任务的一部分，并将其传递给下一层。额外的隐藏层可以添加到多层感知器神经网络，使其有更深入的“理解”。 额外的隐藏层被认为重新组合了来自先前层的学习表示，并在高度抽象层次上找到新的表示。 例如，从线条到形状到对象。</p>\n<h4 id=\"Nested-LSTMs-1\"><a href=\"#Nested-LSTMs-1\" class=\"headerlink\" title=\"Nested LSTMs\"></a>Nested LSTMs</h4><p>在 NLSTM 中，LSTM 的记忆单元可以访问内部记忆。相比于传统的堆栈 LSTM，这一关键特征使得该模型能实现更有效的时间层级。在 NLSTM 中，外部记忆单元可自由选择读取、编写的相关长期信息到内部单元。相比之下，在Stacked LSTM 中，高层级的激活（类似内部记忆）直接生成输出，因此必须包含所有的与当前预测相关的短期信息。换言之，Stacked LSTM 与Nested LSTM 之间的主要不同在于，NLSTM 可以选择性地访问内部记忆。这使得，即使这些事件与当前事件不相关，内部记忆也能够记住、处理更长时间规模上的事件。我们在后面一章更详细地介绍它。</p>\n<h3 id=\"2-Model-of-Nested-LSTMs\"><a href=\"#2-Model-of-Nested-LSTMs\" class=\"headerlink\" title=\"2. Model of Nested LSTMs\"></a>2. Model of Nested LSTMs</h3><p>LSTM 中的输出门会编码可能与当前的时间步骤不相关，但是仍然值得记忆的信息。Nested LSTM 根据这一直观理解来创造一种记忆的时间层级。以同样的方式被gate控访问内部记忆，因此长期信息只有在情景相关的条件下才能选择性地访问。</p>\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202253.jpg\" width=\"80%\">\n\n<h4 id=\"The-architecture\"><a href=\"#The-architecture\" class=\"headerlink\" title=\"The architecture\"></a>The architecture</h4><p>在 LSTM 网络中，单元状态的更新公式和门控机制可以表示为以下方程式：<br>$$<br>i_t = \\sigma_i (x_t W_{xi} + h_{t-1} W_{hi} + b_i) \\<br>f_t = \\sigma_t (x_t W_{xf} + h_{t-1} W_{hf} + b_i) \\<br>c_t = f_t \\odot c_{c-1} + \\sigma_c (x_t W_{xc} + h_{t-1} W_{hc} + b_c) \\<br>o_t = \\sigma_o (x_t W_{xo} + h_{t-1} W_{ho} + b_o) \\<br>h_t = o_t \\odot \\sigma_h(c_t)<br>$$<br>Nested LSTM 使用已学习的状态函数 $c_t = m_t(f_t\\odot c_{t−1}, i_t \\odot g_t)​$ 来替代 LSTM 中计算 $c_t​$ 的加运算。我们将函数的状态表示为 m 在时间 t 的内部记忆（inner memory），调用该函数以计算 $c_t​$ 和 $m_{t+1}​$。我们可以使用另一个 LSTM 单元来实现该记忆函数，就生成了 Nested LSTM。同样，该记忆函数能够由另一个 Nested LSTM 单元替换，因此就能构建任意深的嵌套网络。</p>\n<p>因此，我们得到NLSTM 中记忆函数的输入和隐藏状态：<br>$$<br>\\tilde{h}<em>{t-1} = f_t \\odot c</em>{t-1} \\<br>\\tilde{x}<em>t = i_t \\odot \\sigma_c (x_t W</em>{xc} + h_{t-1} W_{hc} + b_c)<br>$$<br>注意如果记忆函数是加性的，那么$c_t = f_t \\odot c_{c-1} + \\sigma_c (x_t W_{xc} + h_{t-1} W_{hc} + b_c) =  \\tilde{h}_{t-1} + \\tilde{x}_t $，整个系统将退化到经典的 LSTM。</p>\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202250.jpg\">\n\n<p><em>LSTM、Stacked LSTM 和 Nested LSTM 的计算图形。隐藏的状态、外部记忆单元和内部记忆单元分别由h、c和d进行表示。虽然当前的隐藏状态可以直接影响下一个内部记忆单元的内容，但内部记忆只有通过外部记忆才能够影响隐藏状态。</em><br>$$<br>\\widetilde{i}<em>t = \\widetilde{\\sigma}<em>i (\\widetilde{x}<em>t \\widetilde{W}</em>{xi} + \\widetilde{h}</em>{t-1} \\widetilde{W}</em>{hi} + \\widetilde{b}_i) \\<br>\\widetilde{f}<em>t = \\widetilde{\\sigma}<em>t (\\widetilde{x}<em>t \\widetilde{W}</em>{xf} + \\widetilde{h}</em>{t-1} \\widetilde{W}</em>{hf} + \\widetilde{b}_i) \\<br>\\widetilde{c}<em>t = \\widetilde{f}<em>t \\odot \\widetilde{c}</em>{c-1} + \\widetilde{\\sigma}<em>c (\\widetilde{x}<em>t \\widetilde{W}</em>{xc} + \\widetilde{h}</em>{t-1} \\widetilde{W}</em>{hc} + \\widetilde{b}<em>c) \\<br>\\widetilde{o_t} = \\widetilde{\\sigma}<em>o (\\widetilde{x}<em>t \\widetilde{W}</em>{xo} + \\widetilde{h}</em>{t-1} \\widetilde{W}</em>{ho} + \\widetilde{b}_o) \\<br>\\widetilde{h}_t = \\widetilde{o}_t \\odot \\widetilde{\\sigma}_h(\\widetilde{c}<em>t)<br>$$<br>现在，外部 LSTM 的单元状态更新方式为 $ c_t = \\tilde{h}</em>{t} $ 。</p>\n<h3 id=\"3-Experiments\"><a href=\"#3-Experiments\" class=\"headerlink\" title=\"3. Experiments\"></a>3. Experiments</h3><p>见附件论文[1]</p>\n<h3 id=\"4-Conclusion\"><a href=\"#4-Conclusion\" class=\"headerlink\" title=\"4. Conclusion\"></a>4. Conclusion</h3><p>Nested LSTM（NLSTM）是LSTM模型的简单扩展，通过嵌套来增加深度，而不是通过堆叠。 NLSTM的内部存储器单元形成内部存储器，其仅通过外部存储器单元被其他计算元件访问，实现了时间层级的形式。</p>\n<p>论文[1]的实验表明，在相似的参数设置下，Nested LSTM 在多种字符级语言建模任务中的表现都超越了Stacked LSTM和single-layer LSTM，并且和Stacked LSTM 的高层级单元相比，LSTM 的内部记忆可以学习更长期的依赖关系。</p>\n<p><a href=\"https://github.com/hannw/nlstm\">NLSTM的Tensorflow实现</a></p>\n<p><a href=\"https://github.com/titu1994/Nested-LSTM\">NLSTM的Keras实现</a></p>\n<h2 id=\"Bibliographies\"><a href=\"#Bibliographies\" class=\"headerlink\" title=\"Bibliographies\"></a>Bibliographies</h2><p>笔记参考：<a href=\"http://www.sohu.com/a/220745456_390227%EF%BC%8Chttp://posts.careerengine.us/p/5a768ab3381fe136215b3de5?from=latest-posts-panel&amp;type=title\">http://www.sohu.com/a/220745456_390227，http://posts.careerengine.us/p/5a768ab3381fe136215b3de5?from=latest-posts-panel&amp;type=title</a></p>\n<p>[1] Moniz, Joel Ruben Antony, and David Krueger. “Nested LSTMs.” <em>Asian Conference on Machine Learning</em>. 2017.</p>\n<p>[2] Hochreiter, Sepp, and Jürgen Schmidhuber. “Long short-term memory.” <em>Neural computation</em> 9.8 (1997): 1735-1780.</p>\n","site":{"data":{"recommended_posts":{"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"}],"https://LorrinWWW.github.io/posts/complexity/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/hexo-with-latex/":[{"title":"区块链时代的最正确的打赏姿势——在Hexo博客配置Coinhive网页挖矿","permalink":"http://www.davidfnck.com/blockchain/mine-xmr-in-hexo-by-coinhive.html/"}],"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/":[{"title":"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-statistic/":[{"title":"贝叶斯估计 Bayes estimation","permalink":"https://LorrinWWW.github.io/posts/Bayes-estimation/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"}],"https://LorrinWWW.github.io/posts/[2018.2.5]Nested-LSTMs/":[{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"}],"https://LorrinWWW.github.io/posts/datamining-pretreatment/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Several models for knowledge graph representing and completing 几个知识图谱模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/"}],"https://LorrinWWW.github.io/posts/compression/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/Sobolev-space/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/proba-ch5/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch3/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/proba-ch1/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/ML-CNN/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/EDP-basic-models/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/machine-learning/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/datamining-class-pred/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction/":[],"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/":[{"title":"OS notes","permalink":"https://LorrinWWW.github.io/posts/OS-notes/"}],"https://LorrinWWW.github.io/posts/projet-enjeu-plugin-chrome-101/":[{"title":"Chrome油猴脚本","permalink":"http://gubangzhong.cn/2017/06/13/Chrome油猴脚本/"}],"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/":[{"title":"MongoDB, Docker and Python","permalink":"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/":[{"title":"Hands on Scrapy","permalink":"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/OS-notes/":[{"title":"learning OS and building LorriOS","permalink":"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/"}],"https://LorrinWWW.github.io/posts/Bayes-estimation/":[{"title":"Note of statistic","permalink":"https://LorrinWWW.github.io/posts/Note-of-statistic/"}],"https://LorrinWWW.github.io/posts/management-of-the-firm/":[],"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"}],"https://LorrinWWW.github.io/posts/datamining-qualitative-induction/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-finite-element-method/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/QuadTree/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/proba-ch4/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch2/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/":[{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/Note-of-NLP/":[{"title":"Generative Adversarial Network","permalink":"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/"},{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"}],"https://LorrinWWW.github.io/posts/Note-of-datamining/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"数据挖掘-分类与预测","permalink":"https://LorrinWWW.github.io/posts/datamining-class-pred/"},{"title":"数据挖掘-预处理","permalink":"https://LorrinWWW.github.io/posts/datamining-pretreatment/"}],"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/Note-of-probability/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/participe-present-et-gerondif/":[],"https://LorrinWWW.github.io/posts/Hilbert-space/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/proba-ch6/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}]}}},"excerpt":"","more":"<h2 id=\"Nested-LSTMs\"><a href=\"#Nested-LSTMs\" class=\"headerlink\" title=\"Nested LSTMs\"></a>Nested LSTMs</h2><p><strong>摘要</strong>：最近，一种新的 Nested LSTMs 网络被提出。在通常的LSTM网络中，我们通过将LSTM单元堆叠，从而形成深度RNN网络，提高其效果；Nested LSTM则通过嵌套而不是堆栈来增添LSTM的深度。在NLSTM中，记忆单元的值是由LSTM单元计算的，其中，LSTM单元具有自身内在的记忆单元。具体而言，NLSTM记忆单元并不是按照等式：$c_t^{outer} = f_t \\odot c_{t-1} + i_t \\odot g_t$ 对（外部）记忆单元的值进行计算，而是使用级联：$(f_t \\odot c_{t-1}, i_t \\odot g_t)$ 将其作为内部LSTM（或NLSTM）记忆单元的输入，并设定 $c_t^{outer} = h_t^{inner}$。在访问内部记忆时，Nested LSTM 相比传统的堆栈 LSTM 有更高的自由度，从而能处理更长时间规模的内部记忆；实验也表明，在参数数量相似的情况下，NLSTM 在多种任务上都超越了堆栈 LSTM。作者认为Nested LSTM 有潜力直接取代堆栈 LSTM。</p>\n<h3 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1. Introduction\"></a>1. Introduction</h3><p>学习长期的依赖关系是当前人工智能领域中，尤其是在nlp领域，机器学习方法的关键性挑战。基于循环神经网络的体系结构已经在使得机器能够模仿这种能力方面取得了显著进展。</p>\n<h4 id=\"single-layer-LSTM\"><a href=\"#single-layer-LSTM\" class=\"headerlink\" title=\"single-layer LSTM\"></a>single-layer LSTM</h4><img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202251.jpg\" width=\"90%\">\n\n<p>RNN的输入是以当前的状态为依据，适合学习时间上的抽象特征。在实践中，许多专家已经证明，更为复杂的体系结构是解决许多任务的关键。其中一个原因是梯度消失问题（Hochreiter于1991年、Bengio等人于1994年提出），它使得简单的RNN难以学习长期依赖关系。Hochreiter和Schmidhuber于1997年提出了LSTM，包含能够改善梯度消失问题的记忆机制。单层LSTM如上图，图中的三个单元实际上是同一个单元，它循环地将内部的参数传递给自己。</p>\n<h4 id=\"Stacked-LSTMs\"><a href=\"#Stacked-LSTMs\" class=\"headerlink\" title=\"Stacked LSTMs\"></a>Stacked LSTMs</h4><img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202254.jpg\" width=\"60%\">\n\n<p>堆栈 LSTM 架构使用一系列 LSTM 一层层地堆叠在一起来处理数据，一层的输出成为下一层的输入。上图为一个两层的LSTM网络。</p>\n<p>引入多层的结构，即将多个LSTM单元堆叠，每一层的输出成为下一层的输入。 每层处理我们希望解决的任务的一部分，并将其传递给下一层。额外的隐藏层可以添加到多层感知器神经网络，使其有更深入的“理解”。 额外的隐藏层被认为重新组合了来自先前层的学习表示，并在高度抽象层次上找到新的表示。 例如，从线条到形状到对象。</p>\n<h4 id=\"Nested-LSTMs-1\"><a href=\"#Nested-LSTMs-1\" class=\"headerlink\" title=\"Nested LSTMs\"></a>Nested LSTMs</h4><p>在 NLSTM 中，LSTM 的记忆单元可以访问内部记忆。相比于传统的堆栈 LSTM，这一关键特征使得该模型能实现更有效的时间层级。在 NLSTM 中，外部记忆单元可自由选择读取、编写的相关长期信息到内部单元。相比之下，在Stacked LSTM 中，高层级的激活（类似内部记忆）直接生成输出，因此必须包含所有的与当前预测相关的短期信息。换言之，Stacked LSTM 与Nested LSTM 之间的主要不同在于，NLSTM 可以选择性地访问内部记忆。这使得，即使这些事件与当前事件不相关，内部记忆也能够记住、处理更长时间规模上的事件。我们在后面一章更详细地介绍它。</p>\n<h3 id=\"2-Model-of-Nested-LSTMs\"><a href=\"#2-Model-of-Nested-LSTMs\" class=\"headerlink\" title=\"2. Model of Nested LSTMs\"></a>2. Model of Nested LSTMs</h3><p>LSTM 中的输出门会编码可能与当前的时间步骤不相关，但是仍然值得记忆的信息。Nested LSTM 根据这一直观理解来创造一种记忆的时间层级。以同样的方式被gate控访问内部记忆，因此长期信息只有在情景相关的条件下才能选择性地访问。</p>\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202253.jpg\" width=\"80%\">\n\n<h4 id=\"The-architecture\"><a href=\"#The-architecture\" class=\"headerlink\" title=\"The architecture\"></a>The architecture</h4><p>在 LSTM 网络中，单元状态的更新公式和门控机制可以表示为以下方程式：<br>$$<br>i_t = \\sigma_i (x_t W_{xi} + h_{t-1} W_{hi} + b_i) \\<br>f_t = \\sigma_t (x_t W_{xf} + h_{t-1} W_{hf} + b_i) \\<br>c_t = f_t \\odot c_{c-1} + \\sigma_c (x_t W_{xc} + h_{t-1} W_{hc} + b_c) \\<br>o_t = \\sigma_o (x_t W_{xo} + h_{t-1} W_{ho} + b_o) \\<br>h_t = o_t \\odot \\sigma_h(c_t)<br>$$<br>Nested LSTM 使用已学习的状态函数 $c_t = m_t(f_t\\odot c_{t−1}, i_t \\odot g_t)​$ 来替代 LSTM 中计算 $c_t​$ 的加运算。我们将函数的状态表示为 m 在时间 t 的内部记忆（inner memory），调用该函数以计算 $c_t​$ 和 $m_{t+1}​$。我们可以使用另一个 LSTM 单元来实现该记忆函数，就生成了 Nested LSTM。同样，该记忆函数能够由另一个 Nested LSTM 单元替换，因此就能构建任意深的嵌套网络。</p>\n<p>因此，我们得到NLSTM 中记忆函数的输入和隐藏状态：<br>$$<br>\\tilde{h}<em>{t-1} = f_t \\odot c</em>{t-1} \\<br>\\tilde{x}<em>t = i_t \\odot \\sigma_c (x_t W</em>{xc} + h_{t-1} W_{hc} + b_c)<br>$$<br>注意如果记忆函数是加性的，那么$c_t = f_t \\odot c_{c-1} + \\sigma_c (x_t W_{xc} + h_{t-1} W_{hc} + b_c) =  \\tilde{h}_{t-1} + \\tilde{x}_t $，整个系统将退化到经典的 LSTM。</p>\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202250.jpg\">\n\n<p><em>LSTM、Stacked LSTM 和 Nested LSTM 的计算图形。隐藏的状态、外部记忆单元和内部记忆单元分别由h、c和d进行表示。虽然当前的隐藏状态可以直接影响下一个内部记忆单元的内容，但内部记忆只有通过外部记忆才能够影响隐藏状态。</em><br>$$<br>\\widetilde{i}<em>t = \\widetilde{\\sigma}<em>i (\\widetilde{x}<em>t \\widetilde{W}</em>{xi} + \\widetilde{h}</em>{t-1} \\widetilde{W}</em>{hi} + \\widetilde{b}_i) \\<br>\\widetilde{f}<em>t = \\widetilde{\\sigma}<em>t (\\widetilde{x}<em>t \\widetilde{W}</em>{xf} + \\widetilde{h}</em>{t-1} \\widetilde{W}</em>{hf} + \\widetilde{b}_i) \\<br>\\widetilde{c}<em>t = \\widetilde{f}<em>t \\odot \\widetilde{c}</em>{c-1} + \\widetilde{\\sigma}<em>c (\\widetilde{x}<em>t \\widetilde{W}</em>{xc} + \\widetilde{h}</em>{t-1} \\widetilde{W}</em>{hc} + \\widetilde{b}<em>c) \\<br>\\widetilde{o_t} = \\widetilde{\\sigma}<em>o (\\widetilde{x}<em>t \\widetilde{W}</em>{xo} + \\widetilde{h}</em>{t-1} \\widetilde{W}</em>{ho} + \\widetilde{b}_o) \\<br>\\widetilde{h}_t = \\widetilde{o}_t \\odot \\widetilde{\\sigma}_h(\\widetilde{c}<em>t)<br>$$<br>现在，外部 LSTM 的单元状态更新方式为 $ c_t = \\tilde{h}</em>{t} $ 。</p>\n<h3 id=\"3-Experiments\"><a href=\"#3-Experiments\" class=\"headerlink\" title=\"3. Experiments\"></a>3. Experiments</h3><p>见附件论文[1]</p>\n<h3 id=\"4-Conclusion\"><a href=\"#4-Conclusion\" class=\"headerlink\" title=\"4. Conclusion\"></a>4. Conclusion</h3><p>Nested LSTM（NLSTM）是LSTM模型的简单扩展，通过嵌套来增加深度，而不是通过堆叠。 NLSTM的内部存储器单元形成内部存储器，其仅通过外部存储器单元被其他计算元件访问，实现了时间层级的形式。</p>\n<p>论文[1]的实验表明，在相似的参数设置下，Nested LSTM 在多种字符级语言建模任务中的表现都超越了Stacked LSTM和single-layer LSTM，并且和Stacked LSTM 的高层级单元相比，LSTM 的内部记忆可以学习更长期的依赖关系。</p>\n<p><a href=\"https://github.com/hannw/nlstm\">NLSTM的Tensorflow实现</a></p>\n<p><a href=\"https://github.com/titu1994/Nested-LSTM\">NLSTM的Keras实现</a></p>\n<h2 id=\"Bibliographies\"><a href=\"#Bibliographies\" class=\"headerlink\" title=\"Bibliographies\"></a>Bibliographies</h2><p>笔记参考：<a href=\"http://www.sohu.com/a/220745456_390227%EF%BC%8Chttp://posts.careerengine.us/p/5a768ab3381fe136215b3de5?from=latest-posts-panel&amp;type=title\">http://www.sohu.com/a/220745456_390227，http://posts.careerengine.us/p/5a768ab3381fe136215b3de5?from=latest-posts-panel&amp;type=title</a></p>\n<p>[1] Moniz, Joel Ruben Antony, and David Krueger. “Nested LSTMs.” <em>Asian Conference on Machine Learning</em>. 2017.</p>\n<p>[2] Hochreiter, Sepp, and Jürgen Schmidhuber. “Long short-term memory.” <em>Neural computation</em> 9.8 (1997): 1735-1780.</p>\n"},{"title":"Several models for knowledge graph representing and completing 几个知识图谱模型","date":"2018-03-10T00:00:00.000Z","_content":"\n# 2018.3.10\n\n## Several models for knowledge graph representing and completing\n\n**摘要**：上次看到的ConMask在开放领域knowledge graph completion有着不错的表现，这次我们不考虑开放领域，介绍几个经典的模型。\n\n### 1. Series of Trans\n\n#### 1.1 TransE \n\nTransE [^1] 可能是最为常用也最为基础的方法是一种基于强化学习(RL)的模型. 它有一个简单实用的假设：\n$$\nh+r = t\n$$\n其中h是head entity的向量，t是tail entity的向量，r是关系向量。\n\nTransE定义了loss function：\n$$\n\\mathcal{L(T)} = \\sum_{<h,r,t>\\in T} [\\gamma + E(<h,r,t>) - E(<h',r',t'>)]_+\n$$\n其中 $T$ 代表一个三元组的集合；$E(<h,r,t>) = ||h+r-t||_{L_n}$是energy function；$<h,r,t>$是G中的一个三元组；$<h',r',t'>$代表一个不存在于 $T$ 的三元组，通过随机替换一部分$<h,r,t>$来得到；$\\gamma$ 表示边际距离\n\n算法的核心是令正例的 h+r-t 趋近于 0，而负例的 h+r-t 趋近于无穷大。整个 TransE 模型的训练过程比较简单，首先对头尾节点以及关系进行初始化，然后每对一个正例取一个负例样本，然后利用 hinge loss function 尽可能使正例和负例分开，最后采用 SGD(Stochastic Gradient Descent) 方法更新参数。\n\n由TransE又衍生出了许多模型。\n\n#### 1.2 TransH\n\n虽然 TransE 模型训练速度快、易于实现，但是它不能够解决多对一和一对多关系的问题。为了解决这个问题并仍然维持较低的复杂度，TransH [^2] 不再严格要求 h+r-l=0，而是只需要保证头结点和尾节点在关系平面上的投影在一条直线上，因此能够得到图中头结点向量正确的表示。\n\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202332.jpg\">\n\n论文另一个亮点是设计了一种负类抽样的方法，即一对多的时候，给head更多的抽样概率， 同样的多对一的时候，给tail更多抽样概率。\n\n\n\n#### 1.3 TransR/CTrans \n\nTransE和TransH都假设了实体和关系都在同一个空间内，但由于实体和关系本身的不同，简单地认为它们embedding在同一个空间内是不充分的。因此，TransR[^3]对于**每一个r**，我们设置mapping矩阵$M_r$，使得\n$$\nh_{\\perp} + r \\simeq t_{\\perp} \\quad \\text{where } h_{\\perp} = M_{r}\\cdot h, t_{\\perp} = M_{r} \\cdot t\n$$\n后面的方法就和TransE较为类似了。\n\n#### 1.4 TransD\n\n实际上TransD[^4]是对TransR/CTrans的改进。TransR有以下缺点：\n\n- 对于每个r，所有的实体都使用同一个mapping矩阵$M_{r}$，但是实际上对应于一个r的实体有不同的种类、特征，这么做会有一些问题；\n- mapping是entity和relation之间的交互过程，mapping矩阵仅由关系决定是不合理的；\n- 矩阵与向量的运算的计算比较复杂，如果在一个Knowledge graph里有较多的relation，那么就会有大量的参数，以及较高的复杂度，因此导致计算量过大，无法应用到大规模knowledge graph。\n\n对于任意一个entity或relation，TransD定义两个向量，第一个表示entity或relation的含义，另一个用于把entity投影到relation空间（或者说用于构造mapping矩阵）。因此对于每个entity-relation pair，都有一个动态生成的唯一的mapping矩阵。此外，上述过程没有用到矩阵向量运算，而用向量的运算代替了。\n\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202326.jpg\">\n\nTransD大大降低计算复杂度，但仍然保持不错的效果。\n\n#### 1.5 TransA\n\n前面的模型都是基于欧式距离计算，也就是认为每一维的重要性是相同的。但实际上，有一些维度能较好的区分不同的entity和relation，但也有许多不包含什么有效信息，因此甚至可以被认为是噪音，因此不同维度的重要性显然是不同的。\n\nTransA[^5] 模型通过引入加权矩阵，赋给每一维度权重。\n\n结果如下图，欧式距离每一维都是同等重要的，体现为圆形；而TransA体现为椭圆形，显然更符合数据的分布。\n\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202330.jpg\">\n\n\n\n### 2. ProjE\n\nProjE[^6]的作者就是Open-World Knowledge Graph Completion的作者，两篇文章分别被AAAI2017和AAAI2018收录，可以认为是KGC任务的 state-of-the-art。这里提一下ProjE的思路。\n\n#### 2.1 Model Architecture\n\n给出\\<h, r, ?\\>，已知h、r，要求预测 ? 。通常的做法就是把所有的候选entity都拿来打分，得分最高的就是预测的结果。为了得到这一系列打分，首先我们通过一个运算符来合并h和r，得到一个向量，然后我们把所有候选entity投影到这个向量上，随后运算得到分数。\n\n现有的模型（包括上面提到的一系列trans）往往通过mapping矩阵来合并entity和relation，这里作者也是这么做，但是他认为目前还不需要考虑各个维度之间的相互作用，因此这个mapping矩阵应该是一个对角矩阵。所以这个合并操作可以被定义为：\n$$\ne \\oplus r = D_e e + D_r r + b_c\n$$\n其中$D_e$和$D_r$就是两个对角矩阵。\n\n由此，我们可以定义embedding映射函数：\n$$\nh(e, r) = g(W^c f(e \\oplus r) + b_r )\n$$\n其中f和g是激活函数(activation function)，我们在后面定义；$W^c$是候选entity构成的矩阵。因此h就是对所有候选entity的打分矩阵。\n\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202328.jpg\">\n\n上图是ProjE的结构，包括两部分神经网络层，其中上半部分是合并操作，即$e \\oplus r$；下半部分是映射函数，或者说打分函数，即$h(e,r)$。\n\n#### 2.2 Loss function\n\n我们这里分析方便只看pointwise loss function：\n$$\n\\mathcal{L}(e, r, y) = - \\sum_{i\\in\\{i|y_i=1\\} } {log(h(e,r)_i)} - \\sum_{m} {\\mathbb{E}_{j \\sim P_y} log(h(e,r)_j)}\n$$\n其中$y$是一个布尔向量，1为正例，0为反例；m个反例。pointwise ProjE 可以被看作是多类分类问题，所以我们选取 f 和 g 分别为 sigmoid 和 tanh 函数。\n\n代码实现 https://github.com/bxshi/ProjE\n\n## Bibliographies\n\n笔记参考：\n\nhttp://www.infosec-wiki.com/?p=175755\n\nhttps://www.jiqizhixin.com/articles/2017-11-03-5\n\n[^1]: Bordes, A., Usunier, N., Garcia-Duran, A., Weston, J., & Yakhnenko, O. (2013). Translating embeddings for modeling multi-relational data. In *Advances in neural information processing systems* (pp. 2787-2795).\n[^2]: Wang, Z., Zhang, J., Feng, J., & Chen, Z. (2014, July). Knowledge Graph Embedding by Translating on Hyperplanes. In *AAAI* (Vol. 14, pp. 1112-1119).\n\n[^3]: Lin, Y., Liu, Z., Sun, M., Liu, Y., & Zhu, X. (2015, January). Learning entity and relation embeddings for knowledge graph completion. In *AAAI* (Vol. 15, pp. 2181-2187).\n[^4]: Ji, G., He, S., Xu, L., Liu, K., & Zhao, J. (2015). Knowledge graph embedding via dynamic mapping matrix. In *Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)* (Vol. 1, pp. 687-696).\n[^5]: Xiao, H., Huang, M., Hao, Y., & Zhu, X. (2015). TransA: An adaptive approach for knowledge graph embedding. *arXiv preprint arXiv:1509.05490*.\n[^6]: Shi, B., & Weninger, T. (2017, February). ProjE: Embedding Projection for Knowledge Graph Completion. In *AAAI* (Vol. 17, pp. 1236-1242).","source":"_posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing.md","raw":"---\ntitle: Several models for knowledge graph representing and completing 几个知识图谱模型\ndate: 2018-03-10 08:00:00\ncategories: [research]\ntags: [KGC, knowledge-graph]\n---\n\n# 2018.3.10\n\n## Several models for knowledge graph representing and completing\n\n**摘要**：上次看到的ConMask在开放领域knowledge graph completion有着不错的表现，这次我们不考虑开放领域，介绍几个经典的模型。\n\n### 1. Series of Trans\n\n#### 1.1 TransE \n\nTransE [^1] 可能是最为常用也最为基础的方法是一种基于强化学习(RL)的模型. 它有一个简单实用的假设：\n$$\nh+r = t\n$$\n其中h是head entity的向量，t是tail entity的向量，r是关系向量。\n\nTransE定义了loss function：\n$$\n\\mathcal{L(T)} = \\sum_{<h,r,t>\\in T} [\\gamma + E(<h,r,t>) - E(<h',r',t'>)]_+\n$$\n其中 $T$ 代表一个三元组的集合；$E(<h,r,t>) = ||h+r-t||_{L_n}$是energy function；$<h,r,t>$是G中的一个三元组；$<h',r',t'>$代表一个不存在于 $T$ 的三元组，通过随机替换一部分$<h,r,t>$来得到；$\\gamma$ 表示边际距离\n\n算法的核心是令正例的 h+r-t 趋近于 0，而负例的 h+r-t 趋近于无穷大。整个 TransE 模型的训练过程比较简单，首先对头尾节点以及关系进行初始化，然后每对一个正例取一个负例样本，然后利用 hinge loss function 尽可能使正例和负例分开，最后采用 SGD(Stochastic Gradient Descent) 方法更新参数。\n\n由TransE又衍生出了许多模型。\n\n#### 1.2 TransH\n\n虽然 TransE 模型训练速度快、易于实现，但是它不能够解决多对一和一对多关系的问题。为了解决这个问题并仍然维持较低的复杂度，TransH [^2] 不再严格要求 h+r-l=0，而是只需要保证头结点和尾节点在关系平面上的投影在一条直线上，因此能够得到图中头结点向量正确的表示。\n\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202332.jpg\">\n\n论文另一个亮点是设计了一种负类抽样的方法，即一对多的时候，给head更多的抽样概率， 同样的多对一的时候，给tail更多抽样概率。\n\n\n\n#### 1.3 TransR/CTrans \n\nTransE和TransH都假设了实体和关系都在同一个空间内，但由于实体和关系本身的不同，简单地认为它们embedding在同一个空间内是不充分的。因此，TransR[^3]对于**每一个r**，我们设置mapping矩阵$M_r$，使得\n$$\nh_{\\perp} + r \\simeq t_{\\perp} \\quad \\text{where } h_{\\perp} = M_{r}\\cdot h, t_{\\perp} = M_{r} \\cdot t\n$$\n后面的方法就和TransE较为类似了。\n\n#### 1.4 TransD\n\n实际上TransD[^4]是对TransR/CTrans的改进。TransR有以下缺点：\n\n- 对于每个r，所有的实体都使用同一个mapping矩阵$M_{r}$，但是实际上对应于一个r的实体有不同的种类、特征，这么做会有一些问题；\n- mapping是entity和relation之间的交互过程，mapping矩阵仅由关系决定是不合理的；\n- 矩阵与向量的运算的计算比较复杂，如果在一个Knowledge graph里有较多的relation，那么就会有大量的参数，以及较高的复杂度，因此导致计算量过大，无法应用到大规模knowledge graph。\n\n对于任意一个entity或relation，TransD定义两个向量，第一个表示entity或relation的含义，另一个用于把entity投影到relation空间（或者说用于构造mapping矩阵）。因此对于每个entity-relation pair，都有一个动态生成的唯一的mapping矩阵。此外，上述过程没有用到矩阵向量运算，而用向量的运算代替了。\n\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202326.jpg\">\n\nTransD大大降低计算复杂度，但仍然保持不错的效果。\n\n#### 1.5 TransA\n\n前面的模型都是基于欧式距离计算，也就是认为每一维的重要性是相同的。但实际上，有一些维度能较好的区分不同的entity和relation，但也有许多不包含什么有效信息，因此甚至可以被认为是噪音，因此不同维度的重要性显然是不同的。\n\nTransA[^5] 模型通过引入加权矩阵，赋给每一维度权重。\n\n结果如下图，欧式距离每一维都是同等重要的，体现为圆形；而TransA体现为椭圆形，显然更符合数据的分布。\n\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202330.jpg\">\n\n\n\n### 2. ProjE\n\nProjE[^6]的作者就是Open-World Knowledge Graph Completion的作者，两篇文章分别被AAAI2017和AAAI2018收录，可以认为是KGC任务的 state-of-the-art。这里提一下ProjE的思路。\n\n#### 2.1 Model Architecture\n\n给出\\<h, r, ?\\>，已知h、r，要求预测 ? 。通常的做法就是把所有的候选entity都拿来打分，得分最高的就是预测的结果。为了得到这一系列打分，首先我们通过一个运算符来合并h和r，得到一个向量，然后我们把所有候选entity投影到这个向量上，随后运算得到分数。\n\n现有的模型（包括上面提到的一系列trans）往往通过mapping矩阵来合并entity和relation，这里作者也是这么做，但是他认为目前还不需要考虑各个维度之间的相互作用，因此这个mapping矩阵应该是一个对角矩阵。所以这个合并操作可以被定义为：\n$$\ne \\oplus r = D_e e + D_r r + b_c\n$$\n其中$D_e$和$D_r$就是两个对角矩阵。\n\n由此，我们可以定义embedding映射函数：\n$$\nh(e, r) = g(W^c f(e \\oplus r) + b_r )\n$$\n其中f和g是激活函数(activation function)，我们在后面定义；$W^c$是候选entity构成的矩阵。因此h就是对所有候选entity的打分矩阵。\n\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202328.jpg\">\n\n上图是ProjE的结构，包括两部分神经网络层，其中上半部分是合并操作，即$e \\oplus r$；下半部分是映射函数，或者说打分函数，即$h(e,r)$。\n\n#### 2.2 Loss function\n\n我们这里分析方便只看pointwise loss function：\n$$\n\\mathcal{L}(e, r, y) = - \\sum_{i\\in\\{i|y_i=1\\} } {log(h(e,r)_i)} - \\sum_{m} {\\mathbb{E}_{j \\sim P_y} log(h(e,r)_j)}\n$$\n其中$y$是一个布尔向量，1为正例，0为反例；m个反例。pointwise ProjE 可以被看作是多类分类问题，所以我们选取 f 和 g 分别为 sigmoid 和 tanh 函数。\n\n代码实现 https://github.com/bxshi/ProjE\n\n## Bibliographies\n\n笔记参考：\n\nhttp://www.infosec-wiki.com/?p=175755\n\nhttps://www.jiqizhixin.com/articles/2017-11-03-5\n\n[^1]: Bordes, A., Usunier, N., Garcia-Duran, A., Weston, J., & Yakhnenko, O. (2013). Translating embeddings for modeling multi-relational data. In *Advances in neural information processing systems* (pp. 2787-2795).\n[^2]: Wang, Z., Zhang, J., Feng, J., & Chen, Z. (2014, July). Knowledge Graph Embedding by Translating on Hyperplanes. In *AAAI* (Vol. 14, pp. 1112-1119).\n\n[^3]: Lin, Y., Liu, Z., Sun, M., Liu, Y., & Zhu, X. (2015, January). Learning entity and relation embeddings for knowledge graph completion. In *AAAI* (Vol. 15, pp. 2181-2187).\n[^4]: Ji, G., He, S., Xu, L., Liu, K., & Zhao, J. (2015). Knowledge graph embedding via dynamic mapping matrix. In *Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)* (Vol. 1, pp. 687-696).\n[^5]: Xiao, H., Huang, M., Hao, Y., & Zhu, X. (2015). TransA: An adaptive approach for knowledge graph embedding. *arXiv preprint arXiv:1509.05490*.\n[^6]: Shi, B., & Weninger, T. (2017, February). ProjE: Embedding Projection for Knowledge Graph Completion. In *AAAI* (Vol. 17, pp. 1236-1242).","slug":"[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing","published":1,"updated":"2020-11-03T03:26:11.105Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckw4qufpr002dgwtl3nxy0i7w","content":"<h1 id=\"2018-3-10\"><a href=\"#2018-3-10\" class=\"headerlink\" title=\"2018.3.10\"></a>2018.3.10</h1><h2 id=\"Several-models-for-knowledge-graph-representing-and-completing\"><a href=\"#Several-models-for-knowledge-graph-representing-and-completing\" class=\"headerlink\" title=\"Several models for knowledge graph representing and completing\"></a>Several models for knowledge graph representing and completing</h2><p><strong>摘要</strong>：上次看到的ConMask在开放领域knowledge graph completion有着不错的表现，这次我们不考虑开放领域，介绍几个经典的模型。</p>\n<h3 id=\"1-Series-of-Trans\"><a href=\"#1-Series-of-Trans\" class=\"headerlink\" title=\"1. Series of Trans\"></a>1. Series of Trans</h3><h4 id=\"1-1-TransE\"><a href=\"#1-1-TransE\" class=\"headerlink\" title=\"1.1 TransE\"></a>1.1 TransE</h4><p>TransE [^1] 可能是最为常用也最为基础的方法是一种基于强化学习(RL)的模型. 它有一个简单实用的假设：<br>$$<br>h+r = t<br>$$<br>其中h是head entity的向量，t是tail entity的向量，r是关系向量。</p>\n<p>TransE定义了loss function：<br>$$<br>\\mathcal{L(T)} = \\sum_{&lt;h,r,t&gt;\\in T} [\\gamma + E(&lt;h,r,t&gt;) - E(&lt;h’,r’,t’&gt;)]<em>+<br>$$<br>其中 $T$ 代表一个三元组的集合；$E(&lt;h,r,t&gt;) = ||h+r-t||</em>{L_n}$是energy function；$&lt;h,r,t&gt;$是G中的一个三元组；$&lt;h’,r’,t’&gt;$代表一个不存在于 $T$ 的三元组，通过随机替换一部分$&lt;h,r,t&gt;$来得到；$\\gamma$ 表示边际距离</p>\n<p>算法的核心是令正例的 h+r-t 趋近于 0，而负例的 h+r-t 趋近于无穷大。整个 TransE 模型的训练过程比较简单，首先对头尾节点以及关系进行初始化，然后每对一个正例取一个负例样本，然后利用 hinge loss function 尽可能使正例和负例分开，最后采用 SGD(Stochastic Gradient Descent) 方法更新参数。</p>\n<p>由TransE又衍生出了许多模型。</p>\n<h4 id=\"1-2-TransH\"><a href=\"#1-2-TransH\" class=\"headerlink\" title=\"1.2 TransH\"></a>1.2 TransH</h4><p>虽然 TransE 模型训练速度快、易于实现，但是它不能够解决多对一和一对多关系的问题。为了解决这个问题并仍然维持较低的复杂度，TransH [^2] 不再严格要求 h+r-l=0，而是只需要保证头结点和尾节点在关系平面上的投影在一条直线上，因此能够得到图中头结点向量正确的表示。</p>\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202332.jpg\">\n\n<p>论文另一个亮点是设计了一种负类抽样的方法，即一对多的时候，给head更多的抽样概率， 同样的多对一的时候，给tail更多抽样概率。</p>\n<h4 id=\"1-3-TransR-CTrans\"><a href=\"#1-3-TransR-CTrans\" class=\"headerlink\" title=\"1.3 TransR/CTrans\"></a>1.3 TransR/CTrans</h4><p>TransE和TransH都假设了实体和关系都在同一个空间内，但由于实体和关系本身的不同，简单地认为它们embedding在同一个空间内是不充分的。因此，TransR[^3]对于<strong>每一个r</strong>，我们设置mapping矩阵$M_r$，使得<br>$$<br>h_{\\perp} + r \\simeq t_{\\perp} \\quad \\text{where } h_{\\perp} = M_{r}\\cdot h, t_{\\perp} = M_{r} \\cdot t<br>$$<br>后面的方法就和TransE较为类似了。</p>\n<h4 id=\"1-4-TransD\"><a href=\"#1-4-TransD\" class=\"headerlink\" title=\"1.4 TransD\"></a>1.4 TransD</h4><p>实际上TransD[^4]是对TransR/CTrans的改进。TransR有以下缺点：</p>\n<ul>\n<li>对于每个r，所有的实体都使用同一个mapping矩阵$M_{r}$，但是实际上对应于一个r的实体有不同的种类、特征，这么做会有一些问题；</li>\n<li>mapping是entity和relation之间的交互过程，mapping矩阵仅由关系决定是不合理的；</li>\n<li>矩阵与向量的运算的计算比较复杂，如果在一个Knowledge graph里有较多的relation，那么就会有大量的参数，以及较高的复杂度，因此导致计算量过大，无法应用到大规模knowledge graph。</li>\n</ul>\n<p>对于任意一个entity或relation，TransD定义两个向量，第一个表示entity或relation的含义，另一个用于把entity投影到relation空间（或者说用于构造mapping矩阵）。因此对于每个entity-relation pair，都有一个动态生成的唯一的mapping矩阵。此外，上述过程没有用到矩阵向量运算，而用向量的运算代替了。</p>\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202326.jpg\">\n\n<p>TransD大大降低计算复杂度，但仍然保持不错的效果。</p>\n<h4 id=\"1-5-TransA\"><a href=\"#1-5-TransA\" class=\"headerlink\" title=\"1.5 TransA\"></a>1.5 TransA</h4><p>前面的模型都是基于欧式距离计算，也就是认为每一维的重要性是相同的。但实际上，有一些维度能较好的区分不同的entity和relation，但也有许多不包含什么有效信息，因此甚至可以被认为是噪音，因此不同维度的重要性显然是不同的。</p>\n<p>TransA[^5] 模型通过引入加权矩阵，赋给每一维度权重。</p>\n<p>结果如下图，欧式距离每一维都是同等重要的，体现为圆形；而TransA体现为椭圆形，显然更符合数据的分布。</p>\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202330.jpg\">\n\n\n\n<h3 id=\"2-ProjE\"><a href=\"#2-ProjE\" class=\"headerlink\" title=\"2. ProjE\"></a>2. ProjE</h3><p>ProjE[^6]的作者就是Open-World Knowledge Graph Completion的作者，两篇文章分别被AAAI2017和AAAI2018收录，可以认为是KGC任务的 state-of-the-art。这里提一下ProjE的思路。</p>\n<h4 id=\"2-1-Model-Architecture\"><a href=\"#2-1-Model-Architecture\" class=\"headerlink\" title=\"2.1 Model Architecture\"></a>2.1 Model Architecture</h4><p>给出&lt;h, r, ?&gt;，已知h、r，要求预测 ? 。通常的做法就是把所有的候选entity都拿来打分，得分最高的就是预测的结果。为了得到这一系列打分，首先我们通过一个运算符来合并h和r，得到一个向量，然后我们把所有候选entity投影到这个向量上，随后运算得到分数。</p>\n<p>现有的模型（包括上面提到的一系列trans）往往通过mapping矩阵来合并entity和relation，这里作者也是这么做，但是他认为目前还不需要考虑各个维度之间的相互作用，因此这个mapping矩阵应该是一个对角矩阵。所以这个合并操作可以被定义为：<br>$$<br>e \\oplus r = D_e e + D_r r + b_c<br>$$<br>其中$D_e$和$D_r$就是两个对角矩阵。</p>\n<p>由此，我们可以定义embedding映射函数：<br>$$<br>h(e, r) = g(W^c f(e \\oplus r) + b_r )<br>$$<br>其中f和g是激活函数(activation function)，我们在后面定义；$W^c$是候选entity构成的矩阵。因此h就是对所有候选entity的打分矩阵。</p>\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202328.jpg\">\n\n<p>上图是ProjE的结构，包括两部分神经网络层，其中上半部分是合并操作，即$e \\oplus r$；下半部分是映射函数，或者说打分函数，即$h(e,r)$。</p>\n<h4 id=\"2-2-Loss-function\"><a href=\"#2-2-Loss-function\" class=\"headerlink\" title=\"2.2 Loss function\"></a>2.2 Loss function</h4><p>我们这里分析方便只看pointwise loss function：<br>$$<br>\\mathcal{L}(e, r, y) = - \\sum_{i\\in{i|y_i=1} } {log(h(e,r)<em>i)} - \\sum</em>{m} {\\mathbb{E}_{j \\sim P_y} log(h(e,r)_j)}<br>$$<br>其中$y$是一个布尔向量，1为正例，0为反例；m个反例。pointwise ProjE 可以被看作是多类分类问题，所以我们选取 f 和 g 分别为 sigmoid 和 tanh 函数。</p>\n<p>代码实现 <a href=\"https://github.com/bxshi/ProjE\">https://github.com/bxshi/ProjE</a></p>\n<h2 id=\"Bibliographies\"><a href=\"#Bibliographies\" class=\"headerlink\" title=\"Bibliographies\"></a>Bibliographies</h2><p>笔记参考：</p>\n<p><a href=\"http://www.infosec-wiki.com/?p=175755\">http://www.infosec-wiki.com/?p=175755</a></p>\n<p><a href=\"https://www.jiqizhixin.com/articles/2017-11-03-5\">https://www.jiqizhixin.com/articles/2017-11-03-5</a></p>\n<p>[^1]: Bordes, A., Usunier, N., Garcia-Duran, A., Weston, J., &amp; Yakhnenko, O. (2013). Translating embeddings for modeling multi-relational data. In <em>Advances in neural information processing systems</em> (pp. 2787-2795).<br>[^2]: Wang, Z., Zhang, J., Feng, J., &amp; Chen, Z. (2014, July). Knowledge Graph Embedding by Translating on Hyperplanes. In <em>AAAI</em> (Vol. 14, pp. 1112-1119).</p>\n<p>[^3]: Lin, Y., Liu, Z., Sun, M., Liu, Y., &amp; Zhu, X. (2015, January). Learning entity and relation embeddings for knowledge graph completion. In <em>AAAI</em> (Vol. 15, pp. 2181-2187).<br>[^4]: Ji, G., He, S., Xu, L., Liu, K., &amp; Zhao, J. (2015). Knowledge graph embedding via dynamic mapping matrix. In <em>Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</em> (Vol. 1, pp. 687-696).<br>[^5]: Xiao, H., Huang, M., Hao, Y., &amp; Zhu, X. (2015). TransA: An adaptive approach for knowledge graph embedding. <em>arXiv preprint arXiv:1509.05490</em>.<br>[^6]: Shi, B., &amp; Weninger, T. (2017, February). ProjE: Embedding Projection for Knowledge Graph Completion. In <em>AAAI</em> (Vol. 17, pp. 1236-1242).</p>\n","site":{"data":{"recommended_posts":{"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"}],"https://LorrinWWW.github.io/posts/complexity/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/hexo-with-latex/":[{"title":"区块链时代的最正确的打赏姿势——在Hexo博客配置Coinhive网页挖矿","permalink":"http://www.davidfnck.com/blockchain/mine-xmr-in-hexo-by-coinhive.html/"}],"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/":[{"title":"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-statistic/":[{"title":"贝叶斯估计 Bayes estimation","permalink":"https://LorrinWWW.github.io/posts/Bayes-estimation/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"}],"https://LorrinWWW.github.io/posts/[2018.2.5]Nested-LSTMs/":[{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"}],"https://LorrinWWW.github.io/posts/datamining-pretreatment/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Several models for knowledge graph representing and completing 几个知识图谱模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/"}],"https://LorrinWWW.github.io/posts/compression/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/Sobolev-space/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/proba-ch5/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch3/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/proba-ch1/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/ML-CNN/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/EDP-basic-models/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/machine-learning/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/datamining-class-pred/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction/":[],"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/":[{"title":"OS notes","permalink":"https://LorrinWWW.github.io/posts/OS-notes/"}],"https://LorrinWWW.github.io/posts/projet-enjeu-plugin-chrome-101/":[{"title":"Chrome油猴脚本","permalink":"http://gubangzhong.cn/2017/06/13/Chrome油猴脚本/"}],"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/":[{"title":"MongoDB, Docker and Python","permalink":"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/":[{"title":"Hands on Scrapy","permalink":"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/OS-notes/":[{"title":"learning OS and building LorriOS","permalink":"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/"}],"https://LorrinWWW.github.io/posts/Bayes-estimation/":[{"title":"Note of statistic","permalink":"https://LorrinWWW.github.io/posts/Note-of-statistic/"}],"https://LorrinWWW.github.io/posts/management-of-the-firm/":[],"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"}],"https://LorrinWWW.github.io/posts/datamining-qualitative-induction/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-finite-element-method/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/QuadTree/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/proba-ch4/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch2/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/":[{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/Note-of-NLP/":[{"title":"Generative Adversarial Network","permalink":"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/"},{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"}],"https://LorrinWWW.github.io/posts/Note-of-datamining/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"数据挖掘-分类与预测","permalink":"https://LorrinWWW.github.io/posts/datamining-class-pred/"},{"title":"数据挖掘-预处理","permalink":"https://LorrinWWW.github.io/posts/datamining-pretreatment/"}],"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/Note-of-probability/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/participe-present-et-gerondif/":[],"https://LorrinWWW.github.io/posts/Hilbert-space/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/proba-ch6/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}]}}},"excerpt":"","more":"<h1 id=\"2018-3-10\"><a href=\"#2018-3-10\" class=\"headerlink\" title=\"2018.3.10\"></a>2018.3.10</h1><h2 id=\"Several-models-for-knowledge-graph-representing-and-completing\"><a href=\"#Several-models-for-knowledge-graph-representing-and-completing\" class=\"headerlink\" title=\"Several models for knowledge graph representing and completing\"></a>Several models for knowledge graph representing and completing</h2><p><strong>摘要</strong>：上次看到的ConMask在开放领域knowledge graph completion有着不错的表现，这次我们不考虑开放领域，介绍几个经典的模型。</p>\n<h3 id=\"1-Series-of-Trans\"><a href=\"#1-Series-of-Trans\" class=\"headerlink\" title=\"1. Series of Trans\"></a>1. Series of Trans</h3><h4 id=\"1-1-TransE\"><a href=\"#1-1-TransE\" class=\"headerlink\" title=\"1.1 TransE\"></a>1.1 TransE</h4><p>TransE [^1] 可能是最为常用也最为基础的方法是一种基于强化学习(RL)的模型. 它有一个简单实用的假设：<br>$$<br>h+r = t<br>$$<br>其中h是head entity的向量，t是tail entity的向量，r是关系向量。</p>\n<p>TransE定义了loss function：<br>$$<br>\\mathcal{L(T)} = \\sum_{&lt;h,r,t&gt;\\in T} [\\gamma + E(&lt;h,r,t&gt;) - E(&lt;h’,r’,t’&gt;)]<em>+<br>$$<br>其中 $T$ 代表一个三元组的集合；$E(&lt;h,r,t&gt;) = ||h+r-t||</em>{L_n}$是energy function；$&lt;h,r,t&gt;$是G中的一个三元组；$&lt;h’,r’,t’&gt;$代表一个不存在于 $T$ 的三元组，通过随机替换一部分$&lt;h,r,t&gt;$来得到；$\\gamma$ 表示边际距离</p>\n<p>算法的核心是令正例的 h+r-t 趋近于 0，而负例的 h+r-t 趋近于无穷大。整个 TransE 模型的训练过程比较简单，首先对头尾节点以及关系进行初始化，然后每对一个正例取一个负例样本，然后利用 hinge loss function 尽可能使正例和负例分开，最后采用 SGD(Stochastic Gradient Descent) 方法更新参数。</p>\n<p>由TransE又衍生出了许多模型。</p>\n<h4 id=\"1-2-TransH\"><a href=\"#1-2-TransH\" class=\"headerlink\" title=\"1.2 TransH\"></a>1.2 TransH</h4><p>虽然 TransE 模型训练速度快、易于实现，但是它不能够解决多对一和一对多关系的问题。为了解决这个问题并仍然维持较低的复杂度，TransH [^2] 不再严格要求 h+r-l=0，而是只需要保证头结点和尾节点在关系平面上的投影在一条直线上，因此能够得到图中头结点向量正确的表示。</p>\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202332.jpg\">\n\n<p>论文另一个亮点是设计了一种负类抽样的方法，即一对多的时候，给head更多的抽样概率， 同样的多对一的时候，给tail更多抽样概率。</p>\n<h4 id=\"1-3-TransR-CTrans\"><a href=\"#1-3-TransR-CTrans\" class=\"headerlink\" title=\"1.3 TransR/CTrans\"></a>1.3 TransR/CTrans</h4><p>TransE和TransH都假设了实体和关系都在同一个空间内，但由于实体和关系本身的不同，简单地认为它们embedding在同一个空间内是不充分的。因此，TransR[^3]对于<strong>每一个r</strong>，我们设置mapping矩阵$M_r$，使得<br>$$<br>h_{\\perp} + r \\simeq t_{\\perp} \\quad \\text{where } h_{\\perp} = M_{r}\\cdot h, t_{\\perp} = M_{r} \\cdot t<br>$$<br>后面的方法就和TransE较为类似了。</p>\n<h4 id=\"1-4-TransD\"><a href=\"#1-4-TransD\" class=\"headerlink\" title=\"1.4 TransD\"></a>1.4 TransD</h4><p>实际上TransD[^4]是对TransR/CTrans的改进。TransR有以下缺点：</p>\n<ul>\n<li>对于每个r，所有的实体都使用同一个mapping矩阵$M_{r}$，但是实际上对应于一个r的实体有不同的种类、特征，这么做会有一些问题；</li>\n<li>mapping是entity和relation之间的交互过程，mapping矩阵仅由关系决定是不合理的；</li>\n<li>矩阵与向量的运算的计算比较复杂，如果在一个Knowledge graph里有较多的relation，那么就会有大量的参数，以及较高的复杂度，因此导致计算量过大，无法应用到大规模knowledge graph。</li>\n</ul>\n<p>对于任意一个entity或relation，TransD定义两个向量，第一个表示entity或relation的含义，另一个用于把entity投影到relation空间（或者说用于构造mapping矩阵）。因此对于每个entity-relation pair，都有一个动态生成的唯一的mapping矩阵。此外，上述过程没有用到矩阵向量运算，而用向量的运算代替了。</p>\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202326.jpg\">\n\n<p>TransD大大降低计算复杂度，但仍然保持不错的效果。</p>\n<h4 id=\"1-5-TransA\"><a href=\"#1-5-TransA\" class=\"headerlink\" title=\"1.5 TransA\"></a>1.5 TransA</h4><p>前面的模型都是基于欧式距离计算，也就是认为每一维的重要性是相同的。但实际上，有一些维度能较好的区分不同的entity和relation，但也有许多不包含什么有效信息，因此甚至可以被认为是噪音，因此不同维度的重要性显然是不同的。</p>\n<p>TransA[^5] 模型通过引入加权矩阵，赋给每一维度权重。</p>\n<p>结果如下图，欧式距离每一维都是同等重要的，体现为圆形；而TransA体现为椭圆形，显然更符合数据的分布。</p>\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202330.jpg\">\n\n\n\n<h3 id=\"2-ProjE\"><a href=\"#2-ProjE\" class=\"headerlink\" title=\"2. ProjE\"></a>2. ProjE</h3><p>ProjE[^6]的作者就是Open-World Knowledge Graph Completion的作者，两篇文章分别被AAAI2017和AAAI2018收录，可以认为是KGC任务的 state-of-the-art。这里提一下ProjE的思路。</p>\n<h4 id=\"2-1-Model-Architecture\"><a href=\"#2-1-Model-Architecture\" class=\"headerlink\" title=\"2.1 Model Architecture\"></a>2.1 Model Architecture</h4><p>给出&lt;h, r, ?&gt;，已知h、r，要求预测 ? 。通常的做法就是把所有的候选entity都拿来打分，得分最高的就是预测的结果。为了得到这一系列打分，首先我们通过一个运算符来合并h和r，得到一个向量，然后我们把所有候选entity投影到这个向量上，随后运算得到分数。</p>\n<p>现有的模型（包括上面提到的一系列trans）往往通过mapping矩阵来合并entity和relation，这里作者也是这么做，但是他认为目前还不需要考虑各个维度之间的相互作用，因此这个mapping矩阵应该是一个对角矩阵。所以这个合并操作可以被定义为：<br>$$<br>e \\oplus r = D_e e + D_r r + b_c<br>$$<br>其中$D_e$和$D_r$就是两个对角矩阵。</p>\n<p>由此，我们可以定义embedding映射函数：<br>$$<br>h(e, r) = g(W^c f(e \\oplus r) + b_r )<br>$$<br>其中f和g是激活函数(activation function)，我们在后面定义；$W^c$是候选entity构成的矩阵。因此h就是对所有候选entity的打分矩阵。</p>\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202328.jpg\">\n\n<p>上图是ProjE的结构，包括两部分神经网络层，其中上半部分是合并操作，即$e \\oplus r$；下半部分是映射函数，或者说打分函数，即$h(e,r)$。</p>\n<h4 id=\"2-2-Loss-function\"><a href=\"#2-2-Loss-function\" class=\"headerlink\" title=\"2.2 Loss function\"></a>2.2 Loss function</h4><p>我们这里分析方便只看pointwise loss function：<br>$$<br>\\mathcal{L}(e, r, y) = - \\sum_{i\\in{i|y_i=1} } {log(h(e,r)<em>i)} - \\sum</em>{m} {\\mathbb{E}_{j \\sim P_y} log(h(e,r)_j)}<br>$$<br>其中$y$是一个布尔向量，1为正例，0为反例；m个反例。pointwise ProjE 可以被看作是多类分类问题，所以我们选取 f 和 g 分别为 sigmoid 和 tanh 函数。</p>\n<p>代码实现 <a href=\"https://github.com/bxshi/ProjE\">https://github.com/bxshi/ProjE</a></p>\n<h2 id=\"Bibliographies\"><a href=\"#Bibliographies\" class=\"headerlink\" title=\"Bibliographies\"></a>Bibliographies</h2><p>笔记参考：</p>\n<p><a href=\"http://www.infosec-wiki.com/?p=175755\">http://www.infosec-wiki.com/?p=175755</a></p>\n<p><a href=\"https://www.jiqizhixin.com/articles/2017-11-03-5\">https://www.jiqizhixin.com/articles/2017-11-03-5</a></p>\n<p>[^1]: Bordes, A., Usunier, N., Garcia-Duran, A., Weston, J., &amp; Yakhnenko, O. (2013). Translating embeddings for modeling multi-relational data. In <em>Advances in neural information processing systems</em> (pp. 2787-2795).<br>[^2]: Wang, Z., Zhang, J., Feng, J., &amp; Chen, Z. (2014, July). Knowledge Graph Embedding by Translating on Hyperplanes. In <em>AAAI</em> (Vol. 14, pp. 1112-1119).</p>\n<p>[^3]: Lin, Y., Liu, Z., Sun, M., Liu, Y., &amp; Zhu, X. (2015, January). Learning entity and relation embeddings for knowledge graph completion. In <em>AAAI</em> (Vol. 15, pp. 2181-2187).<br>[^4]: Ji, G., He, S., Xu, L., Liu, K., &amp; Zhao, J. (2015). Knowledge graph embedding via dynamic mapping matrix. In <em>Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</em> (Vol. 1, pp. 687-696).<br>[^5]: Xiao, H., Huang, M., Hao, Y., &amp; Zhu, X. (2015). TransA: An adaptive approach for knowledge graph embedding. <em>arXiv preprint arXiv:1509.05490</em>.<br>[^6]: Shi, B., &amp; Weninger, T. (2017, February). ProjE: Embedding Projection for Knowledge Graph Completion. In <em>AAAI</em> (Vol. 17, pp. 1236-1242).</p>\n"},{"title":"Event detection 的几个神经网络模型","date":"2018-03-20T00:00:00.000Z","_content":"\n## Event detection 的几个神经网络模型\n\n**摘要：** 根据ace的定义，事件被分为 trigger word 和 attributes，因此 event detection 也可以被认为是 trigger word detection。目前基于神经网络的方法的思路基本大同小异，本文挑选并阐述3篇paper的主要内容，并比较其特点。\n\n### 1. Dual CNN\n\n这篇[^1]主要是对通常的CNN的改进，增加了一层语义层用以感知上下文信息。\n\n整个pipline可以总结为如下图：\n\n![WX20180320-221054@2x](https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-20-WX20180320-221054%402x.png)\n\n1. Text Processing: 数据清洗、分词等，便于后续处理；\n2. Word Vector Initialisation: 初始化词向量，包括加载 pre-trained word embedding 等；\n3. Concept Extraction: 与2.并行运行，这里利用外部工具实现实体的语意概念；\n4. Concept Vector Initialisation: 将实体和实体相关的概念向量化；\n5. Dual-CNN Training: 这一步利用我们提出的 Dual-CNN 训练；\n\n#### Dual-CNN\n\n我们知道CNN可以用来作为分类器，因此也可以被构造为一个事件检测模型，并能够分出类别。在这个模型中，我们增加一层语意层。一般从正常逻辑出发，我们可以增加一个channel来存放entity related embedding，就像我们图像的多个channel一样；但是这要求实体和原来的句子完全对齐，因此作者用两个CNN并行训练。\n\n![WX20180320-224531@2x](https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-20-WX20180320-224531%402x.png)\n\n事实上，这篇文章提出的embedding方法并不能跟原来的句子对齐。我们有一个句子 $D = $ 'Obama attends vigil for Boston Marathon bombing victims.'；分词为 $T_w = $ \\['obama', 'attends', 'vigil', 'for', 'boston', 'marathon', 'bombing', 'victims'\\]；而语意分词将分为$T_s = $ ['obama', 'politician', 'none', 'none', 'none', 'boston', 'location', 'none', 'none', 'none']。我们可以看到语意分词采用了 entity-type 的方法，这导致了$T_w$和$T_s$长度并不相同 ，因此无法把他们并为两个channels。（其实这里我感觉可以把entity和type分别embedding之后级联起来，这样就能够对齐了，不知道这样可不可行？）\n\n最后的结果显示，它能够较好地检测和识别事件类别，对比CNN有一些提升；但是对于颗粒度较细的事件反而有所下降。但是这些结果都好于传统的机器学习方法。\n\n### 2. convolution BiLSTM\n\n文献[^2]其实之前就看过，详细写在2018.1.29的笔记里，这里再简单提一下，模型如下。\n\n![WX20180320-224725@2x](https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-20-WX20180320-224725%402x.png)\n\n简单来说，就是在通常的biLSTM模型下，并行地训练一个CNN模型，其输出和biLSTM的输出的向量进行连接，Output layer接Softmax输出标签的概率分布。创新点在于引入了CNN捕获局部语意信息，也获得不错的效果。本模型也适用于其他sequence labeling任务。\n\n### 3. BiLSTM + CNN\n\n这篇文章[^3]和文章[^2]的思路也很相似，主要的想法是BiLSTM对文本的语意进行编码，后面串联CNN来捕获局部结构信息。\n\n![WX20180320-225956@2x](https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-20-WX20180320-225956%402x.png)\n\n### 4. Conclusion\n\n如果认为是一个文本分类任务，CNN能够很好的完成任务，而且由于它本身的特性训练速度比较快；另一方面可以用RNN来做数据标注任务，仅标注触发词；此外可以利用好biLSTM能够处理长距离前后文信息、CNN着重局部信息的关系等特性，构造不同的变体，对于实际任务可能也有不错的效果。\n\n从结果上来看实际上各种变体效果差距并不大，对特定种类特定体裁可能会有较大的差别；可能更重要的可能是如何构造特征（除了word embedding 之外，还可以考虑entity embedding？entity type？还有词性的embedding？）\n\n## Bibliographies\n\n[^1]: Burel, G., Saif, H., Fernandez, M., & Alani, H. (2017). On semantics and deep learning for event detection in crisis situations.\n\n[^2]: Zeng, Y., Yang, H., Feng, Y., Wang, Z., & Zhao, D. (2016). A convolution BiLSTM neural network model for Chinese event extraction. In *Natural Language Understanding and Intelligent Applications* (pp. 275-287). Springer, Cham.\n[^3]: Feng, X., Huang, L., Tang, D., Ji, H., Qin, B., & Liu, T. (2016). A language-independent neural network for event detection. In *Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)* (Vol. 2, pp. 66-71).","source":"_posts/[2018.3.20]Event-detection.md","raw":"---\ntitle: Event detection 的几个神经网络模型\ndate: 2018-03-20 08:00:00\ncategories: [research]\ntags: [event-detection, neural-network]\n---\n\n## Event detection 的几个神经网络模型\n\n**摘要：** 根据ace的定义，事件被分为 trigger word 和 attributes，因此 event detection 也可以被认为是 trigger word detection。目前基于神经网络的方法的思路基本大同小异，本文挑选并阐述3篇paper的主要内容，并比较其特点。\n\n### 1. Dual CNN\n\n这篇[^1]主要是对通常的CNN的改进，增加了一层语义层用以感知上下文信息。\n\n整个pipline可以总结为如下图：\n\n![WX20180320-221054@2x](https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-20-WX20180320-221054%402x.png)\n\n1. Text Processing: 数据清洗、分词等，便于后续处理；\n2. Word Vector Initialisation: 初始化词向量，包括加载 pre-trained word embedding 等；\n3. Concept Extraction: 与2.并行运行，这里利用外部工具实现实体的语意概念；\n4. Concept Vector Initialisation: 将实体和实体相关的概念向量化；\n5. Dual-CNN Training: 这一步利用我们提出的 Dual-CNN 训练；\n\n#### Dual-CNN\n\n我们知道CNN可以用来作为分类器，因此也可以被构造为一个事件检测模型，并能够分出类别。在这个模型中，我们增加一层语意层。一般从正常逻辑出发，我们可以增加一个channel来存放entity related embedding，就像我们图像的多个channel一样；但是这要求实体和原来的句子完全对齐，因此作者用两个CNN并行训练。\n\n![WX20180320-224531@2x](https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-20-WX20180320-224531%402x.png)\n\n事实上，这篇文章提出的embedding方法并不能跟原来的句子对齐。我们有一个句子 $D = $ 'Obama attends vigil for Boston Marathon bombing victims.'；分词为 $T_w = $ \\['obama', 'attends', 'vigil', 'for', 'boston', 'marathon', 'bombing', 'victims'\\]；而语意分词将分为$T_s = $ ['obama', 'politician', 'none', 'none', 'none', 'boston', 'location', 'none', 'none', 'none']。我们可以看到语意分词采用了 entity-type 的方法，这导致了$T_w$和$T_s$长度并不相同 ，因此无法把他们并为两个channels。（其实这里我感觉可以把entity和type分别embedding之后级联起来，这样就能够对齐了，不知道这样可不可行？）\n\n最后的结果显示，它能够较好地检测和识别事件类别，对比CNN有一些提升；但是对于颗粒度较细的事件反而有所下降。但是这些结果都好于传统的机器学习方法。\n\n### 2. convolution BiLSTM\n\n文献[^2]其实之前就看过，详细写在2018.1.29的笔记里，这里再简单提一下，模型如下。\n\n![WX20180320-224725@2x](https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-20-WX20180320-224725%402x.png)\n\n简单来说，就是在通常的biLSTM模型下，并行地训练一个CNN模型，其输出和biLSTM的输出的向量进行连接，Output layer接Softmax输出标签的概率分布。创新点在于引入了CNN捕获局部语意信息，也获得不错的效果。本模型也适用于其他sequence labeling任务。\n\n### 3. BiLSTM + CNN\n\n这篇文章[^3]和文章[^2]的思路也很相似，主要的想法是BiLSTM对文本的语意进行编码，后面串联CNN来捕获局部结构信息。\n\n![WX20180320-225956@2x](https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-20-WX20180320-225956%402x.png)\n\n### 4. Conclusion\n\n如果认为是一个文本分类任务，CNN能够很好的完成任务，而且由于它本身的特性训练速度比较快；另一方面可以用RNN来做数据标注任务，仅标注触发词；此外可以利用好biLSTM能够处理长距离前后文信息、CNN着重局部信息的关系等特性，构造不同的变体，对于实际任务可能也有不错的效果。\n\n从结果上来看实际上各种变体效果差距并不大，对特定种类特定体裁可能会有较大的差别；可能更重要的可能是如何构造特征（除了word embedding 之外，还可以考虑entity embedding？entity type？还有词性的embedding？）\n\n## Bibliographies\n\n[^1]: Burel, G., Saif, H., Fernandez, M., & Alani, H. (2017). On semantics and deep learning for event detection in crisis situations.\n\n[^2]: Zeng, Y., Yang, H., Feng, Y., Wang, Z., & Zhao, D. (2016). A convolution BiLSTM neural network model for Chinese event extraction. In *Natural Language Understanding and Intelligent Applications* (pp. 275-287). Springer, Cham.\n[^3]: Feng, X., Huang, L., Tang, D., Ji, H., Qin, B., & Liu, T. (2016). A language-independent neural network for event detection. In *Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)* (Vol. 2, pp. 66-71).","slug":"[2018.3.20]Event-detection","published":1,"updated":"2020-11-03T03:26:07.533Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckw4qufps002fgwtl1uppb39d","content":"<h2 id=\"Event-detection-的几个神经网络模型\"><a href=\"#Event-detection-的几个神经网络模型\" class=\"headerlink\" title=\"Event detection 的几个神经网络模型\"></a>Event detection 的几个神经网络模型</h2><p><strong>摘要：</strong> 根据ace的定义，事件被分为 trigger word 和 attributes，因此 event detection 也可以被认为是 trigger word detection。目前基于神经网络的方法的思路基本大同小异，本文挑选并阐述3篇paper的主要内容，并比较其特点。</p>\n<h3 id=\"1-Dual-CNN\"><a href=\"#1-Dual-CNN\" class=\"headerlink\" title=\"1. Dual CNN\"></a>1. Dual CNN</h3><p>这篇[^1]主要是对通常的CNN的改进，增加了一层语义层用以感知上下文信息。</p>\n<p>整个pipline可以总结为如下图：</p>\n<p><img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-20-WX20180320-221054%402x.png\" alt=\"WX20180320-221054@2x\"></p>\n<ol>\n<li>Text Processing: 数据清洗、分词等，便于后续处理；</li>\n<li>Word Vector Initialisation: 初始化词向量，包括加载 pre-trained word embedding 等；</li>\n<li>Concept Extraction: 与2.并行运行，这里利用外部工具实现实体的语意概念；</li>\n<li>Concept Vector Initialisation: 将实体和实体相关的概念向量化；</li>\n<li>Dual-CNN Training: 这一步利用我们提出的 Dual-CNN 训练；</li>\n</ol>\n<h4 id=\"Dual-CNN\"><a href=\"#Dual-CNN\" class=\"headerlink\" title=\"Dual-CNN\"></a>Dual-CNN</h4><p>我们知道CNN可以用来作为分类器，因此也可以被构造为一个事件检测模型，并能够分出类别。在这个模型中，我们增加一层语意层。一般从正常逻辑出发，我们可以增加一个channel来存放entity related embedding，就像我们图像的多个channel一样；但是这要求实体和原来的句子完全对齐，因此作者用两个CNN并行训练。</p>\n<p><img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-20-WX20180320-224531%402x.png\" alt=\"WX20180320-224531@2x\"></p>\n<p>事实上，这篇文章提出的embedding方法并不能跟原来的句子对齐。我们有一个句子 $D = $ ‘Obama attends vigil for Boston Marathon bombing victims.’；分词为 $T_w = $ [‘obama’, ‘attends’, ‘vigil’, ‘for’, ‘boston’, ‘marathon’, ‘bombing’, ‘victims’]；而语意分词将分为$T_s = $ [‘obama’, ‘politician’, ‘none’, ‘none’, ‘none’, ‘boston’, ‘location’, ‘none’, ‘none’, ‘none’]。我们可以看到语意分词采用了 entity-type 的方法，这导致了$T_w$和$T_s$长度并不相同 ，因此无法把他们并为两个channels。（其实这里我感觉可以把entity和type分别embedding之后级联起来，这样就能够对齐了，不知道这样可不可行？）</p>\n<p>最后的结果显示，它能够较好地检测和识别事件类别，对比CNN有一些提升；但是对于颗粒度较细的事件反而有所下降。但是这些结果都好于传统的机器学习方法。</p>\n<h3 id=\"2-convolution-BiLSTM\"><a href=\"#2-convolution-BiLSTM\" class=\"headerlink\" title=\"2. convolution BiLSTM\"></a>2. convolution BiLSTM</h3><p>文献[^2]其实之前就看过，详细写在2018.1.29的笔记里，这里再简单提一下，模型如下。</p>\n<p><img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-20-WX20180320-224725%402x.png\" alt=\"WX20180320-224725@2x\"></p>\n<p>简单来说，就是在通常的biLSTM模型下，并行地训练一个CNN模型，其输出和biLSTM的输出的向量进行连接，Output layer接Softmax输出标签的概率分布。创新点在于引入了CNN捕获局部语意信息，也获得不错的效果。本模型也适用于其他sequence labeling任务。</p>\n<h3 id=\"3-BiLSTM-CNN\"><a href=\"#3-BiLSTM-CNN\" class=\"headerlink\" title=\"3. BiLSTM + CNN\"></a>3. BiLSTM + CNN</h3><p>这篇文章[^3]和文章[^2]的思路也很相似，主要的想法是BiLSTM对文本的语意进行编码，后面串联CNN来捕获局部结构信息。</p>\n<p><img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-20-WX20180320-225956%402x.png\" alt=\"WX20180320-225956@2x\"></p>\n<h3 id=\"4-Conclusion\"><a href=\"#4-Conclusion\" class=\"headerlink\" title=\"4. Conclusion\"></a>4. Conclusion</h3><p>如果认为是一个文本分类任务，CNN能够很好的完成任务，而且由于它本身的特性训练速度比较快；另一方面可以用RNN来做数据标注任务，仅标注触发词；此外可以利用好biLSTM能够处理长距离前后文信息、CNN着重局部信息的关系等特性，构造不同的变体，对于实际任务可能也有不错的效果。</p>\n<p>从结果上来看实际上各种变体效果差距并不大，对特定种类特定体裁可能会有较大的差别；可能更重要的可能是如何构造特征（除了word embedding 之外，还可以考虑entity embedding？entity type？还有词性的embedding？）</p>\n<h2 id=\"Bibliographies\"><a href=\"#Bibliographies\" class=\"headerlink\" title=\"Bibliographies\"></a>Bibliographies</h2><p>[^1]: Burel, G., Saif, H., Fernandez, M., &amp; Alani, H. (2017). On semantics and deep learning for event detection in crisis situations.</p>\n<p>[^2]: Zeng, Y., Yang, H., Feng, Y., Wang, Z., &amp; Zhao, D. (2016). A convolution BiLSTM neural network model for Chinese event extraction. In <em>Natural Language Understanding and Intelligent Applications</em> (pp. 275-287). Springer, Cham.<br>[^3]: Feng, X., Huang, L., Tang, D., Ji, H., Qin, B., &amp; Liu, T. (2016). A language-independent neural network for event detection. In <em>Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</em> (Vol. 2, pp. 66-71).</p>\n","site":{"data":{"recommended_posts":{"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"}],"https://LorrinWWW.github.io/posts/complexity/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/hexo-with-latex/":[{"title":"区块链时代的最正确的打赏姿势——在Hexo博客配置Coinhive网页挖矿","permalink":"http://www.davidfnck.com/blockchain/mine-xmr-in-hexo-by-coinhive.html/"}],"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/":[{"title":"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-statistic/":[{"title":"贝叶斯估计 Bayes estimation","permalink":"https://LorrinWWW.github.io/posts/Bayes-estimation/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"}],"https://LorrinWWW.github.io/posts/[2018.2.5]Nested-LSTMs/":[{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"}],"https://LorrinWWW.github.io/posts/datamining-pretreatment/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Several models for knowledge graph representing and completing 几个知识图谱模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/"}],"https://LorrinWWW.github.io/posts/compression/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/Sobolev-space/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/proba-ch5/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch3/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/proba-ch1/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/ML-CNN/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/EDP-basic-models/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/machine-learning/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/datamining-class-pred/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction/":[],"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/":[{"title":"OS notes","permalink":"https://LorrinWWW.github.io/posts/OS-notes/"}],"https://LorrinWWW.github.io/posts/projet-enjeu-plugin-chrome-101/":[{"title":"Chrome油猴脚本","permalink":"http://gubangzhong.cn/2017/06/13/Chrome油猴脚本/"}],"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/":[{"title":"MongoDB, Docker and Python","permalink":"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/":[{"title":"Hands on Scrapy","permalink":"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/OS-notes/":[{"title":"learning OS and building LorriOS","permalink":"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/"}],"https://LorrinWWW.github.io/posts/Bayes-estimation/":[{"title":"Note of statistic","permalink":"https://LorrinWWW.github.io/posts/Note-of-statistic/"}],"https://LorrinWWW.github.io/posts/management-of-the-firm/":[],"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"}],"https://LorrinWWW.github.io/posts/datamining-qualitative-induction/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-finite-element-method/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/QuadTree/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/proba-ch4/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch2/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/":[{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/Note-of-NLP/":[{"title":"Generative Adversarial Network","permalink":"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/"},{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"}],"https://LorrinWWW.github.io/posts/Note-of-datamining/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"数据挖掘-分类与预测","permalink":"https://LorrinWWW.github.io/posts/datamining-class-pred/"},{"title":"数据挖掘-预处理","permalink":"https://LorrinWWW.github.io/posts/datamining-pretreatment/"}],"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/Note-of-probability/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/participe-present-et-gerondif/":[],"https://LorrinWWW.github.io/posts/Hilbert-space/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/proba-ch6/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}]}}},"excerpt":"","more":"<h2 id=\"Event-detection-的几个神经网络模型\"><a href=\"#Event-detection-的几个神经网络模型\" class=\"headerlink\" title=\"Event detection 的几个神经网络模型\"></a>Event detection 的几个神经网络模型</h2><p><strong>摘要：</strong> 根据ace的定义，事件被分为 trigger word 和 attributes，因此 event detection 也可以被认为是 trigger word detection。目前基于神经网络的方法的思路基本大同小异，本文挑选并阐述3篇paper的主要内容，并比较其特点。</p>\n<h3 id=\"1-Dual-CNN\"><a href=\"#1-Dual-CNN\" class=\"headerlink\" title=\"1. Dual CNN\"></a>1. Dual CNN</h3><p>这篇[^1]主要是对通常的CNN的改进，增加了一层语义层用以感知上下文信息。</p>\n<p>整个pipline可以总结为如下图：</p>\n<p><img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-20-WX20180320-221054%402x.png\" alt=\"WX20180320-221054@2x\"></p>\n<ol>\n<li>Text Processing: 数据清洗、分词等，便于后续处理；</li>\n<li>Word Vector Initialisation: 初始化词向量，包括加载 pre-trained word embedding 等；</li>\n<li>Concept Extraction: 与2.并行运行，这里利用外部工具实现实体的语意概念；</li>\n<li>Concept Vector Initialisation: 将实体和实体相关的概念向量化；</li>\n<li>Dual-CNN Training: 这一步利用我们提出的 Dual-CNN 训练；</li>\n</ol>\n<h4 id=\"Dual-CNN\"><a href=\"#Dual-CNN\" class=\"headerlink\" title=\"Dual-CNN\"></a>Dual-CNN</h4><p>我们知道CNN可以用来作为分类器，因此也可以被构造为一个事件检测模型，并能够分出类别。在这个模型中，我们增加一层语意层。一般从正常逻辑出发，我们可以增加一个channel来存放entity related embedding，就像我们图像的多个channel一样；但是这要求实体和原来的句子完全对齐，因此作者用两个CNN并行训练。</p>\n<p><img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-20-WX20180320-224531%402x.png\" alt=\"WX20180320-224531@2x\"></p>\n<p>事实上，这篇文章提出的embedding方法并不能跟原来的句子对齐。我们有一个句子 $D = $ ‘Obama attends vigil for Boston Marathon bombing victims.’；分词为 $T_w = $ [‘obama’, ‘attends’, ‘vigil’, ‘for’, ‘boston’, ‘marathon’, ‘bombing’, ‘victims’]；而语意分词将分为$T_s = $ [‘obama’, ‘politician’, ‘none’, ‘none’, ‘none’, ‘boston’, ‘location’, ‘none’, ‘none’, ‘none’]。我们可以看到语意分词采用了 entity-type 的方法，这导致了$T_w$和$T_s$长度并不相同 ，因此无法把他们并为两个channels。（其实这里我感觉可以把entity和type分别embedding之后级联起来，这样就能够对齐了，不知道这样可不可行？）</p>\n<p>最后的结果显示，它能够较好地检测和识别事件类别，对比CNN有一些提升；但是对于颗粒度较细的事件反而有所下降。但是这些结果都好于传统的机器学习方法。</p>\n<h3 id=\"2-convolution-BiLSTM\"><a href=\"#2-convolution-BiLSTM\" class=\"headerlink\" title=\"2. convolution BiLSTM\"></a>2. convolution BiLSTM</h3><p>文献[^2]其实之前就看过，详细写在2018.1.29的笔记里，这里再简单提一下，模型如下。</p>\n<p><img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-20-WX20180320-224725%402x.png\" alt=\"WX20180320-224725@2x\"></p>\n<p>简单来说，就是在通常的biLSTM模型下，并行地训练一个CNN模型，其输出和biLSTM的输出的向量进行连接，Output layer接Softmax输出标签的概率分布。创新点在于引入了CNN捕获局部语意信息，也获得不错的效果。本模型也适用于其他sequence labeling任务。</p>\n<h3 id=\"3-BiLSTM-CNN\"><a href=\"#3-BiLSTM-CNN\" class=\"headerlink\" title=\"3. BiLSTM + CNN\"></a>3. BiLSTM + CNN</h3><p>这篇文章[^3]和文章[^2]的思路也很相似，主要的想法是BiLSTM对文本的语意进行编码，后面串联CNN来捕获局部结构信息。</p>\n<p><img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-20-WX20180320-225956%402x.png\" alt=\"WX20180320-225956@2x\"></p>\n<h3 id=\"4-Conclusion\"><a href=\"#4-Conclusion\" class=\"headerlink\" title=\"4. Conclusion\"></a>4. Conclusion</h3><p>如果认为是一个文本分类任务，CNN能够很好的完成任务，而且由于它本身的特性训练速度比较快；另一方面可以用RNN来做数据标注任务，仅标注触发词；此外可以利用好biLSTM能够处理长距离前后文信息、CNN着重局部信息的关系等特性，构造不同的变体，对于实际任务可能也有不错的效果。</p>\n<p>从结果上来看实际上各种变体效果差距并不大，对特定种类特定体裁可能会有较大的差别；可能更重要的可能是如何构造特征（除了word embedding 之外，还可以考虑entity embedding？entity type？还有词性的embedding？）</p>\n<h2 id=\"Bibliographies\"><a href=\"#Bibliographies\" class=\"headerlink\" title=\"Bibliographies\"></a>Bibliographies</h2><p>[^1]: Burel, G., Saif, H., Fernandez, M., &amp; Alani, H. (2017). On semantics and deep learning for event detection in crisis situations.</p>\n<p>[^2]: Zeng, Y., Yang, H., Feng, Y., Wang, Z., &amp; Zhao, D. (2016). A convolution BiLSTM neural network model for Chinese event extraction. In <em>Natural Language Understanding and Intelligent Applications</em> (pp. 275-287). Springer, Cham.<br>[^3]: Feng, X., Huang, L., Tang, D., Ji, H., Qin, B., &amp; Liu, T. (2016). A language-independent neural network for event detection. In <em>Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</em> (Vol. 2, pp. 66-71).</p>\n"},{"title":"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记","date":"2018-04-14T00:00:00.000Z","_content":"\n## Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记\n\n**摘要**：现有关系分类方法依赖远程监督(distant supervision)，它假定提到实体对的句子都描述了这个实体对的关系。这样的方法一般在句子集合进行分类，不能识别关系和句子之间的映射，并且很大程度上受到标签噪音问题的影响。在这篇论文[^1]中，作者提出了一个从有噪声多数据的句子层次的关系分类模型。该模型有两个模块：一个实例选择器和一个关系分类器。实例选择器通过增强学习选择高质量的句子，并将选定的句子输入到关系分类器中，关系分类器进行句子级预测，并向实例选择器提供奖励。这两个模块共同训练以优化实例选择和关系分类过程。实验结果表明，我们的模型可以有效地处理数据中的噪音，并在句子级别获得更好的关系分类性能。\n\n## 1. Introduction\n\n关系抽取是nlp领域中一个非常重要的任务，尤其在知识图谱构建等任务上。相关的工作可以参考我之前写的笔记，主要还是分为两种：传统的手工特征方法，和深度神经网络。\n\n为了获得更大量的训练数据集，半监督、远程监督，甚至无监督模型被提出。半监督模型对一开始的少量数据要求较高，容易产生较大的偏差；无监督学习目前还没有比较成熟的解决方案。\n\n这里主要提一下远程监督模型。远程监督模型有一个很强的假设：如果两个实体在给定的知识库中有一种关系，则包含这两个实体的所有句子都会提及该关系，实际上当然会有很大问题，会带来很多标注错误的信息。有一些解决方法就是转化为bag-level的关系标注。一个bag包含提及相同实体对的句子，但有可能描述不同的关系，如下图。\n\n![X20180414-152317@2](https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-04-14-WX20180414-152317%402x.png)\n\n不过实际上还是会有问题：1. 不能处理句子级别的关系分类；2. 如果一个bag里的句子都不含知识库中的关系，即都是噪声，这样对性能会有很大影响。\n\n为解决上述的两个缺陷，作者提出了实例选取器，并将其定义为一个强化学习任务。它有两个特征：1. 句子选择是一个反复试错的过程，需要从分类器中得到选取句子质量的反馈；2. 反馈在挑选结束后得到，因此是滞后的。这两点非常满足强化学习的特点。\n\n## 2. Methodology\n\n作者提出了一个新的关系分类框架，它可以从噪音数据中选择正确的句子用于更好的关系分类。 所提出的框架可以从清理的数据中预测句子级别的关系，而不是在bag级别。句子级别的预测对需要理解句子的任务（如QA和语义分析）更加友好。\n我们的框架包含两个关键模块：从噪声数据中选择正确句子的实例选择器，以及预测关系并使用清理数据更新其参数的关系分类器。 这两个模块相互作用共同训练。\n\n### Problem definition\n\n我们定义两个子任务：实例选择和一个关系分类。\n\n我们定义实例选择问题：给定一组(sentence, relation label)，如$X = \\{(x_1,r_1),(x_2,r_2),…,(x_n,r_n)\\}$，其中$x_i$是与两个实体$(e_{1i},e_{2i})$相关的句子，$r_i$是由远程监督产生的关系标签。我们的目标是确定哪个句子真正描述了这种关系，并且应该被选作训练实例。\n\n关系分类问题表述如下：给定一个句子$x_i$和所提到的实体对$(h_i,t_i)$，目标是预测$x_i$中的语义关系$r_i$。模型估计概率：$p_{\\Phi}(r_i | x_i,h_i,t_i)$。\n\n### Overview\n\n![X20180414-165503@2](https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-04-14-WX20180414-165503%402x.png)\n\n### Instance Selector\n\n这里的Instance Selector被当作强化学习问题来处理，因此policy的更新有滞后性，作者为了加快更新速度，把所有句子实例$X=\\{x_1, …,x_n\\}$分为N个bag $B = \\{B^1, B^2, …, B^N\\}$，每一个$B^k$都包含同一个实体对，且标注关系都为$r^k$。\n\n- **State**：编码以下信息：1) 当前句子的向量表示，从CNN的非线性层获得用于关系分类的当前句子的向量表示; 2) 被选的句子集合的表示，它是所有选择句子的向量表示的平均值; 3) 一个句子中实体对的向量表示，从预先训练的知识图谱embedding中获得。\n\n- **Action**：定义action $a_i \\in \\{0,1\\}$ 表示是否选取第i个句子，$a_i$的取值由policy function得到，定义如下：\n  $$\n  \\pi_{\\Theta}(s_i,a_i) = a_i \\sigma(W * F(s_i) + b) + (1 - a_i)(1 - \\sigma(W * F(s_i)+b))\n  $$\n\n- **Reward**：reward function是所选句子效用的指标。对于某些bag $B = \\{x_1,... ,x_{| B |}\\}$，我们为每个句子选择一个action，以确定是否应该选择当前句子。我们假设该模型在完成所有选择时具有最终奖励。 因此，我们只在终端状态$s_{| B | +1}$收到延迟奖励，其他的奖励为零。 reward是基于CNN的分类反馈得到。\n\n### Relation Classifier\n\n这里用的分类模型比较简单，一个经典的CNN。\n\n输入层可以被分为两部分：\n\n1. word embedding\n2. position embedding：两个固定长度的向量，表示该词分别到两个实体店距离。\n\n## 3. Analysis\n\n作者用的数据集是New York Times，作者随机挑选300句子人工标注，再对其做评测。对比的baseline包括CNN、CNN+Max(每个bag选一个正确的句子)、CNN+ATT。最后结果上看出，本文的CNN+RL模型取得了最好的结果，这表明强化学习对于该任务是行之有效的；并且句子层次的模型在评测中普遍好于bag模型。\n\n强化学习是目前比较火热的技术，它在nlp相关任务的应用仍在探索中，但是最近的论文确实有很多都在讨论它，并且也做到了不错的效果。希望从这篇文章为入口，开始了解强化学习及其在nlp上的应用。\n\n这篇文章中，强化学习主要是用于选择噪声数据，用以减少数据集的偏差等。但是我们相信强化学习能做的不仅如此，事实上最近的顶会还是有一些相关的工作的，可以放到以后再看。\n\n## Bibliography\n\n本文的c++实现：https://github.com/JuneFeng/RelationClassification-RL\n\n[^1]: Feng, J., Huang, M., Zhao, L., Yang, Y., & Zhu, X. (2018). Reinforcement Learning for Relation Classification from Noisy Data.","source":"_posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data.md","raw":"---\ntitle: Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记\ndate: 2018-04-14 08:00:00\ncategories: [research]\ntags: [relation-classification, relation-extraction, reinforcement-learning]\n---\n\n## Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记\n\n**摘要**：现有关系分类方法依赖远程监督(distant supervision)，它假定提到实体对的句子都描述了这个实体对的关系。这样的方法一般在句子集合进行分类，不能识别关系和句子之间的映射，并且很大程度上受到标签噪音问题的影响。在这篇论文[^1]中，作者提出了一个从有噪声多数据的句子层次的关系分类模型。该模型有两个模块：一个实例选择器和一个关系分类器。实例选择器通过增强学习选择高质量的句子，并将选定的句子输入到关系分类器中，关系分类器进行句子级预测，并向实例选择器提供奖励。这两个模块共同训练以优化实例选择和关系分类过程。实验结果表明，我们的模型可以有效地处理数据中的噪音，并在句子级别获得更好的关系分类性能。\n\n## 1. Introduction\n\n关系抽取是nlp领域中一个非常重要的任务，尤其在知识图谱构建等任务上。相关的工作可以参考我之前写的笔记，主要还是分为两种：传统的手工特征方法，和深度神经网络。\n\n为了获得更大量的训练数据集，半监督、远程监督，甚至无监督模型被提出。半监督模型对一开始的少量数据要求较高，容易产生较大的偏差；无监督学习目前还没有比较成熟的解决方案。\n\n这里主要提一下远程监督模型。远程监督模型有一个很强的假设：如果两个实体在给定的知识库中有一种关系，则包含这两个实体的所有句子都会提及该关系，实际上当然会有很大问题，会带来很多标注错误的信息。有一些解决方法就是转化为bag-level的关系标注。一个bag包含提及相同实体对的句子，但有可能描述不同的关系，如下图。\n\n![X20180414-152317@2](https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-04-14-WX20180414-152317%402x.png)\n\n不过实际上还是会有问题：1. 不能处理句子级别的关系分类；2. 如果一个bag里的句子都不含知识库中的关系，即都是噪声，这样对性能会有很大影响。\n\n为解决上述的两个缺陷，作者提出了实例选取器，并将其定义为一个强化学习任务。它有两个特征：1. 句子选择是一个反复试错的过程，需要从分类器中得到选取句子质量的反馈；2. 反馈在挑选结束后得到，因此是滞后的。这两点非常满足强化学习的特点。\n\n## 2. Methodology\n\n作者提出了一个新的关系分类框架，它可以从噪音数据中选择正确的句子用于更好的关系分类。 所提出的框架可以从清理的数据中预测句子级别的关系，而不是在bag级别。句子级别的预测对需要理解句子的任务（如QA和语义分析）更加友好。\n我们的框架包含两个关键模块：从噪声数据中选择正确句子的实例选择器，以及预测关系并使用清理数据更新其参数的关系分类器。 这两个模块相互作用共同训练。\n\n### Problem definition\n\n我们定义两个子任务：实例选择和一个关系分类。\n\n我们定义实例选择问题：给定一组(sentence, relation label)，如$X = \\{(x_1,r_1),(x_2,r_2),…,(x_n,r_n)\\}$，其中$x_i$是与两个实体$(e_{1i},e_{2i})$相关的句子，$r_i$是由远程监督产生的关系标签。我们的目标是确定哪个句子真正描述了这种关系，并且应该被选作训练实例。\n\n关系分类问题表述如下：给定一个句子$x_i$和所提到的实体对$(h_i,t_i)$，目标是预测$x_i$中的语义关系$r_i$。模型估计概率：$p_{\\Phi}(r_i | x_i,h_i,t_i)$。\n\n### Overview\n\n![X20180414-165503@2](https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-04-14-WX20180414-165503%402x.png)\n\n### Instance Selector\n\n这里的Instance Selector被当作强化学习问题来处理，因此policy的更新有滞后性，作者为了加快更新速度，把所有句子实例$X=\\{x_1, …,x_n\\}$分为N个bag $B = \\{B^1, B^2, …, B^N\\}$，每一个$B^k$都包含同一个实体对，且标注关系都为$r^k$。\n\n- **State**：编码以下信息：1) 当前句子的向量表示，从CNN的非线性层获得用于关系分类的当前句子的向量表示; 2) 被选的句子集合的表示，它是所有选择句子的向量表示的平均值; 3) 一个句子中实体对的向量表示，从预先训练的知识图谱embedding中获得。\n\n- **Action**：定义action $a_i \\in \\{0,1\\}$ 表示是否选取第i个句子，$a_i$的取值由policy function得到，定义如下：\n  $$\n  \\pi_{\\Theta}(s_i,a_i) = a_i \\sigma(W * F(s_i) + b) + (1 - a_i)(1 - \\sigma(W * F(s_i)+b))\n  $$\n\n- **Reward**：reward function是所选句子效用的指标。对于某些bag $B = \\{x_1,... ,x_{| B |}\\}$，我们为每个句子选择一个action，以确定是否应该选择当前句子。我们假设该模型在完成所有选择时具有最终奖励。 因此，我们只在终端状态$s_{| B | +1}$收到延迟奖励，其他的奖励为零。 reward是基于CNN的分类反馈得到。\n\n### Relation Classifier\n\n这里用的分类模型比较简单，一个经典的CNN。\n\n输入层可以被分为两部分：\n\n1. word embedding\n2. position embedding：两个固定长度的向量，表示该词分别到两个实体店距离。\n\n## 3. Analysis\n\n作者用的数据集是New York Times，作者随机挑选300句子人工标注，再对其做评测。对比的baseline包括CNN、CNN+Max(每个bag选一个正确的句子)、CNN+ATT。最后结果上看出，本文的CNN+RL模型取得了最好的结果，这表明强化学习对于该任务是行之有效的；并且句子层次的模型在评测中普遍好于bag模型。\n\n强化学习是目前比较火热的技术，它在nlp相关任务的应用仍在探索中，但是最近的论文确实有很多都在讨论它，并且也做到了不错的效果。希望从这篇文章为入口，开始了解强化学习及其在nlp上的应用。\n\n这篇文章中，强化学习主要是用于选择噪声数据，用以减少数据集的偏差等。但是我们相信强化学习能做的不仅如此，事实上最近的顶会还是有一些相关的工作的，可以放到以后再看。\n\n## Bibliography\n\n本文的c++实现：https://github.com/JuneFeng/RelationClassification-RL\n\n[^1]: Feng, J., Huang, M., Zhao, L., Yang, Y., & Zhu, X. (2018). Reinforcement Learning for Relation Classification from Noisy Data.","slug":"[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data","published":1,"updated":"2020-11-03T03:26:07.006Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckw4qufpt002jgwtl6mih547q","content":"<h2 id=\"Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data-阅读笔记\"><a href=\"#Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data-阅读笔记\" class=\"headerlink\" title=\"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记\"></a>Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记</h2><p><strong>摘要</strong>：现有关系分类方法依赖远程监督(distant supervision)，它假定提到实体对的句子都描述了这个实体对的关系。这样的方法一般在句子集合进行分类，不能识别关系和句子之间的映射，并且很大程度上受到标签噪音问题的影响。在这篇论文[^1]中，作者提出了一个从有噪声多数据的句子层次的关系分类模型。该模型有两个模块：一个实例选择器和一个关系分类器。实例选择器通过增强学习选择高质量的句子，并将选定的句子输入到关系分类器中，关系分类器进行句子级预测，并向实例选择器提供奖励。这两个模块共同训练以优化实例选择和关系分类过程。实验结果表明，我们的模型可以有效地处理数据中的噪音，并在句子级别获得更好的关系分类性能。</p>\n<h2 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1. Introduction\"></a>1. Introduction</h2><p>关系抽取是nlp领域中一个非常重要的任务，尤其在知识图谱构建等任务上。相关的工作可以参考我之前写的笔记，主要还是分为两种：传统的手工特征方法，和深度神经网络。</p>\n<p>为了获得更大量的训练数据集，半监督、远程监督，甚至无监督模型被提出。半监督模型对一开始的少量数据要求较高，容易产生较大的偏差；无监督学习目前还没有比较成熟的解决方案。</p>\n<p>这里主要提一下远程监督模型。远程监督模型有一个很强的假设：如果两个实体在给定的知识库中有一种关系，则包含这两个实体的所有句子都会提及该关系，实际上当然会有很大问题，会带来很多标注错误的信息。有一些解决方法就是转化为bag-level的关系标注。一个bag包含提及相同实体对的句子，但有可能描述不同的关系，如下图。</p>\n<p><img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-04-14-WX20180414-152317%402x.png\" alt=\"X20180414-152317@2\"></p>\n<p>不过实际上还是会有问题：1. 不能处理句子级别的关系分类；2. 如果一个bag里的句子都不含知识库中的关系，即都是噪声，这样对性能会有很大影响。</p>\n<p>为解决上述的两个缺陷，作者提出了实例选取器，并将其定义为一个强化学习任务。它有两个特征：1. 句子选择是一个反复试错的过程，需要从分类器中得到选取句子质量的反馈；2. 反馈在挑选结束后得到，因此是滞后的。这两点非常满足强化学习的特点。</p>\n<h2 id=\"2-Methodology\"><a href=\"#2-Methodology\" class=\"headerlink\" title=\"2. Methodology\"></a>2. Methodology</h2><p>作者提出了一个新的关系分类框架，它可以从噪音数据中选择正确的句子用于更好的关系分类。 所提出的框架可以从清理的数据中预测句子级别的关系，而不是在bag级别。句子级别的预测对需要理解句子的任务（如QA和语义分析）更加友好。<br>我们的框架包含两个关键模块：从噪声数据中选择正确句子的实例选择器，以及预测关系并使用清理数据更新其参数的关系分类器。 这两个模块相互作用共同训练。</p>\n<h3 id=\"Problem-definition\"><a href=\"#Problem-definition\" class=\"headerlink\" title=\"Problem definition\"></a>Problem definition</h3><p>我们定义两个子任务：实例选择和一个关系分类。</p>\n<p>我们定义实例选择问题：给定一组(sentence, relation label)，如$X = {(x_1,r_1),(x_2,r_2),…,(x_n,r_n)}$，其中$x_i$是与两个实体$(e_{1i},e_{2i})$相关的句子，$r_i$是由远程监督产生的关系标签。我们的目标是确定哪个句子真正描述了这种关系，并且应该被选作训练实例。</p>\n<p>关系分类问题表述如下：给定一个句子$x_i$和所提到的实体对$(h_i,t_i)$，目标是预测$x_i$中的语义关系$r_i$。模型估计概率：$p_{\\Phi}(r_i | x_i,h_i,t_i)$。</p>\n<h3 id=\"Overview\"><a href=\"#Overview\" class=\"headerlink\" title=\"Overview\"></a>Overview</h3><p><img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-04-14-WX20180414-165503%402x.png\" alt=\"X20180414-165503@2\"></p>\n<h3 id=\"Instance-Selector\"><a href=\"#Instance-Selector\" class=\"headerlink\" title=\"Instance Selector\"></a>Instance Selector</h3><p>这里的Instance Selector被当作强化学习问题来处理，因此policy的更新有滞后性，作者为了加快更新速度，把所有句子实例$X={x_1, …,x_n}$分为N个bag $B = {B^1, B^2, …, B^N}$，每一个$B^k$都包含同一个实体对，且标注关系都为$r^k$。</p>\n<ul>\n<li><p><strong>State</strong>：编码以下信息：1) 当前句子的向量表示，从CNN的非线性层获得用于关系分类的当前句子的向量表示; 2) 被选的句子集合的表示，它是所有选择句子的向量表示的平均值; 3) 一个句子中实体对的向量表示，从预先训练的知识图谱embedding中获得。</p>\n</li>\n<li><p><strong>Action</strong>：定义action $a_i \\in {0,1}$ 表示是否选取第i个句子，$a_i$的取值由policy function得到，定义如下：<br>$$<br>\\pi_{\\Theta}(s_i,a_i) = a_i \\sigma(W * F(s_i) + b) + (1 - a_i)(1 - \\sigma(W * F(s_i)+b))<br>$$</p>\n</li>\n<li><p><strong>Reward</strong>：reward function是所选句子效用的指标。对于某些bag $B = {x_1,… ,x_{| B |}}$，我们为每个句子选择一个action，以确定是否应该选择当前句子。我们假设该模型在完成所有选择时具有最终奖励。 因此，我们只在终端状态$s_{| B | +1}$收到延迟奖励，其他的奖励为零。 reward是基于CNN的分类反馈得到。</p>\n</li>\n</ul>\n<h3 id=\"Relation-Classifier\"><a href=\"#Relation-Classifier\" class=\"headerlink\" title=\"Relation Classifier\"></a>Relation Classifier</h3><p>这里用的分类模型比较简单，一个经典的CNN。</p>\n<p>输入层可以被分为两部分：</p>\n<ol>\n<li>word embedding</li>\n<li>position embedding：两个固定长度的向量，表示该词分别到两个实体店距离。</li>\n</ol>\n<h2 id=\"3-Analysis\"><a href=\"#3-Analysis\" class=\"headerlink\" title=\"3. Analysis\"></a>3. Analysis</h2><p>作者用的数据集是New York Times，作者随机挑选300句子人工标注，再对其做评测。对比的baseline包括CNN、CNN+Max(每个bag选一个正确的句子)、CNN+ATT。最后结果上看出，本文的CNN+RL模型取得了最好的结果，这表明强化学习对于该任务是行之有效的；并且句子层次的模型在评测中普遍好于bag模型。</p>\n<p>强化学习是目前比较火热的技术，它在nlp相关任务的应用仍在探索中，但是最近的论文确实有很多都在讨论它，并且也做到了不错的效果。希望从这篇文章为入口，开始了解强化学习及其在nlp上的应用。</p>\n<p>这篇文章中，强化学习主要是用于选择噪声数据，用以减少数据集的偏差等。但是我们相信强化学习能做的不仅如此，事实上最近的顶会还是有一些相关的工作的，可以放到以后再看。</p>\n<h2 id=\"Bibliography\"><a href=\"#Bibliography\" class=\"headerlink\" title=\"Bibliography\"></a>Bibliography</h2><p>本文的c++实现：<a href=\"https://github.com/JuneFeng/RelationClassification-RL\">https://github.com/JuneFeng/RelationClassification-RL</a></p>\n<p>[^1]: Feng, J., Huang, M., Zhao, L., Yang, Y., &amp; Zhu, X. (2018). Reinforcement Learning for Relation Classification from Noisy Data.</p>\n","site":{"data":{"recommended_posts":{"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"}],"https://LorrinWWW.github.io/posts/complexity/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/hexo-with-latex/":[{"title":"区块链时代的最正确的打赏姿势——在Hexo博客配置Coinhive网页挖矿","permalink":"http://www.davidfnck.com/blockchain/mine-xmr-in-hexo-by-coinhive.html/"}],"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/":[{"title":"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-statistic/":[{"title":"贝叶斯估计 Bayes estimation","permalink":"https://LorrinWWW.github.io/posts/Bayes-estimation/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"}],"https://LorrinWWW.github.io/posts/[2018.2.5]Nested-LSTMs/":[{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"}],"https://LorrinWWW.github.io/posts/datamining-pretreatment/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Several models for knowledge graph representing and completing 几个知识图谱模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/"}],"https://LorrinWWW.github.io/posts/compression/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/Sobolev-space/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/proba-ch5/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch3/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/proba-ch1/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/ML-CNN/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/EDP-basic-models/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/machine-learning/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/datamining-class-pred/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction/":[],"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/":[{"title":"OS notes","permalink":"https://LorrinWWW.github.io/posts/OS-notes/"}],"https://LorrinWWW.github.io/posts/projet-enjeu-plugin-chrome-101/":[{"title":"Chrome油猴脚本","permalink":"http://gubangzhong.cn/2017/06/13/Chrome油猴脚本/"}],"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/":[{"title":"MongoDB, Docker and Python","permalink":"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/":[{"title":"Hands on Scrapy","permalink":"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/OS-notes/":[{"title":"learning OS and building LorriOS","permalink":"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/"}],"https://LorrinWWW.github.io/posts/Bayes-estimation/":[{"title":"Note of statistic","permalink":"https://LorrinWWW.github.io/posts/Note-of-statistic/"}],"https://LorrinWWW.github.io/posts/management-of-the-firm/":[],"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"}],"https://LorrinWWW.github.io/posts/datamining-qualitative-induction/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-finite-element-method/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/QuadTree/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/proba-ch4/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch2/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/":[{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/Note-of-NLP/":[{"title":"Generative Adversarial Network","permalink":"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/"},{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"}],"https://LorrinWWW.github.io/posts/Note-of-datamining/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"数据挖掘-分类与预测","permalink":"https://LorrinWWW.github.io/posts/datamining-class-pred/"},{"title":"数据挖掘-预处理","permalink":"https://LorrinWWW.github.io/posts/datamining-pretreatment/"}],"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/Note-of-probability/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/participe-present-et-gerondif/":[],"https://LorrinWWW.github.io/posts/Hilbert-space/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/proba-ch6/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}]}}},"excerpt":"","more":"<h2 id=\"Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data-阅读笔记\"><a href=\"#Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data-阅读笔记\" class=\"headerlink\" title=\"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记\"></a>Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记</h2><p><strong>摘要</strong>：现有关系分类方法依赖远程监督(distant supervision)，它假定提到实体对的句子都描述了这个实体对的关系。这样的方法一般在句子集合进行分类，不能识别关系和句子之间的映射，并且很大程度上受到标签噪音问题的影响。在这篇论文[^1]中，作者提出了一个从有噪声多数据的句子层次的关系分类模型。该模型有两个模块：一个实例选择器和一个关系分类器。实例选择器通过增强学习选择高质量的句子，并将选定的句子输入到关系分类器中，关系分类器进行句子级预测，并向实例选择器提供奖励。这两个模块共同训练以优化实例选择和关系分类过程。实验结果表明，我们的模型可以有效地处理数据中的噪音，并在句子级别获得更好的关系分类性能。</p>\n<h2 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1. Introduction\"></a>1. Introduction</h2><p>关系抽取是nlp领域中一个非常重要的任务，尤其在知识图谱构建等任务上。相关的工作可以参考我之前写的笔记，主要还是分为两种：传统的手工特征方法，和深度神经网络。</p>\n<p>为了获得更大量的训练数据集，半监督、远程监督，甚至无监督模型被提出。半监督模型对一开始的少量数据要求较高，容易产生较大的偏差；无监督学习目前还没有比较成熟的解决方案。</p>\n<p>这里主要提一下远程监督模型。远程监督模型有一个很强的假设：如果两个实体在给定的知识库中有一种关系，则包含这两个实体的所有句子都会提及该关系，实际上当然会有很大问题，会带来很多标注错误的信息。有一些解决方法就是转化为bag-level的关系标注。一个bag包含提及相同实体对的句子，但有可能描述不同的关系，如下图。</p>\n<p><img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-04-14-WX20180414-152317%402x.png\" alt=\"X20180414-152317@2\"></p>\n<p>不过实际上还是会有问题：1. 不能处理句子级别的关系分类；2. 如果一个bag里的句子都不含知识库中的关系，即都是噪声，这样对性能会有很大影响。</p>\n<p>为解决上述的两个缺陷，作者提出了实例选取器，并将其定义为一个强化学习任务。它有两个特征：1. 句子选择是一个反复试错的过程，需要从分类器中得到选取句子质量的反馈；2. 反馈在挑选结束后得到，因此是滞后的。这两点非常满足强化学习的特点。</p>\n<h2 id=\"2-Methodology\"><a href=\"#2-Methodology\" class=\"headerlink\" title=\"2. Methodology\"></a>2. Methodology</h2><p>作者提出了一个新的关系分类框架，它可以从噪音数据中选择正确的句子用于更好的关系分类。 所提出的框架可以从清理的数据中预测句子级别的关系，而不是在bag级别。句子级别的预测对需要理解句子的任务（如QA和语义分析）更加友好。<br>我们的框架包含两个关键模块：从噪声数据中选择正确句子的实例选择器，以及预测关系并使用清理数据更新其参数的关系分类器。 这两个模块相互作用共同训练。</p>\n<h3 id=\"Problem-definition\"><a href=\"#Problem-definition\" class=\"headerlink\" title=\"Problem definition\"></a>Problem definition</h3><p>我们定义两个子任务：实例选择和一个关系分类。</p>\n<p>我们定义实例选择问题：给定一组(sentence, relation label)，如$X = {(x_1,r_1),(x_2,r_2),…,(x_n,r_n)}$，其中$x_i$是与两个实体$(e_{1i},e_{2i})$相关的句子，$r_i$是由远程监督产生的关系标签。我们的目标是确定哪个句子真正描述了这种关系，并且应该被选作训练实例。</p>\n<p>关系分类问题表述如下：给定一个句子$x_i$和所提到的实体对$(h_i,t_i)$，目标是预测$x_i$中的语义关系$r_i$。模型估计概率：$p_{\\Phi}(r_i | x_i,h_i,t_i)$。</p>\n<h3 id=\"Overview\"><a href=\"#Overview\" class=\"headerlink\" title=\"Overview\"></a>Overview</h3><p><img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-04-14-WX20180414-165503%402x.png\" alt=\"X20180414-165503@2\"></p>\n<h3 id=\"Instance-Selector\"><a href=\"#Instance-Selector\" class=\"headerlink\" title=\"Instance Selector\"></a>Instance Selector</h3><p>这里的Instance Selector被当作强化学习问题来处理，因此policy的更新有滞后性，作者为了加快更新速度，把所有句子实例$X={x_1, …,x_n}$分为N个bag $B = {B^1, B^2, …, B^N}$，每一个$B^k$都包含同一个实体对，且标注关系都为$r^k$。</p>\n<ul>\n<li><p><strong>State</strong>：编码以下信息：1) 当前句子的向量表示，从CNN的非线性层获得用于关系分类的当前句子的向量表示; 2) 被选的句子集合的表示，它是所有选择句子的向量表示的平均值; 3) 一个句子中实体对的向量表示，从预先训练的知识图谱embedding中获得。</p>\n</li>\n<li><p><strong>Action</strong>：定义action $a_i \\in {0,1}$ 表示是否选取第i个句子，$a_i$的取值由policy function得到，定义如下：<br>$$<br>\\pi_{\\Theta}(s_i,a_i) = a_i \\sigma(W * F(s_i) + b) + (1 - a_i)(1 - \\sigma(W * F(s_i)+b))<br>$$</p>\n</li>\n<li><p><strong>Reward</strong>：reward function是所选句子效用的指标。对于某些bag $B = {x_1,… ,x_{| B |}}$，我们为每个句子选择一个action，以确定是否应该选择当前句子。我们假设该模型在完成所有选择时具有最终奖励。 因此，我们只在终端状态$s_{| B | +1}$收到延迟奖励，其他的奖励为零。 reward是基于CNN的分类反馈得到。</p>\n</li>\n</ul>\n<h3 id=\"Relation-Classifier\"><a href=\"#Relation-Classifier\" class=\"headerlink\" title=\"Relation Classifier\"></a>Relation Classifier</h3><p>这里用的分类模型比较简单，一个经典的CNN。</p>\n<p>输入层可以被分为两部分：</p>\n<ol>\n<li>word embedding</li>\n<li>position embedding：两个固定长度的向量，表示该词分别到两个实体店距离。</li>\n</ol>\n<h2 id=\"3-Analysis\"><a href=\"#3-Analysis\" class=\"headerlink\" title=\"3. Analysis\"></a>3. Analysis</h2><p>作者用的数据集是New York Times，作者随机挑选300句子人工标注，再对其做评测。对比的baseline包括CNN、CNN+Max(每个bag选一个正确的句子)、CNN+ATT。最后结果上看出，本文的CNN+RL模型取得了最好的结果，这表明强化学习对于该任务是行之有效的；并且句子层次的模型在评测中普遍好于bag模型。</p>\n<p>强化学习是目前比较火热的技术，它在nlp相关任务的应用仍在探索中，但是最近的论文确实有很多都在讨论它，并且也做到了不错的效果。希望从这篇文章为入口，开始了解强化学习及其在nlp上的应用。</p>\n<p>这篇文章中，强化学习主要是用于选择噪声数据，用以减少数据集的偏差等。但是我们相信强化学习能做的不仅如此，事实上最近的顶会还是有一些相关的工作的，可以放到以后再看。</p>\n<h2 id=\"Bibliography\"><a href=\"#Bibliography\" class=\"headerlink\" title=\"Bibliography\"></a>Bibliography</h2><p>本文的c++实现：<a href=\"https://github.com/JuneFeng/RelationClassification-RL\">https://github.com/JuneFeng/RelationClassification-RL</a></p>\n<p>[^1]: Feng, J., Huang, M., Zhao, L., Yang, Y., &amp; Zhu, X. (2018). Reinforcement Learning for Relation Classification from Noisy Data.</p>\n"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","date":"2018-04-04T00:00:00.000Z","_content":"\n## A Neural Model for Joint Event Detection and Summarization 阅读笔记\n\n**摘要**：Twitter事件检测旨在识别推文流中的first stories。一般认为由两个子任务组成。首先，过滤掉普通的或不相关的推文。其次，推文会被自动分类到event cluster中。传统上，尽管这两个任务之间存在相互依赖关系，它们仍被单独处理，并通过pipeline整合。另外，还有一个相关的任务是摘要，即提取能够代表event cluster的简洁摘要。这里和上个暑假看的Wang, Z.[^2]的工作比较相似。\n\n在本文[^1]中，我们构建了一个joint model来筛选、聚类和摘要推文中的event。特别的，我们利用深度表示学习来对推文进行矢量化处理。Neural stacking model用于整合不同子任务的pipeline，并更好地共享前后参数。实验表明，我们提出的neural joint model比pipeline更有效。\n\n### 1. Introduction\n\n有文献证明了推特、微博这类体裁比起传统的媒体，对新闻事件有更快的反应速度，因此今年对于推特的事件监测也是今年的热点之一，引起了广泛关注。我们在本文主要检测一些典型的事件类别，比如地震、DDos攻击等。我们提出了一个神经网络模型，该模型监视特定事件类别的推特流，共同检测并摘要该类别下的新闻事件。\n\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-04-04-WX20180404-225501%402x.png\" width=\"60%\">\n\n整体的架构如上图所示，给定一个推特流，我们的模型考虑三个子任务：推文过滤，事件聚类和事件摘要。\n\n典型的推特事件检测模型的核心部分是**聚类**，其中包括增量聚类和locality sensitive hashing。主要的想法是将同一主题的推文进行分组，以便在新推文不属于现有主题类的情况下检测到新主题。这样的聚类算法通常依赖于推特内容的特征，如TFIDF，用于测量推文之间的相似度。\n\n第二个子任务是**摘要**，它并不直接涉及event detection，但仍然与之高度相关，因为检测到的事件群可能比较大并且包含不同程度信息的推文。 从系统功能的角度来说，对于推特事件检测系统，摘要是十分必要的，因为我们没办法直接读取事件群，只有将其抽取摘要，并将事件摘要作为输出，才能够为我们所用。\n\n此外，由于大部分推文流包含普通或不相关的信息，推文**过滤**是我们考虑的第三个子任务。 我们的目标是根据其与潜在新事件的相关性对传入推文进行分类，以便只保留信息性推文。 过滤可以在聚类之前或之后进行。 在本文中，我们在聚类之前执行这项任务。\n\n这三个子任务形成了一个三阶段pipeline（过滤→聚类→摘要），其中各个阶段是密切相关的。 例如，完整描述事件的推文应该在相关性过滤和抽象摘要步骤中得到高分。 另外，更好地理解推文对于相关性过滤和事件聚类都有帮助。 受此启发，我们考虑使用一个joint model进行筛选、聚类和摘要。\n\n神经网络在近年也在类似任务上展示了良好的表现。我们使用两种联合建模策略。首先，我们以推文的语义表示作为关键连接因素，通过参数共享整合三项任务。其次，我们将neural stacking应用于pipeline，将前一个子模型的隐藏神经层作为其后继子模型的附加输入特征，并将后继的误差传播给前者，使得在培训期间，让前后子模型之间的信息得到更好的共享。\n\n### 2. Model\n\n我们系统的输入是一个推特流，实时输出事件报告。三个主要的子任务定义如下：\n\n- **事件筛选**：我们将流中的每条推文分类为与相关事件相关或不相关的事件。由于我们的目标是仅检测某些类型的事件（即地震和DDOS攻击事件），因此我们使用相应的一组关键字来过滤推文流作为预处理步骤。相关分类在预处理步骤之后执行，因为并非所有包含关键字的推文都是相关的。\n- **事件聚类**：我们在事件检测后对推文进行增量聚类。给定一个提到事件的推文，其任务是确定它是否存在于现有的事件集群中、是否是一个新的事件。这个任务的关键是推文之间的相似度计算。\n- **事件摘要**：当一个事件集群足够大时，我们通过提取前n个包含最多信息的推文来创建相应事件的报告。这个子任务可以被看作是一个多文档摘要任务。\n\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-04-04-WX20180404-232216%402x.png\" width=\"60%\">\n\n模型的整体构架如上图，H是推文的embedding，Hd是事件筛选步骤的隐藏层，Hc是聚类的隐藏层，Ps是摘要的输出。我们可以看到，每一个子任务都充分利用了前面的信息，使得各个子模型项目补充辅助，让结果更理想。\n\n结果表明，与独立的pipeline相比，这种neural stacking方法可以产生更好的子模型。请注意，虽然计算两个tweets之间的相似性的过程是用LSTM模型监督的，但聚类算法是一种无监督的在线聚类算法。\n\n#### 2.1 Shared Tweet Representation\n\n我们使用标准的LSTM模型来学习不同任务之间的推文表示。 假设$X =(w_1,w_2,…,w_n)$是推文，其中$n$是推文长度，$w_i$是第i个标记。 我们使用$w_i$的word embedding将每个$w_i$转换为实值向量$x_i$，通过查找预先训练的word embedding表D获得。我们使用skip-gram算法来训练embedding。\nLSTM用以生成隐藏序列$(h_1,h_2,…,h_n)$。 在每个步骤t，基于当前向量$x_t$和前一个向量$h_{t-1}$和$h_t = LSTM(x_t，h_{t-1})$计算LSTM模型的隐藏向量$h_t$。 初始状态和所有LSTM参数随机初始化并在训练过程中调整。 我们使用$H = h_n$作为X的共享表示。\n\n#### 2.2 Joint Model \n\n##### Event mention detection\n\n事件提及检测是一个二元分类任务，使用多层感知器进行描述。给出输入向量H，隐含层用于激发一组高级特征：\n$$\nH_d = \\sigma(W_d^h H + b_d^h)\n$$\n$H_d$被用于softmax输出层的输入：\n$$\nP_d = softmax(W_dH_d + B_d)\n$$\n这里$W_d^h, b_d^b, W_d$都是模型参数。$P_d$ 长度为2，$P_d(0)$表示推文X相关的概率，$P_d(1)$表示不相关的概率。\n\n##### Event clustering\n\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-04-04-WX20180404-234532%402x.png\" width=\"60%\">\n\n我们使用基于流的聚类算法将传入的推文分为不同的事件组，每个事件组表示一个特定的事件。该算法随着流中的每个传入推文增长式地工作，计算新推文与现有事件群中的每条推文之间的相似度分数。新传入的推文与每个事件群中最相似的对应文件之间的相似度用于衡量新推文与事件群集之间的相似度。使用阈值$\\mu - 3\\cdot \\sigma$来检测新推文是否属于现有聚类，其中μ是所有先前相似度分数的均值，σ是标准偏差。如果推特和所有现有群集之间的相似度低于阈值，则建立新的事件群集。否则，将推文添加到最相似的现有事件群集中。\n为了计算两条推文$X_i$和$X_j$之间的相似度，我们使用了一个连体网络，它采用共享表示向量$H_i$和$H_j$，并通过参数化计算相似概率得分$P_c$:\n$$\nH_c = \\sigma(W_c^h(H_i \\oplus H_j)+ b_c^h) \\\\\nP_c = softmax(W_cH_c + B_c)\n$$\n⊕表示向量级联。 $W_c^h，b_b^c,W_c,b_c$是模型参数。\n\n为了更好地整合事件提及检测和事件聚类，我们还将$X_i$和$X_j$的隐藏特征矢量$H_d$馈送到Siamese网络，从而\n$$\nH_c = \\sigma(W_c^h(H_i \\oplus H_j \\oplus H_{d_i} \\oplus H_{d_j})+ b_c^h)\n$$\n\n##### Event summarization\n\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-04-04-WX20180405-000354%402x.png\" width=\"60%\">\n\n为了生成事件集群的摘要，我们使用概率分数$P_s$对集群中的所有推文进行排序。 对于集群中的每个推特X，使用多层感知器来估计$P_s$，其中输入是$H \\oplus \\bar{H_c^h}$。 这里的矢量$\\bar{H_c^h}$是推文X与同一集群中所有其他推特之间的$H_c^h$之和。 它有两个用途。首先，$H_c^h$提供有关整个群集的信息，这对于更好地确定群集中给定推文的相关性很有用。 其次，$H_c^h$还把聚类和摘要联系了起来，从而强化了信息共享。\n$$\nP_s = softmax(W_sH_s + B_s)\n$$\nwhere\n$$\nH_s = \\sigma(W_s^h(H\\oplus \\bar{H_c^h})+ b_s^h)\n$$\n$W_s^h, b_s^b, Ws$是模型的参数\n\n#### 2.3 Training\n\n我们的培训目标是尽量减少这三项任务中标注的标签和预测标签之间的cross-entropy loss。 我们应用在线培训，使用Adagrad调整模型参数。为了避免过拟合，对word embedding使用0.2点dropout。隐藏层$H_d, H_c,H_s$的大小都设置为32。我们使用Skip-gram算法训练word embedding，并在训练期间对它们进行微调。 Word embedding的大小是128。\n\n### 3. Experiment\n\n见paper原文\n\n### 4. Conclusion\n\n文献[^1]提出了一个joint model，通过使用全局共享表示和不同子任务之间的堆叠来共同检测、聚类和摘要事件。 实验表明，我们提出的joint model比pipeline模型更有效。该联合神经系统优于采用离散或神经网络进行新闻事件检测和摘要的最新baseline。\n\n## Bibliography\n\n代码开源：https://github.com/wangzq870305/joint_event_detection\n\n[^1]: Wang, Z., & Zhang, Y. (2017, August). A neural model for joint event detection and summarization. In *Proceedings of the 26th International Joint Conference on Artificial Intelligence* (pp. 4158-4164). AAAI Press.\n[^2]: Wang, Z., Shou, L., Chen, K., Chen, G., & Mehrotra, S. (2015). On summarization and timeline generation for evolutionary tweet streams. *IEEE Transactions on Knowledge and Data Engineering*, *27*(5), 1301-1315.","source":"_posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization.md","raw":"---\ntitle: A Neural Model for Joint Event Detection and Summarization 阅读笔记\ndate: 2018-04-04 08:00:00\ncategories: [research]\ntags: [event-detection, summarization]\n---\n\n## A Neural Model for Joint Event Detection and Summarization 阅读笔记\n\n**摘要**：Twitter事件检测旨在识别推文流中的first stories。一般认为由两个子任务组成。首先，过滤掉普通的或不相关的推文。其次，推文会被自动分类到event cluster中。传统上，尽管这两个任务之间存在相互依赖关系，它们仍被单独处理，并通过pipeline整合。另外，还有一个相关的任务是摘要，即提取能够代表event cluster的简洁摘要。这里和上个暑假看的Wang, Z.[^2]的工作比较相似。\n\n在本文[^1]中，我们构建了一个joint model来筛选、聚类和摘要推文中的event。特别的，我们利用深度表示学习来对推文进行矢量化处理。Neural stacking model用于整合不同子任务的pipeline，并更好地共享前后参数。实验表明，我们提出的neural joint model比pipeline更有效。\n\n### 1. Introduction\n\n有文献证明了推特、微博这类体裁比起传统的媒体，对新闻事件有更快的反应速度，因此今年对于推特的事件监测也是今年的热点之一，引起了广泛关注。我们在本文主要检测一些典型的事件类别，比如地震、DDos攻击等。我们提出了一个神经网络模型，该模型监视特定事件类别的推特流，共同检测并摘要该类别下的新闻事件。\n\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-04-04-WX20180404-225501%402x.png\" width=\"60%\">\n\n整体的架构如上图所示，给定一个推特流，我们的模型考虑三个子任务：推文过滤，事件聚类和事件摘要。\n\n典型的推特事件检测模型的核心部分是**聚类**，其中包括增量聚类和locality sensitive hashing。主要的想法是将同一主题的推文进行分组，以便在新推文不属于现有主题类的情况下检测到新主题。这样的聚类算法通常依赖于推特内容的特征，如TFIDF，用于测量推文之间的相似度。\n\n第二个子任务是**摘要**，它并不直接涉及event detection，但仍然与之高度相关，因为检测到的事件群可能比较大并且包含不同程度信息的推文。 从系统功能的角度来说，对于推特事件检测系统，摘要是十分必要的，因为我们没办法直接读取事件群，只有将其抽取摘要，并将事件摘要作为输出，才能够为我们所用。\n\n此外，由于大部分推文流包含普通或不相关的信息，推文**过滤**是我们考虑的第三个子任务。 我们的目标是根据其与潜在新事件的相关性对传入推文进行分类，以便只保留信息性推文。 过滤可以在聚类之前或之后进行。 在本文中，我们在聚类之前执行这项任务。\n\n这三个子任务形成了一个三阶段pipeline（过滤→聚类→摘要），其中各个阶段是密切相关的。 例如，完整描述事件的推文应该在相关性过滤和抽象摘要步骤中得到高分。 另外，更好地理解推文对于相关性过滤和事件聚类都有帮助。 受此启发，我们考虑使用一个joint model进行筛选、聚类和摘要。\n\n神经网络在近年也在类似任务上展示了良好的表现。我们使用两种联合建模策略。首先，我们以推文的语义表示作为关键连接因素，通过参数共享整合三项任务。其次，我们将neural stacking应用于pipeline，将前一个子模型的隐藏神经层作为其后继子模型的附加输入特征，并将后继的误差传播给前者，使得在培训期间，让前后子模型之间的信息得到更好的共享。\n\n### 2. Model\n\n我们系统的输入是一个推特流，实时输出事件报告。三个主要的子任务定义如下：\n\n- **事件筛选**：我们将流中的每条推文分类为与相关事件相关或不相关的事件。由于我们的目标是仅检测某些类型的事件（即地震和DDOS攻击事件），因此我们使用相应的一组关键字来过滤推文流作为预处理步骤。相关分类在预处理步骤之后执行，因为并非所有包含关键字的推文都是相关的。\n- **事件聚类**：我们在事件检测后对推文进行增量聚类。给定一个提到事件的推文，其任务是确定它是否存在于现有的事件集群中、是否是一个新的事件。这个任务的关键是推文之间的相似度计算。\n- **事件摘要**：当一个事件集群足够大时，我们通过提取前n个包含最多信息的推文来创建相应事件的报告。这个子任务可以被看作是一个多文档摘要任务。\n\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-04-04-WX20180404-232216%402x.png\" width=\"60%\">\n\n模型的整体构架如上图，H是推文的embedding，Hd是事件筛选步骤的隐藏层，Hc是聚类的隐藏层，Ps是摘要的输出。我们可以看到，每一个子任务都充分利用了前面的信息，使得各个子模型项目补充辅助，让结果更理想。\n\n结果表明，与独立的pipeline相比，这种neural stacking方法可以产生更好的子模型。请注意，虽然计算两个tweets之间的相似性的过程是用LSTM模型监督的，但聚类算法是一种无监督的在线聚类算法。\n\n#### 2.1 Shared Tweet Representation\n\n我们使用标准的LSTM模型来学习不同任务之间的推文表示。 假设$X =(w_1,w_2,…,w_n)$是推文，其中$n$是推文长度，$w_i$是第i个标记。 我们使用$w_i$的word embedding将每个$w_i$转换为实值向量$x_i$，通过查找预先训练的word embedding表D获得。我们使用skip-gram算法来训练embedding。\nLSTM用以生成隐藏序列$(h_1,h_2,…,h_n)$。 在每个步骤t，基于当前向量$x_t$和前一个向量$h_{t-1}$和$h_t = LSTM(x_t，h_{t-1})$计算LSTM模型的隐藏向量$h_t$。 初始状态和所有LSTM参数随机初始化并在训练过程中调整。 我们使用$H = h_n$作为X的共享表示。\n\n#### 2.2 Joint Model \n\n##### Event mention detection\n\n事件提及检测是一个二元分类任务，使用多层感知器进行描述。给出输入向量H，隐含层用于激发一组高级特征：\n$$\nH_d = \\sigma(W_d^h H + b_d^h)\n$$\n$H_d$被用于softmax输出层的输入：\n$$\nP_d = softmax(W_dH_d + B_d)\n$$\n这里$W_d^h, b_d^b, W_d$都是模型参数。$P_d$ 长度为2，$P_d(0)$表示推文X相关的概率，$P_d(1)$表示不相关的概率。\n\n##### Event clustering\n\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-04-04-WX20180404-234532%402x.png\" width=\"60%\">\n\n我们使用基于流的聚类算法将传入的推文分为不同的事件组，每个事件组表示一个特定的事件。该算法随着流中的每个传入推文增长式地工作，计算新推文与现有事件群中的每条推文之间的相似度分数。新传入的推文与每个事件群中最相似的对应文件之间的相似度用于衡量新推文与事件群集之间的相似度。使用阈值$\\mu - 3\\cdot \\sigma$来检测新推文是否属于现有聚类，其中μ是所有先前相似度分数的均值，σ是标准偏差。如果推特和所有现有群集之间的相似度低于阈值，则建立新的事件群集。否则，将推文添加到最相似的现有事件群集中。\n为了计算两条推文$X_i$和$X_j$之间的相似度，我们使用了一个连体网络，它采用共享表示向量$H_i$和$H_j$，并通过参数化计算相似概率得分$P_c$:\n$$\nH_c = \\sigma(W_c^h(H_i \\oplus H_j)+ b_c^h) \\\\\nP_c = softmax(W_cH_c + B_c)\n$$\n⊕表示向量级联。 $W_c^h，b_b^c,W_c,b_c$是模型参数。\n\n为了更好地整合事件提及检测和事件聚类，我们还将$X_i$和$X_j$的隐藏特征矢量$H_d$馈送到Siamese网络，从而\n$$\nH_c = \\sigma(W_c^h(H_i \\oplus H_j \\oplus H_{d_i} \\oplus H_{d_j})+ b_c^h)\n$$\n\n##### Event summarization\n\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-04-04-WX20180405-000354%402x.png\" width=\"60%\">\n\n为了生成事件集群的摘要，我们使用概率分数$P_s$对集群中的所有推文进行排序。 对于集群中的每个推特X，使用多层感知器来估计$P_s$，其中输入是$H \\oplus \\bar{H_c^h}$。 这里的矢量$\\bar{H_c^h}$是推文X与同一集群中所有其他推特之间的$H_c^h$之和。 它有两个用途。首先，$H_c^h$提供有关整个群集的信息，这对于更好地确定群集中给定推文的相关性很有用。 其次，$H_c^h$还把聚类和摘要联系了起来，从而强化了信息共享。\n$$\nP_s = softmax(W_sH_s + B_s)\n$$\nwhere\n$$\nH_s = \\sigma(W_s^h(H\\oplus \\bar{H_c^h})+ b_s^h)\n$$\n$W_s^h, b_s^b, Ws$是模型的参数\n\n#### 2.3 Training\n\n我们的培训目标是尽量减少这三项任务中标注的标签和预测标签之间的cross-entropy loss。 我们应用在线培训，使用Adagrad调整模型参数。为了避免过拟合，对word embedding使用0.2点dropout。隐藏层$H_d, H_c,H_s$的大小都设置为32。我们使用Skip-gram算法训练word embedding，并在训练期间对它们进行微调。 Word embedding的大小是128。\n\n### 3. Experiment\n\n见paper原文\n\n### 4. Conclusion\n\n文献[^1]提出了一个joint model，通过使用全局共享表示和不同子任务之间的堆叠来共同检测、聚类和摘要事件。 实验表明，我们提出的joint model比pipeline模型更有效。该联合神经系统优于采用离散或神经网络进行新闻事件检测和摘要的最新baseline。\n\n## Bibliography\n\n代码开源：https://github.com/wangzq870305/joint_event_detection\n\n[^1]: Wang, Z., & Zhang, Y. (2017, August). A neural model for joint event detection and summarization. In *Proceedings of the 26th International Joint Conference on Artificial Intelligence* (pp. 4158-4164). AAAI Press.\n[^2]: Wang, Z., Shou, L., Chen, K., Chen, G., & Mehrotra, S. (2015). On summarization and timeline generation for evolutionary tweet streams. *IEEE Transactions on Knowledge and Data Engineering*, *27*(5), 1301-1315.","slug":"[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization","published":1,"updated":"2020-11-03T03:26:06.064Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckw4qufpu002mgwtl1ggb971o","content":"<h2 id=\"A-Neural-Model-for-Joint-Event-Detection-and-Summarization-阅读笔记\"><a href=\"#A-Neural-Model-for-Joint-Event-Detection-and-Summarization-阅读笔记\" class=\"headerlink\" title=\"A Neural Model for Joint Event Detection and Summarization 阅读笔记\"></a>A Neural Model for Joint Event Detection and Summarization 阅读笔记</h2><p><strong>摘要</strong>：Twitter事件检测旨在识别推文流中的first stories。一般认为由两个子任务组成。首先，过滤掉普通的或不相关的推文。其次，推文会被自动分类到event cluster中。传统上，尽管这两个任务之间存在相互依赖关系，它们仍被单独处理，并通过pipeline整合。另外，还有一个相关的任务是摘要，即提取能够代表event cluster的简洁摘要。这里和上个暑假看的Wang, Z.[^2]的工作比较相似。</p>\n<p>在本文[^1]中，我们构建了一个joint model来筛选、聚类和摘要推文中的event。特别的，我们利用深度表示学习来对推文进行矢量化处理。Neural stacking model用于整合不同子任务的pipeline，并更好地共享前后参数。实验表明，我们提出的neural joint model比pipeline更有效。</p>\n<h3 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1. Introduction\"></a>1. Introduction</h3><p>有文献证明了推特、微博这类体裁比起传统的媒体，对新闻事件有更快的反应速度，因此今年对于推特的事件监测也是今年的热点之一，引起了广泛关注。我们在本文主要检测一些典型的事件类别，比如地震、DDos攻击等。我们提出了一个神经网络模型，该模型监视特定事件类别的推特流，共同检测并摘要该类别下的新闻事件。</p>\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-04-04-WX20180404-225501%402x.png\" width=\"60%\">\n\n<p>整体的架构如上图所示，给定一个推特流，我们的模型考虑三个子任务：推文过滤，事件聚类和事件摘要。</p>\n<p>典型的推特事件检测模型的核心部分是<strong>聚类</strong>，其中包括增量聚类和locality sensitive hashing。主要的想法是将同一主题的推文进行分组，以便在新推文不属于现有主题类的情况下检测到新主题。这样的聚类算法通常依赖于推特内容的特征，如TFIDF，用于测量推文之间的相似度。</p>\n<p>第二个子任务是<strong>摘要</strong>，它并不直接涉及event detection，但仍然与之高度相关，因为检测到的事件群可能比较大并且包含不同程度信息的推文。 从系统功能的角度来说，对于推特事件检测系统，摘要是十分必要的，因为我们没办法直接读取事件群，只有将其抽取摘要，并将事件摘要作为输出，才能够为我们所用。</p>\n<p>此外，由于大部分推文流包含普通或不相关的信息，推文<strong>过滤</strong>是我们考虑的第三个子任务。 我们的目标是根据其与潜在新事件的相关性对传入推文进行分类，以便只保留信息性推文。 过滤可以在聚类之前或之后进行。 在本文中，我们在聚类之前执行这项任务。</p>\n<p>这三个子任务形成了一个三阶段pipeline（过滤→聚类→摘要），其中各个阶段是密切相关的。 例如，完整描述事件的推文应该在相关性过滤和抽象摘要步骤中得到高分。 另外，更好地理解推文对于相关性过滤和事件聚类都有帮助。 受此启发，我们考虑使用一个joint model进行筛选、聚类和摘要。</p>\n<p>神经网络在近年也在类似任务上展示了良好的表现。我们使用两种联合建模策略。首先，我们以推文的语义表示作为关键连接因素，通过参数共享整合三项任务。其次，我们将neural stacking应用于pipeline，将前一个子模型的隐藏神经层作为其后继子模型的附加输入特征，并将后继的误差传播给前者，使得在培训期间，让前后子模型之间的信息得到更好的共享。</p>\n<h3 id=\"2-Model\"><a href=\"#2-Model\" class=\"headerlink\" title=\"2. Model\"></a>2. Model</h3><p>我们系统的输入是一个推特流，实时输出事件报告。三个主要的子任务定义如下：</p>\n<ul>\n<li><strong>事件筛选</strong>：我们将流中的每条推文分类为与相关事件相关或不相关的事件。由于我们的目标是仅检测某些类型的事件（即地震和DDOS攻击事件），因此我们使用相应的一组关键字来过滤推文流作为预处理步骤。相关分类在预处理步骤之后执行，因为并非所有包含关键字的推文都是相关的。</li>\n<li><strong>事件聚类</strong>：我们在事件检测后对推文进行增量聚类。给定一个提到事件的推文，其任务是确定它是否存在于现有的事件集群中、是否是一个新的事件。这个任务的关键是推文之间的相似度计算。</li>\n<li><strong>事件摘要</strong>：当一个事件集群足够大时，我们通过提取前n个包含最多信息的推文来创建相应事件的报告。这个子任务可以被看作是一个多文档摘要任务。</li>\n</ul>\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-04-04-WX20180404-232216%402x.png\" width=\"60%\">\n\n<p>模型的整体构架如上图，H是推文的embedding，Hd是事件筛选步骤的隐藏层，Hc是聚类的隐藏层，Ps是摘要的输出。我们可以看到，每一个子任务都充分利用了前面的信息，使得各个子模型项目补充辅助，让结果更理想。</p>\n<p>结果表明，与独立的pipeline相比，这种neural stacking方法可以产生更好的子模型。请注意，虽然计算两个tweets之间的相似性的过程是用LSTM模型监督的，但聚类算法是一种无监督的在线聚类算法。</p>\n<h4 id=\"2-1-Shared-Tweet-Representation\"><a href=\"#2-1-Shared-Tweet-Representation\" class=\"headerlink\" title=\"2.1 Shared Tweet Representation\"></a>2.1 Shared Tweet Representation</h4><p>我们使用标准的LSTM模型来学习不同任务之间的推文表示。 假设$X =(w_1,w_2,…,w_n)$是推文，其中$n$是推文长度，$w_i$是第i个标记。 我们使用$w_i$的word embedding将每个$w_i$转换为实值向量$x_i$，通过查找预先训练的word embedding表D获得。我们使用skip-gram算法来训练embedding。<br>LSTM用以生成隐藏序列$(h_1,h_2,…,h_n)$。 在每个步骤t，基于当前向量$x_t$和前一个向量$h_{t-1}$和$h_t = LSTM(x_t，h_{t-1})$计算LSTM模型的隐藏向量$h_t$。 初始状态和所有LSTM参数随机初始化并在训练过程中调整。 我们使用$H = h_n$作为X的共享表示。</p>\n<h4 id=\"2-2-Joint-Model\"><a href=\"#2-2-Joint-Model\" class=\"headerlink\" title=\"2.2 Joint Model\"></a>2.2 Joint Model</h4><h5 id=\"Event-mention-detection\"><a href=\"#Event-mention-detection\" class=\"headerlink\" title=\"Event mention detection\"></a>Event mention detection</h5><p>事件提及检测是一个二元分类任务，使用多层感知器进行描述。给出输入向量H，隐含层用于激发一组高级特征：<br>$$<br>H_d = \\sigma(W_d^h H + b_d^h)<br>$$<br>$H_d$被用于softmax输出层的输入：<br>$$<br>P_d = softmax(W_dH_d + B_d)<br>$$<br>这里$W_d^h, b_d^b, W_d$都是模型参数。$P_d$ 长度为2，$P_d(0)$表示推文X相关的概率，$P_d(1)$表示不相关的概率。</p>\n<h5 id=\"Event-clustering\"><a href=\"#Event-clustering\" class=\"headerlink\" title=\"Event clustering\"></a>Event clustering</h5><img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-04-04-WX20180404-234532%402x.png\" width=\"60%\">\n\n<p>我们使用基于流的聚类算法将传入的推文分为不同的事件组，每个事件组表示一个特定的事件。该算法随着流中的每个传入推文增长式地工作，计算新推文与现有事件群中的每条推文之间的相似度分数。新传入的推文与每个事件群中最相似的对应文件之间的相似度用于衡量新推文与事件群集之间的相似度。使用阈值$\\mu - 3\\cdot \\sigma$来检测新推文是否属于现有聚类，其中μ是所有先前相似度分数的均值，σ是标准偏差。如果推特和所有现有群集之间的相似度低于阈值，则建立新的事件群集。否则，将推文添加到最相似的现有事件群集中。<br>为了计算两条推文$X_i$和$X_j$之间的相似度，我们使用了一个连体网络，它采用共享表示向量$H_i$和$H_j$，并通过参数化计算相似概率得分$P_c$:<br>$$<br>H_c = \\sigma(W_c^h(H_i \\oplus H_j)+ b_c^h) \\<br>P_c = softmax(W_cH_c + B_c)<br>$$<br>⊕表示向量级联。 $W_c^h，b_b^c,W_c,b_c$是模型参数。</p>\n<p>为了更好地整合事件提及检测和事件聚类，我们还将$X_i$和$X_j$的隐藏特征矢量$H_d$馈送到Siamese网络，从而<br>$$<br>H_c = \\sigma(W_c^h(H_i \\oplus H_j \\oplus H_{d_i} \\oplus H_{d_j})+ b_c^h)<br>$$</p>\n<h5 id=\"Event-summarization\"><a href=\"#Event-summarization\" class=\"headerlink\" title=\"Event summarization\"></a>Event summarization</h5><img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-04-04-WX20180405-000354%402x.png\" width=\"60%\">\n\n<p>为了生成事件集群的摘要，我们使用概率分数$P_s$对集群中的所有推文进行排序。 对于集群中的每个推特X，使用多层感知器来估计$P_s$，其中输入是$H \\oplus \\bar{H_c^h}$。 这里的矢量$\\bar{H_c^h}$是推文X与同一集群中所有其他推特之间的$H_c^h$之和。 它有两个用途。首先，$H_c^h$提供有关整个群集的信息，这对于更好地确定群集中给定推文的相关性很有用。 其次，$H_c^h$还把聚类和摘要联系了起来，从而强化了信息共享。<br>$$<br>P_s = softmax(W_sH_s + B_s)<br>$$<br>where<br>$$<br>H_s = \\sigma(W_s^h(H\\oplus \\bar{H_c^h})+ b_s^h)<br>$$<br>$W_s^h, b_s^b, Ws$是模型的参数</p>\n<h4 id=\"2-3-Training\"><a href=\"#2-3-Training\" class=\"headerlink\" title=\"2.3 Training\"></a>2.3 Training</h4><p>我们的培训目标是尽量减少这三项任务中标注的标签和预测标签之间的cross-entropy loss。 我们应用在线培训，使用Adagrad调整模型参数。为了避免过拟合，对word embedding使用0.2点dropout。隐藏层$H_d, H_c,H_s$的大小都设置为32。我们使用Skip-gram算法训练word embedding，并在训练期间对它们进行微调。 Word embedding的大小是128。</p>\n<h3 id=\"3-Experiment\"><a href=\"#3-Experiment\" class=\"headerlink\" title=\"3. Experiment\"></a>3. Experiment</h3><p>见paper原文</p>\n<h3 id=\"4-Conclusion\"><a href=\"#4-Conclusion\" class=\"headerlink\" title=\"4. Conclusion\"></a>4. Conclusion</h3><p>文献[^1]提出了一个joint model，通过使用全局共享表示和不同子任务之间的堆叠来共同检测、聚类和摘要事件。 实验表明，我们提出的joint model比pipeline模型更有效。该联合神经系统优于采用离散或神经网络进行新闻事件检测和摘要的最新baseline。</p>\n<h2 id=\"Bibliography\"><a href=\"#Bibliography\" class=\"headerlink\" title=\"Bibliography\"></a>Bibliography</h2><p>代码开源：<a href=\"https://github.com/wangzq870305/joint_event_detection\">https://github.com/wangzq870305/joint_event_detection</a></p>\n<p>[^1]: Wang, Z., &amp; Zhang, Y. (2017, August). A neural model for joint event detection and summarization. In <em>Proceedings of the 26th International Joint Conference on Artificial Intelligence</em> (pp. 4158-4164). AAAI Press.<br>[^2]: Wang, Z., Shou, L., Chen, K., Chen, G., &amp; Mehrotra, S. (2015). On summarization and timeline generation for evolutionary tweet streams. <em>IEEE Transactions on Knowledge and Data Engineering</em>, <em>27</em>(5), 1301-1315.</p>\n","site":{"data":{"recommended_posts":{"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"}],"https://LorrinWWW.github.io/posts/complexity/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/hexo-with-latex/":[{"title":"区块链时代的最正确的打赏姿势——在Hexo博客配置Coinhive网页挖矿","permalink":"http://www.davidfnck.com/blockchain/mine-xmr-in-hexo-by-coinhive.html/"}],"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/":[{"title":"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-statistic/":[{"title":"贝叶斯估计 Bayes estimation","permalink":"https://LorrinWWW.github.io/posts/Bayes-estimation/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"}],"https://LorrinWWW.github.io/posts/[2018.2.5]Nested-LSTMs/":[{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"}],"https://LorrinWWW.github.io/posts/datamining-pretreatment/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Several models for knowledge graph representing and completing 几个知识图谱模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/"}],"https://LorrinWWW.github.io/posts/compression/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/Sobolev-space/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/proba-ch5/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch3/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/proba-ch1/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/ML-CNN/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/EDP-basic-models/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/machine-learning/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/datamining-class-pred/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction/":[],"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/":[{"title":"OS notes","permalink":"https://LorrinWWW.github.io/posts/OS-notes/"}],"https://LorrinWWW.github.io/posts/projet-enjeu-plugin-chrome-101/":[{"title":"Chrome油猴脚本","permalink":"http://gubangzhong.cn/2017/06/13/Chrome油猴脚本/"}],"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/":[{"title":"MongoDB, Docker and Python","permalink":"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/":[{"title":"Hands on Scrapy","permalink":"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/OS-notes/":[{"title":"learning OS and building LorriOS","permalink":"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/"}],"https://LorrinWWW.github.io/posts/Bayes-estimation/":[{"title":"Note of statistic","permalink":"https://LorrinWWW.github.io/posts/Note-of-statistic/"}],"https://LorrinWWW.github.io/posts/management-of-the-firm/":[],"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"}],"https://LorrinWWW.github.io/posts/datamining-qualitative-induction/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-finite-element-method/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/QuadTree/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/proba-ch4/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch2/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/":[{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/Note-of-NLP/":[{"title":"Generative Adversarial Network","permalink":"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/"},{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"}],"https://LorrinWWW.github.io/posts/Note-of-datamining/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"数据挖掘-分类与预测","permalink":"https://LorrinWWW.github.io/posts/datamining-class-pred/"},{"title":"数据挖掘-预处理","permalink":"https://LorrinWWW.github.io/posts/datamining-pretreatment/"}],"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/Note-of-probability/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/participe-present-et-gerondif/":[],"https://LorrinWWW.github.io/posts/Hilbert-space/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/proba-ch6/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}]}}},"excerpt":"","more":"<h2 id=\"A-Neural-Model-for-Joint-Event-Detection-and-Summarization-阅读笔记\"><a href=\"#A-Neural-Model-for-Joint-Event-Detection-and-Summarization-阅读笔记\" class=\"headerlink\" title=\"A Neural Model for Joint Event Detection and Summarization 阅读笔记\"></a>A Neural Model for Joint Event Detection and Summarization 阅读笔记</h2><p><strong>摘要</strong>：Twitter事件检测旨在识别推文流中的first stories。一般认为由两个子任务组成。首先，过滤掉普通的或不相关的推文。其次，推文会被自动分类到event cluster中。传统上，尽管这两个任务之间存在相互依赖关系，它们仍被单独处理，并通过pipeline整合。另外，还有一个相关的任务是摘要，即提取能够代表event cluster的简洁摘要。这里和上个暑假看的Wang, Z.[^2]的工作比较相似。</p>\n<p>在本文[^1]中，我们构建了一个joint model来筛选、聚类和摘要推文中的event。特别的，我们利用深度表示学习来对推文进行矢量化处理。Neural stacking model用于整合不同子任务的pipeline，并更好地共享前后参数。实验表明，我们提出的neural joint model比pipeline更有效。</p>\n<h3 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1. Introduction\"></a>1. Introduction</h3><p>有文献证明了推特、微博这类体裁比起传统的媒体，对新闻事件有更快的反应速度，因此今年对于推特的事件监测也是今年的热点之一，引起了广泛关注。我们在本文主要检测一些典型的事件类别，比如地震、DDos攻击等。我们提出了一个神经网络模型，该模型监视特定事件类别的推特流，共同检测并摘要该类别下的新闻事件。</p>\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-04-04-WX20180404-225501%402x.png\" width=\"60%\">\n\n<p>整体的架构如上图所示，给定一个推特流，我们的模型考虑三个子任务：推文过滤，事件聚类和事件摘要。</p>\n<p>典型的推特事件检测模型的核心部分是<strong>聚类</strong>，其中包括增量聚类和locality sensitive hashing。主要的想法是将同一主题的推文进行分组，以便在新推文不属于现有主题类的情况下检测到新主题。这样的聚类算法通常依赖于推特内容的特征，如TFIDF，用于测量推文之间的相似度。</p>\n<p>第二个子任务是<strong>摘要</strong>，它并不直接涉及event detection，但仍然与之高度相关，因为检测到的事件群可能比较大并且包含不同程度信息的推文。 从系统功能的角度来说，对于推特事件检测系统，摘要是十分必要的，因为我们没办法直接读取事件群，只有将其抽取摘要，并将事件摘要作为输出，才能够为我们所用。</p>\n<p>此外，由于大部分推文流包含普通或不相关的信息，推文<strong>过滤</strong>是我们考虑的第三个子任务。 我们的目标是根据其与潜在新事件的相关性对传入推文进行分类，以便只保留信息性推文。 过滤可以在聚类之前或之后进行。 在本文中，我们在聚类之前执行这项任务。</p>\n<p>这三个子任务形成了一个三阶段pipeline（过滤→聚类→摘要），其中各个阶段是密切相关的。 例如，完整描述事件的推文应该在相关性过滤和抽象摘要步骤中得到高分。 另外，更好地理解推文对于相关性过滤和事件聚类都有帮助。 受此启发，我们考虑使用一个joint model进行筛选、聚类和摘要。</p>\n<p>神经网络在近年也在类似任务上展示了良好的表现。我们使用两种联合建模策略。首先，我们以推文的语义表示作为关键连接因素，通过参数共享整合三项任务。其次，我们将neural stacking应用于pipeline，将前一个子模型的隐藏神经层作为其后继子模型的附加输入特征，并将后继的误差传播给前者，使得在培训期间，让前后子模型之间的信息得到更好的共享。</p>\n<h3 id=\"2-Model\"><a href=\"#2-Model\" class=\"headerlink\" title=\"2. Model\"></a>2. Model</h3><p>我们系统的输入是一个推特流，实时输出事件报告。三个主要的子任务定义如下：</p>\n<ul>\n<li><strong>事件筛选</strong>：我们将流中的每条推文分类为与相关事件相关或不相关的事件。由于我们的目标是仅检测某些类型的事件（即地震和DDOS攻击事件），因此我们使用相应的一组关键字来过滤推文流作为预处理步骤。相关分类在预处理步骤之后执行，因为并非所有包含关键字的推文都是相关的。</li>\n<li><strong>事件聚类</strong>：我们在事件检测后对推文进行增量聚类。给定一个提到事件的推文，其任务是确定它是否存在于现有的事件集群中、是否是一个新的事件。这个任务的关键是推文之间的相似度计算。</li>\n<li><strong>事件摘要</strong>：当一个事件集群足够大时，我们通过提取前n个包含最多信息的推文来创建相应事件的报告。这个子任务可以被看作是一个多文档摘要任务。</li>\n</ul>\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-04-04-WX20180404-232216%402x.png\" width=\"60%\">\n\n<p>模型的整体构架如上图，H是推文的embedding，Hd是事件筛选步骤的隐藏层，Hc是聚类的隐藏层，Ps是摘要的输出。我们可以看到，每一个子任务都充分利用了前面的信息，使得各个子模型项目补充辅助，让结果更理想。</p>\n<p>结果表明，与独立的pipeline相比，这种neural stacking方法可以产生更好的子模型。请注意，虽然计算两个tweets之间的相似性的过程是用LSTM模型监督的，但聚类算法是一种无监督的在线聚类算法。</p>\n<h4 id=\"2-1-Shared-Tweet-Representation\"><a href=\"#2-1-Shared-Tweet-Representation\" class=\"headerlink\" title=\"2.1 Shared Tweet Representation\"></a>2.1 Shared Tweet Representation</h4><p>我们使用标准的LSTM模型来学习不同任务之间的推文表示。 假设$X =(w_1,w_2,…,w_n)$是推文，其中$n$是推文长度，$w_i$是第i个标记。 我们使用$w_i$的word embedding将每个$w_i$转换为实值向量$x_i$，通过查找预先训练的word embedding表D获得。我们使用skip-gram算法来训练embedding。<br>LSTM用以生成隐藏序列$(h_1,h_2,…,h_n)$。 在每个步骤t，基于当前向量$x_t$和前一个向量$h_{t-1}$和$h_t = LSTM(x_t，h_{t-1})$计算LSTM模型的隐藏向量$h_t$。 初始状态和所有LSTM参数随机初始化并在训练过程中调整。 我们使用$H = h_n$作为X的共享表示。</p>\n<h4 id=\"2-2-Joint-Model\"><a href=\"#2-2-Joint-Model\" class=\"headerlink\" title=\"2.2 Joint Model\"></a>2.2 Joint Model</h4><h5 id=\"Event-mention-detection\"><a href=\"#Event-mention-detection\" class=\"headerlink\" title=\"Event mention detection\"></a>Event mention detection</h5><p>事件提及检测是一个二元分类任务，使用多层感知器进行描述。给出输入向量H，隐含层用于激发一组高级特征：<br>$$<br>H_d = \\sigma(W_d^h H + b_d^h)<br>$$<br>$H_d$被用于softmax输出层的输入：<br>$$<br>P_d = softmax(W_dH_d + B_d)<br>$$<br>这里$W_d^h, b_d^b, W_d$都是模型参数。$P_d$ 长度为2，$P_d(0)$表示推文X相关的概率，$P_d(1)$表示不相关的概率。</p>\n<h5 id=\"Event-clustering\"><a href=\"#Event-clustering\" class=\"headerlink\" title=\"Event clustering\"></a>Event clustering</h5><img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-04-04-WX20180404-234532%402x.png\" width=\"60%\">\n\n<p>我们使用基于流的聚类算法将传入的推文分为不同的事件组，每个事件组表示一个特定的事件。该算法随着流中的每个传入推文增长式地工作，计算新推文与现有事件群中的每条推文之间的相似度分数。新传入的推文与每个事件群中最相似的对应文件之间的相似度用于衡量新推文与事件群集之间的相似度。使用阈值$\\mu - 3\\cdot \\sigma$来检测新推文是否属于现有聚类，其中μ是所有先前相似度分数的均值，σ是标准偏差。如果推特和所有现有群集之间的相似度低于阈值，则建立新的事件群集。否则，将推文添加到最相似的现有事件群集中。<br>为了计算两条推文$X_i$和$X_j$之间的相似度，我们使用了一个连体网络，它采用共享表示向量$H_i$和$H_j$，并通过参数化计算相似概率得分$P_c$:<br>$$<br>H_c = \\sigma(W_c^h(H_i \\oplus H_j)+ b_c^h) \\<br>P_c = softmax(W_cH_c + B_c)<br>$$<br>⊕表示向量级联。 $W_c^h，b_b^c,W_c,b_c$是模型参数。</p>\n<p>为了更好地整合事件提及检测和事件聚类，我们还将$X_i$和$X_j$的隐藏特征矢量$H_d$馈送到Siamese网络，从而<br>$$<br>H_c = \\sigma(W_c^h(H_i \\oplus H_j \\oplus H_{d_i} \\oplus H_{d_j})+ b_c^h)<br>$$</p>\n<h5 id=\"Event-summarization\"><a href=\"#Event-summarization\" class=\"headerlink\" title=\"Event summarization\"></a>Event summarization</h5><img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-04-04-WX20180405-000354%402x.png\" width=\"60%\">\n\n<p>为了生成事件集群的摘要，我们使用概率分数$P_s$对集群中的所有推文进行排序。 对于集群中的每个推特X，使用多层感知器来估计$P_s$，其中输入是$H \\oplus \\bar{H_c^h}$。 这里的矢量$\\bar{H_c^h}$是推文X与同一集群中所有其他推特之间的$H_c^h$之和。 它有两个用途。首先，$H_c^h$提供有关整个群集的信息，这对于更好地确定群集中给定推文的相关性很有用。 其次，$H_c^h$还把聚类和摘要联系了起来，从而强化了信息共享。<br>$$<br>P_s = softmax(W_sH_s + B_s)<br>$$<br>where<br>$$<br>H_s = \\sigma(W_s^h(H\\oplus \\bar{H_c^h})+ b_s^h)<br>$$<br>$W_s^h, b_s^b, Ws$是模型的参数</p>\n<h4 id=\"2-3-Training\"><a href=\"#2-3-Training\" class=\"headerlink\" title=\"2.3 Training\"></a>2.3 Training</h4><p>我们的培训目标是尽量减少这三项任务中标注的标签和预测标签之间的cross-entropy loss。 我们应用在线培训，使用Adagrad调整模型参数。为了避免过拟合，对word embedding使用0.2点dropout。隐藏层$H_d, H_c,H_s$的大小都设置为32。我们使用Skip-gram算法训练word embedding，并在训练期间对它们进行微调。 Word embedding的大小是128。</p>\n<h3 id=\"3-Experiment\"><a href=\"#3-Experiment\" class=\"headerlink\" title=\"3. Experiment\"></a>3. Experiment</h3><p>见paper原文</p>\n<h3 id=\"4-Conclusion\"><a href=\"#4-Conclusion\" class=\"headerlink\" title=\"4. Conclusion\"></a>4. Conclusion</h3><p>文献[^1]提出了一个joint model，通过使用全局共享表示和不同子任务之间的堆叠来共同检测、聚类和摘要事件。 实验表明，我们提出的joint model比pipeline模型更有效。该联合神经系统优于采用离散或神经网络进行新闻事件检测和摘要的最新baseline。</p>\n<h2 id=\"Bibliography\"><a href=\"#Bibliography\" class=\"headerlink\" title=\"Bibliography\"></a>Bibliography</h2><p>代码开源：<a href=\"https://github.com/wangzq870305/joint_event_detection\">https://github.com/wangzq870305/joint_event_detection</a></p>\n<p>[^1]: Wang, Z., &amp; Zhang, Y. (2017, August). A neural model for joint event detection and summarization. In <em>Proceedings of the 26th International Joint Conference on Artificial Intelligence</em> (pp. 4158-4164). AAAI Press.<br>[^2]: Wang, Z., Shou, L., Chen, K., Chen, G., &amp; Mehrotra, S. (2015). On summarization and timeline generation for evolutionary tweet streams. <em>IEEE Transactions on Knowledge and Data Engineering</em>, <em>27</em>(5), 1301-1315.</p>\n"},{"title":"Learning beyond datasets - Knowledge Graph Augmented Neural Networks for Natural language Processing 阅读笔记","date":"2018-05-10T00:00:00.000Z","_content":"\n## Knowledge Graph Augmented Neural Networks for Natural language Processing 阅读笔记\n\n**摘要**：机器学习的效果一般依赖于具体的训练数据。一些学习模型可以结合贝叶斯中的先验知识，但是这些模型不具备根据需要访问任何有组织的知识的能力。在这项工作[^1]中，我们以知识图谱（KG）的形式为NLP模型提供先验知识，使得模型取得更好的效果。我们的目标是开发一种深度学习模型，可以根据任务使用attention机制从知识图谱中提取相关的先验支持事实。为了减少attention空间，我们引入了基于卷积的模型来学习知识图谱实体和关系集的表示。提出的方法是高度可扩展的，并可应用于常用的NLP任务。使用这种方法，我们在实验中显示了文本分类性能的显着提高。我们还证明了，当深度学习模型使用知识图谱以辅助时，可以用较为少量的标记训练数据进行训练。\n\n## 1. Introduction\n\n现在机器学习主要是针对特定任务、特定训练数据进行训练的模型。虽然transfer learning试图将学习从一个任务迁移到另一个任务，但在可扩展性方面有局限性，通常是具体地针对某个的任务。另一方面，我们知道人类具有一种内在的能力，可以根据需求从脑中获取所需的知识，并结合我们新学习的概念来解决问题。\n\n这就引出了我们要在本文中讨论的问题：是否有可能设计一个学习模型，除了从培训数据集中学习外，还可以在预测时利用大量外在的知识？![X20180503-190446@2](https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-05-03-WX20180503-190446%402x.png)\n\n我们有一个基本的想法如上图，$\\mathcal{X}$是原本的输入，$\\mathcal{Y}$是输出。通过知识库补充、增强$\\mathcal{X}$，以得到$\\mathcal{X_w}$，将两这串联，获得$\\mathcal{X'}$作为新的输入。\n\n这里我们知识库以知识图谱的形势呈现，主要将一个事实（fact）表现为三元组：(subject entity, relation, object entity)简记为(h,r,t)。其他关于知识图谱的介绍可以参考以前的笔记和相关的文献，这里不再赘述。\n\n通常我们通过训练集来得到我们所需的模型，但是它往往缺乏world knowledge或者常识，结果往往会有偏差。例如 ：“特朗普慰问了得克萨斯州的飓风幸存者和他们的家人”，我们需要知道$\\langle \\text{特朗普},\\text{总统},\\text{美国} \\rangle$和$\\langle \\text{得克萨斯州},\\text{州},\\text{美国} \\rangle$才能判断这是一个政治事件。因此我们认为对于机器学习模型，除了代表ground-truth的用于训练的数据集以外，我们还可以从结构化的知识库获取相关知识，以提高整体性能。\n\n因此我们提出了一个深度学习模型，可以根据需求从知识图谱中提取相关的事实，并将其也作为输入特征加以补充。特别的，当知识图谱非常大的时候，即其中的三元组数量非常大，以至于我们不可能逐一比较来提出相关信息时，我们提出了一种基于深度学习的搜寻机制，来大大提高搜寻速度，我们将在后文具体描述。\n\n## 2. Knowledge graph representations\n\n实体和关系需要进行embedding以进行后续处理，目前有很多种知识图谱的表示方法，主要可以被分为：structure-based embedding，semantically-enriched embedding。\n\n#### structure-based embedding\n\n其中包括经典的TransE以及其各种变体，它的基本假设就是$h + r = t$. 之前有一篇笔记主要介绍的就是这类知识图谱表示[这里](https://blog.lorrin.info/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/)。\n\n#### semantically-enriched embedding\n\n这些embedding技术学习表示KG的实体/关系及其语义信息。 神经张量网络（NTN）Socher et al [2011]是该领域的先驱工作，它使用平均词嵌入和基于张量的操作初始化实体向量。 最近涉及这个想法的作品是“联合对齐”Zhong et al。 [2015]和SSP Xiao et al。[2017]。 DKRL Xie et al [2016]是一种KG表示技术，它也保留了TransE模型简单结构的文本描述性。 预训过的word2vec被用来形成实体表示，通过一个卷积神经网络（CNN）来约束要保持的关系。\n\n作者采用了DKRL，因为它强调了文本的语义描述，同时，它也继承了TransE的方法。因此我们能够通过$t = h + r$的关系来提取相关实体或关系。这大大减少了提取fact（三元组）的复杂度，因为实体关系的组合数远小于三元组的数量，因此能够让这个过程的速度更快。\n\n## 3. The proposed model\n\n我们以一个文本分类模型为例，模型的参数为$\\Theta$，训练集为$x$，标签为$y$，我们需要最大化下列方程：\n$$\n\\max_{\\Theta}{P(y|x, \\Theta)}\n$$\n\n因此：\n$$\n\\Theta = \\arg\\max_{\\Theta} {\\log{P(y|x, \\Theta)}}\n$$\n这里，我们通过结合world knowledge特征$x_w$来增强监督学习过程。使用数据$x$检索world knowledge特征，使用单独的模型，其中$x_w = F(x, \\Theta^{(2)})$。 因此，我们修改的目标函数为：\n$$\n\\max_{\\Theta}{P(y|x, x_w, \\Theta^{(1)})}\n$$\n其中$\\Theta = \\{\\Theta^{(1)}, \\Theta^{(2)}\\}$。可以获得优化的参数：\n$$\n\\Theta = \\arg\\max_{\\Theta} {\\log{P(y|x,F(x, \\Theta^{(2)}), \\Theta^{(1)})}}\n$$\n后面的部分着重于函数F的表达，该函数使用数据x进行事实三重检索。在实验中，我们使用经过softmax的输入的LSTM Greff et al. [2015]编码作为P的形式。对于F，我们使用soft attention。\n\n基于此，我们提出两种模型：A. 朴素模型；B. 基于卷积的实体和关系集群表示\n\n#### A. 朴素模型\n\n前面解释过，KG的实体和关系使用DKRL进行编码。 令$e_i \\in \\mathbb{R}^m$代表实体i的编码，$r_j\\in \\mathbb R^m$代表KG中第j个关系。 输入文本以串联的单词向量$x =(x_1,x_2,…,x_T)$的形式首先使用LSTM Greff et al. [2015]模块如下，\n$$\nh_t = f(x_t, h_{t-1})\n$$\n以及\n$$\no = \\frac{1}{T}\\sum_{t=1}^{T}{h_t}\n$$\n$h_t \\in \\mathbb{R}^n$是LSTM的隐藏状态，f是非线性函数，T是序列长度。 然后如下形成一个上下文向量，\n$$\nC = ReLU(o^T W)\n$$\n其中，$W\\in \\mathbb R^n \\times m$表示权重参数。设置两个同样的过程以形成两个独立的上下文向量，一个用于实体检索($C_E$)和一个用于关系检索($C_R$)。\n\n由于在朴素模型中KG的事实三元组的数量是数以百万计的，所以我们分别对实体和关系空间产生注意力。 然后使用检索到的实体和关系形成事实。使用实体上下文向量的实体的注意力由下式给出：\n$$\n\\alpha_{e_i} = \\frac{\\exp{C_E^T{e_i}}}{\\sum_{j=0}^{|E|} \\exp{C_E^T{e_j}}}\n$$\n同理，关系的注意力：\n$$\n\\alpha_{r_i} = \\frac{\\exp{C_R^T{r_i}}}{\\sum_{j=0}^{|R|} \\exp{C_R^T{r_j}}}\n$$\n![mage-20180511000712](https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-05-10-image-201805110007120.png)\n\n图2显示了实体/关系检索的示意图。在计算出最终的实体和关系向量之后，我们希望补充事实三元组。用于实验的KG技术是DKRL，其使用TransE模型假设($h + r = t$)。因此，使用主题实体(subject entity)和关系，我们将对象实体形成为$t = e + r$。 因此，检索的事实三元组是$\\mathcal F = [e，r，e + r]$，其中$F \\in \\mathbb R^{3m}$。 该检索到的事实信息与使用LSTM模块获得的输入x的上下文向量(C)一起连接。 最终分类标签$\\mathbb y$的计算如下，\n$$\n\\mathcal F' = ReLU(\\mathcal F^T V) \\\\\n\\mathbb y = softmax([\\mathcal F' : C]^T U)\n$$\n其中，$V∈\\mathbb R^{3m \\times u}$和$U\\in \\mathbb R^{2u\\times u}$是要学习的模型参数。 $\\mathbb y$是预测结果，用于计算交叉熵损失。我们尽量减少训练样本的平均损失，以便使用随机梯度下降来学习各种模型参数。最后的预测$\\mathbb y$现在包含了来自数据集特定信息和世界知识的信息，以帮助提高性能。在联合培训注意力机制的同时调整自己，以检索进行最终分类所需的相关事实。\n\n#### A+. 预训练KG检索模型\n\n![mage-20180511003414](https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-05-10-image-201805110034147.png)\n\n朴素模型需要考虑整个实体/关系空间，这不是一个好的方法，因为我们观察到每个attention的容易饱和。在一起训练分类和检索模块时，模型往往会忽略KG部分，而梯度只通过分类模块进行传播。这在一定程度上是可以预料的，因为当前任务的大多数相关信息来自训练样本，只有背景辅助信息来自KG。经过几个训练阶段后，KG检索到的事实总是收敛到一个固定的向量。为了克服这个问题，我们试图单独预先训练KG检索部分。预训练的KG模型用于检索事实，然后与分类模块连接，同时，在联合训练时通过预先训练的模型可能会导致传递误差。我们推断，KG不会返回噪音，并且对于任务具有基本信息，因为单独的KG部件单独显示出显着的性能（News20为59％，SNLI为66％）。图3描述了整个训练过程。该程序解决了联合训练时KG检索部分中的梯度饱和问题。但是，attention机制必须覆盖大量实体/关系的关键问题依然存在。\n\n#### B. 基于卷积的实体和关系集群表示\n\n在本节中，我们提出了一种机制来减少知识图谱中需要attention的大量实体/关系。 我们通过学习类似实体/关系向量的表示来减少attention空间。\n\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-05-10-image-201805110044225.png\" width=\"60%\">\n\n为了聚类相似的实体/关系向量，我们使用k-means聚类，并在每个聚类中形成具有相同数量的实体/关系向量的$l$个聚类。然后使用CNN对每个cluster进行编码。k-means聚类的输出是一个实体/关系向量序列$\\{e^T_1,e^T_2,…,e^T_q\\}$，其中$e_i \\in\\mathbb R^m$，每个聚类中的元素个数为$ q =⌈\\frac{| E |}{l}⌉$。对于每个cluster，这些矢量被堆叠形成$\\mathcal E$作为到CNN编码器的2D输入，其中$\\mathcal E\\in\\mathbb R^{m\\times q}$。 在寻找合适滤波器形状的实验中，我们发现使用2-D滤波器，该模型无法收敛。因此，我们推断，向量$e_i$中两个不同索引的潜在表示不应该被卷积修改。然后，我们采用一维卷积滤波器，只沿$\\mathcal E$列滑动，如图4所示。沿着y轴的步长是窗口长度k，卷积层的输出表示为：\n$$\n\\mathcal E'(i,j) = W^T[e_{i,j}, e_{i+1,j},...,e_{i+k-1, j}]^T\n$$\n其中，$\\mathcal E'(i,j)$是输出矩阵$\\mathcal E'$的第(i, j)个元素，$W\\in \\mathbb R^k$是卷积权重滤波器。为了减少参数空间，在卷积层之后放置一个pooling层，我们只沿y轴使用一维窗口，类似于上面提到的卷积核。我们使用了一个双层卷积网络，其步长k和最大池窗口n被调整以获得输出$\\mathcal E_i\\in \\mathbb R^m$，其中i是聚类索引。对于关系也进行类似的聚类过程，接着对聚类实体进行编码。这样，实体和关系空间都被缩减为包含更少的元素，每个cluster都有一个元素。在形成紧凑的实体空间$E$和关系空间$R$之后，我们采用了与之前相同的步骤来形成attention，但是现在，由于梯度有效地传递并且没有被过大的空间所阻塞，所以训练更有效。此外，由于卷积架构也同时得到训练，所以attention机制并没有像以前那样通过实体和关系的巨大空间来学习。\n\n## 4. Conclusion\n\n实验表明，引入KG不仅降低了深度学习模型对训练集的依赖，还显著地提高了预测结果的准确度，在数据集不够的情况下效果更佳拔群。此外，本文的方法对world knowledge的处理、embedding的方法是高度可扩展，可以应用于各种NLP任务。\n\n\n\n## Bibliography\n\n笔记参考 https://blog.csdn.net/TgqDT3gGaMdkHasLZv/article/details/80118742\n\n[^1]: Annervaz, K. M., Somnath Basu Roy Chowdhury, and Ambedkar Dukkipati. 'Learning beyond datasets: Knowledge Graph Augmented Neural Networks for Natural language Processing.' arXiv preprint arXiv:1802.05930 (2018).","source":"_posts/[2018.5.10]Knowledge-Graph-Augmented-Neural-Networks-for-NLP.md","raw":"---\ntitle: Learning beyond datasets - Knowledge Graph Augmented Neural Networks for Natural language Processing 阅读笔记\ndate: 2018-05-10 08:00:00\ncategories: [research]\ntags: [neural-network, NLP, knowledge-graph]\n---\n\n## Knowledge Graph Augmented Neural Networks for Natural language Processing 阅读笔记\n\n**摘要**：机器学习的效果一般依赖于具体的训练数据。一些学习模型可以结合贝叶斯中的先验知识，但是这些模型不具备根据需要访问任何有组织的知识的能力。在这项工作[^1]中，我们以知识图谱（KG）的形式为NLP模型提供先验知识，使得模型取得更好的效果。我们的目标是开发一种深度学习模型，可以根据任务使用attention机制从知识图谱中提取相关的先验支持事实。为了减少attention空间，我们引入了基于卷积的模型来学习知识图谱实体和关系集的表示。提出的方法是高度可扩展的，并可应用于常用的NLP任务。使用这种方法，我们在实验中显示了文本分类性能的显着提高。我们还证明了，当深度学习模型使用知识图谱以辅助时，可以用较为少量的标记训练数据进行训练。\n\n## 1. Introduction\n\n现在机器学习主要是针对特定任务、特定训练数据进行训练的模型。虽然transfer learning试图将学习从一个任务迁移到另一个任务，但在可扩展性方面有局限性，通常是具体地针对某个的任务。另一方面，我们知道人类具有一种内在的能力，可以根据需求从脑中获取所需的知识，并结合我们新学习的概念来解决问题。\n\n这就引出了我们要在本文中讨论的问题：是否有可能设计一个学习模型，除了从培训数据集中学习外，还可以在预测时利用大量外在的知识？![X20180503-190446@2](https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-05-03-WX20180503-190446%402x.png)\n\n我们有一个基本的想法如上图，$\\mathcal{X}$是原本的输入，$\\mathcal{Y}$是输出。通过知识库补充、增强$\\mathcal{X}$，以得到$\\mathcal{X_w}$，将两这串联，获得$\\mathcal{X'}$作为新的输入。\n\n这里我们知识库以知识图谱的形势呈现，主要将一个事实（fact）表现为三元组：(subject entity, relation, object entity)简记为(h,r,t)。其他关于知识图谱的介绍可以参考以前的笔记和相关的文献，这里不再赘述。\n\n通常我们通过训练集来得到我们所需的模型，但是它往往缺乏world knowledge或者常识，结果往往会有偏差。例如 ：“特朗普慰问了得克萨斯州的飓风幸存者和他们的家人”，我们需要知道$\\langle \\text{特朗普},\\text{总统},\\text{美国} \\rangle$和$\\langle \\text{得克萨斯州},\\text{州},\\text{美国} \\rangle$才能判断这是一个政治事件。因此我们认为对于机器学习模型，除了代表ground-truth的用于训练的数据集以外，我们还可以从结构化的知识库获取相关知识，以提高整体性能。\n\n因此我们提出了一个深度学习模型，可以根据需求从知识图谱中提取相关的事实，并将其也作为输入特征加以补充。特别的，当知识图谱非常大的时候，即其中的三元组数量非常大，以至于我们不可能逐一比较来提出相关信息时，我们提出了一种基于深度学习的搜寻机制，来大大提高搜寻速度，我们将在后文具体描述。\n\n## 2. Knowledge graph representations\n\n实体和关系需要进行embedding以进行后续处理，目前有很多种知识图谱的表示方法，主要可以被分为：structure-based embedding，semantically-enriched embedding。\n\n#### structure-based embedding\n\n其中包括经典的TransE以及其各种变体，它的基本假设就是$h + r = t$. 之前有一篇笔记主要介绍的就是这类知识图谱表示[这里](https://blog.lorrin.info/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/)。\n\n#### semantically-enriched embedding\n\n这些embedding技术学习表示KG的实体/关系及其语义信息。 神经张量网络（NTN）Socher et al [2011]是该领域的先驱工作，它使用平均词嵌入和基于张量的操作初始化实体向量。 最近涉及这个想法的作品是“联合对齐”Zhong et al。 [2015]和SSP Xiao et al。[2017]。 DKRL Xie et al [2016]是一种KG表示技术，它也保留了TransE模型简单结构的文本描述性。 预训过的word2vec被用来形成实体表示，通过一个卷积神经网络（CNN）来约束要保持的关系。\n\n作者采用了DKRL，因为它强调了文本的语义描述，同时，它也继承了TransE的方法。因此我们能够通过$t = h + r$的关系来提取相关实体或关系。这大大减少了提取fact（三元组）的复杂度，因为实体关系的组合数远小于三元组的数量，因此能够让这个过程的速度更快。\n\n## 3. The proposed model\n\n我们以一个文本分类模型为例，模型的参数为$\\Theta$，训练集为$x$，标签为$y$，我们需要最大化下列方程：\n$$\n\\max_{\\Theta}{P(y|x, \\Theta)}\n$$\n\n因此：\n$$\n\\Theta = \\arg\\max_{\\Theta} {\\log{P(y|x, \\Theta)}}\n$$\n这里，我们通过结合world knowledge特征$x_w$来增强监督学习过程。使用数据$x$检索world knowledge特征，使用单独的模型，其中$x_w = F(x, \\Theta^{(2)})$。 因此，我们修改的目标函数为：\n$$\n\\max_{\\Theta}{P(y|x, x_w, \\Theta^{(1)})}\n$$\n其中$\\Theta = \\{\\Theta^{(1)}, \\Theta^{(2)}\\}$。可以获得优化的参数：\n$$\n\\Theta = \\arg\\max_{\\Theta} {\\log{P(y|x,F(x, \\Theta^{(2)}), \\Theta^{(1)})}}\n$$\n后面的部分着重于函数F的表达，该函数使用数据x进行事实三重检索。在实验中，我们使用经过softmax的输入的LSTM Greff et al. [2015]编码作为P的形式。对于F，我们使用soft attention。\n\n基于此，我们提出两种模型：A. 朴素模型；B. 基于卷积的实体和关系集群表示\n\n#### A. 朴素模型\n\n前面解释过，KG的实体和关系使用DKRL进行编码。 令$e_i \\in \\mathbb{R}^m$代表实体i的编码，$r_j\\in \\mathbb R^m$代表KG中第j个关系。 输入文本以串联的单词向量$x =(x_1,x_2,…,x_T)$的形式首先使用LSTM Greff et al. [2015]模块如下，\n$$\nh_t = f(x_t, h_{t-1})\n$$\n以及\n$$\no = \\frac{1}{T}\\sum_{t=1}^{T}{h_t}\n$$\n$h_t \\in \\mathbb{R}^n$是LSTM的隐藏状态，f是非线性函数，T是序列长度。 然后如下形成一个上下文向量，\n$$\nC = ReLU(o^T W)\n$$\n其中，$W\\in \\mathbb R^n \\times m$表示权重参数。设置两个同样的过程以形成两个独立的上下文向量，一个用于实体检索($C_E$)和一个用于关系检索($C_R$)。\n\n由于在朴素模型中KG的事实三元组的数量是数以百万计的，所以我们分别对实体和关系空间产生注意力。 然后使用检索到的实体和关系形成事实。使用实体上下文向量的实体的注意力由下式给出：\n$$\n\\alpha_{e_i} = \\frac{\\exp{C_E^T{e_i}}}{\\sum_{j=0}^{|E|} \\exp{C_E^T{e_j}}}\n$$\n同理，关系的注意力：\n$$\n\\alpha_{r_i} = \\frac{\\exp{C_R^T{r_i}}}{\\sum_{j=0}^{|R|} \\exp{C_R^T{r_j}}}\n$$\n![mage-20180511000712](https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-05-10-image-201805110007120.png)\n\n图2显示了实体/关系检索的示意图。在计算出最终的实体和关系向量之后，我们希望补充事实三元组。用于实验的KG技术是DKRL，其使用TransE模型假设($h + r = t$)。因此，使用主题实体(subject entity)和关系，我们将对象实体形成为$t = e + r$。 因此，检索的事实三元组是$\\mathcal F = [e，r，e + r]$，其中$F \\in \\mathbb R^{3m}$。 该检索到的事实信息与使用LSTM模块获得的输入x的上下文向量(C)一起连接。 最终分类标签$\\mathbb y$的计算如下，\n$$\n\\mathcal F' = ReLU(\\mathcal F^T V) \\\\\n\\mathbb y = softmax([\\mathcal F' : C]^T U)\n$$\n其中，$V∈\\mathbb R^{3m \\times u}$和$U\\in \\mathbb R^{2u\\times u}$是要学习的模型参数。 $\\mathbb y$是预测结果，用于计算交叉熵损失。我们尽量减少训练样本的平均损失，以便使用随机梯度下降来学习各种模型参数。最后的预测$\\mathbb y$现在包含了来自数据集特定信息和世界知识的信息，以帮助提高性能。在联合培训注意力机制的同时调整自己，以检索进行最终分类所需的相关事实。\n\n#### A+. 预训练KG检索模型\n\n![mage-20180511003414](https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-05-10-image-201805110034147.png)\n\n朴素模型需要考虑整个实体/关系空间，这不是一个好的方法，因为我们观察到每个attention的容易饱和。在一起训练分类和检索模块时，模型往往会忽略KG部分，而梯度只通过分类模块进行传播。这在一定程度上是可以预料的，因为当前任务的大多数相关信息来自训练样本，只有背景辅助信息来自KG。经过几个训练阶段后，KG检索到的事实总是收敛到一个固定的向量。为了克服这个问题，我们试图单独预先训练KG检索部分。预训练的KG模型用于检索事实，然后与分类模块连接，同时，在联合训练时通过预先训练的模型可能会导致传递误差。我们推断，KG不会返回噪音，并且对于任务具有基本信息，因为单独的KG部件单独显示出显着的性能（News20为59％，SNLI为66％）。图3描述了整个训练过程。该程序解决了联合训练时KG检索部分中的梯度饱和问题。但是，attention机制必须覆盖大量实体/关系的关键问题依然存在。\n\n#### B. 基于卷积的实体和关系集群表示\n\n在本节中，我们提出了一种机制来减少知识图谱中需要attention的大量实体/关系。 我们通过学习类似实体/关系向量的表示来减少attention空间。\n\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-05-10-image-201805110044225.png\" width=\"60%\">\n\n为了聚类相似的实体/关系向量，我们使用k-means聚类，并在每个聚类中形成具有相同数量的实体/关系向量的$l$个聚类。然后使用CNN对每个cluster进行编码。k-means聚类的输出是一个实体/关系向量序列$\\{e^T_1,e^T_2,…,e^T_q\\}$，其中$e_i \\in\\mathbb R^m$，每个聚类中的元素个数为$ q =⌈\\frac{| E |}{l}⌉$。对于每个cluster，这些矢量被堆叠形成$\\mathcal E$作为到CNN编码器的2D输入，其中$\\mathcal E\\in\\mathbb R^{m\\times q}$。 在寻找合适滤波器形状的实验中，我们发现使用2-D滤波器，该模型无法收敛。因此，我们推断，向量$e_i$中两个不同索引的潜在表示不应该被卷积修改。然后，我们采用一维卷积滤波器，只沿$\\mathcal E$列滑动，如图4所示。沿着y轴的步长是窗口长度k，卷积层的输出表示为：\n$$\n\\mathcal E'(i,j) = W^T[e_{i,j}, e_{i+1,j},...,e_{i+k-1, j}]^T\n$$\n其中，$\\mathcal E'(i,j)$是输出矩阵$\\mathcal E'$的第(i, j)个元素，$W\\in \\mathbb R^k$是卷积权重滤波器。为了减少参数空间，在卷积层之后放置一个pooling层，我们只沿y轴使用一维窗口，类似于上面提到的卷积核。我们使用了一个双层卷积网络，其步长k和最大池窗口n被调整以获得输出$\\mathcal E_i\\in \\mathbb R^m$，其中i是聚类索引。对于关系也进行类似的聚类过程，接着对聚类实体进行编码。这样，实体和关系空间都被缩减为包含更少的元素，每个cluster都有一个元素。在形成紧凑的实体空间$E$和关系空间$R$之后，我们采用了与之前相同的步骤来形成attention，但是现在，由于梯度有效地传递并且没有被过大的空间所阻塞，所以训练更有效。此外，由于卷积架构也同时得到训练，所以attention机制并没有像以前那样通过实体和关系的巨大空间来学习。\n\n## 4. Conclusion\n\n实验表明，引入KG不仅降低了深度学习模型对训练集的依赖，还显著地提高了预测结果的准确度，在数据集不够的情况下效果更佳拔群。此外，本文的方法对world knowledge的处理、embedding的方法是高度可扩展，可以应用于各种NLP任务。\n\n\n\n## Bibliography\n\n笔记参考 https://blog.csdn.net/TgqDT3gGaMdkHasLZv/article/details/80118742\n\n[^1]: Annervaz, K. M., Somnath Basu Roy Chowdhury, and Ambedkar Dukkipati. 'Learning beyond datasets: Knowledge Graph Augmented Neural Networks for Natural language Processing.' arXiv preprint arXiv:1802.05930 (2018).","slug":"[2018.5.10]Knowledge-Graph-Augmented-Neural-Networks-for-NLP","published":1,"updated":"2020-11-03T03:26:05.090Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckw4qufpv002qgwtl5hcyhnfp","content":"<h2 id=\"Knowledge-Graph-Augmented-Neural-Networks-for-Natural-language-Processing-阅读笔记\"><a href=\"#Knowledge-Graph-Augmented-Neural-Networks-for-Natural-language-Processing-阅读笔记\" class=\"headerlink\" title=\"Knowledge Graph Augmented Neural Networks for Natural language Processing 阅读笔记\"></a>Knowledge Graph Augmented Neural Networks for Natural language Processing 阅读笔记</h2><p><strong>摘要</strong>：机器学习的效果一般依赖于具体的训练数据。一些学习模型可以结合贝叶斯中的先验知识，但是这些模型不具备根据需要访问任何有组织的知识的能力。在这项工作[^1]中，我们以知识图谱（KG）的形式为NLP模型提供先验知识，使得模型取得更好的效果。我们的目标是开发一种深度学习模型，可以根据任务使用attention机制从知识图谱中提取相关的先验支持事实。为了减少attention空间，我们引入了基于卷积的模型来学习知识图谱实体和关系集的表示。提出的方法是高度可扩展的，并可应用于常用的NLP任务。使用这种方法，我们在实验中显示了文本分类性能的显着提高。我们还证明了，当深度学习模型使用知识图谱以辅助时，可以用较为少量的标记训练数据进行训练。</p>\n<h2 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1. Introduction\"></a>1. Introduction</h2><p>现在机器学习主要是针对特定任务、特定训练数据进行训练的模型。虽然transfer learning试图将学习从一个任务迁移到另一个任务，但在可扩展性方面有局限性，通常是具体地针对某个的任务。另一方面，我们知道人类具有一种内在的能力，可以根据需求从脑中获取所需的知识，并结合我们新学习的概念来解决问题。</p>\n<p>这就引出了我们要在本文中讨论的问题：是否有可能设计一个学习模型，除了从培训数据集中学习外，还可以在预测时利用大量外在的知识？<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-05-03-WX20180503-190446%402x.png\" alt=\"X20180503-190446@2\"></p>\n<p>我们有一个基本的想法如上图，$\\mathcal{X}$是原本的输入，$\\mathcal{Y}$是输出。通过知识库补充、增强$\\mathcal{X}$，以得到$\\mathcal{X_w}$，将两这串联，获得$\\mathcal{X’}$作为新的输入。</p>\n<p>这里我们知识库以知识图谱的形势呈现，主要将一个事实（fact）表现为三元组：(subject entity, relation, object entity)简记为(h,r,t)。其他关于知识图谱的介绍可以参考以前的笔记和相关的文献，这里不再赘述。</p>\n<p>通常我们通过训练集来得到我们所需的模型，但是它往往缺乏world knowledge或者常识，结果往往会有偏差。例如 ：“特朗普慰问了得克萨斯州的飓风幸存者和他们的家人”，我们需要知道$\\langle \\text{特朗普},\\text{总统},\\text{美国} \\rangle$和$\\langle \\text{得克萨斯州},\\text{州},\\text{美国} \\rangle$才能判断这是一个政治事件。因此我们认为对于机器学习模型，除了代表ground-truth的用于训练的数据集以外，我们还可以从结构化的知识库获取相关知识，以提高整体性能。</p>\n<p>因此我们提出了一个深度学习模型，可以根据需求从知识图谱中提取相关的事实，并将其也作为输入特征加以补充。特别的，当知识图谱非常大的时候，即其中的三元组数量非常大，以至于我们不可能逐一比较来提出相关信息时，我们提出了一种基于深度学习的搜寻机制，来大大提高搜寻速度，我们将在后文具体描述。</p>\n<h2 id=\"2-Knowledge-graph-representations\"><a href=\"#2-Knowledge-graph-representations\" class=\"headerlink\" title=\"2. Knowledge graph representations\"></a>2. Knowledge graph representations</h2><p>实体和关系需要进行embedding以进行后续处理，目前有很多种知识图谱的表示方法，主要可以被分为：structure-based embedding，semantically-enriched embedding。</p>\n<h4 id=\"structure-based-embedding\"><a href=\"#structure-based-embedding\" class=\"headerlink\" title=\"structure-based embedding\"></a>structure-based embedding</h4><p>其中包括经典的TransE以及其各种变体，它的基本假设就是$h + r = t$. 之前有一篇笔记主要介绍的就是这类知识图谱表示<a href=\"https://blog.lorrin.info/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/\">这里</a>。</p>\n<h4 id=\"semantically-enriched-embedding\"><a href=\"#semantically-enriched-embedding\" class=\"headerlink\" title=\"semantically-enriched embedding\"></a>semantically-enriched embedding</h4><p>这些embedding技术学习表示KG的实体/关系及其语义信息。 神经张量网络（NTN）Socher et al [2011]是该领域的先驱工作，它使用平均词嵌入和基于张量的操作初始化实体向量。 最近涉及这个想法的作品是“联合对齐”Zhong et al。 [2015]和SSP Xiao et al。[2017]。 DKRL Xie et al [2016]是一种KG表示技术，它也保留了TransE模型简单结构的文本描述性。 预训过的word2vec被用来形成实体表示，通过一个卷积神经网络（CNN）来约束要保持的关系。</p>\n<p>作者采用了DKRL，因为它强调了文本的语义描述，同时，它也继承了TransE的方法。因此我们能够通过$t = h + r$的关系来提取相关实体或关系。这大大减少了提取fact（三元组）的复杂度，因为实体关系的组合数远小于三元组的数量，因此能够让这个过程的速度更快。</p>\n<h2 id=\"3-The-proposed-model\"><a href=\"#3-The-proposed-model\" class=\"headerlink\" title=\"3. The proposed model\"></a>3. The proposed model</h2><p>我们以一个文本分类模型为例，模型的参数为$\\Theta$，训练集为$x$，标签为$y$，我们需要最大化下列方程：<br>$$<br>\\max_{\\Theta}{P(y|x, \\Theta)}<br>$$</p>\n<p>因此：<br>$$<br>\\Theta = \\arg\\max_{\\Theta} {\\log{P(y|x, \\Theta)}}<br>$$<br>这里，我们通过结合world knowledge特征$x_w$来增强监督学习过程。使用数据$x$检索world knowledge特征，使用单独的模型，其中$x_w = F(x, \\Theta^{(2)})$。 因此，我们修改的目标函数为：<br>$$<br>\\max_{\\Theta}{P(y|x, x_w, \\Theta^{(1)})}<br>$$<br>其中$\\Theta = {\\Theta^{(1)}, \\Theta^{(2)}}$。可以获得优化的参数：<br>$$<br>\\Theta = \\arg\\max_{\\Theta} {\\log{P(y|x,F(x, \\Theta^{(2)}), \\Theta^{(1)})}}<br>$$<br>后面的部分着重于函数F的表达，该函数使用数据x进行事实三重检索。在实验中，我们使用经过softmax的输入的LSTM Greff et al. [2015]编码作为P的形式。对于F，我们使用soft attention。</p>\n<p>基于此，我们提出两种模型：A. 朴素模型；B. 基于卷积的实体和关系集群表示</p>\n<h4 id=\"A-朴素模型\"><a href=\"#A-朴素模型\" class=\"headerlink\" title=\"A. 朴素模型\"></a>A. 朴素模型</h4><p>前面解释过，KG的实体和关系使用DKRL进行编码。 令$e_i \\in \\mathbb{R}^m$代表实体i的编码，$r_j\\in \\mathbb R^m$代表KG中第j个关系。 输入文本以串联的单词向量$x =(x_1,x_2,…,x_T)$的形式首先使用LSTM Greff et al. [2015]模块如下，<br>$$<br>h_t = f(x_t, h_{t-1})<br>$$<br>以及<br>$$<br>o = \\frac{1}{T}\\sum_{t=1}^{T}{h_t}<br>$$<br>$h_t \\in \\mathbb{R}^n$是LSTM的隐藏状态，f是非线性函数，T是序列长度。 然后如下形成一个上下文向量，<br>$$<br>C = ReLU(o^T W)<br>$$<br>其中，$W\\in \\mathbb R^n \\times m$表示权重参数。设置两个同样的过程以形成两个独立的上下文向量，一个用于实体检索($C_E$)和一个用于关系检索($C_R$)。</p>\n<p>由于在朴素模型中KG的事实三元组的数量是数以百万计的，所以我们分别对实体和关系空间产生注意力。 然后使用检索到的实体和关系形成事实。使用实体上下文向量的实体的注意力由下式给出：<br>$$<br>\\alpha_{e_i} = \\frac{\\exp{C_E^T{e_i}}}{\\sum_{j=0}^{|E|} \\exp{C_E^T{e_j}}}<br>$$<br>同理，关系的注意力：<br>$$<br>\\alpha_{r_i} = \\frac{\\exp{C_R^T{r_i}}}{\\sum_{j=0}^{|R|} \\exp{C_R^T{r_j}}}<br>$$<br><img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-05-10-image-201805110007120.png\" alt=\"mage-20180511000712\"></p>\n<p>图2显示了实体/关系检索的示意图。在计算出最终的实体和关系向量之后，我们希望补充事实三元组。用于实验的KG技术是DKRL，其使用TransE模型假设($h + r = t$)。因此，使用主题实体(subject entity)和关系，我们将对象实体形成为$t = e + r$。 因此，检索的事实三元组是$\\mathcal F = [e，r，e + r]$，其中$F \\in \\mathbb R^{3m}$。 该检索到的事实信息与使用LSTM模块获得的输入x的上下文向量(C)一起连接。 最终分类标签$\\mathbb y$的计算如下，<br>$$<br>\\mathcal F’ = ReLU(\\mathcal F^T V) \\<br>\\mathbb y = softmax([\\mathcal F’ : C]^T U)<br>$$<br>其中，$V∈\\mathbb R^{3m \\times u}$和$U\\in \\mathbb R^{2u\\times u}$是要学习的模型参数。 $\\mathbb y$是预测结果，用于计算交叉熵损失。我们尽量减少训练样本的平均损失，以便使用随机梯度下降来学习各种模型参数。最后的预测$\\mathbb y$现在包含了来自数据集特定信息和世界知识的信息，以帮助提高性能。在联合培训注意力机制的同时调整自己，以检索进行最终分类所需的相关事实。</p>\n<h4 id=\"A-预训练KG检索模型\"><a href=\"#A-预训练KG检索模型\" class=\"headerlink\" title=\"A+. 预训练KG检索模型\"></a>A+. 预训练KG检索模型</h4><p><img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-05-10-image-201805110034147.png\" alt=\"mage-20180511003414\"></p>\n<p>朴素模型需要考虑整个实体/关系空间，这不是一个好的方法，因为我们观察到每个attention的容易饱和。在一起训练分类和检索模块时，模型往往会忽略KG部分，而梯度只通过分类模块进行传播。这在一定程度上是可以预料的，因为当前任务的大多数相关信息来自训练样本，只有背景辅助信息来自KG。经过几个训练阶段后，KG检索到的事实总是收敛到一个固定的向量。为了克服这个问题，我们试图单独预先训练KG检索部分。预训练的KG模型用于检索事实，然后与分类模块连接，同时，在联合训练时通过预先训练的模型可能会导致传递误差。我们推断，KG不会返回噪音，并且对于任务具有基本信息，因为单独的KG部件单独显示出显着的性能（News20为59％，SNLI为66％）。图3描述了整个训练过程。该程序解决了联合训练时KG检索部分中的梯度饱和问题。但是，attention机制必须覆盖大量实体/关系的关键问题依然存在。</p>\n<h4 id=\"B-基于卷积的实体和关系集群表示\"><a href=\"#B-基于卷积的实体和关系集群表示\" class=\"headerlink\" title=\"B. 基于卷积的实体和关系集群表示\"></a>B. 基于卷积的实体和关系集群表示</h4><p>在本节中，我们提出了一种机制来减少知识图谱中需要attention的大量实体/关系。 我们通过学习类似实体/关系向量的表示来减少attention空间。</p>\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-05-10-image-201805110044225.png\" width=\"60%\">\n\n<p>为了聚类相似的实体/关系向量，我们使用k-means聚类，并在每个聚类中形成具有相同数量的实体/关系向量的$l$个聚类。然后使用CNN对每个cluster进行编码。k-means聚类的输出是一个实体/关系向量序列${e^T_1,e^T_2,…,e^T_q}$，其中$e_i \\in\\mathbb R^m$，每个聚类中的元素个数为$ q =⌈\\frac{| E |}{l}⌉$。对于每个cluster，这些矢量被堆叠形成$\\mathcal E$作为到CNN编码器的2D输入，其中$\\mathcal E\\in\\mathbb R^{m\\times q}$。 在寻找合适滤波器形状的实验中，我们发现使用2-D滤波器，该模型无法收敛。因此，我们推断，向量$e_i$中两个不同索引的潜在表示不应该被卷积修改。然后，我们采用一维卷积滤波器，只沿$\\mathcal E$列滑动，如图4所示。沿着y轴的步长是窗口长度k，卷积层的输出表示为：<br>$$<br>\\mathcal E’(i,j) = W^T[e_{i,j}, e_{i+1,j},…,e_{i+k-1, j}]^T<br>$$<br>其中，$\\mathcal E’(i,j)$是输出矩阵$\\mathcal E’$的第(i, j)个元素，$W\\in \\mathbb R^k$是卷积权重滤波器。为了减少参数空间，在卷积层之后放置一个pooling层，我们只沿y轴使用一维窗口，类似于上面提到的卷积核。我们使用了一个双层卷积网络，其步长k和最大池窗口n被调整以获得输出$\\mathcal E_i\\in \\mathbb R^m$，其中i是聚类索引。对于关系也进行类似的聚类过程，接着对聚类实体进行编码。这样，实体和关系空间都被缩减为包含更少的元素，每个cluster都有一个元素。在形成紧凑的实体空间$E$和关系空间$R$之后，我们采用了与之前相同的步骤来形成attention，但是现在，由于梯度有效地传递并且没有被过大的空间所阻塞，所以训练更有效。此外，由于卷积架构也同时得到训练，所以attention机制并没有像以前那样通过实体和关系的巨大空间来学习。</p>\n<h2 id=\"4-Conclusion\"><a href=\"#4-Conclusion\" class=\"headerlink\" title=\"4. Conclusion\"></a>4. Conclusion</h2><p>实验表明，引入KG不仅降低了深度学习模型对训练集的依赖，还显著地提高了预测结果的准确度，在数据集不够的情况下效果更佳拔群。此外，本文的方法对world knowledge的处理、embedding的方法是高度可扩展，可以应用于各种NLP任务。</p>\n<h2 id=\"Bibliography\"><a href=\"#Bibliography\" class=\"headerlink\" title=\"Bibliography\"></a>Bibliography</h2><p>笔记参考 <a href=\"https://blog.csdn.net/TgqDT3gGaMdkHasLZv/article/details/80118742\">https://blog.csdn.net/TgqDT3gGaMdkHasLZv/article/details/80118742</a></p>\n<p>[^1]: Annervaz, K. M., Somnath Basu Roy Chowdhury, and Ambedkar Dukkipati. ‘Learning beyond datasets: Knowledge Graph Augmented Neural Networks for Natural language Processing.’ arXiv preprint arXiv:1802.05930 (2018).</p>\n","site":{"data":{"recommended_posts":{"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"}],"https://LorrinWWW.github.io/posts/complexity/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/hexo-with-latex/":[{"title":"区块链时代的最正确的打赏姿势——在Hexo博客配置Coinhive网页挖矿","permalink":"http://www.davidfnck.com/blockchain/mine-xmr-in-hexo-by-coinhive.html/"}],"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/":[{"title":"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-statistic/":[{"title":"贝叶斯估计 Bayes estimation","permalink":"https://LorrinWWW.github.io/posts/Bayes-estimation/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"}],"https://LorrinWWW.github.io/posts/[2018.2.5]Nested-LSTMs/":[{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"}],"https://LorrinWWW.github.io/posts/datamining-pretreatment/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Several models for knowledge graph representing and completing 几个知识图谱模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/"}],"https://LorrinWWW.github.io/posts/compression/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/Sobolev-space/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/proba-ch5/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch3/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/proba-ch1/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/ML-CNN/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/EDP-basic-models/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/machine-learning/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/datamining-class-pred/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction/":[],"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/":[{"title":"OS notes","permalink":"https://LorrinWWW.github.io/posts/OS-notes/"}],"https://LorrinWWW.github.io/posts/projet-enjeu-plugin-chrome-101/":[{"title":"Chrome油猴脚本","permalink":"http://gubangzhong.cn/2017/06/13/Chrome油猴脚本/"}],"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/":[{"title":"MongoDB, Docker and Python","permalink":"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/":[{"title":"Hands on Scrapy","permalink":"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/OS-notes/":[{"title":"learning OS and building LorriOS","permalink":"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/"}],"https://LorrinWWW.github.io/posts/Bayes-estimation/":[{"title":"Note of statistic","permalink":"https://LorrinWWW.github.io/posts/Note-of-statistic/"}],"https://LorrinWWW.github.io/posts/management-of-the-firm/":[],"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"}],"https://LorrinWWW.github.io/posts/datamining-qualitative-induction/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-finite-element-method/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/QuadTree/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/proba-ch4/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch2/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/":[{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/Note-of-NLP/":[{"title":"Generative Adversarial Network","permalink":"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/"},{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"}],"https://LorrinWWW.github.io/posts/Note-of-datamining/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"数据挖掘-分类与预测","permalink":"https://LorrinWWW.github.io/posts/datamining-class-pred/"},{"title":"数据挖掘-预处理","permalink":"https://LorrinWWW.github.io/posts/datamining-pretreatment/"}],"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/Note-of-probability/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/participe-present-et-gerondif/":[],"https://LorrinWWW.github.io/posts/Hilbert-space/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/proba-ch6/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}]}}},"excerpt":"","more":"<h2 id=\"Knowledge-Graph-Augmented-Neural-Networks-for-Natural-language-Processing-阅读笔记\"><a href=\"#Knowledge-Graph-Augmented-Neural-Networks-for-Natural-language-Processing-阅读笔记\" class=\"headerlink\" title=\"Knowledge Graph Augmented Neural Networks for Natural language Processing 阅读笔记\"></a>Knowledge Graph Augmented Neural Networks for Natural language Processing 阅读笔记</h2><p><strong>摘要</strong>：机器学习的效果一般依赖于具体的训练数据。一些学习模型可以结合贝叶斯中的先验知识，但是这些模型不具备根据需要访问任何有组织的知识的能力。在这项工作[^1]中，我们以知识图谱（KG）的形式为NLP模型提供先验知识，使得模型取得更好的效果。我们的目标是开发一种深度学习模型，可以根据任务使用attention机制从知识图谱中提取相关的先验支持事实。为了减少attention空间，我们引入了基于卷积的模型来学习知识图谱实体和关系集的表示。提出的方法是高度可扩展的，并可应用于常用的NLP任务。使用这种方法，我们在实验中显示了文本分类性能的显着提高。我们还证明了，当深度学习模型使用知识图谱以辅助时，可以用较为少量的标记训练数据进行训练。</p>\n<h2 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1. Introduction\"></a>1. Introduction</h2><p>现在机器学习主要是针对特定任务、特定训练数据进行训练的模型。虽然transfer learning试图将学习从一个任务迁移到另一个任务，但在可扩展性方面有局限性，通常是具体地针对某个的任务。另一方面，我们知道人类具有一种内在的能力，可以根据需求从脑中获取所需的知识，并结合我们新学习的概念来解决问题。</p>\n<p>这就引出了我们要在本文中讨论的问题：是否有可能设计一个学习模型，除了从培训数据集中学习外，还可以在预测时利用大量外在的知识？<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-05-03-WX20180503-190446%402x.png\" alt=\"X20180503-190446@2\"></p>\n<p>我们有一个基本的想法如上图，$\\mathcal{X}$是原本的输入，$\\mathcal{Y}$是输出。通过知识库补充、增强$\\mathcal{X}$，以得到$\\mathcal{X_w}$，将两这串联，获得$\\mathcal{X’}$作为新的输入。</p>\n<p>这里我们知识库以知识图谱的形势呈现，主要将一个事实（fact）表现为三元组：(subject entity, relation, object entity)简记为(h,r,t)。其他关于知识图谱的介绍可以参考以前的笔记和相关的文献，这里不再赘述。</p>\n<p>通常我们通过训练集来得到我们所需的模型，但是它往往缺乏world knowledge或者常识，结果往往会有偏差。例如 ：“特朗普慰问了得克萨斯州的飓风幸存者和他们的家人”，我们需要知道$\\langle \\text{特朗普},\\text{总统},\\text{美国} \\rangle$和$\\langle \\text{得克萨斯州},\\text{州},\\text{美国} \\rangle$才能判断这是一个政治事件。因此我们认为对于机器学习模型，除了代表ground-truth的用于训练的数据集以外，我们还可以从结构化的知识库获取相关知识，以提高整体性能。</p>\n<p>因此我们提出了一个深度学习模型，可以根据需求从知识图谱中提取相关的事实，并将其也作为输入特征加以补充。特别的，当知识图谱非常大的时候，即其中的三元组数量非常大，以至于我们不可能逐一比较来提出相关信息时，我们提出了一种基于深度学习的搜寻机制，来大大提高搜寻速度，我们将在后文具体描述。</p>\n<h2 id=\"2-Knowledge-graph-representations\"><a href=\"#2-Knowledge-graph-representations\" class=\"headerlink\" title=\"2. Knowledge graph representations\"></a>2. Knowledge graph representations</h2><p>实体和关系需要进行embedding以进行后续处理，目前有很多种知识图谱的表示方法，主要可以被分为：structure-based embedding，semantically-enriched embedding。</p>\n<h4 id=\"structure-based-embedding\"><a href=\"#structure-based-embedding\" class=\"headerlink\" title=\"structure-based embedding\"></a>structure-based embedding</h4><p>其中包括经典的TransE以及其各种变体，它的基本假设就是$h + r = t$. 之前有一篇笔记主要介绍的就是这类知识图谱表示<a href=\"https://blog.lorrin.info/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/\">这里</a>。</p>\n<h4 id=\"semantically-enriched-embedding\"><a href=\"#semantically-enriched-embedding\" class=\"headerlink\" title=\"semantically-enriched embedding\"></a>semantically-enriched embedding</h4><p>这些embedding技术学习表示KG的实体/关系及其语义信息。 神经张量网络（NTN）Socher et al [2011]是该领域的先驱工作，它使用平均词嵌入和基于张量的操作初始化实体向量。 最近涉及这个想法的作品是“联合对齐”Zhong et al。 [2015]和SSP Xiao et al。[2017]。 DKRL Xie et al [2016]是一种KG表示技术，它也保留了TransE模型简单结构的文本描述性。 预训过的word2vec被用来形成实体表示，通过一个卷积神经网络（CNN）来约束要保持的关系。</p>\n<p>作者采用了DKRL，因为它强调了文本的语义描述，同时，它也继承了TransE的方法。因此我们能够通过$t = h + r$的关系来提取相关实体或关系。这大大减少了提取fact（三元组）的复杂度，因为实体关系的组合数远小于三元组的数量，因此能够让这个过程的速度更快。</p>\n<h2 id=\"3-The-proposed-model\"><a href=\"#3-The-proposed-model\" class=\"headerlink\" title=\"3. The proposed model\"></a>3. The proposed model</h2><p>我们以一个文本分类模型为例，模型的参数为$\\Theta$，训练集为$x$，标签为$y$，我们需要最大化下列方程：<br>$$<br>\\max_{\\Theta}{P(y|x, \\Theta)}<br>$$</p>\n<p>因此：<br>$$<br>\\Theta = \\arg\\max_{\\Theta} {\\log{P(y|x, \\Theta)}}<br>$$<br>这里，我们通过结合world knowledge特征$x_w$来增强监督学习过程。使用数据$x$检索world knowledge特征，使用单独的模型，其中$x_w = F(x, \\Theta^{(2)})$。 因此，我们修改的目标函数为：<br>$$<br>\\max_{\\Theta}{P(y|x, x_w, \\Theta^{(1)})}<br>$$<br>其中$\\Theta = {\\Theta^{(1)}, \\Theta^{(2)}}$。可以获得优化的参数：<br>$$<br>\\Theta = \\arg\\max_{\\Theta} {\\log{P(y|x,F(x, \\Theta^{(2)}), \\Theta^{(1)})}}<br>$$<br>后面的部分着重于函数F的表达，该函数使用数据x进行事实三重检索。在实验中，我们使用经过softmax的输入的LSTM Greff et al. [2015]编码作为P的形式。对于F，我们使用soft attention。</p>\n<p>基于此，我们提出两种模型：A. 朴素模型；B. 基于卷积的实体和关系集群表示</p>\n<h4 id=\"A-朴素模型\"><a href=\"#A-朴素模型\" class=\"headerlink\" title=\"A. 朴素模型\"></a>A. 朴素模型</h4><p>前面解释过，KG的实体和关系使用DKRL进行编码。 令$e_i \\in \\mathbb{R}^m$代表实体i的编码，$r_j\\in \\mathbb R^m$代表KG中第j个关系。 输入文本以串联的单词向量$x =(x_1,x_2,…,x_T)$的形式首先使用LSTM Greff et al. [2015]模块如下，<br>$$<br>h_t = f(x_t, h_{t-1})<br>$$<br>以及<br>$$<br>o = \\frac{1}{T}\\sum_{t=1}^{T}{h_t}<br>$$<br>$h_t \\in \\mathbb{R}^n$是LSTM的隐藏状态，f是非线性函数，T是序列长度。 然后如下形成一个上下文向量，<br>$$<br>C = ReLU(o^T W)<br>$$<br>其中，$W\\in \\mathbb R^n \\times m$表示权重参数。设置两个同样的过程以形成两个独立的上下文向量，一个用于实体检索($C_E$)和一个用于关系检索($C_R$)。</p>\n<p>由于在朴素模型中KG的事实三元组的数量是数以百万计的，所以我们分别对实体和关系空间产生注意力。 然后使用检索到的实体和关系形成事实。使用实体上下文向量的实体的注意力由下式给出：<br>$$<br>\\alpha_{e_i} = \\frac{\\exp{C_E^T{e_i}}}{\\sum_{j=0}^{|E|} \\exp{C_E^T{e_j}}}<br>$$<br>同理，关系的注意力：<br>$$<br>\\alpha_{r_i} = \\frac{\\exp{C_R^T{r_i}}}{\\sum_{j=0}^{|R|} \\exp{C_R^T{r_j}}}<br>$$<br><img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-05-10-image-201805110007120.png\" alt=\"mage-20180511000712\"></p>\n<p>图2显示了实体/关系检索的示意图。在计算出最终的实体和关系向量之后，我们希望补充事实三元组。用于实验的KG技术是DKRL，其使用TransE模型假设($h + r = t$)。因此，使用主题实体(subject entity)和关系，我们将对象实体形成为$t = e + r$。 因此，检索的事实三元组是$\\mathcal F = [e，r，e + r]$，其中$F \\in \\mathbb R^{3m}$。 该检索到的事实信息与使用LSTM模块获得的输入x的上下文向量(C)一起连接。 最终分类标签$\\mathbb y$的计算如下，<br>$$<br>\\mathcal F’ = ReLU(\\mathcal F^T V) \\<br>\\mathbb y = softmax([\\mathcal F’ : C]^T U)<br>$$<br>其中，$V∈\\mathbb R^{3m \\times u}$和$U\\in \\mathbb R^{2u\\times u}$是要学习的模型参数。 $\\mathbb y$是预测结果，用于计算交叉熵损失。我们尽量减少训练样本的平均损失，以便使用随机梯度下降来学习各种模型参数。最后的预测$\\mathbb y$现在包含了来自数据集特定信息和世界知识的信息，以帮助提高性能。在联合培训注意力机制的同时调整自己，以检索进行最终分类所需的相关事实。</p>\n<h4 id=\"A-预训练KG检索模型\"><a href=\"#A-预训练KG检索模型\" class=\"headerlink\" title=\"A+. 预训练KG检索模型\"></a>A+. 预训练KG检索模型</h4><p><img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-05-10-image-201805110034147.png\" alt=\"mage-20180511003414\"></p>\n<p>朴素模型需要考虑整个实体/关系空间，这不是一个好的方法，因为我们观察到每个attention的容易饱和。在一起训练分类和检索模块时，模型往往会忽略KG部分，而梯度只通过分类模块进行传播。这在一定程度上是可以预料的，因为当前任务的大多数相关信息来自训练样本，只有背景辅助信息来自KG。经过几个训练阶段后，KG检索到的事实总是收敛到一个固定的向量。为了克服这个问题，我们试图单独预先训练KG检索部分。预训练的KG模型用于检索事实，然后与分类模块连接，同时，在联合训练时通过预先训练的模型可能会导致传递误差。我们推断，KG不会返回噪音，并且对于任务具有基本信息，因为单独的KG部件单独显示出显着的性能（News20为59％，SNLI为66％）。图3描述了整个训练过程。该程序解决了联合训练时KG检索部分中的梯度饱和问题。但是，attention机制必须覆盖大量实体/关系的关键问题依然存在。</p>\n<h4 id=\"B-基于卷积的实体和关系集群表示\"><a href=\"#B-基于卷积的实体和关系集群表示\" class=\"headerlink\" title=\"B. 基于卷积的实体和关系集群表示\"></a>B. 基于卷积的实体和关系集群表示</h4><p>在本节中，我们提出了一种机制来减少知识图谱中需要attention的大量实体/关系。 我们通过学习类似实体/关系向量的表示来减少attention空间。</p>\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-05-10-image-201805110044225.png\" width=\"60%\">\n\n<p>为了聚类相似的实体/关系向量，我们使用k-means聚类，并在每个聚类中形成具有相同数量的实体/关系向量的$l$个聚类。然后使用CNN对每个cluster进行编码。k-means聚类的输出是一个实体/关系向量序列${e^T_1,e^T_2,…,e^T_q}$，其中$e_i \\in\\mathbb R^m$，每个聚类中的元素个数为$ q =⌈\\frac{| E |}{l}⌉$。对于每个cluster，这些矢量被堆叠形成$\\mathcal E$作为到CNN编码器的2D输入，其中$\\mathcal E\\in\\mathbb R^{m\\times q}$。 在寻找合适滤波器形状的实验中，我们发现使用2-D滤波器，该模型无法收敛。因此，我们推断，向量$e_i$中两个不同索引的潜在表示不应该被卷积修改。然后，我们采用一维卷积滤波器，只沿$\\mathcal E$列滑动，如图4所示。沿着y轴的步长是窗口长度k，卷积层的输出表示为：<br>$$<br>\\mathcal E’(i,j) = W^T[e_{i,j}, e_{i+1,j},…,e_{i+k-1, j}]^T<br>$$<br>其中，$\\mathcal E’(i,j)$是输出矩阵$\\mathcal E’$的第(i, j)个元素，$W\\in \\mathbb R^k$是卷积权重滤波器。为了减少参数空间，在卷积层之后放置一个pooling层，我们只沿y轴使用一维窗口，类似于上面提到的卷积核。我们使用了一个双层卷积网络，其步长k和最大池窗口n被调整以获得输出$\\mathcal E_i\\in \\mathbb R^m$，其中i是聚类索引。对于关系也进行类似的聚类过程，接着对聚类实体进行编码。这样，实体和关系空间都被缩减为包含更少的元素，每个cluster都有一个元素。在形成紧凑的实体空间$E$和关系空间$R$之后，我们采用了与之前相同的步骤来形成attention，但是现在，由于梯度有效地传递并且没有被过大的空间所阻塞，所以训练更有效。此外，由于卷积架构也同时得到训练，所以attention机制并没有像以前那样通过实体和关系的巨大空间来学习。</p>\n<h2 id=\"4-Conclusion\"><a href=\"#4-Conclusion\" class=\"headerlink\" title=\"4. Conclusion\"></a>4. Conclusion</h2><p>实验表明，引入KG不仅降低了深度学习模型对训练集的依赖，还显著地提高了预测结果的准确度，在数据集不够的情况下效果更佳拔群。此外，本文的方法对world knowledge的处理、embedding的方法是高度可扩展，可以应用于各种NLP任务。</p>\n<h2 id=\"Bibliography\"><a href=\"#Bibliography\" class=\"headerlink\" title=\"Bibliography\"></a>Bibliography</h2><p>笔记参考 <a href=\"https://blog.csdn.net/TgqDT3gGaMdkHasLZv/article/details/80118742\">https://blog.csdn.net/TgqDT3gGaMdkHasLZv/article/details/80118742</a></p>\n<p>[^1]: Annervaz, K. M., Somnath Basu Roy Chowdhury, and Ambedkar Dukkipati. ‘Learning beyond datasets: Knowledge Graph Augmented Neural Networks for Natural language Processing.’ arXiv preprint arXiv:1802.05930 (2018).</p>\n"},{"title":"RegEx with NN","date":"2018-08-22T05:58:00.000Z","_content":"\n这两天在实习没有太多的时间写笔记orz，正好趁着公司内部分享的时候稍微写几笔。（然而一直没发出来）\n\n# Marrying Up Regexs with Neural Networks\n\n## 概述\n\n- 正则表达式 \n\n  - 简明、扼要、可调，不依赖大规模标注数据\n\n  - 泛化性能差，所以变体、同义词都需要人为编写\n\n- 神经网络 \n  - 拟合能力强、泛化性能强  \n  - 需要大量标注数据，解释性差\n\n因此工程上常常结合两个，正则解决部分cases，剩下交给统计模型，一般来说就是神经网络了。\n\n那么有没有可能正则和神经网络结合起来？Bingfeng et al. 2018[^1]给出了一些思路。\n\n## Problem def. and the baselines \n\n文章主要解决两个问题，intent detection和slot filling，也可以认为是classification和seq2seq的任务。这里的baselines主要有两个，正则表达式的尝试和Liu, Bing, and Ian Lane. 2016[^2]提出的attention-based rnn。思路可以见下图。\n\n![image-20180916143407395](https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-09-16-063410.png)\n\n## Approaches\n\n文章主要在三个方面进行尝试：input level、network level、output level。\n\n### Input level\n\n- For intent detection, \n\n  two possible approach:\n\n  - Append the embedding to all words (deprecated <= 从结果上看会导致网络过于依赖正则)\n  - Append the embedding to the input of softmax layer(① in Fig(a) )\n\n- For slot filling, \n\n  Embed and average the REtags into a vector fi for each word and append it to the corresponding word embedding wi (① in Fig(b) )\n\n![image-20180916144059096](https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-09-16-064101.png)\n\n### Network level\n\n- For intent detection, \n\n  For each intent label k, use different attention ak , which is used to generate the sentence embedding sk   (② in Fig(a) )\n\n  Note that a RE can also indicate that a sentence does not express intent k (negative REs), it is also necessary to set another group of attention. \n\n\n$$\n  s_{k} = \\sum_i {\\alpha_{ki} h_i}, \\quad \\alpha_{ki} = \\frac{\\exp(h_i^T W_{a} c_k)}{\\sum_i {\\exp(h_i^T W_{a} c_k)}}\n$$\n\n- For slot filling, \n\n  The mechanism introduced for intent detection is unsuitable for slot filling.\n\n  A simple version of the two-side attention, where all the slot labels share the same set of positive and negative attention. (② in Fig(b) )\n\n  $$\n  s_{pi} = \\sum_j {\\alpha_{pij} h_j}, \\quad \\alpha_{pij} = \\frac{\\exp(h_j^T W_{sp} h_i)}{\\sum_j {\\exp(h_j^T W_{sp} h_i)}}\n  $$\n\n\n\n\n![image-20180916144733081](https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-09-16-064735.png)\n\n### Output level\n\nLet $z_k$ be a 0-1 indicator of whether there is at least one matched RE that leads to target label $k$ (intent or slot label), the final logits of label k for a sentence (or a spefic word for slot filling) is:\n$$\nlogit_k = logit_k' + w_k z_k\n$$\nwhere $logit′_k$ is the logit produced by the original NN, and $w_k$ is a trainable weight indicating the overall confidence for REs that lead to target label $k$.\n\n## Experimental Results \n\n![image-20180916145004296](https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-09-16-065006.png) \n\n![image-20180916145033498](https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-09-16-065035.png)\n\n## Bibliography\n\n[^1]: Luo, Bingfeng, et al. Marrying up Regular Expressions with Neural Networks: A Case Study for Spoken Language Understanding. arXiv preprint arXiv:1805.05588 (2018). \n[^2]: Liu, Bing, and Ian Lane. Attention-based recurrent neural network models for joint intent detection and slot filling. arXiv preprint arXiv:1609.01454 (2016). \n\n\n","source":"_posts/[2018.8.22]RegEx-with-NN.md","raw":"---\ntitle: RegEx with NN\ncategories:\n  - research\ntags:\n  - neural-network\n  - regular-expression\ndate: 2018-08-22 13:58:00\n---\n\n这两天在实习没有太多的时间写笔记orz，正好趁着公司内部分享的时候稍微写几笔。（然而一直没发出来）\n\n# Marrying Up Regexs with Neural Networks\n\n## 概述\n\n- 正则表达式 \n\n  - 简明、扼要、可调，不依赖大规模标注数据\n\n  - 泛化性能差，所以变体、同义词都需要人为编写\n\n- 神经网络 \n  - 拟合能力强、泛化性能强  \n  - 需要大量标注数据，解释性差\n\n因此工程上常常结合两个，正则解决部分cases，剩下交给统计模型，一般来说就是神经网络了。\n\n那么有没有可能正则和神经网络结合起来？Bingfeng et al. 2018[^1]给出了一些思路。\n\n## Problem def. and the baselines \n\n文章主要解决两个问题，intent detection和slot filling，也可以认为是classification和seq2seq的任务。这里的baselines主要有两个，正则表达式的尝试和Liu, Bing, and Ian Lane. 2016[^2]提出的attention-based rnn。思路可以见下图。\n\n![image-20180916143407395](https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-09-16-063410.png)\n\n## Approaches\n\n文章主要在三个方面进行尝试：input level、network level、output level。\n\n### Input level\n\n- For intent detection, \n\n  two possible approach:\n\n  - Append the embedding to all words (deprecated <= 从结果上看会导致网络过于依赖正则)\n  - Append the embedding to the input of softmax layer(① in Fig(a) )\n\n- For slot filling, \n\n  Embed and average the REtags into a vector fi for each word and append it to the corresponding word embedding wi (① in Fig(b) )\n\n![image-20180916144059096](https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-09-16-064101.png)\n\n### Network level\n\n- For intent detection, \n\n  For each intent label k, use different attention ak , which is used to generate the sentence embedding sk   (② in Fig(a) )\n\n  Note that a RE can also indicate that a sentence does not express intent k (negative REs), it is also necessary to set another group of attention. \n\n\n$$\n  s_{k} = \\sum_i {\\alpha_{ki} h_i}, \\quad \\alpha_{ki} = \\frac{\\exp(h_i^T W_{a} c_k)}{\\sum_i {\\exp(h_i^T W_{a} c_k)}}\n$$\n\n- For slot filling, \n\n  The mechanism introduced for intent detection is unsuitable for slot filling.\n\n  A simple version of the two-side attention, where all the slot labels share the same set of positive and negative attention. (② in Fig(b) )\n\n  $$\n  s_{pi} = \\sum_j {\\alpha_{pij} h_j}, \\quad \\alpha_{pij} = \\frac{\\exp(h_j^T W_{sp} h_i)}{\\sum_j {\\exp(h_j^T W_{sp} h_i)}}\n  $$\n\n\n\n\n![image-20180916144733081](https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-09-16-064735.png)\n\n### Output level\n\nLet $z_k$ be a 0-1 indicator of whether there is at least one matched RE that leads to target label $k$ (intent or slot label), the final logits of label k for a sentence (or a spefic word for slot filling) is:\n$$\nlogit_k = logit_k' + w_k z_k\n$$\nwhere $logit′_k$ is the logit produced by the original NN, and $w_k$ is a trainable weight indicating the overall confidence for REs that lead to target label $k$.\n\n## Experimental Results \n\n![image-20180916145004296](https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-09-16-065006.png) \n\n![image-20180916145033498](https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-09-16-065035.png)\n\n## Bibliography\n\n[^1]: Luo, Bingfeng, et al. Marrying up Regular Expressions with Neural Networks: A Case Study for Spoken Language Understanding. arXiv preprint arXiv:1805.05588 (2018). \n[^2]: Liu, Bing, and Ian Lane. Attention-based recurrent neural network models for joint intent detection and slot filling. arXiv preprint arXiv:1609.01454 (2016). \n\n\n","slug":"[2018.8.22]RegEx-with-NN","published":1,"updated":"2020-11-03T03:26:04.399Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckw4qufpw002ugwtl2aagaeeh","content":"<p>这两天在实习没有太多的时间写笔记orz，正好趁着公司内部分享的时候稍微写几笔。（然而一直没发出来）</p>\n<h1 id=\"Marrying-Up-Regexs-with-Neural-Networks\"><a href=\"#Marrying-Up-Regexs-with-Neural-Networks\" class=\"headerlink\" title=\"Marrying Up Regexs with Neural Networks\"></a>Marrying Up Regexs with Neural Networks</h1><h2 id=\"概述\"><a href=\"#概述\" class=\"headerlink\" title=\"概述\"></a>概述</h2><ul>\n<li><p>正则表达式 </p>\n<ul>\n<li><p>简明、扼要、可调，不依赖大规模标注数据</p>\n</li>\n<li><p>泛化性能差，所以变体、同义词都需要人为编写</p>\n</li>\n</ul>\n</li>\n<li><p>神经网络 </p>\n<ul>\n<li>拟合能力强、泛化性能强  </li>\n<li>需要大量标注数据，解释性差</li>\n</ul>\n</li>\n</ul>\n<p>因此工程上常常结合两个，正则解决部分cases，剩下交给统计模型，一般来说就是神经网络了。</p>\n<p>那么有没有可能正则和神经网络结合起来？Bingfeng et al. 2018[^1]给出了一些思路。</p>\n<h2 id=\"Problem-def-and-the-baselines\"><a href=\"#Problem-def-and-the-baselines\" class=\"headerlink\" title=\"Problem def. and the baselines\"></a>Problem def. and the baselines</h2><p>文章主要解决两个问题，intent detection和slot filling，也可以认为是classification和seq2seq的任务。这里的baselines主要有两个，正则表达式的尝试和Liu, Bing, and Ian Lane. 2016[^2]提出的attention-based rnn。思路可以见下图。</p>\n<p><img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-09-16-063410.png\" alt=\"image-20180916143407395\"></p>\n<h2 id=\"Approaches\"><a href=\"#Approaches\" class=\"headerlink\" title=\"Approaches\"></a>Approaches</h2><p>文章主要在三个方面进行尝试：input level、network level、output level。</p>\n<h3 id=\"Input-level\"><a href=\"#Input-level\" class=\"headerlink\" title=\"Input level\"></a>Input level</h3><ul>\n<li><p>For intent detection, </p>\n<p>two possible approach:</p>\n<ul>\n<li>Append the embedding to all words (deprecated &lt;= 从结果上看会导致网络过于依赖正则)</li>\n<li>Append the embedding to the input of softmax layer(① in Fig(a) )</li>\n</ul>\n</li>\n<li><p>For slot filling, </p>\n<p>Embed and average the REtags into a vector fi for each word and append it to the corresponding word embedding wi (① in Fig(b) )</p>\n</li>\n</ul>\n<p><img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-09-16-064101.png\" alt=\"image-20180916144059096\"></p>\n<h3 id=\"Network-level\"><a href=\"#Network-level\" class=\"headerlink\" title=\"Network level\"></a>Network level</h3><ul>\n<li><p>For intent detection, </p>\n<p>For each intent label k, use different attention ak , which is used to generate the sentence embedding sk   (② in Fig(a) )</p>\n<p>Note that a RE can also indicate that a sentence does not express intent k (negative REs), it is also necessary to set another group of attention. </p>\n</li>\n</ul>\n<p>$$<br>  s_{k} = \\sum_i {\\alpha_{ki} h_i}, \\quad \\alpha_{ki} = \\frac{\\exp(h_i^T W_{a} c_k)}{\\sum_i {\\exp(h_i^T W_{a} c_k)}}<br>$$</p>\n<ul>\n<li><p>For slot filling, </p>\n<p>The mechanism introduced for intent detection is unsuitable for slot filling.</p>\n<p>A simple version of the two-side attention, where all the slot labels share the same set of positive and negative attention. (② in Fig(b) )</p>\n<p>$$<br>s_{pi} = \\sum_j {\\alpha_{pij} h_j}, \\quad \\alpha_{pij} = \\frac{\\exp(h_j^T W_{sp} h_i)}{\\sum_j {\\exp(h_j^T W_{sp} h_i)}}<br>$$</p>\n</li>\n</ul>\n<p><img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-09-16-064735.png\" alt=\"image-20180916144733081\"></p>\n<h3 id=\"Output-level\"><a href=\"#Output-level\" class=\"headerlink\" title=\"Output level\"></a>Output level</h3><p>Let $z_k$ be a 0-1 indicator of whether there is at least one matched RE that leads to target label $k$ (intent or slot label), the final logits of label k for a sentence (or a spefic word for slot filling) is:<br>$$<br>logit_k = logit_k’ + w_k z_k<br>$$<br>where $logit′_k$ is the logit produced by the original NN, and $w_k$ is a trainable weight indicating the overall confidence for REs that lead to target label $k$.</p>\n<h2 id=\"Experimental-Results\"><a href=\"#Experimental-Results\" class=\"headerlink\" title=\"Experimental Results\"></a>Experimental Results</h2><p><img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-09-16-065006.png\" alt=\"image-20180916145004296\"> </p>\n<p><img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-09-16-065035.png\" alt=\"image-20180916145033498\"></p>\n<h2 id=\"Bibliography\"><a href=\"#Bibliography\" class=\"headerlink\" title=\"Bibliography\"></a>Bibliography</h2><p>[^1]: Luo, Bingfeng, et al. Marrying up Regular Expressions with Neural Networks: A Case Study for Spoken Language Understanding.&nbsp;arXiv preprint arXiv:1805.05588&nbsp;(2018).<br>[^2]: Liu, Bing, and Ian Lane. Attention-based recurrent neural network models for joint intent detection and slot filling. arXiv preprint arXiv:1609.01454 (2016). </p>\n","site":{"data":{"recommended_posts":{"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"}],"https://LorrinWWW.github.io/posts/complexity/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/hexo-with-latex/":[{"title":"区块链时代的最正确的打赏姿势——在Hexo博客配置Coinhive网页挖矿","permalink":"http://www.davidfnck.com/blockchain/mine-xmr-in-hexo-by-coinhive.html/"}],"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/":[{"title":"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-statistic/":[{"title":"贝叶斯估计 Bayes estimation","permalink":"https://LorrinWWW.github.io/posts/Bayes-estimation/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"}],"https://LorrinWWW.github.io/posts/[2018.2.5]Nested-LSTMs/":[{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"}],"https://LorrinWWW.github.io/posts/datamining-pretreatment/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Several models for knowledge graph representing and completing 几个知识图谱模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/"}],"https://LorrinWWW.github.io/posts/compression/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/Sobolev-space/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/proba-ch5/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch3/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/proba-ch1/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/ML-CNN/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/EDP-basic-models/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/machine-learning/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/datamining-class-pred/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction/":[],"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/":[{"title":"OS notes","permalink":"https://LorrinWWW.github.io/posts/OS-notes/"}],"https://LorrinWWW.github.io/posts/projet-enjeu-plugin-chrome-101/":[{"title":"Chrome油猴脚本","permalink":"http://gubangzhong.cn/2017/06/13/Chrome油猴脚本/"}],"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/":[{"title":"MongoDB, Docker and Python","permalink":"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/":[{"title":"Hands on Scrapy","permalink":"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/OS-notes/":[{"title":"learning OS and building LorriOS","permalink":"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/"}],"https://LorrinWWW.github.io/posts/Bayes-estimation/":[{"title":"Note of statistic","permalink":"https://LorrinWWW.github.io/posts/Note-of-statistic/"}],"https://LorrinWWW.github.io/posts/management-of-the-firm/":[],"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"}],"https://LorrinWWW.github.io/posts/datamining-qualitative-induction/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-finite-element-method/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/QuadTree/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/proba-ch4/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch2/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/":[{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/Note-of-NLP/":[{"title":"Generative Adversarial Network","permalink":"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/"},{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"}],"https://LorrinWWW.github.io/posts/Note-of-datamining/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"数据挖掘-分类与预测","permalink":"https://LorrinWWW.github.io/posts/datamining-class-pred/"},{"title":"数据挖掘-预处理","permalink":"https://LorrinWWW.github.io/posts/datamining-pretreatment/"}],"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/Note-of-probability/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/participe-present-et-gerondif/":[],"https://LorrinWWW.github.io/posts/Hilbert-space/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/proba-ch6/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}]}}},"excerpt":"","more":"<p>这两天在实习没有太多的时间写笔记orz，正好趁着公司内部分享的时候稍微写几笔。（然而一直没发出来）</p>\n<h1 id=\"Marrying-Up-Regexs-with-Neural-Networks\"><a href=\"#Marrying-Up-Regexs-with-Neural-Networks\" class=\"headerlink\" title=\"Marrying Up Regexs with Neural Networks\"></a>Marrying Up Regexs with Neural Networks</h1><h2 id=\"概述\"><a href=\"#概述\" class=\"headerlink\" title=\"概述\"></a>概述</h2><ul>\n<li><p>正则表达式 </p>\n<ul>\n<li><p>简明、扼要、可调，不依赖大规模标注数据</p>\n</li>\n<li><p>泛化性能差，所以变体、同义词都需要人为编写</p>\n</li>\n</ul>\n</li>\n<li><p>神经网络 </p>\n<ul>\n<li>拟合能力强、泛化性能强  </li>\n<li>需要大量标注数据，解释性差</li>\n</ul>\n</li>\n</ul>\n<p>因此工程上常常结合两个，正则解决部分cases，剩下交给统计模型，一般来说就是神经网络了。</p>\n<p>那么有没有可能正则和神经网络结合起来？Bingfeng et al. 2018[^1]给出了一些思路。</p>\n<h2 id=\"Problem-def-and-the-baselines\"><a href=\"#Problem-def-and-the-baselines\" class=\"headerlink\" title=\"Problem def. and the baselines\"></a>Problem def. and the baselines</h2><p>文章主要解决两个问题，intent detection和slot filling，也可以认为是classification和seq2seq的任务。这里的baselines主要有两个，正则表达式的尝试和Liu, Bing, and Ian Lane. 2016[^2]提出的attention-based rnn。思路可以见下图。</p>\n<p><img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-09-16-063410.png\" alt=\"image-20180916143407395\"></p>\n<h2 id=\"Approaches\"><a href=\"#Approaches\" class=\"headerlink\" title=\"Approaches\"></a>Approaches</h2><p>文章主要在三个方面进行尝试：input level、network level、output level。</p>\n<h3 id=\"Input-level\"><a href=\"#Input-level\" class=\"headerlink\" title=\"Input level\"></a>Input level</h3><ul>\n<li><p>For intent detection, </p>\n<p>two possible approach:</p>\n<ul>\n<li>Append the embedding to all words (deprecated &lt;= 从结果上看会导致网络过于依赖正则)</li>\n<li>Append the embedding to the input of softmax layer(① in Fig(a) )</li>\n</ul>\n</li>\n<li><p>For slot filling, </p>\n<p>Embed and average the REtags into a vector fi for each word and append it to the corresponding word embedding wi (① in Fig(b) )</p>\n</li>\n</ul>\n<p><img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-09-16-064101.png\" alt=\"image-20180916144059096\"></p>\n<h3 id=\"Network-level\"><a href=\"#Network-level\" class=\"headerlink\" title=\"Network level\"></a>Network level</h3><ul>\n<li><p>For intent detection, </p>\n<p>For each intent label k, use different attention ak , which is used to generate the sentence embedding sk   (② in Fig(a) )</p>\n<p>Note that a RE can also indicate that a sentence does not express intent k (negative REs), it is also necessary to set another group of attention. </p>\n</li>\n</ul>\n<p>$$<br>  s_{k} = \\sum_i {\\alpha_{ki} h_i}, \\quad \\alpha_{ki} = \\frac{\\exp(h_i^T W_{a} c_k)}{\\sum_i {\\exp(h_i^T W_{a} c_k)}}<br>$$</p>\n<ul>\n<li><p>For slot filling, </p>\n<p>The mechanism introduced for intent detection is unsuitable for slot filling.</p>\n<p>A simple version of the two-side attention, where all the slot labels share the same set of positive and negative attention. (② in Fig(b) )</p>\n<p>$$<br>s_{pi} = \\sum_j {\\alpha_{pij} h_j}, \\quad \\alpha_{pij} = \\frac{\\exp(h_j^T W_{sp} h_i)}{\\sum_j {\\exp(h_j^T W_{sp} h_i)}}<br>$$</p>\n</li>\n</ul>\n<p><img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-09-16-064735.png\" alt=\"image-20180916144733081\"></p>\n<h3 id=\"Output-level\"><a href=\"#Output-level\" class=\"headerlink\" title=\"Output level\"></a>Output level</h3><p>Let $z_k$ be a 0-1 indicator of whether there is at least one matched RE that leads to target label $k$ (intent or slot label), the final logits of label k for a sentence (or a spefic word for slot filling) is:<br>$$<br>logit_k = logit_k’ + w_k z_k<br>$$<br>where $logit′_k$ is the logit produced by the original NN, and $w_k$ is a trainable weight indicating the overall confidence for REs that lead to target label $k$.</p>\n<h2 id=\"Experimental-Results\"><a href=\"#Experimental-Results\" class=\"headerlink\" title=\"Experimental Results\"></a>Experimental Results</h2><p><img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-09-16-065006.png\" alt=\"image-20180916145004296\"> </p>\n<p><img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-09-16-065035.png\" alt=\"image-20180916145033498\"></p>\n<h2 id=\"Bibliography\"><a href=\"#Bibliography\" class=\"headerlink\" title=\"Bibliography\"></a>Bibliography</h2><p>[^1]: Luo, Bingfeng, et al. Marrying up Regular Expressions with Neural Networks: A Case Study for Spoken Language Understanding. arXiv preprint arXiv:1805.05588 (2018).<br>[^2]: Liu, Bing, and Ian Lane. Attention-based recurrent neural network models for joint intent detection and slot filling. arXiv preprint arXiv:1609.01454 (2016). </p>\n"},{"title":"nlp short reviews - week 1","date":"2018-09-18T03:24:29.000Z","_content":"\n## 9.21\n\n### Robust Spoken Language Understanding via Paraphrasing. (arXiv:1809.06444v1 [cs.CL])\n\n***Abstract:*** Learning intents and slot labels from user utterances is a fundamental step in all spoken language understanding (SLU) and dialog systems. State-of-the-art neural network based methods, after deployment, often suffer from performance degradation on encountering paraphrased utterances, and out-of-vocabulary words, rarely observed in their training set. We address this challenging problem by introducing a novel paraphrasing based SLU model which can be integrated with any existing SLU model in order to improve their overall performance. We propose two new paraphrase generators using RNN and sequence-to-sequence based neural networks, which are suitable for our application. Our experiments on existing benchmark and in house datasets demonstrate the robustness of our models to rare and complex paraphrased utterances, even under adversarial test distributions.\n\n***Comment:***  一篇利用句子改写优化SLU的文章。目前基于神经网络的state-of-the-art需要大量的语料，并且对语料库中少见的说法支持较差，因此我们寄希望于句子改写。在state-of-the-art的基础上，如果产生的结果置信度低于阈值，则尝试使用句子改写，从而让SLU模型更好地工作。具体来讲，这里我们把第一次置信度较低的“O”label替换成《?》，再将《?》填充成常见的语句。但是最后的试验未能表现出较大的进步，推测句子改写仅仅只是改变几个word，对整句的判断影响并不大；另一方面它不能对句子结构进行改写，因此提升有限。\n\n## 9.20\n\n### User Information Augmented Semantic Frame Parsing using Coarse-to-Fine Neural Networks. (arXiv:1809.06559v1 [cs.CL])\n\n***Abstract:*** Semantic frame parsing is a crucial component in spoken language understanding (SLU) to build spoken dialog systems. It has two main tasks: intent detection and slot filling. Although state-of-the-art approaches showed good results, they require large annotated training data and long training time. In this paper, we aim to alleviate these drawbacks for semantic frame parsing by utilizing the ubiquitous user information. We design a novel coarse-to-fine deep neural network model to incorporate prior knowledge of user information intermediately to better and quickly train a semantic frame parser. Due to the lack of benchmark dataset with real user information, we synthesize the simplest type of user information (location and time) on ATIS benchmark data. The results show that our approach leverages such simple user information to outperform state-of-the-art approaches by 0.25% for intent detection and 0.31% for slot filling using standard training data. When using smaller training data, the performance improvement on intent detection and slot filling reaches up to 1.35% and 1.20% respectively. We also show that our approach can achieve similar performance as state-of-the-art approaches by using less than 80% annotated training data. Moreover, the training time to achieve the similar performance is also reduced by over 60%.\n\n***Comment:***  工作相关。目前intent classification和slot filling的state-of-the-art是attention-based-biLSTM，这篇文章的创新点在于利用了用户的信息，并将其融合进上下文，使模型对数据的依赖减少并让结果更加准确。但是似乎提升比较有限，不过思考的方向可以参考。\n\n### Learning Universal Sentence Representations with Mean-Max Attention Autoencoder. (arXiv:1809.06590v1 [cs.CL])\n\n***Abstract:*** In order to learn universal sentence representations, previous methods focus on complex recurrent neural networks or supervised learning. In this paper, we propose a mean-max attention autoencoder (mean-max AAE) within the encoder-decoder framework. Our autoencoder rely entirely on the MultiHead self-attention mechanism to reconstruct the input sequence. In the encoding we propose a mean-max strategy that applies both mean and max pooling operations over the hidden vectors to capture diverse information of the input. To enable the information to steer the reconstruction process dynamically, the decoder performs attention over the mean-max representation. By training our model on a large collection of unlabelled data, we obtain high-quality representations of sentences. Experimental results on a broad range of 10 transfer tasks demonstrate that our model outperforms the state-of-the-art unsupervised single methods, including the classical skip-thoughts and the advanced skip-thoughts+LN model. Furthermore, compared with the traditional recurrent neural network, our mean-max AAE greatly reduce the training time.\n\n***Comment:***  提出了一种mean-max AAE，用self-attention和mean-max pooling组成encoder和decoder，最后的结果在无监督单模型中达到了state-of-the-art，并且能够有效利用并行计算，使训练过程非常快。\n\n### Transfer and Multi-Task Learning for Noun-Noun Compound Interpretation. (arXiv:1809.06748v1 [cs.CL])\n\n***Abstract:*** In this paper, we empirically evaluate the utility of transfer and multi-task learning on a challenging semantic classification task: semantic interpretation of noun--noun compounds. Through a comprehensive series of experiments and in-depth error analysis, we show that transfer learning via parameter initialization and multi-task learning via parameter sharing can help a neural classification model generalize over a highly skewed distribution of relations. Further, we demonstrate how dual annotation with two distinct sets of relations over the same set of compounds can be exploited to improve the overall accuracy of a neural classifier and its F1 scores on the less frequent, but more difficult relations.\n\n***Comment:*** 这是一篇interpretation，主要分析transfer learning和multi-task learning对于noun-noun compound。一般来说像relation extraction等任务主要研究的也是名词与名词的关系，文中提到的方法对于很多信息抽取任务都可以参考一下。\n\n## 9.18\n\n### Learning to Accept New Classes without Training. (arXiv:1809.06004v1 \\[cs.CL\\])]\n\n***Abstract:*** Classic supervised learning makes the closed-world assumption, meaning that classes seen in testing must have been seen in training. However, in the dynamic world, new or unseen class examples may appear constantly. A model working in such an environment must be able to reject unseen classes (not seen or used in training). If enough data is collected for the unseen classes, the system should incrementally learn to accept/classify them. This learning paradigm is called open-world learning (OWL). Existing OWL methods all need some form of re-training to accept or include the new classes in the overall model. In this paper, we propose a meta-learning approach to the problem. Its key novelty is that it only needs to train a meta-classifier, which can then continually accept new classes when they have enough labeled data for the meta-classifier to use, and also detect/reject future unseen classes. No re-training of the meta-classifier or a new overall classifier covering all old and new classes is needed. In testing, the method only uses the examples of the seen classes (including the newly added classes) on-the-fly for classification and rejection. Experimental results demonstrate the effectiveness of the new approach.\n\n***Comment:*** 想法还是值得借鉴的，用一个meta-classifier来避免新类需要重新训练、缺少数据集等问题。但是最后的实现有点像knn，这样纯粹的非监督学习总觉得效果可能还不能达到现有监督学习。可以考虑怎么把meta-classifier加到常见的有监督学习模型中，不降低准确率的情况下，提高泛化效果。\n\n### Events Beyond ACE: Curated Training for Events. (arXiv:1809.05576v1 \\[cs.CL\\])\n\n***Abstract:*** We explore a human-driven approach to annotation, curated training (CT), in which annotation is framed as teaching the system by using interactive search to identify informative snippets of text to annotate, unlike traditional approaches which either annotate preselected text or use active learning. A trained annotator performed 80 hours of CT for the thirty event types of the NIST TAC KBP Event Argument Extraction evaluation. Combining this annotation with ACE results in a 6% reduction in error and the learning curve of CT plateaus more slowly than for full-document annotation. 3 NLP researchers performed CT for one event type and showed much sharper learning curves with all three exceeding ACE performance in less than ninety minutes, suggesting that CT can provide further benefits when the annotator deeply understands the system.\n\n***Comment:*** 模型驱动标注，使得标注效率更高，mark。\n\n### Extending Neural Generative Conversational Model using External Knowledge Sources. (arXiv:1809.05524v1 [cs.CL])\n\n***Abstract:*** The use of connectionist approaches in conversational agents has been progressing rapidly due to the availability of large corpora. However current generative dialogue models often lack coherence and are content poor. This work proposes an architecture to incorporate unstructured knowledge sources to enhance the next utterance prediction in chit-chat type of generative dialogue models. We focus on Sequence-to-Sequence (Seq2Seq) conversational agents trained with the Reddit News dataset, and consider incorporating external knowledge from Wikipedia summaries as well as from the NELL knowledge base. Our experiments show faster training time and improved perplexity when leveraging external knowledge.\n\n***Comment:*** 近期这一类利用外部知识库的paper挺多的，到时候需要用到知识图谱的时候都可以参考一下。\n","source":"_posts/[2018.9]nlp-short-reviews-week-1.md","raw":"---\ntitle: nlp short reviews - week 1\ndate: 2018-09-18 11:24:29\ncategories: [research]\ntags: [review, nlp]\n---\n\n## 9.21\n\n### Robust Spoken Language Understanding via Paraphrasing. (arXiv:1809.06444v1 [cs.CL])\n\n***Abstract:*** Learning intents and slot labels from user utterances is a fundamental step in all spoken language understanding (SLU) and dialog systems. State-of-the-art neural network based methods, after deployment, often suffer from performance degradation on encountering paraphrased utterances, and out-of-vocabulary words, rarely observed in their training set. We address this challenging problem by introducing a novel paraphrasing based SLU model which can be integrated with any existing SLU model in order to improve their overall performance. We propose two new paraphrase generators using RNN and sequence-to-sequence based neural networks, which are suitable for our application. Our experiments on existing benchmark and in house datasets demonstrate the robustness of our models to rare and complex paraphrased utterances, even under adversarial test distributions.\n\n***Comment:***  一篇利用句子改写优化SLU的文章。目前基于神经网络的state-of-the-art需要大量的语料，并且对语料库中少见的说法支持较差，因此我们寄希望于句子改写。在state-of-the-art的基础上，如果产生的结果置信度低于阈值，则尝试使用句子改写，从而让SLU模型更好地工作。具体来讲，这里我们把第一次置信度较低的“O”label替换成《?》，再将《?》填充成常见的语句。但是最后的试验未能表现出较大的进步，推测句子改写仅仅只是改变几个word，对整句的判断影响并不大；另一方面它不能对句子结构进行改写，因此提升有限。\n\n## 9.20\n\n### User Information Augmented Semantic Frame Parsing using Coarse-to-Fine Neural Networks. (arXiv:1809.06559v1 [cs.CL])\n\n***Abstract:*** Semantic frame parsing is a crucial component in spoken language understanding (SLU) to build spoken dialog systems. It has two main tasks: intent detection and slot filling. Although state-of-the-art approaches showed good results, they require large annotated training data and long training time. In this paper, we aim to alleviate these drawbacks for semantic frame parsing by utilizing the ubiquitous user information. We design a novel coarse-to-fine deep neural network model to incorporate prior knowledge of user information intermediately to better and quickly train a semantic frame parser. Due to the lack of benchmark dataset with real user information, we synthesize the simplest type of user information (location and time) on ATIS benchmark data. The results show that our approach leverages such simple user information to outperform state-of-the-art approaches by 0.25% for intent detection and 0.31% for slot filling using standard training data. When using smaller training data, the performance improvement on intent detection and slot filling reaches up to 1.35% and 1.20% respectively. We also show that our approach can achieve similar performance as state-of-the-art approaches by using less than 80% annotated training data. Moreover, the training time to achieve the similar performance is also reduced by over 60%.\n\n***Comment:***  工作相关。目前intent classification和slot filling的state-of-the-art是attention-based-biLSTM，这篇文章的创新点在于利用了用户的信息，并将其融合进上下文，使模型对数据的依赖减少并让结果更加准确。但是似乎提升比较有限，不过思考的方向可以参考。\n\n### Learning Universal Sentence Representations with Mean-Max Attention Autoencoder. (arXiv:1809.06590v1 [cs.CL])\n\n***Abstract:*** In order to learn universal sentence representations, previous methods focus on complex recurrent neural networks or supervised learning. In this paper, we propose a mean-max attention autoencoder (mean-max AAE) within the encoder-decoder framework. Our autoencoder rely entirely on the MultiHead self-attention mechanism to reconstruct the input sequence. In the encoding we propose a mean-max strategy that applies both mean and max pooling operations over the hidden vectors to capture diverse information of the input. To enable the information to steer the reconstruction process dynamically, the decoder performs attention over the mean-max representation. By training our model on a large collection of unlabelled data, we obtain high-quality representations of sentences. Experimental results on a broad range of 10 transfer tasks demonstrate that our model outperforms the state-of-the-art unsupervised single methods, including the classical skip-thoughts and the advanced skip-thoughts+LN model. Furthermore, compared with the traditional recurrent neural network, our mean-max AAE greatly reduce the training time.\n\n***Comment:***  提出了一种mean-max AAE，用self-attention和mean-max pooling组成encoder和decoder，最后的结果在无监督单模型中达到了state-of-the-art，并且能够有效利用并行计算，使训练过程非常快。\n\n### Transfer and Multi-Task Learning for Noun-Noun Compound Interpretation. (arXiv:1809.06748v1 [cs.CL])\n\n***Abstract:*** In this paper, we empirically evaluate the utility of transfer and multi-task learning on a challenging semantic classification task: semantic interpretation of noun--noun compounds. Through a comprehensive series of experiments and in-depth error analysis, we show that transfer learning via parameter initialization and multi-task learning via parameter sharing can help a neural classification model generalize over a highly skewed distribution of relations. Further, we demonstrate how dual annotation with two distinct sets of relations over the same set of compounds can be exploited to improve the overall accuracy of a neural classifier and its F1 scores on the less frequent, but more difficult relations.\n\n***Comment:*** 这是一篇interpretation，主要分析transfer learning和multi-task learning对于noun-noun compound。一般来说像relation extraction等任务主要研究的也是名词与名词的关系，文中提到的方法对于很多信息抽取任务都可以参考一下。\n\n## 9.18\n\n### Learning to Accept New Classes without Training. (arXiv:1809.06004v1 \\[cs.CL\\])]\n\n***Abstract:*** Classic supervised learning makes the closed-world assumption, meaning that classes seen in testing must have been seen in training. However, in the dynamic world, new or unseen class examples may appear constantly. A model working in such an environment must be able to reject unseen classes (not seen or used in training). If enough data is collected for the unseen classes, the system should incrementally learn to accept/classify them. This learning paradigm is called open-world learning (OWL). Existing OWL methods all need some form of re-training to accept or include the new classes in the overall model. In this paper, we propose a meta-learning approach to the problem. Its key novelty is that it only needs to train a meta-classifier, which can then continually accept new classes when they have enough labeled data for the meta-classifier to use, and also detect/reject future unseen classes. No re-training of the meta-classifier or a new overall classifier covering all old and new classes is needed. In testing, the method only uses the examples of the seen classes (including the newly added classes) on-the-fly for classification and rejection. Experimental results demonstrate the effectiveness of the new approach.\n\n***Comment:*** 想法还是值得借鉴的，用一个meta-classifier来避免新类需要重新训练、缺少数据集等问题。但是最后的实现有点像knn，这样纯粹的非监督学习总觉得效果可能还不能达到现有监督学习。可以考虑怎么把meta-classifier加到常见的有监督学习模型中，不降低准确率的情况下，提高泛化效果。\n\n### Events Beyond ACE: Curated Training for Events. (arXiv:1809.05576v1 \\[cs.CL\\])\n\n***Abstract:*** We explore a human-driven approach to annotation, curated training (CT), in which annotation is framed as teaching the system by using interactive search to identify informative snippets of text to annotate, unlike traditional approaches which either annotate preselected text or use active learning. A trained annotator performed 80 hours of CT for the thirty event types of the NIST TAC KBP Event Argument Extraction evaluation. Combining this annotation with ACE results in a 6% reduction in error and the learning curve of CT plateaus more slowly than for full-document annotation. 3 NLP researchers performed CT for one event type and showed much sharper learning curves with all three exceeding ACE performance in less than ninety minutes, suggesting that CT can provide further benefits when the annotator deeply understands the system.\n\n***Comment:*** 模型驱动标注，使得标注效率更高，mark。\n\n### Extending Neural Generative Conversational Model using External Knowledge Sources. (arXiv:1809.05524v1 [cs.CL])\n\n***Abstract:*** The use of connectionist approaches in conversational agents has been progressing rapidly due to the availability of large corpora. However current generative dialogue models often lack coherence and are content poor. This work proposes an architecture to incorporate unstructured knowledge sources to enhance the next utterance prediction in chit-chat type of generative dialogue models. We focus on Sequence-to-Sequence (Seq2Seq) conversational agents trained with the Reddit News dataset, and consider incorporating external knowledge from Wikipedia summaries as well as from the NELL knowledge base. Our experiments show faster training time and improved perplexity when leveraging external knowledge.\n\n***Comment:*** 近期这一类利用外部知识库的paper挺多的，到时候需要用到知识图谱的时候都可以参考一下。\n","slug":"[2018.9]nlp-short-reviews-week-1","published":1,"updated":"2018-10-06T02:10:34.775Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckw4qufpx002ygwtlfb0shkrr","content":"<h2 id=\"9-21\"><a href=\"#9-21\" class=\"headerlink\" title=\"9.21\"></a>9.21</h2><h3 id=\"Robust-Spoken-Language-Understanding-via-Paraphrasing-arXiv-1809-06444v1-cs-CL\"><a href=\"#Robust-Spoken-Language-Understanding-via-Paraphrasing-arXiv-1809-06444v1-cs-CL\" class=\"headerlink\" title=\"Robust Spoken Language Understanding via Paraphrasing. (arXiv:1809.06444v1 [cs.CL])\"></a>Robust Spoken Language Understanding via Paraphrasing. (arXiv:1809.06444v1 [cs.CL])</h3><p><em><strong>Abstract:</strong></em> Learning intents and slot labels from user utterances is a fundamental step in all spoken language understanding (SLU) and dialog systems. State-of-the-art neural network based methods, after deployment, often suffer from performance degradation on encountering paraphrased utterances, and out-of-vocabulary words, rarely observed in their training set. We address this challenging problem by introducing a novel paraphrasing based SLU model which can be integrated with any existing SLU model in order to improve their overall performance. We propose two new paraphrase generators using RNN and sequence-to-sequence based neural networks, which are suitable for our application. Our experiments on existing benchmark and in house datasets demonstrate the robustness of our models to rare and complex paraphrased utterances, even under adversarial test distributions.</p>\n<p><em><strong>Comment:</strong></em>  一篇利用句子改写优化SLU的文章。目前基于神经网络的state-of-the-art需要大量的语料，并且对语料库中少见的说法支持较差，因此我们寄希望于句子改写。在state-of-the-art的基础上，如果产生的结果置信度低于阈值，则尝试使用句子改写，从而让SLU模型更好地工作。具体来讲，这里我们把第一次置信度较低的“O”label替换成《?》，再将《?》填充成常见的语句。但是最后的试验未能表现出较大的进步，推测句子改写仅仅只是改变几个word，对整句的判断影响并不大；另一方面它不能对句子结构进行改写，因此提升有限。</p>\n<h2 id=\"9-20\"><a href=\"#9-20\" class=\"headerlink\" title=\"9.20\"></a>9.20</h2><h3 id=\"User-Information-Augmented-Semantic-Frame-Parsing-using-Coarse-to-Fine-Neural-Networks-arXiv-1809-06559v1-cs-CL\"><a href=\"#User-Information-Augmented-Semantic-Frame-Parsing-using-Coarse-to-Fine-Neural-Networks-arXiv-1809-06559v1-cs-CL\" class=\"headerlink\" title=\"User Information Augmented Semantic Frame Parsing using Coarse-to-Fine Neural Networks. (arXiv:1809.06559v1 [cs.CL])\"></a>User Information Augmented Semantic Frame Parsing using Coarse-to-Fine Neural Networks. (arXiv:1809.06559v1 [cs.CL])</h3><p><em><strong>Abstract:</strong></em> Semantic frame parsing is a crucial component in spoken language understanding (SLU) to build spoken dialog systems. It has two main tasks: intent detection and slot filling. Although state-of-the-art approaches showed good results, they require large annotated training data and long training time. In this paper, we aim to alleviate these drawbacks for semantic frame parsing by utilizing the ubiquitous user information. We design a novel coarse-to-fine deep neural network model to incorporate prior knowledge of user information intermediately to better and quickly train a semantic frame parser. Due to the lack of benchmark dataset with real user information, we synthesize the simplest type of user information (location and time) on ATIS benchmark data. The results show that our approach leverages such simple user information to outperform state-of-the-art approaches by 0.25% for intent detection and 0.31% for slot filling using standard training data. When using smaller training data, the performance improvement on intent detection and slot filling reaches up to 1.35% and 1.20% respectively. We also show that our approach can achieve similar performance as state-of-the-art approaches by using less than 80% annotated training data. Moreover, the training time to achieve the similar performance is also reduced by over 60%.</p>\n<p><em><strong>Comment:</strong></em>  工作相关。目前intent classification和slot filling的state-of-the-art是attention-based-biLSTM，这篇文章的创新点在于利用了用户的信息，并将其融合进上下文，使模型对数据的依赖减少并让结果更加准确。但是似乎提升比较有限，不过思考的方向可以参考。</p>\n<h3 id=\"Learning-Universal-Sentence-Representations-with-Mean-Max-Attention-Autoencoder-arXiv-1809-06590v1-cs-CL\"><a href=\"#Learning-Universal-Sentence-Representations-with-Mean-Max-Attention-Autoencoder-arXiv-1809-06590v1-cs-CL\" class=\"headerlink\" title=\"Learning Universal Sentence Representations with Mean-Max Attention Autoencoder. (arXiv:1809.06590v1 [cs.CL])\"></a>Learning Universal Sentence Representations with Mean-Max Attention Autoencoder. (arXiv:1809.06590v1 [cs.CL])</h3><p><em><strong>Abstract:</strong></em> In order to learn universal sentence representations, previous methods focus on complex recurrent neural networks or supervised learning. In this paper, we propose a mean-max attention autoencoder (mean-max AAE) within the encoder-decoder framework. Our autoencoder rely entirely on the MultiHead self-attention mechanism to reconstruct the input sequence. In the encoding we propose a mean-max strategy that applies both mean and max pooling operations over the hidden vectors to capture diverse information of the input. To enable the information to steer the reconstruction process dynamically, the decoder performs attention over the mean-max representation. By training our model on a large collection of unlabelled data, we obtain high-quality representations of sentences. Experimental results on a broad range of 10 transfer tasks demonstrate that our model outperforms the state-of-the-art unsupervised single methods, including the classical skip-thoughts and the advanced skip-thoughts+LN model. Furthermore, compared with the traditional recurrent neural network, our mean-max AAE greatly reduce the training time.</p>\n<p><em><strong>Comment:</strong></em>  提出了一种mean-max AAE，用self-attention和mean-max pooling组成encoder和decoder，最后的结果在无监督单模型中达到了state-of-the-art，并且能够有效利用并行计算，使训练过程非常快。</p>\n<h3 id=\"Transfer-and-Multi-Task-Learning-for-Noun-Noun-Compound-Interpretation-arXiv-1809-06748v1-cs-CL\"><a href=\"#Transfer-and-Multi-Task-Learning-for-Noun-Noun-Compound-Interpretation-arXiv-1809-06748v1-cs-CL\" class=\"headerlink\" title=\"Transfer and Multi-Task Learning for Noun-Noun Compound Interpretation. (arXiv:1809.06748v1 [cs.CL])\"></a>Transfer and Multi-Task Learning for Noun-Noun Compound Interpretation. (arXiv:1809.06748v1 [cs.CL])</h3><p><em><strong>Abstract:</strong></em> In this paper, we empirically evaluate the utility of transfer and multi-task learning on a challenging semantic classification task: semantic interpretation of noun–noun compounds. Through a comprehensive series of experiments and in-depth error analysis, we show that transfer learning via parameter initialization and multi-task learning via parameter sharing can help a neural classification model generalize over a highly skewed distribution of relations. Further, we demonstrate how dual annotation with two distinct sets of relations over the same set of compounds can be exploited to improve the overall accuracy of a neural classifier and its F1 scores on the less frequent, but more difficult relations.</p>\n<p><em><strong>Comment:</strong></em> 这是一篇interpretation，主要分析transfer learning和multi-task learning对于noun-noun compound。一般来说像relation extraction等任务主要研究的也是名词与名词的关系，文中提到的方法对于很多信息抽取任务都可以参考一下。</p>\n<h2 id=\"9-18\"><a href=\"#9-18\" class=\"headerlink\" title=\"9.18\"></a>9.18</h2><h3 id=\"Learning-to-Accept-New-Classes-without-Training-arXiv-1809-06004v1-cs-CL\"><a href=\"#Learning-to-Accept-New-Classes-without-Training-arXiv-1809-06004v1-cs-CL\" class=\"headerlink\" title=\"Learning to Accept New Classes without Training. (arXiv:1809.06004v1 [cs.CL])]\"></a>Learning to Accept New Classes without Training. (arXiv:1809.06004v1 [cs.CL])]</h3><p><em><strong>Abstract:</strong></em> Classic supervised learning makes the closed-world assumption, meaning that classes seen in testing must have been seen in training. However, in the dynamic world, new or unseen class examples may appear constantly. A model working in such an environment must be able to reject unseen classes (not seen or used in training). If enough data is collected for the unseen classes, the system should incrementally learn to accept/classify them. This learning paradigm is called open-world learning (OWL). Existing OWL methods all need some form of re-training to accept or include the new classes in the overall model. In this paper, we propose a meta-learning approach to the problem. Its key novelty is that it only needs to train a meta-classifier, which can then continually accept new classes when they have enough labeled data for the meta-classifier to use, and also detect/reject future unseen classes. No re-training of the meta-classifier or a new overall classifier covering all old and new classes is needed. In testing, the method only uses the examples of the seen classes (including the newly added classes) on-the-fly for classification and rejection. Experimental results demonstrate the effectiveness of the new approach.</p>\n<p><em><strong>Comment:</strong></em> 想法还是值得借鉴的，用一个meta-classifier来避免新类需要重新训练、缺少数据集等问题。但是最后的实现有点像knn，这样纯粹的非监督学习总觉得效果可能还不能达到现有监督学习。可以考虑怎么把meta-classifier加到常见的有监督学习模型中，不降低准确率的情况下，提高泛化效果。</p>\n<h3 id=\"Events-Beyond-ACE-Curated-Training-for-Events-arXiv-1809-05576v1-cs-CL\"><a href=\"#Events-Beyond-ACE-Curated-Training-for-Events-arXiv-1809-05576v1-cs-CL\" class=\"headerlink\" title=\"Events Beyond ACE: Curated Training for Events. (arXiv:1809.05576v1 [cs.CL])\"></a>Events Beyond ACE: Curated Training for Events. (arXiv:1809.05576v1 [cs.CL])</h3><p><em><strong>Abstract:</strong></em> We explore a human-driven approach to annotation, curated training (CT), in which annotation is framed as teaching the system by using interactive search to identify informative snippets of text to annotate, unlike traditional approaches which either annotate preselected text or use active learning. A trained annotator performed 80 hours of CT for the thirty event types of the NIST TAC KBP Event Argument Extraction evaluation. Combining this annotation with ACE results in a 6% reduction in error and the learning curve of CT plateaus more slowly than for full-document annotation. 3 NLP researchers performed CT for one event type and showed much sharper learning curves with all three exceeding ACE performance in less than ninety minutes, suggesting that CT can provide further benefits when the annotator deeply understands the system.</p>\n<p><em><strong>Comment:</strong></em> 模型驱动标注，使得标注效率更高，mark。</p>\n<h3 id=\"Extending-Neural-Generative-Conversational-Model-using-External-Knowledge-Sources-arXiv-1809-05524v1-cs-CL\"><a href=\"#Extending-Neural-Generative-Conversational-Model-using-External-Knowledge-Sources-arXiv-1809-05524v1-cs-CL\" class=\"headerlink\" title=\"Extending Neural Generative Conversational Model using External Knowledge Sources. (arXiv:1809.05524v1 [cs.CL])\"></a>Extending Neural Generative Conversational Model using External Knowledge Sources. (arXiv:1809.05524v1 [cs.CL])</h3><p><em><strong>Abstract:</strong></em> The use of connectionist approaches in conversational agents has been progressing rapidly due to the availability of large corpora. However current generative dialogue models often lack coherence and are content poor. This work proposes an architecture to incorporate unstructured knowledge sources to enhance the next utterance prediction in chit-chat type of generative dialogue models. We focus on Sequence-to-Sequence (Seq2Seq) conversational agents trained with the Reddit News dataset, and consider incorporating external knowledge from Wikipedia summaries as well as from the NELL knowledge base. Our experiments show faster training time and improved perplexity when leveraging external knowledge.</p>\n<p><em><strong>Comment:</strong></em> 近期这一类利用外部知识库的paper挺多的，到时候需要用到知识图谱的时候都可以参考一下。</p>\n","site":{"data":{"recommended_posts":{"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"}],"https://LorrinWWW.github.io/posts/complexity/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/hexo-with-latex/":[{"title":"区块链时代的最正确的打赏姿势——在Hexo博客配置Coinhive网页挖矿","permalink":"http://www.davidfnck.com/blockchain/mine-xmr-in-hexo-by-coinhive.html/"}],"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/":[{"title":"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-statistic/":[{"title":"贝叶斯估计 Bayes estimation","permalink":"https://LorrinWWW.github.io/posts/Bayes-estimation/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"}],"https://LorrinWWW.github.io/posts/[2018.2.5]Nested-LSTMs/":[{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"}],"https://LorrinWWW.github.io/posts/datamining-pretreatment/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Several models for knowledge graph representing and completing 几个知识图谱模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/"}],"https://LorrinWWW.github.io/posts/compression/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/Sobolev-space/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/proba-ch5/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch3/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/proba-ch1/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/ML-CNN/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/EDP-basic-models/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/machine-learning/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/datamining-class-pred/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction/":[],"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/":[{"title":"OS notes","permalink":"https://LorrinWWW.github.io/posts/OS-notes/"}],"https://LorrinWWW.github.io/posts/projet-enjeu-plugin-chrome-101/":[{"title":"Chrome油猴脚本","permalink":"http://gubangzhong.cn/2017/06/13/Chrome油猴脚本/"}],"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/":[{"title":"MongoDB, Docker and Python","permalink":"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/":[{"title":"Hands on Scrapy","permalink":"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/OS-notes/":[{"title":"learning OS and building LorriOS","permalink":"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/"}],"https://LorrinWWW.github.io/posts/Bayes-estimation/":[{"title":"Note of statistic","permalink":"https://LorrinWWW.github.io/posts/Note-of-statistic/"}],"https://LorrinWWW.github.io/posts/management-of-the-firm/":[],"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"}],"https://LorrinWWW.github.io/posts/datamining-qualitative-induction/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-finite-element-method/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/QuadTree/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/proba-ch4/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch2/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/":[{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/Note-of-NLP/":[{"title":"Generative Adversarial Network","permalink":"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/"},{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"}],"https://LorrinWWW.github.io/posts/Note-of-datamining/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"数据挖掘-分类与预测","permalink":"https://LorrinWWW.github.io/posts/datamining-class-pred/"},{"title":"数据挖掘-预处理","permalink":"https://LorrinWWW.github.io/posts/datamining-pretreatment/"}],"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/Note-of-probability/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/participe-present-et-gerondif/":[],"https://LorrinWWW.github.io/posts/Hilbert-space/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/proba-ch6/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}]}}},"excerpt":"","more":"<h2 id=\"9-21\"><a href=\"#9-21\" class=\"headerlink\" title=\"9.21\"></a>9.21</h2><h3 id=\"Robust-Spoken-Language-Understanding-via-Paraphrasing-arXiv-1809-06444v1-cs-CL\"><a href=\"#Robust-Spoken-Language-Understanding-via-Paraphrasing-arXiv-1809-06444v1-cs-CL\" class=\"headerlink\" title=\"Robust Spoken Language Understanding via Paraphrasing. (arXiv:1809.06444v1 [cs.CL])\"></a>Robust Spoken Language Understanding via Paraphrasing. (arXiv:1809.06444v1 [cs.CL])</h3><p><em><strong>Abstract:</strong></em> Learning intents and slot labels from user utterances is a fundamental step in all spoken language understanding (SLU) and dialog systems. State-of-the-art neural network based methods, after deployment, often suffer from performance degradation on encountering paraphrased utterances, and out-of-vocabulary words, rarely observed in their training set. We address this challenging problem by introducing a novel paraphrasing based SLU model which can be integrated with any existing SLU model in order to improve their overall performance. We propose two new paraphrase generators using RNN and sequence-to-sequence based neural networks, which are suitable for our application. Our experiments on existing benchmark and in house datasets demonstrate the robustness of our models to rare and complex paraphrased utterances, even under adversarial test distributions.</p>\n<p><em><strong>Comment:</strong></em>  一篇利用句子改写优化SLU的文章。目前基于神经网络的state-of-the-art需要大量的语料，并且对语料库中少见的说法支持较差，因此我们寄希望于句子改写。在state-of-the-art的基础上，如果产生的结果置信度低于阈值，则尝试使用句子改写，从而让SLU模型更好地工作。具体来讲，这里我们把第一次置信度较低的“O”label替换成《?》，再将《?》填充成常见的语句。但是最后的试验未能表现出较大的进步，推测句子改写仅仅只是改变几个word，对整句的判断影响并不大；另一方面它不能对句子结构进行改写，因此提升有限。</p>\n<h2 id=\"9-20\"><a href=\"#9-20\" class=\"headerlink\" title=\"9.20\"></a>9.20</h2><h3 id=\"User-Information-Augmented-Semantic-Frame-Parsing-using-Coarse-to-Fine-Neural-Networks-arXiv-1809-06559v1-cs-CL\"><a href=\"#User-Information-Augmented-Semantic-Frame-Parsing-using-Coarse-to-Fine-Neural-Networks-arXiv-1809-06559v1-cs-CL\" class=\"headerlink\" title=\"User Information Augmented Semantic Frame Parsing using Coarse-to-Fine Neural Networks. (arXiv:1809.06559v1 [cs.CL])\"></a>User Information Augmented Semantic Frame Parsing using Coarse-to-Fine Neural Networks. (arXiv:1809.06559v1 [cs.CL])</h3><p><em><strong>Abstract:</strong></em> Semantic frame parsing is a crucial component in spoken language understanding (SLU) to build spoken dialog systems. It has two main tasks: intent detection and slot filling. Although state-of-the-art approaches showed good results, they require large annotated training data and long training time. In this paper, we aim to alleviate these drawbacks for semantic frame parsing by utilizing the ubiquitous user information. We design a novel coarse-to-fine deep neural network model to incorporate prior knowledge of user information intermediately to better and quickly train a semantic frame parser. Due to the lack of benchmark dataset with real user information, we synthesize the simplest type of user information (location and time) on ATIS benchmark data. The results show that our approach leverages such simple user information to outperform state-of-the-art approaches by 0.25% for intent detection and 0.31% for slot filling using standard training data. When using smaller training data, the performance improvement on intent detection and slot filling reaches up to 1.35% and 1.20% respectively. We also show that our approach can achieve similar performance as state-of-the-art approaches by using less than 80% annotated training data. Moreover, the training time to achieve the similar performance is also reduced by over 60%.</p>\n<p><em><strong>Comment:</strong></em>  工作相关。目前intent classification和slot filling的state-of-the-art是attention-based-biLSTM，这篇文章的创新点在于利用了用户的信息，并将其融合进上下文，使模型对数据的依赖减少并让结果更加准确。但是似乎提升比较有限，不过思考的方向可以参考。</p>\n<h3 id=\"Learning-Universal-Sentence-Representations-with-Mean-Max-Attention-Autoencoder-arXiv-1809-06590v1-cs-CL\"><a href=\"#Learning-Universal-Sentence-Representations-with-Mean-Max-Attention-Autoencoder-arXiv-1809-06590v1-cs-CL\" class=\"headerlink\" title=\"Learning Universal Sentence Representations with Mean-Max Attention Autoencoder. (arXiv:1809.06590v1 [cs.CL])\"></a>Learning Universal Sentence Representations with Mean-Max Attention Autoencoder. (arXiv:1809.06590v1 [cs.CL])</h3><p><em><strong>Abstract:</strong></em> In order to learn universal sentence representations, previous methods focus on complex recurrent neural networks or supervised learning. In this paper, we propose a mean-max attention autoencoder (mean-max AAE) within the encoder-decoder framework. Our autoencoder rely entirely on the MultiHead self-attention mechanism to reconstruct the input sequence. In the encoding we propose a mean-max strategy that applies both mean and max pooling operations over the hidden vectors to capture diverse information of the input. To enable the information to steer the reconstruction process dynamically, the decoder performs attention over the mean-max representation. By training our model on a large collection of unlabelled data, we obtain high-quality representations of sentences. Experimental results on a broad range of 10 transfer tasks demonstrate that our model outperforms the state-of-the-art unsupervised single methods, including the classical skip-thoughts and the advanced skip-thoughts+LN model. Furthermore, compared with the traditional recurrent neural network, our mean-max AAE greatly reduce the training time.</p>\n<p><em><strong>Comment:</strong></em>  提出了一种mean-max AAE，用self-attention和mean-max pooling组成encoder和decoder，最后的结果在无监督单模型中达到了state-of-the-art，并且能够有效利用并行计算，使训练过程非常快。</p>\n<h3 id=\"Transfer-and-Multi-Task-Learning-for-Noun-Noun-Compound-Interpretation-arXiv-1809-06748v1-cs-CL\"><a href=\"#Transfer-and-Multi-Task-Learning-for-Noun-Noun-Compound-Interpretation-arXiv-1809-06748v1-cs-CL\" class=\"headerlink\" title=\"Transfer and Multi-Task Learning for Noun-Noun Compound Interpretation. (arXiv:1809.06748v1 [cs.CL])\"></a>Transfer and Multi-Task Learning for Noun-Noun Compound Interpretation. (arXiv:1809.06748v1 [cs.CL])</h3><p><em><strong>Abstract:</strong></em> In this paper, we empirically evaluate the utility of transfer and multi-task learning on a challenging semantic classification task: semantic interpretation of noun–noun compounds. Through a comprehensive series of experiments and in-depth error analysis, we show that transfer learning via parameter initialization and multi-task learning via parameter sharing can help a neural classification model generalize over a highly skewed distribution of relations. Further, we demonstrate how dual annotation with two distinct sets of relations over the same set of compounds can be exploited to improve the overall accuracy of a neural classifier and its F1 scores on the less frequent, but more difficult relations.</p>\n<p><em><strong>Comment:</strong></em> 这是一篇interpretation，主要分析transfer learning和multi-task learning对于noun-noun compound。一般来说像relation extraction等任务主要研究的也是名词与名词的关系，文中提到的方法对于很多信息抽取任务都可以参考一下。</p>\n<h2 id=\"9-18\"><a href=\"#9-18\" class=\"headerlink\" title=\"9.18\"></a>9.18</h2><h3 id=\"Learning-to-Accept-New-Classes-without-Training-arXiv-1809-06004v1-cs-CL\"><a href=\"#Learning-to-Accept-New-Classes-without-Training-arXiv-1809-06004v1-cs-CL\" class=\"headerlink\" title=\"Learning to Accept New Classes without Training. (arXiv:1809.06004v1 [cs.CL])]\"></a>Learning to Accept New Classes without Training. (arXiv:1809.06004v1 [cs.CL])]</h3><p><em><strong>Abstract:</strong></em> Classic supervised learning makes the closed-world assumption, meaning that classes seen in testing must have been seen in training. However, in the dynamic world, new or unseen class examples may appear constantly. A model working in such an environment must be able to reject unseen classes (not seen or used in training). If enough data is collected for the unseen classes, the system should incrementally learn to accept/classify them. This learning paradigm is called open-world learning (OWL). Existing OWL methods all need some form of re-training to accept or include the new classes in the overall model. In this paper, we propose a meta-learning approach to the problem. Its key novelty is that it only needs to train a meta-classifier, which can then continually accept new classes when they have enough labeled data for the meta-classifier to use, and also detect/reject future unseen classes. No re-training of the meta-classifier or a new overall classifier covering all old and new classes is needed. In testing, the method only uses the examples of the seen classes (including the newly added classes) on-the-fly for classification and rejection. Experimental results demonstrate the effectiveness of the new approach.</p>\n<p><em><strong>Comment:</strong></em> 想法还是值得借鉴的，用一个meta-classifier来避免新类需要重新训练、缺少数据集等问题。但是最后的实现有点像knn，这样纯粹的非监督学习总觉得效果可能还不能达到现有监督学习。可以考虑怎么把meta-classifier加到常见的有监督学习模型中，不降低准确率的情况下，提高泛化效果。</p>\n<h3 id=\"Events-Beyond-ACE-Curated-Training-for-Events-arXiv-1809-05576v1-cs-CL\"><a href=\"#Events-Beyond-ACE-Curated-Training-for-Events-arXiv-1809-05576v1-cs-CL\" class=\"headerlink\" title=\"Events Beyond ACE: Curated Training for Events. (arXiv:1809.05576v1 [cs.CL])\"></a>Events Beyond ACE: Curated Training for Events. (arXiv:1809.05576v1 [cs.CL])</h3><p><em><strong>Abstract:</strong></em> We explore a human-driven approach to annotation, curated training (CT), in which annotation is framed as teaching the system by using interactive search to identify informative snippets of text to annotate, unlike traditional approaches which either annotate preselected text or use active learning. A trained annotator performed 80 hours of CT for the thirty event types of the NIST TAC KBP Event Argument Extraction evaluation. Combining this annotation with ACE results in a 6% reduction in error and the learning curve of CT plateaus more slowly than for full-document annotation. 3 NLP researchers performed CT for one event type and showed much sharper learning curves with all three exceeding ACE performance in less than ninety minutes, suggesting that CT can provide further benefits when the annotator deeply understands the system.</p>\n<p><em><strong>Comment:</strong></em> 模型驱动标注，使得标注效率更高，mark。</p>\n<h3 id=\"Extending-Neural-Generative-Conversational-Model-using-External-Knowledge-Sources-arXiv-1809-05524v1-cs-CL\"><a href=\"#Extending-Neural-Generative-Conversational-Model-using-External-Knowledge-Sources-arXiv-1809-05524v1-cs-CL\" class=\"headerlink\" title=\"Extending Neural Generative Conversational Model using External Knowledge Sources. (arXiv:1809.05524v1 [cs.CL])\"></a>Extending Neural Generative Conversational Model using External Knowledge Sources. (arXiv:1809.05524v1 [cs.CL])</h3><p><em><strong>Abstract:</strong></em> The use of connectionist approaches in conversational agents has been progressing rapidly due to the availability of large corpora. However current generative dialogue models often lack coherence and are content poor. This work proposes an architecture to incorporate unstructured knowledge sources to enhance the next utterance prediction in chit-chat type of generative dialogue models. We focus on Sequence-to-Sequence (Seq2Seq) conversational agents trained with the Reddit News dataset, and consider incorporating external knowledge from Wikipedia summaries as well as from the NELL knowledge base. Our experiments show faster training time and improved perplexity when leveraging external knowledge.</p>\n<p><em><strong>Comment:</strong></em> 近期这一类利用外部知识库的paper挺多的，到时候需要用到知识图谱的时候都可以参考一下。</p>\n"},{"title":"Jue Wang","date":"2021-06-01T08:00:00.000Z","top":100,"_content":"\nHello, I am a PhD student in [Data Intelligence Lab](http://59.111.103.237:8081/) of Zhejiang University, advised by [Prof. Lidan Shou](https://person.zju.edu.cn/en/should).\n\nI work on Natural Language Processing and Data Mining. More specifically, my research interests lie in Information Extraction (e.g., Named Entity Recognition and Relation Extraction), NLP in low-resource scenarios (e.g., Weak/Semi-Supervised Learning), and Efficient Algorithms for NLP (e.g., Knowledge Distillation and Network Pruning). If you want to get in touch, please [send me an email](mailto:zjuwangjue@gmail.com). \n\nMy [resume](resume-Jue.Wang.pdf). \n\n## Updates\n\n- Jun 2021: I graduated from [CentraleSupélec](https://www.centralesupelec.fr/) with diplôme d'Ingénieur (master degree), cheers!\n- Dec 2020: As the first author, I had one long paper accepted to AAAI 2021.\n- Sep 2020: As the first author, I had one long paper accepted to EMNLP 2020.\n- Apr 2020: As the first author, I had one long paper accepted to ACL 2020.\n- Feb 2020: I had a remote internship at [StatNLP](https://statnlp-research.github.io/) under the guidance of [Prof. Wei Lu](https://istd.sutd.edu.sg/people/faculty/lu-wei).\n- Aug 2019: I was enrolled in ByteCamp hosted by [ByteDance](https://bytedance.com/en), where I mainly deal with Multimodal Classification.\n- July 2019: We got one demo paper accepted to SIGIR 2019. I attended the conference as the assistant presenter.\n- Jun 2018 to Dec 2018: I did an internship in [Rokid](https://www.rokid.com/), where I mainly deal with Spoken Language Understanding.\n- Jun 2017 to Aug 2018: I did an research internship in [Data Intelligence Lab](http://59.111.103.237:8081/).\n\n## Education\n\n- **Zhejiang University**, PhD student in Computer Science (Current), Sep 2018 - Jun 2023 (Expected)\n- **Université Paris Saclay (CentraleSupélec)**, Master (Engineer) in General Engineering, Sep 2016 - May 2021\n- **Zhejiang University**, Bachelor in Electrical Engineering, Sep 2014 - Jun 2018\n\n## Contact\n\nCollege of Computer Science and Technology, Zhejiang University\n\n38 Zheda Rd, Xihu Qu, Hangzhou, Zhejiang, 310027\n\nEmail: zjuwangjue@gmail.com\n\n\n\n([中文版](/about-zh))\n\n---\n\n[Blog](https://blog.lorrin.info)([RSS](https://blog.lorrin.info/atom.xml)), [Github](https://github.com/LorrinWWW), [知乎](https://www.zhihu.com/people/wang-jue-9/activities), 欢迎关注～","source":"_posts/about.md","raw":"---\ntitle: \"Jue Wang\"\ndate: 2021-6-1 16:00:00\ntop: 100\n---\n\nHello, I am a PhD student in [Data Intelligence Lab](http://59.111.103.237:8081/) of Zhejiang University, advised by [Prof. Lidan Shou](https://person.zju.edu.cn/en/should).\n\nI work on Natural Language Processing and Data Mining. More specifically, my research interests lie in Information Extraction (e.g., Named Entity Recognition and Relation Extraction), NLP in low-resource scenarios (e.g., Weak/Semi-Supervised Learning), and Efficient Algorithms for NLP (e.g., Knowledge Distillation and Network Pruning). If you want to get in touch, please [send me an email](mailto:zjuwangjue@gmail.com). \n\nMy [resume](resume-Jue.Wang.pdf). \n\n## Updates\n\n- Jun 2021: I graduated from [CentraleSupélec](https://www.centralesupelec.fr/) with diplôme d'Ingénieur (master degree), cheers!\n- Dec 2020: As the first author, I had one long paper accepted to AAAI 2021.\n- Sep 2020: As the first author, I had one long paper accepted to EMNLP 2020.\n- Apr 2020: As the first author, I had one long paper accepted to ACL 2020.\n- Feb 2020: I had a remote internship at [StatNLP](https://statnlp-research.github.io/) under the guidance of [Prof. Wei Lu](https://istd.sutd.edu.sg/people/faculty/lu-wei).\n- Aug 2019: I was enrolled in ByteCamp hosted by [ByteDance](https://bytedance.com/en), where I mainly deal with Multimodal Classification.\n- July 2019: We got one demo paper accepted to SIGIR 2019. I attended the conference as the assistant presenter.\n- Jun 2018 to Dec 2018: I did an internship in [Rokid](https://www.rokid.com/), where I mainly deal with Spoken Language Understanding.\n- Jun 2017 to Aug 2018: I did an research internship in [Data Intelligence Lab](http://59.111.103.237:8081/).\n\n## Education\n\n- **Zhejiang University**, PhD student in Computer Science (Current), Sep 2018 - Jun 2023 (Expected)\n- **Université Paris Saclay (CentraleSupélec)**, Master (Engineer) in General Engineering, Sep 2016 - May 2021\n- **Zhejiang University**, Bachelor in Electrical Engineering, Sep 2014 - Jun 2018\n\n## Contact\n\nCollege of Computer Science and Technology, Zhejiang University\n\n38 Zheda Rd, Xihu Qu, Hangzhou, Zhejiang, 310027\n\nEmail: zjuwangjue@gmail.com\n\n\n\n([中文版](/about-zh))\n\n---\n\n[Blog](https://blog.lorrin.info)([RSS](https://blog.lorrin.info/atom.xml)), [Github](https://github.com/LorrinWWW), [知乎](https://www.zhihu.com/people/wang-jue-9/activities), 欢迎关注～","slug":"about","published":1,"updated":"2021-11-18T09:18:19.227Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckw4qufpy0031gwtlfssibi1s","content":"<p>Hello, I am a PhD student in <a href=\"http://59.111.103.237:8081/\">Data Intelligence Lab</a> of Zhejiang University, advised by <a href=\"https://person.zju.edu.cn/en/should\">Prof. Lidan Shou</a>.</p>\n<p>I work on Natural Language Processing and Data Mining. More specifically, my research interests lie in Information Extraction (e.g., Named Entity Recognition and Relation Extraction), NLP in low-resource scenarios (e.g., Weak/Semi-Supervised Learning), and Efficient Algorithms for NLP (e.g., Knowledge Distillation and Network Pruning). If you want to get in touch, please <a href=\"mailto:zjuwangjue@gmail.com\">send me an email</a>. </p>\n<p>My <a href=\"resume-Jue.Wang.pdf\">resume</a>. </p>\n<h2 id=\"Updates\"><a href=\"#Updates\" class=\"headerlink\" title=\"Updates\"></a>Updates</h2><ul>\n<li>Jun 2021: I graduated from <a href=\"https://www.centralesupelec.fr/\">CentraleSupélec</a> with diplôme d’Ingénieur (master degree), cheers!</li>\n<li>Dec 2020: As the first author, I had one long paper accepted to AAAI 2021.</li>\n<li>Sep 2020: As the first author, I had one long paper accepted to EMNLP 2020.</li>\n<li>Apr 2020: As the first author, I had one long paper accepted to ACL 2020.</li>\n<li>Feb 2020: I had a remote internship at <a href=\"https://statnlp-research.github.io/\">StatNLP</a> under the guidance of <a href=\"https://istd.sutd.edu.sg/people/faculty/lu-wei\">Prof. Wei Lu</a>.</li>\n<li>Aug 2019: I was enrolled in ByteCamp hosted by <a href=\"https://bytedance.com/en\">ByteDance</a>, where I mainly deal with Multimodal Classification.</li>\n<li>July 2019: We got one demo paper accepted to SIGIR 2019. I attended the conference as the assistant presenter.</li>\n<li>Jun 2018 to Dec 2018: I did an internship in <a href=\"https://www.rokid.com/\">Rokid</a>, where I mainly deal with Spoken Language Understanding.</li>\n<li>Jun 2017 to Aug 2018: I did an research internship in <a href=\"http://59.111.103.237:8081/\">Data Intelligence Lab</a>.</li>\n</ul>\n<h2 id=\"Education\"><a href=\"#Education\" class=\"headerlink\" title=\"Education\"></a>Education</h2><ul>\n<li><strong>Zhejiang University</strong>, PhD student in Computer Science (Current), Sep 2018 - Jun 2023 (Expected)</li>\n<li><strong>Université Paris Saclay (CentraleSupélec)</strong>, Master (Engineer) in General Engineering, Sep 2016 - May 2021</li>\n<li><strong>Zhejiang University</strong>, Bachelor in Electrical Engineering, Sep 2014 - Jun 2018</li>\n</ul>\n<h2 id=\"Contact\"><a href=\"#Contact\" class=\"headerlink\" title=\"Contact\"></a>Contact</h2><p>College of Computer Science and Technology, Zhejiang University</p>\n<p>38 Zheda Rd, Xihu Qu, Hangzhou, Zhejiang, 310027</p>\n<p>Email: <a href=\"mailto:zjuwangjue@gmail.com\">zjuwangjue@gmail.com</a></p>\n<p>(<a href=\"/about-zh\">中文版</a>)</p>\n<hr>\n<p><a href=\"https://blog.lorrin.info/\">Blog</a>(<a href=\"https://blog.lorrin.info/atom.xml\">RSS</a>), <a href=\"https://github.com/LorrinWWW\">Github</a>, <a href=\"https://www.zhihu.com/people/wang-jue-9/activities\">知乎</a>, 欢迎关注～</p>\n","site":{"data":{"recommended_posts":{"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"}],"https://LorrinWWW.github.io/posts/complexity/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/hexo-with-latex/":[{"title":"区块链时代的最正确的打赏姿势——在Hexo博客配置Coinhive网页挖矿","permalink":"http://www.davidfnck.com/blockchain/mine-xmr-in-hexo-by-coinhive.html/"}],"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/":[{"title":"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-statistic/":[{"title":"贝叶斯估计 Bayes estimation","permalink":"https://LorrinWWW.github.io/posts/Bayes-estimation/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"}],"https://LorrinWWW.github.io/posts/[2018.2.5]Nested-LSTMs/":[{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"}],"https://LorrinWWW.github.io/posts/datamining-pretreatment/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Several models for knowledge graph representing and completing 几个知识图谱模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/"}],"https://LorrinWWW.github.io/posts/compression/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/Sobolev-space/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/proba-ch5/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch3/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/proba-ch1/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/ML-CNN/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/EDP-basic-models/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/machine-learning/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/datamining-class-pred/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction/":[],"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/":[{"title":"OS notes","permalink":"https://LorrinWWW.github.io/posts/OS-notes/"}],"https://LorrinWWW.github.io/posts/projet-enjeu-plugin-chrome-101/":[{"title":"Chrome油猴脚本","permalink":"http://gubangzhong.cn/2017/06/13/Chrome油猴脚本/"}],"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/":[{"title":"MongoDB, Docker and Python","permalink":"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/":[{"title":"Hands on Scrapy","permalink":"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/OS-notes/":[{"title":"learning OS and building LorriOS","permalink":"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/"}],"https://LorrinWWW.github.io/posts/Bayes-estimation/":[{"title":"Note of statistic","permalink":"https://LorrinWWW.github.io/posts/Note-of-statistic/"}],"https://LorrinWWW.github.io/posts/management-of-the-firm/":[],"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"}],"https://LorrinWWW.github.io/posts/datamining-qualitative-induction/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-finite-element-method/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/QuadTree/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/proba-ch4/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch2/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/":[{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/Note-of-NLP/":[{"title":"Generative Adversarial Network","permalink":"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/"},{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"}],"https://LorrinWWW.github.io/posts/Note-of-datamining/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"数据挖掘-分类与预测","permalink":"https://LorrinWWW.github.io/posts/datamining-class-pred/"},{"title":"数据挖掘-预处理","permalink":"https://LorrinWWW.github.io/posts/datamining-pretreatment/"}],"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/Note-of-probability/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/participe-present-et-gerondif/":[],"https://LorrinWWW.github.io/posts/Hilbert-space/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/proba-ch6/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}]}}},"excerpt":"","more":"<p>Hello, I am a PhD student in <a href=\"http://59.111.103.237:8081/\">Data Intelligence Lab</a> of Zhejiang University, advised by <a href=\"https://person.zju.edu.cn/en/should\">Prof. Lidan Shou</a>.</p>\n<p>I work on Natural Language Processing and Data Mining. More specifically, my research interests lie in Information Extraction (e.g., Named Entity Recognition and Relation Extraction), NLP in low-resource scenarios (e.g., Weak/Semi-Supervised Learning), and Efficient Algorithms for NLP (e.g., Knowledge Distillation and Network Pruning). If you want to get in touch, please <a href=\"mailto:zjuwangjue@gmail.com\">send me an email</a>. </p>\n<p>My <a href=\"resume-Jue.Wang.pdf\">resume</a>. </p>\n<h2 id=\"Updates\"><a href=\"#Updates\" class=\"headerlink\" title=\"Updates\"></a>Updates</h2><ul>\n<li>Jun 2021: I graduated from <a href=\"https://www.centralesupelec.fr/\">CentraleSupélec</a> with diplôme d’Ingénieur (master degree), cheers!</li>\n<li>Dec 2020: As the first author, I had one long paper accepted to AAAI 2021.</li>\n<li>Sep 2020: As the first author, I had one long paper accepted to EMNLP 2020.</li>\n<li>Apr 2020: As the first author, I had one long paper accepted to ACL 2020.</li>\n<li>Feb 2020: I had a remote internship at <a href=\"https://statnlp-research.github.io/\">StatNLP</a> under the guidance of <a href=\"https://istd.sutd.edu.sg/people/faculty/lu-wei\">Prof. Wei Lu</a>.</li>\n<li>Aug 2019: I was enrolled in ByteCamp hosted by <a href=\"https://bytedance.com/en\">ByteDance</a>, where I mainly deal with Multimodal Classification.</li>\n<li>July 2019: We got one demo paper accepted to SIGIR 2019. I attended the conference as the assistant presenter.</li>\n<li>Jun 2018 to Dec 2018: I did an internship in <a href=\"https://www.rokid.com/\">Rokid</a>, where I mainly deal with Spoken Language Understanding.</li>\n<li>Jun 2017 to Aug 2018: I did an research internship in <a href=\"http://59.111.103.237:8081/\">Data Intelligence Lab</a>.</li>\n</ul>\n<h2 id=\"Education\"><a href=\"#Education\" class=\"headerlink\" title=\"Education\"></a>Education</h2><ul>\n<li><strong>Zhejiang University</strong>, PhD student in Computer Science (Current), Sep 2018 - Jun 2023 (Expected)</li>\n<li><strong>Université Paris Saclay (CentraleSupélec)</strong>, Master (Engineer) in General Engineering, Sep 2016 - May 2021</li>\n<li><strong>Zhejiang University</strong>, Bachelor in Electrical Engineering, Sep 2014 - Jun 2018</li>\n</ul>\n<h2 id=\"Contact\"><a href=\"#Contact\" class=\"headerlink\" title=\"Contact\"></a>Contact</h2><p>College of Computer Science and Technology, Zhejiang University</p>\n<p>38 Zheda Rd, Xihu Qu, Hangzhou, Zhejiang, 310027</p>\n<p>Email: <a href=\"mailto:&#122;&#x6a;&#x75;&#x77;&#x61;&#x6e;&#103;&#106;&#x75;&#x65;&#64;&#103;&#x6d;&#97;&#105;&#108;&#x2e;&#x63;&#x6f;&#109;\">&#122;&#x6a;&#x75;&#x77;&#x61;&#x6e;&#103;&#106;&#x75;&#x65;&#64;&#103;&#x6d;&#97;&#105;&#108;&#x2e;&#x63;&#x6f;&#109;</a></p>\n<p>(<a href=\"/about-zh\">中文版</a>)</p>\n<hr>\n<p><a href=\"https://blog.lorrin.info/\">Blog</a>(<a href=\"https://blog.lorrin.info/atom.xml\">RSS</a>), <a href=\"https://github.com/LorrinWWW\">Github</a>, <a href=\"https://www.zhihu.com/people/wang-jue-9/activities\">知乎</a>, 欢迎关注～</p>\n"},{"title":"复杂度 complexity","date":"2016-11-27T02:52:05.000Z","_content":"\n# 计算复杂度 - Calcul the complexity\n\n这里我们只考虑时间复杂度。\n\nThere, we only discuss the time complexity.\n\n## 通常 - Normal\n\n1. Single operation - O(1)\n\n2. Loop\n\n   ```Python\n   def fun(n):\n   \tfor i in range(n):\n      \t\tpass\n   ```\n\n   ```python\n   def fun(n):\n   \twhile i < n :\n       \ti += 1\n   ```\n\n   — O(n)\n\n   ```python\n   def fun(n):\n       while i < n :\n           i *= 2\n   ```\n\n   — O(logn)\n\n   Etc.\n\n   ## 递归 - Recursion\n   1.  代入法\n\n       预估其复杂度，代入原方程，若相符，则可能为解。\n\n       ex:\n\n       T(n) = 2*T(n/2) + O(n)\n\n       假设 T(n) = kn^2\n\n       代入原式，成立。\n\n       接下来用数学归纳法验证。\n\n   2.  迭代法\n\n          迭代地展开递归方程的右端，使其成为一个非递归的和式，随后进行复杂度计算\n\n          ex:\n\n          T(n) = T(n-1) + O(1)\n\n          T(1) = O(1)\n\n          所以，T(n) = T(n-1) + O(1) = T(n-1) + 2 * O(1) = … = n*O(1) = O(n)\n\n   3.  套用公式\n\n          对于形如\n\n          T(n) = aT(n/b) +f(n)\n\n          的方程已有固定判断法，套用公式即可。\n\n   4.  差分方程法\n\n          利用查分方程解，这个比较复杂，这里先占个位，以后在补。","source":"_posts/complexity.md","raw":"---\ntitle: 复杂度 complexity\ndate: 2016-11-27 10:52:05\ncategories: programming\ntags: [algo, programming, complexity]\n---\n\n# 计算复杂度 - Calcul the complexity\n\n这里我们只考虑时间复杂度。\n\nThere, we only discuss the time complexity.\n\n## 通常 - Normal\n\n1. Single operation - O(1)\n\n2. Loop\n\n   ```Python\n   def fun(n):\n   \tfor i in range(n):\n      \t\tpass\n   ```\n\n   ```python\n   def fun(n):\n   \twhile i < n :\n       \ti += 1\n   ```\n\n   — O(n)\n\n   ```python\n   def fun(n):\n       while i < n :\n           i *= 2\n   ```\n\n   — O(logn)\n\n   Etc.\n\n   ## 递归 - Recursion\n   1.  代入法\n\n       预估其复杂度，代入原方程，若相符，则可能为解。\n\n       ex:\n\n       T(n) = 2*T(n/2) + O(n)\n\n       假设 T(n) = kn^2\n\n       代入原式，成立。\n\n       接下来用数学归纳法验证。\n\n   2.  迭代法\n\n          迭代地展开递归方程的右端，使其成为一个非递归的和式，随后进行复杂度计算\n\n          ex:\n\n          T(n) = T(n-1) + O(1)\n\n          T(1) = O(1)\n\n          所以，T(n) = T(n-1) + O(1) = T(n-1) + 2 * O(1) = … = n*O(1) = O(n)\n\n   3.  套用公式\n\n          对于形如\n\n          T(n) = aT(n/b) +f(n)\n\n          的方程已有固定判断法，套用公式即可。\n\n   4.  差分方程法\n\n          利用查分方程解，这个比较复杂，这里先占个位，以后在补。","slug":"complexity","published":1,"updated":"2016-11-30T10:43:08.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckw4qufpz0034gwtl8swk33xy","content":"<h1 id=\"计算复杂度-Calcul-the-complexity\"><a href=\"#计算复杂度-Calcul-the-complexity\" class=\"headerlink\" title=\"计算复杂度 - Calcul the complexity\"></a>计算复杂度 - Calcul the complexity</h1><p>这里我们只考虑时间复杂度。</p>\n<p>There, we only discuss the time complexity.</p>\n<h2 id=\"通常-Normal\"><a href=\"#通常-Normal\" class=\"headerlink\" title=\"通常 - Normal\"></a>通常 - Normal</h2><ol>\n<li><p>Single operation - O(1)</p>\n</li>\n<li><p>Loop</p>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">fun</span>(<span class=\"params\">n</span>):</span></span><br><span class=\"line\">\t<span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(n):</span><br><span class=\"line\">   \t\t<span class=\"keyword\">pass</span></span><br></pre></td></tr></tbody></table></figure>\n\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">fun</span>(<span class=\"params\">n</span>):</span></span><br><span class=\"line\">\t<span class=\"keyword\">while</span> i &lt; n :</span><br><span class=\"line\">    \ti += <span class=\"number\">1</span></span><br></pre></td></tr></tbody></table></figure>\n\n<p>— O(n)</p>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">fun</span>(<span class=\"params\">n</span>):</span></span><br><span class=\"line\">    <span class=\"keyword\">while</span> i &lt; n :</span><br><span class=\"line\">        i *= <span class=\"number\">2</span></span><br></pre></td></tr></tbody></table></figure>\n\n<p>— O(logn)</p>\n<p>Etc.</p>\n<h2 id=\"递归-Recursion\"><a href=\"#递归-Recursion\" class=\"headerlink\" title=\"递归 - Recursion\"></a>递归 - Recursion</h2><ol>\n<li><p>代入法</p>\n<p> 预估其复杂度，代入原方程，若相符，则可能为解。</p>\n<p> ex:</p>\n<p> T(n) = 2*T(n/2) + O(n)</p>\n<p> 假设 T(n) = kn^2</p>\n<p> 代入原式，成立。</p>\n<p> 接下来用数学归纳法验证。</p>\n</li>\n<li><p>迭代法</p>\n<pre><code>迭代地展开递归方程的右端，使其成为一个非递归的和式，随后进行复杂度计算\n\nex:\n\nT(n) = T(n-1) + O(1)\n\nT(1) = O(1)\n\n所以，T(n) = T(n-1) + O(1) = T(n-1) + 2 * O(1) = … = n*O(1) = O(n)\n</code></pre>\n</li>\n<li><p>套用公式</p>\n<pre><code>对于形如\n\nT(n) = aT(n/b) +f(n)\n\n的方程已有固定判断法，套用公式即可。\n</code></pre>\n</li>\n<li><p>差分方程法</p>\n<pre><code>利用查分方程解，这个比较复杂，这里先占个位，以后在补。\n</code></pre>\n</li>\n</ol>\n</li>\n</ol>\n","site":{"data":{"recommended_posts":{"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"}],"https://LorrinWWW.github.io/posts/complexity/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/hexo-with-latex/":[{"title":"区块链时代的最正确的打赏姿势——在Hexo博客配置Coinhive网页挖矿","permalink":"http://www.davidfnck.com/blockchain/mine-xmr-in-hexo-by-coinhive.html/"}],"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/":[{"title":"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-statistic/":[{"title":"贝叶斯估计 Bayes estimation","permalink":"https://LorrinWWW.github.io/posts/Bayes-estimation/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"}],"https://LorrinWWW.github.io/posts/[2018.2.5]Nested-LSTMs/":[{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"}],"https://LorrinWWW.github.io/posts/datamining-pretreatment/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Several models for knowledge graph representing and completing 几个知识图谱模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/"}],"https://LorrinWWW.github.io/posts/compression/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/Sobolev-space/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/proba-ch5/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch3/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/proba-ch1/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/ML-CNN/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/EDP-basic-models/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/machine-learning/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/datamining-class-pred/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction/":[],"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/":[{"title":"OS notes","permalink":"https://LorrinWWW.github.io/posts/OS-notes/"}],"https://LorrinWWW.github.io/posts/projet-enjeu-plugin-chrome-101/":[{"title":"Chrome油猴脚本","permalink":"http://gubangzhong.cn/2017/06/13/Chrome油猴脚本/"}],"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/":[{"title":"MongoDB, Docker and Python","permalink":"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/":[{"title":"Hands on Scrapy","permalink":"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/OS-notes/":[{"title":"learning OS and building LorriOS","permalink":"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/"}],"https://LorrinWWW.github.io/posts/Bayes-estimation/":[{"title":"Note of statistic","permalink":"https://LorrinWWW.github.io/posts/Note-of-statistic/"}],"https://LorrinWWW.github.io/posts/management-of-the-firm/":[],"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"}],"https://LorrinWWW.github.io/posts/datamining-qualitative-induction/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-finite-element-method/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/QuadTree/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/proba-ch4/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch2/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/":[{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/Note-of-NLP/":[{"title":"Generative Adversarial Network","permalink":"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/"},{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"}],"https://LorrinWWW.github.io/posts/Note-of-datamining/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"数据挖掘-分类与预测","permalink":"https://LorrinWWW.github.io/posts/datamining-class-pred/"},{"title":"数据挖掘-预处理","permalink":"https://LorrinWWW.github.io/posts/datamining-pretreatment/"}],"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/Note-of-probability/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/participe-present-et-gerondif/":[],"https://LorrinWWW.github.io/posts/Hilbert-space/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/proba-ch6/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}]}}},"excerpt":"","more":"<h1 id=\"计算复杂度-Calcul-the-complexity\"><a href=\"#计算复杂度-Calcul-the-complexity\" class=\"headerlink\" title=\"计算复杂度 - Calcul the complexity\"></a>计算复杂度 - Calcul the complexity</h1><p>这里我们只考虑时间复杂度。</p>\n<p>There, we only discuss the time complexity.</p>\n<h2 id=\"通常-Normal\"><a href=\"#通常-Normal\" class=\"headerlink\" title=\"通常 - Normal\"></a>通常 - Normal</h2><ol>\n<li><p>Single operation - O(1)</p>\n</li>\n<li><p>Loop</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">fun</span>(<span class=\"params\">n</span>):</span></span><br><span class=\"line\">\t<span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(n):</span><br><span class=\"line\">   \t\t<span class=\"keyword\">pass</span></span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">fun</span>(<span class=\"params\">n</span>):</span></span><br><span class=\"line\">\t<span class=\"keyword\">while</span> i &lt; n :</span><br><span class=\"line\">    \ti += <span class=\"number\">1</span></span><br></pre></td></tr></table></figure>\n\n<p>— O(n)</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">fun</span>(<span class=\"params\">n</span>):</span></span><br><span class=\"line\">    <span class=\"keyword\">while</span> i &lt; n :</span><br><span class=\"line\">        i *= <span class=\"number\">2</span></span><br></pre></td></tr></table></figure>\n\n<p>— O(logn)</p>\n<p>Etc.</p>\n<h2 id=\"递归-Recursion\"><a href=\"#递归-Recursion\" class=\"headerlink\" title=\"递归 - Recursion\"></a>递归 - Recursion</h2><ol>\n<li><p>代入法</p>\n<p> 预估其复杂度，代入原方程，若相符，则可能为解。</p>\n<p> ex:</p>\n<p> T(n) = 2*T(n/2) + O(n)</p>\n<p> 假设 T(n) = kn^2</p>\n<p> 代入原式，成立。</p>\n<p> 接下来用数学归纳法验证。</p>\n</li>\n<li><p>迭代法</p>\n<pre><code>迭代地展开递归方程的右端，使其成为一个非递归的和式，随后进行复杂度计算\n\nex:\n\nT(n) = T(n-1) + O(1)\n\nT(1) = O(1)\n\n所以，T(n) = T(n-1) + O(1) = T(n-1) + 2 * O(1) = … = n*O(1) = O(n)\n</code></pre>\n</li>\n<li><p>套用公式</p>\n<pre><code>对于形如\n\nT(n) = aT(n/b) +f(n)\n\n的方程已有固定判断法，套用公式即可。\n</code></pre>\n</li>\n<li><p>差分方程法</p>\n<pre><code>利用查分方程解，这个比较复杂，这里先占个位，以后在补。\n</code></pre>\n</li>\n</ol>\n</li>\n</ol>\n"},{"title":"编码解码 compression","date":"2016-11-27T06:22:11.000Z","_content":"\n1. Run length encoding\n\n   ex:\n\n   0101 — 0,101 — \"3 pixels are color '0'\"\n\n   1101 — 1,101 — \"6 pixels are color '1'\"\n\n   You can also define other signification like:\n\n   1 |1111|1111|1111|1111|0111|0111|0111|0111| with the first 1 meaning encoding in rank, while 0 meaning encoding in row.\n\n2. Huffman\n\n   Defined in wiki\n\n   Normally we supppose higher number with \"1\".\n\n   ex:\n\n   ​                        (1)\n\n   ​                   0/       \\1\n\n   ​            a(0.45)     (0.55)\n\n   ​                             0/     \\1\n\n   ​                     b(0.25)    c(0.30)\n\n3. Lempel-Ziv\n\n   Encode:\n\n   - Origin: ababcbab...\n   - Init: a:0, b:1, c:2\n   - Extensions du dico: ab: 3, ba: 4, abc: 5, cb: 6...\n   - Result: 01324...\n\n   Decode: pass\n\n   ​","source":"_posts/compression.md","raw":"---\ntitle: 编码解码 compression\ndate: 2016-11-27 14:22:11\ncategories: programming\ntags: [algo, compression, programming]\n---\n\n1. Run length encoding\n\n   ex:\n\n   0101 — 0,101 — \"3 pixels are color '0'\"\n\n   1101 — 1,101 — \"6 pixels are color '1'\"\n\n   You can also define other signification like:\n\n   1 |1111|1111|1111|1111|0111|0111|0111|0111| with the first 1 meaning encoding in rank, while 0 meaning encoding in row.\n\n2. Huffman\n\n   Defined in wiki\n\n   Normally we supppose higher number with \"1\".\n\n   ex:\n\n   ​                        (1)\n\n   ​                   0/       \\1\n\n   ​            a(0.45)     (0.55)\n\n   ​                             0/     \\1\n\n   ​                     b(0.25)    c(0.30)\n\n3. Lempel-Ziv\n\n   Encode:\n\n   - Origin: ababcbab...\n   - Init: a:0, b:1, c:2\n   - Extensions du dico: ab: 3, ba: 4, abc: 5, cb: 6...\n   - Result: 01324...\n\n   Decode: pass\n\n   ​","slug":"compression","published":1,"updated":"2016-11-30T10:46:12.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckw4qufq10038gwtle9ta0x74","content":"<ol>\n<li><p>Run length encoding</p>\n<p>ex:</p>\n<p>0101 — 0,101 — “3 pixels are color ‘0’”</p>\n<p>1101 — 1,101 — “6 pixels are color ‘1’”</p>\n<p>You can also define other signification like:</p>\n<p>1 |1111|1111|1111|1111|0111|0111|0111|0111| with the first 1 meaning encoding in rank, while 0 meaning encoding in row.</p>\n</li>\n<li><p>Huffman</p>\n<p>Defined in wiki</p>\n<p>Normally we supppose higher number with “1”.</p>\n<p>ex:</p>\n<p>​                        (1)</p>\n<p>​                   0/       \\1</p>\n<p>​            a(0.45)     (0.55)</p>\n<p>​                             0/     \\1</p>\n<p>​                     b(0.25)    c(0.30)</p>\n</li>\n<li><p>Lempel-Ziv</p>\n<p>Encode:</p>\n<ul>\n<li>Origin: ababcbab…</li>\n<li>Init: a:0, b:1, c:2</li>\n<li>Extensions du dico: ab: 3, ba: 4, abc: 5, cb: 6…</li>\n<li>Result: 01324…</li>\n</ul>\n<p>Decode: pass</p>\n<p>​</p>\n</li>\n</ol>\n","site":{"data":{"recommended_posts":{"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"}],"https://LorrinWWW.github.io/posts/complexity/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/hexo-with-latex/":[{"title":"区块链时代的最正确的打赏姿势——在Hexo博客配置Coinhive网页挖矿","permalink":"http://www.davidfnck.com/blockchain/mine-xmr-in-hexo-by-coinhive.html/"}],"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/":[{"title":"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-statistic/":[{"title":"贝叶斯估计 Bayes estimation","permalink":"https://LorrinWWW.github.io/posts/Bayes-estimation/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"}],"https://LorrinWWW.github.io/posts/[2018.2.5]Nested-LSTMs/":[{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"}],"https://LorrinWWW.github.io/posts/datamining-pretreatment/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Several models for knowledge graph representing and completing 几个知识图谱模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/"}],"https://LorrinWWW.github.io/posts/compression/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/Sobolev-space/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/proba-ch5/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch3/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/proba-ch1/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/ML-CNN/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/EDP-basic-models/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/machine-learning/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/datamining-class-pred/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction/":[],"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/":[{"title":"OS notes","permalink":"https://LorrinWWW.github.io/posts/OS-notes/"}],"https://LorrinWWW.github.io/posts/projet-enjeu-plugin-chrome-101/":[{"title":"Chrome油猴脚本","permalink":"http://gubangzhong.cn/2017/06/13/Chrome油猴脚本/"}],"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/":[{"title":"MongoDB, Docker and Python","permalink":"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/":[{"title":"Hands on Scrapy","permalink":"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/OS-notes/":[{"title":"learning OS and building LorriOS","permalink":"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/"}],"https://LorrinWWW.github.io/posts/Bayes-estimation/":[{"title":"Note of statistic","permalink":"https://LorrinWWW.github.io/posts/Note-of-statistic/"}],"https://LorrinWWW.github.io/posts/management-of-the-firm/":[],"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"}],"https://LorrinWWW.github.io/posts/datamining-qualitative-induction/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-finite-element-method/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/QuadTree/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/proba-ch4/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch2/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/":[{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/Note-of-NLP/":[{"title":"Generative Adversarial Network","permalink":"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/"},{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"}],"https://LorrinWWW.github.io/posts/Note-of-datamining/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"数据挖掘-分类与预测","permalink":"https://LorrinWWW.github.io/posts/datamining-class-pred/"},{"title":"数据挖掘-预处理","permalink":"https://LorrinWWW.github.io/posts/datamining-pretreatment/"}],"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/Note-of-probability/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/participe-present-et-gerondif/":[],"https://LorrinWWW.github.io/posts/Hilbert-space/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/proba-ch6/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}]}}},"excerpt":"","more":"<ol>\n<li><p>Run length encoding</p>\n<p>ex:</p>\n<p>0101 — 0,101 — “3 pixels are color ‘0’”</p>\n<p>1101 — 1,101 — “6 pixels are color ‘1’”</p>\n<p>You can also define other signification like:</p>\n<p>1 |1111|1111|1111|1111|0111|0111|0111|0111| with the first 1 meaning encoding in rank, while 0 meaning encoding in row.</p>\n</li>\n<li><p>Huffman</p>\n<p>Defined in wiki</p>\n<p>Normally we supppose higher number with “1”.</p>\n<p>ex:</p>\n<p>​                        (1)</p>\n<p>​                   0/       \\1</p>\n<p>​            a(0.45)     (0.55)</p>\n<p>​                             0/     \\1</p>\n<p>​                     b(0.25)    c(0.30)</p>\n</li>\n<li><p>Lempel-Ziv</p>\n<p>Encode:</p>\n<ul>\n<li>Origin: ababcbab…</li>\n<li>Init: a:0, b:1, c:2</li>\n<li>Extensions du dico: ab: 3, ba: 4, abc: 5, cb: 6…</li>\n<li>Result: 01324…</li>\n</ul>\n<p>Decode: pass</p>\n<p>​</p>\n</li>\n</ol>\n"},{"title":"数据挖掘-分类与预测","date":"2016-12-26T06:56:57.000Z","_content":"\n# 基本知识\n\n主要分为两个步骤：\n\n1. 建立一个描述已知数据集类别或概念的模型。\n\n   分类规则形式、决策树形式，或数学公式形式。\n\n2. 利用所获得的模型进行分类操作。\n\n   首先要对模型分类准确率进行估计。\n\n   若其准确率可以接受，则可以使用该模型进行分类。   \t\n\n可以根据以下几条标准对各种分类方法进行比较:\n\n1. 预测准确率，它描述(学习所获)模型能够正确预测未知对象类别或(类别)数值的能力。\n2. 速度，它描述在构造和使用模型时的计算效率。 \n3. 鲁棒性，它描述在数据带有噪声和有数据遗失情况下，(学习所获)模型仍能进行正确预测的能力。 \n4. 可扩展性，它描述对处理大量数据并构造相应学习模型所需要的能力。\n5. 易理解性，它描述学习所获模型表示的可理解程度。 在本章的后面各节，将要陆续介绍上述有关问题的实现方法。\n\n# 基于决策树的分类\n\n1. 决策树的生成算法\n\n   - 基本决策树算法\n\n     本质是一个贪心算法，自上而下分而治之。\n\n2. 属性选择方法\n\n   信息量\n   $$\n   I(s_1,…,s_m) = - \\sum_{i=1}^{m}{p_i \\log (p_i)}\n   $$\n   利用属性A划分当前样本集合所需要的信息可以计算：\n   $$\n   E(A) = \\sum_{j=1}^{v}{\\frac{s_{1j}+s_{2j}+...+s_{mj}}{s}I(s_1,…,s_m) }\n   $$\n   E(A)的值越小，说明其子集划分结果越纯，即越好。而对于一个子集Sj，它的信息为\n   $$\n   I(s_{1j},…,s_{mj}) = - \\sum_{i=1}^{m}{p_{ij} \\log (p_{ij})}\n   $$\n   这样利用属性A对当前分支结点进行相应样本集合划分所获得的信息增益是:\n   $$\n   Gain(A) =I(s_1,…,s_m)- E(A)\n   $$\n   也就是说Gain(A)被认为是利用属性A进行划分后，所获的的信息熵的减少量。\n\n   决策树归纳算法计算每个属性的Gain，选择信息增益最大的属性，作为测试属性并由此产生相应的分支节点。\n\n3. 树枝的修剪\n\n   1. 事前修剪\n\n      基本原理是设置一个阀值，当某一节点的的样本数少于阀值，则停止细分。难点在于设置一个合理的阀值。\n\n   2. 事后修剪\n\n      从一个充分生长的树中修建。\n\n      可以使用基于代价成本的方法，也可以使用最短描述长度(MDL)。\n\n      前者计算其分类错误率，若修剪枝导致分类错误率变高，则保留，否则剪枝；后者选择最简单的，无需独立的数据测试\n\n4. 规则获取\n\n   在已经获得了一个决策树的基础上，可以使用\"if...else...\"语句描述该决策树。\n\n5. 基本决策树方法改进及其扩展性\n\n#  贝叶斯分类方法\n\n贝叶斯分类器是一个统计分类器，基于贝叶斯定理。\n\n简单贝叶斯分类器的分类性能可以与决策树和神经网络相比。\n\n简单贝叶斯分类器假设一个指定类别中各属性的取值是互相独立的，它可以帮助减少计算量。\n\n1. 贝叶斯定理\n\n   设X为一个类别未知的数据样本，H为某个假设，则：\n   $$\n   P(H|X) = \\frac{P(X|H)P(H)}{P(X)}\n   $$\n\n2. 简单贝叶斯分类方法\n\n   步骤说明如下：\n\n   1. 每一个数据都是有一个n维特征向量X={x1,…,xn}构成，分别描述其n个属性(A1,…,An)。\n\n   2. 若有m个不同的类别(C1,…,Cm)，分类器将未知X归属到类别Ci，当仅当\n      $$\n      P(C_i|X) = \\max(P(C_j|X)|1\\le j\\le m)\n      $$\n      所以我们要找到最大的\n      $$\n      P(C_i|X) = \\frac{P(X|C_i)P(C_i)}{P(X)}\n      $$\n\n   3. 要找到它，只需要P(X|Ci)P(Ci)取最大即可。由于各类别的事前概率是未知的，我们假设P(Ci)是互相相等的，这样，第二步中的式子取最大就转化为了寻找最大的P(X|Ci)\n\n   4. 由于所含的属性比较多，直接计算P(X|Ci)的计算量还是很大的。由于简单贝叶斯分类器假设各属性独立，则：\n      $$\n      P(X|C_i) = \\prod{P(x_k|C_i)}\n      $$\n      可以根据训练数据样本估算P(xk|Ci)的值。具体如下：\n\n      - 若Ak是符号量\n        $$\n        P(x_k|Ci)=\\frac{s_{ik}}{s_i}\n        $$\n        这里sik为训练样本中类别为Ci且属性Ak取vk的样本数。si为训练样本中类别为Ci的样本数。\n\n      - 若Ak是连续量\n        $$\n        P(x_k|Ci)=g(x_k,\\mu_{C_i},\\sigma_{C_i}) = \\frac{1}{\\sqrt{2\\pi}\\sigma_{C_i}}e^{-\\frac{(x-\\mu_{C_i})^2}{2\\sigma^2_{C_i}}}\n        $$\n        其中$g(x_k,\\mu_{C_i},\\sigma_{C_i})$ï为属性为Ak的高斯规范密度函数。\n\n   5. 为了预测X的分类，我们通过上述方法估计各个类别的正确率，将X归属到可能性最高的类别。\n\n3. 贝叶斯信念网络\n\n   简单贝叶斯假设属性相互独立，从而简化计算，而实际上属性相互依赖的情况比较常见，所以又出现了贝叶斯信念网络，用于描述这种相互关联的概率分布。\n\n   贝叶斯信念网络提供了一个图形模型来描述其中的因果关系。信念网络包括两个部分。\n\n   - 第一部分是有向无环图\n\n     每一个节点代表随机变量。\n\n     每一条弧代表一个概率依赖。\n\n     给定父节点，每个变量有条件的独立于图中非子节点。\n\n   - 第二部分是包含所有变量的条件概率表(CPT)\n\n     对于一个变量Z，CPT定义了P(Z|parent(Z))，由此可以得到一个表。\n\n     例如，LunCancer的父节点是FamilyHistory和Smoker，\n\n|             | FH, S | FH, ~S | ~FH, S | ~FH, ~S |\n| ----------- | ----- | ------ | ------ | ------- |\n| LungCancer  | 0.8   | 0.5    | 0.7    | 0.1     |\n| ~LungCancer | 0.2   | 0.5    | 0.3    | 0.9     |\n\n\n数据对象的联合概率通过如下公式计算。\n$$\nP(z_1,...,z_n) = \\prod{P(z_i|parent(z_i))}\n$$\n\n4. 贝叶斯信念网络的学习\n\n   学习算法步骤如下：\n\n   1. 计算下降梯度\n      $$\n      \\frac{\\partial\\ln{P_w(S)}}{\\partial w_{ijk}} = \\sum_{d=1}^{s}{\\frac{P(Y_i=y_{ij}, U_i=u_{ik} |X_d)}{w_{ijk}}}\n      $$\n      ​\n      左边是计算训练集合S中每个样本Xd的概率。设这一概率为p。\n\n      由Yi和Ui表示隐含变量，p可通过样本中可观察到的变量以及标准贝叶斯网络推理计算得到。\n\n   2. 沿梯度方向前进一小步，权重更新计算如下\n      $$\n      w_{ijk} \\leftarrow w_{ijk} + (l)\\frac{\\partial\\ln{P_w(S)}}{\\partial w_{ijk}}\n      $$\n      l为学习速率代表学习步长。通常学习速率为较小的常数。\n\n   3. 重新规格化权重\n\n      保证wijk的取值在0～1，其和等于1.\n\n# 神经网络分类方法\n\n1. 多层反馈神经网络\n\n   输入同时赋给第一层(输入层)单元，这些单元输出赋予权重，输出给第二层(隐藏层)。\n\n   隐藏层的带权输出又作为输入再馈给另一隐层。\n\n   最后的隐层结点带权输出馈给输出层单元，最终给出相应样本的预测输出。\n\n   该网络是前馈的，即每一个反馈只能发送到前面的输出层或隐含层。\n\n   它也是全连接的，即每一个层中单元均与前面一层的各单元相连接。\n\n   只要中间隐层足够多的话，多层前馈网络中的线性阈值函数，可以充分逼近任何函数。\n\n2. 神经网络结构\n\n   确定结构，即：\n\n   - 输入层的单元数\n   - 隐含层的个数(和层数)\n   - 每个隐含层的单元数目\n   - 输出层单元数目\n\n   对输入值规格化，一般取值在0～1.\n\n   离散数据可以为每一个取值增加一个节点进行编码。例如A={a0,a1,a2}则我们设立三个输入单元I0, I1, I2，每个单元默认为0，若A=a0，则I1置为1.\n\n   此外没有什么特定的规律或规则来指导隐含层的最佳单元数量，它的确定是一个不断试错的过程。\n\n3. 后传方法\n\n   后传方法不断地处理一个训练样本集。将处理结果与训练样本已知类别比较所获误差，修改权重，使网络输出与实际类别之间的均方差最小。权重的修改是后传的，即从前向后的。\n\n   其收敛性不能保证。\n\n   流程(伪代码)：\n\n   ```python\n   # 定义sum函数，以变量x对f(x)求和\n   def sum(f(x), x):\n       pass\n\n   # 初始化\n   init();\n\n   while !conditions:           # 条件不满足时\n       for X in samples:\n           for each layer:      # 每个隐含层和输出层\n               O[j] = 1 / (1 + exp( - I[j]))\n               I[j] = sum(w[i][j]O[i], i) + theta[j]\n           for each unit of output layer as j:    # 每个输出层单元j，计算向后传播误差\n               Err[j] = O[j] * (1 - O[j]) * (T[j] - O[j])\n           for each unit of hidden layer as j:    # 每个隐含层单元j，从最后一层到第一层隐含层\n               Err[j] = O[j] * (1 - O[j]) * sum(Err[k] * w[i][j][k], k)\n           for each w[i][j] in the network:       # network中的权重wij\n               delta_w[i][j] = (l) * Err[j] * O[i]     # (l)是学习速率，取值在0～1\n               w[i][j] += delta_w[i][j]\n           for each theta[j] in the network:      # network中的偏差thetaij\n               delta_theta[j] = (l) * Err[j]\n               v[i] = theta[j] + delta_theta[j]\n           \n   ```\n\n   看伪代码基本就能明白它的过程了。虽然伪代码可能写的比较迷。。。\n\n# 基于关联的分类方法\n\n略，后面会详细说。\n\n# 其他分类\n\n其他分类我也没有细看，纪录一下名字，以后有时间再看。\n\n1. k-最邻近方法\n\n   这个很简单，就是比较两个点的欧式距离。比较基本的分类方法，略。\n\n2. 基于示例推理\n\n3. 遗传算法\n\n4. 粗糙集方法\n\n5. 模糊集合方法\n\n# 预测方法\n\n1. 线形与多变量回归\n\n   线性代数、概率统计、数值计算方法里都学过。核心是利用最小二乘法。\n\n2. 非线性回归\n\n   例如多项式回归\n   $$\n   Y = \\alpha + \\beta_1 X+\\beta_2 X^2+\\beta_3 X^3\n   $$\n   为了将其转化为线性形式，令：\n   $$\n   X_1 = X;X_2 = X^2 ; X_3 = X^3\n   $$\n   接下来就用最小二乘法即可。\n\n3. 其他回归模型\n\n   对数回归、泊松回归等\n\n# 分类器准确性\n\n分层k次交叉验证方法普遍应用于对分类器预测准确性的评估方面。而bagging方法和boosting方法则通过学习和组合多个(单)分类器来帮助提高整个(数据训练样本所获)分类器的预测准确性。","source":"_posts/datamining-class-pred.md","raw":"---\ntitle: 数据挖掘-分类与预测\ndate: 2016-12-26 14:56:57\ncategories: [programming]\ntags: [datamining, programming, classification, prediction]\n---\n\n# 基本知识\n\n主要分为两个步骤：\n\n1. 建立一个描述已知数据集类别或概念的模型。\n\n   分类规则形式、决策树形式，或数学公式形式。\n\n2. 利用所获得的模型进行分类操作。\n\n   首先要对模型分类准确率进行估计。\n\n   若其准确率可以接受，则可以使用该模型进行分类。   \t\n\n可以根据以下几条标准对各种分类方法进行比较:\n\n1. 预测准确率，它描述(学习所获)模型能够正确预测未知对象类别或(类别)数值的能力。\n2. 速度，它描述在构造和使用模型时的计算效率。 \n3. 鲁棒性，它描述在数据带有噪声和有数据遗失情况下，(学习所获)模型仍能进行正确预测的能力。 \n4. 可扩展性，它描述对处理大量数据并构造相应学习模型所需要的能力。\n5. 易理解性，它描述学习所获模型表示的可理解程度。 在本章的后面各节，将要陆续介绍上述有关问题的实现方法。\n\n# 基于决策树的分类\n\n1. 决策树的生成算法\n\n   - 基本决策树算法\n\n     本质是一个贪心算法，自上而下分而治之。\n\n2. 属性选择方法\n\n   信息量\n   $$\n   I(s_1,…,s_m) = - \\sum_{i=1}^{m}{p_i \\log (p_i)}\n   $$\n   利用属性A划分当前样本集合所需要的信息可以计算：\n   $$\n   E(A) = \\sum_{j=1}^{v}{\\frac{s_{1j}+s_{2j}+...+s_{mj}}{s}I(s_1,…,s_m) }\n   $$\n   E(A)的值越小，说明其子集划分结果越纯，即越好。而对于一个子集Sj，它的信息为\n   $$\n   I(s_{1j},…,s_{mj}) = - \\sum_{i=1}^{m}{p_{ij} \\log (p_{ij})}\n   $$\n   这样利用属性A对当前分支结点进行相应样本集合划分所获得的信息增益是:\n   $$\n   Gain(A) =I(s_1,…,s_m)- E(A)\n   $$\n   也就是说Gain(A)被认为是利用属性A进行划分后，所获的的信息熵的减少量。\n\n   决策树归纳算法计算每个属性的Gain，选择信息增益最大的属性，作为测试属性并由此产生相应的分支节点。\n\n3. 树枝的修剪\n\n   1. 事前修剪\n\n      基本原理是设置一个阀值，当某一节点的的样本数少于阀值，则停止细分。难点在于设置一个合理的阀值。\n\n   2. 事后修剪\n\n      从一个充分生长的树中修建。\n\n      可以使用基于代价成本的方法，也可以使用最短描述长度(MDL)。\n\n      前者计算其分类错误率，若修剪枝导致分类错误率变高，则保留，否则剪枝；后者选择最简单的，无需独立的数据测试\n\n4. 规则获取\n\n   在已经获得了一个决策树的基础上，可以使用\"if...else...\"语句描述该决策树。\n\n5. 基本决策树方法改进及其扩展性\n\n#  贝叶斯分类方法\n\n贝叶斯分类器是一个统计分类器，基于贝叶斯定理。\n\n简单贝叶斯分类器的分类性能可以与决策树和神经网络相比。\n\n简单贝叶斯分类器假设一个指定类别中各属性的取值是互相独立的，它可以帮助减少计算量。\n\n1. 贝叶斯定理\n\n   设X为一个类别未知的数据样本，H为某个假设，则：\n   $$\n   P(H|X) = \\frac{P(X|H)P(H)}{P(X)}\n   $$\n\n2. 简单贝叶斯分类方法\n\n   步骤说明如下：\n\n   1. 每一个数据都是有一个n维特征向量X={x1,…,xn}构成，分别描述其n个属性(A1,…,An)。\n\n   2. 若有m个不同的类别(C1,…,Cm)，分类器将未知X归属到类别Ci，当仅当\n      $$\n      P(C_i|X) = \\max(P(C_j|X)|1\\le j\\le m)\n      $$\n      所以我们要找到最大的\n      $$\n      P(C_i|X) = \\frac{P(X|C_i)P(C_i)}{P(X)}\n      $$\n\n   3. 要找到它，只需要P(X|Ci)P(Ci)取最大即可。由于各类别的事前概率是未知的，我们假设P(Ci)是互相相等的，这样，第二步中的式子取最大就转化为了寻找最大的P(X|Ci)\n\n   4. 由于所含的属性比较多，直接计算P(X|Ci)的计算量还是很大的。由于简单贝叶斯分类器假设各属性独立，则：\n      $$\n      P(X|C_i) = \\prod{P(x_k|C_i)}\n      $$\n      可以根据训练数据样本估算P(xk|Ci)的值。具体如下：\n\n      - 若Ak是符号量\n        $$\n        P(x_k|Ci)=\\frac{s_{ik}}{s_i}\n        $$\n        这里sik为训练样本中类别为Ci且属性Ak取vk的样本数。si为训练样本中类别为Ci的样本数。\n\n      - 若Ak是连续量\n        $$\n        P(x_k|Ci)=g(x_k,\\mu_{C_i},\\sigma_{C_i}) = \\frac{1}{\\sqrt{2\\pi}\\sigma_{C_i}}e^{-\\frac{(x-\\mu_{C_i})^2}{2\\sigma^2_{C_i}}}\n        $$\n        其中$g(x_k,\\mu_{C_i},\\sigma_{C_i})$ï为属性为Ak的高斯规范密度函数。\n\n   5. 为了预测X的分类，我们通过上述方法估计各个类别的正确率，将X归属到可能性最高的类别。\n\n3. 贝叶斯信念网络\n\n   简单贝叶斯假设属性相互独立，从而简化计算，而实际上属性相互依赖的情况比较常见，所以又出现了贝叶斯信念网络，用于描述这种相互关联的概率分布。\n\n   贝叶斯信念网络提供了一个图形模型来描述其中的因果关系。信念网络包括两个部分。\n\n   - 第一部分是有向无环图\n\n     每一个节点代表随机变量。\n\n     每一条弧代表一个概率依赖。\n\n     给定父节点，每个变量有条件的独立于图中非子节点。\n\n   - 第二部分是包含所有变量的条件概率表(CPT)\n\n     对于一个变量Z，CPT定义了P(Z|parent(Z))，由此可以得到一个表。\n\n     例如，LunCancer的父节点是FamilyHistory和Smoker，\n\n|             | FH, S | FH, ~S | ~FH, S | ~FH, ~S |\n| ----------- | ----- | ------ | ------ | ------- |\n| LungCancer  | 0.8   | 0.5    | 0.7    | 0.1     |\n| ~LungCancer | 0.2   | 0.5    | 0.3    | 0.9     |\n\n\n数据对象的联合概率通过如下公式计算。\n$$\nP(z_1,...,z_n) = \\prod{P(z_i|parent(z_i))}\n$$\n\n4. 贝叶斯信念网络的学习\n\n   学习算法步骤如下：\n\n   1. 计算下降梯度\n      $$\n      \\frac{\\partial\\ln{P_w(S)}}{\\partial w_{ijk}} = \\sum_{d=1}^{s}{\\frac{P(Y_i=y_{ij}, U_i=u_{ik} |X_d)}{w_{ijk}}}\n      $$\n      ​\n      左边是计算训练集合S中每个样本Xd的概率。设这一概率为p。\n\n      由Yi和Ui表示隐含变量，p可通过样本中可观察到的变量以及标准贝叶斯网络推理计算得到。\n\n   2. 沿梯度方向前进一小步，权重更新计算如下\n      $$\n      w_{ijk} \\leftarrow w_{ijk} + (l)\\frac{\\partial\\ln{P_w(S)}}{\\partial w_{ijk}}\n      $$\n      l为学习速率代表学习步长。通常学习速率为较小的常数。\n\n   3. 重新规格化权重\n\n      保证wijk的取值在0～1，其和等于1.\n\n# 神经网络分类方法\n\n1. 多层反馈神经网络\n\n   输入同时赋给第一层(输入层)单元，这些单元输出赋予权重，输出给第二层(隐藏层)。\n\n   隐藏层的带权输出又作为输入再馈给另一隐层。\n\n   最后的隐层结点带权输出馈给输出层单元，最终给出相应样本的预测输出。\n\n   该网络是前馈的，即每一个反馈只能发送到前面的输出层或隐含层。\n\n   它也是全连接的，即每一个层中单元均与前面一层的各单元相连接。\n\n   只要中间隐层足够多的话，多层前馈网络中的线性阈值函数，可以充分逼近任何函数。\n\n2. 神经网络结构\n\n   确定结构，即：\n\n   - 输入层的单元数\n   - 隐含层的个数(和层数)\n   - 每个隐含层的单元数目\n   - 输出层单元数目\n\n   对输入值规格化，一般取值在0～1.\n\n   离散数据可以为每一个取值增加一个节点进行编码。例如A={a0,a1,a2}则我们设立三个输入单元I0, I1, I2，每个单元默认为0，若A=a0，则I1置为1.\n\n   此外没有什么特定的规律或规则来指导隐含层的最佳单元数量，它的确定是一个不断试错的过程。\n\n3. 后传方法\n\n   后传方法不断地处理一个训练样本集。将处理结果与训练样本已知类别比较所获误差，修改权重，使网络输出与实际类别之间的均方差最小。权重的修改是后传的，即从前向后的。\n\n   其收敛性不能保证。\n\n   流程(伪代码)：\n\n   ```python\n   # 定义sum函数，以变量x对f(x)求和\n   def sum(f(x), x):\n       pass\n\n   # 初始化\n   init();\n\n   while !conditions:           # 条件不满足时\n       for X in samples:\n           for each layer:      # 每个隐含层和输出层\n               O[j] = 1 / (1 + exp( - I[j]))\n               I[j] = sum(w[i][j]O[i], i) + theta[j]\n           for each unit of output layer as j:    # 每个输出层单元j，计算向后传播误差\n               Err[j] = O[j] * (1 - O[j]) * (T[j] - O[j])\n           for each unit of hidden layer as j:    # 每个隐含层单元j，从最后一层到第一层隐含层\n               Err[j] = O[j] * (1 - O[j]) * sum(Err[k] * w[i][j][k], k)\n           for each w[i][j] in the network:       # network中的权重wij\n               delta_w[i][j] = (l) * Err[j] * O[i]     # (l)是学习速率，取值在0～1\n               w[i][j] += delta_w[i][j]\n           for each theta[j] in the network:      # network中的偏差thetaij\n               delta_theta[j] = (l) * Err[j]\n               v[i] = theta[j] + delta_theta[j]\n           \n   ```\n\n   看伪代码基本就能明白它的过程了。虽然伪代码可能写的比较迷。。。\n\n# 基于关联的分类方法\n\n略，后面会详细说。\n\n# 其他分类\n\n其他分类我也没有细看，纪录一下名字，以后有时间再看。\n\n1. k-最邻近方法\n\n   这个很简单，就是比较两个点的欧式距离。比较基本的分类方法，略。\n\n2. 基于示例推理\n\n3. 遗传算法\n\n4. 粗糙集方法\n\n5. 模糊集合方法\n\n# 预测方法\n\n1. 线形与多变量回归\n\n   线性代数、概率统计、数值计算方法里都学过。核心是利用最小二乘法。\n\n2. 非线性回归\n\n   例如多项式回归\n   $$\n   Y = \\alpha + \\beta_1 X+\\beta_2 X^2+\\beta_3 X^3\n   $$\n   为了将其转化为线性形式，令：\n   $$\n   X_1 = X;X_2 = X^2 ; X_3 = X^3\n   $$\n   接下来就用最小二乘法即可。\n\n3. 其他回归模型\n\n   对数回归、泊松回归等\n\n# 分类器准确性\n\n分层k次交叉验证方法普遍应用于对分类器预测准确性的评估方面。而bagging方法和boosting方法则通过学习和组合多个(单)分类器来帮助提高整个(数据训练样本所获)分类器的预测准确性。","slug":"datamining-class-pred","published":1,"updated":"2016-12-26T15:52:42.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckw4qufq1003bgwtlfgnpfpxn","content":"<h1 id=\"基本知识\"><a href=\"#基本知识\" class=\"headerlink\" title=\"基本知识\"></a>基本知识</h1><p>主要分为两个步骤：</p>\n<ol>\n<li><p>建立一个描述已知数据集类别或概念的模型。</p>\n<p>分类规则形式、决策树形式，或数学公式形式。</p>\n</li>\n<li><p>利用所获得的模型进行分类操作。</p>\n<p>首先要对模型分类准确率进行估计。</p>\n<p>若其准确率可以接受，则可以使用该模型进行分类。       </p>\n</li>\n</ol>\n<p>可以根据以下几条标准对各种分类方法进行比较:</p>\n<ol>\n<li>预测准确率，它描述(学习所获)模型能够正确预测未知对象类别或(类别)数值的能力。</li>\n<li>速度，它描述在构造和使用模型时的计算效率。 </li>\n<li>鲁棒性，它描述在数据带有噪声和有数据遗失情况下，(学习所获)模型仍能进行正确预测的能力。 </li>\n<li>可扩展性，它描述对处理大量数据并构造相应学习模型所需要的能力。</li>\n<li>易理解性，它描述学习所获模型表示的可理解程度。 在本章的后面各节，将要陆续介绍上述有关问题的实现方法。</li>\n</ol>\n<h1 id=\"基于决策树的分类\"><a href=\"#基于决策树的分类\" class=\"headerlink\" title=\"基于决策树的分类\"></a>基于决策树的分类</h1><ol>\n<li><p>决策树的生成算法</p>\n<ul>\n<li><p>基本决策树算法</p>\n<p>本质是一个贪心算法，自上而下分而治之。</p>\n</li>\n</ul>\n</li>\n<li><p>属性选择方法</p>\n<p>信息量<br>$$<br>I(s_1,…,s_m) = - \\sum_{i=1}^{m}{p_i \\log (p_i)}<br>$$<br>利用属性A划分当前样本集合所需要的信息可以计算：<br>$$<br>E(A) = \\sum_{j=1}^{v}{\\frac{s_{1j}+s_{2j}+…+s_{mj}}{s}I(s_1,…,s_m) }<br>$$<br>E(A)的值越小，说明其子集划分结果越纯，即越好。而对于一个子集Sj，它的信息为<br>$$<br>I(s_{1j},…,s_{mj}) = - \\sum_{i=1}^{m}{p_{ij} \\log (p_{ij})}<br>$$<br>这样利用属性A对当前分支结点进行相应样本集合划分所获得的信息增益是:<br>$$<br>Gain(A) =I(s_1,…,s_m)- E(A)<br>$$<br>也就是说Gain(A)被认为是利用属性A进行划分后，所获的的信息熵的减少量。</p>\n<p>决策树归纳算法计算每个属性的Gain，选择信息增益最大的属性，作为测试属性并由此产生相应的分支节点。</p>\n</li>\n<li><p>树枝的修剪</p>\n<ol>\n<li><p>事前修剪</p>\n<p>基本原理是设置一个阀值，当某一节点的的样本数少于阀值，则停止细分。难点在于设置一个合理的阀值。</p>\n</li>\n<li><p>事后修剪</p>\n<p>从一个充分生长的树中修建。</p>\n<p>可以使用基于代价成本的方法，也可以使用最短描述长度(MDL)。</p>\n<p>前者计算其分类错误率，若修剪枝导致分类错误率变高，则保留，否则剪枝；后者选择最简单的，无需独立的数据测试</p>\n</li>\n</ol>\n</li>\n<li><p>规则获取</p>\n<p>在已经获得了一个决策树的基础上，可以使用”if…else…”语句描述该决策树。</p>\n</li>\n<li><p>基本决策树方法改进及其扩展性</p>\n</li>\n</ol>\n<h1 id=\"贝叶斯分类方法\"><a href=\"#贝叶斯分类方法\" class=\"headerlink\" title=\"贝叶斯分类方法\"></a>贝叶斯分类方法</h1><p>贝叶斯分类器是一个统计分类器，基于贝叶斯定理。</p>\n<p>简单贝叶斯分类器的分类性能可以与决策树和神经网络相比。</p>\n<p>简单贝叶斯分类器假设一个指定类别中各属性的取值是互相独立的，它可以帮助减少计算量。</p>\n<ol>\n<li><p>贝叶斯定理</p>\n<p>设X为一个类别未知的数据样本，H为某个假设，则：<br>$$<br>P(H|X) = \\frac{P(X|H)P(H)}{P(X)}<br>$$</p>\n</li>\n<li><p>简单贝叶斯分类方法</p>\n<p>步骤说明如下：</p>\n<ol>\n<li><p>每一个数据都是有一个n维特征向量X={x1,…,xn}构成，分别描述其n个属性(A1,…,An)。</p>\n</li>\n<li><p>若有m个不同的类别(C1,…,Cm)，分类器将未知X归属到类别Ci，当仅当<br>$$<br>P(C_i|X) = \\max(P(C_j|X)|1\\le j\\le m)<br>$$<br>所以我们要找到最大的<br>$$<br>P(C_i|X) = \\frac{P(X|C_i)P(C_i)}{P(X)}<br>$$</p>\n</li>\n<li><p>要找到它，只需要P(X|Ci)P(Ci)取最大即可。由于各类别的事前概率是未知的，我们假设P(Ci)是互相相等的，这样，第二步中的式子取最大就转化为了寻找最大的P(X|Ci)</p>\n</li>\n<li><p>由于所含的属性比较多，直接计算P(X|Ci)的计算量还是很大的。由于简单贝叶斯分类器假设各属性独立，则：<br>$$<br>P(X|C_i) = \\prod{P(x_k|C_i)}<br>$$<br>可以根据训练数据样本估算P(xk|Ci)的值。具体如下：</p>\n<ul>\n<li><p>若Ak是符号量<br>$$<br>P(x_k|Ci)=\\frac{s_{ik}}{s_i}<br>$$<br>这里sik为训练样本中类别为Ci且属性Ak取vk的样本数。si为训练样本中类别为Ci的样本数。</p>\n</li>\n<li><p>若Ak是连续量<br>$$<br>P(x_k|Ci)=g(x_k,\\mu_{C_i},\\sigma_{C_i}) = \\frac{1}{\\sqrt{2\\pi}\\sigma_{C_i}}e^{-\\frac{(x-\\mu_{C_i})^2}{2\\sigma^2_{C_i}}}<br>$$<br>其中$g(x_k,\\mu_{C_i},\\sigma_{C_i})$ï为属性为Ak的高斯规范密度函数。</p>\n</li>\n</ul>\n</li>\n<li><p>为了预测X的分类，我们通过上述方法估计各个类别的正确率，将X归属到可能性最高的类别。</p>\n</li>\n</ol>\n</li>\n<li><p>贝叶斯信念网络</p>\n<p>简单贝叶斯假设属性相互独立，从而简化计算，而实际上属性相互依赖的情况比较常见，所以又出现了贝叶斯信念网络，用于描述这种相互关联的概率分布。</p>\n<p>贝叶斯信念网络提供了一个图形模型来描述其中的因果关系。信念网络包括两个部分。</p>\n<ul>\n<li><p>第一部分是有向无环图</p>\n<p>每一个节点代表随机变量。</p>\n<p>每一条弧代表一个概率依赖。</p>\n<p>给定父节点，每个变量有条件的独立于图中非子节点。</p>\n</li>\n<li><p>第二部分是包含所有变量的条件概率表(CPT)</p>\n<p>对于一个变量Z，CPT定义了P(Z|parent(Z))，由此可以得到一个表。</p>\n<p>例如，LunCancer的父节点是FamilyHistory和Smoker，</p>\n</li>\n</ul>\n</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>FH, S</th>\n<th>FH, ~S</th>\n<th>~FH, S</th>\n<th>~FH, ~S</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>LungCancer</td>\n<td>0.8</td>\n<td>0.5</td>\n<td>0.7</td>\n<td>0.1</td>\n</tr>\n<tr>\n<td>~LungCancer</td>\n<td>0.2</td>\n<td>0.5</td>\n<td>0.3</td>\n<td>0.9</td>\n</tr>\n</tbody></table>\n<p>数据对象的联合概率通过如下公式计算。<br>$$<br>P(z_1,…,z_n) = \\prod{P(z_i|parent(z_i))}<br>$$</p>\n<ol start=\"4\">\n<li><p>贝叶斯信念网络的学习</p>\n<p>学习算法步骤如下：</p>\n<ol>\n<li><p>计算下降梯度<br>$$<br>\\frac{\\partial\\ln{P_w(S)}}{\\partial w_{ijk}} = \\sum_{d=1}^{s}{\\frac{P(Y_i=y_{ij}, U_i=u_{ik} |X_d)}{w_{ijk}}}<br>$$<br>​<br>左边是计算训练集合S中每个样本Xd的概率。设这一概率为p。</p>\n<p>由Yi和Ui表示隐含变量，p可通过样本中可观察到的变量以及标准贝叶斯网络推理计算得到。</p>\n</li>\n<li><p>沿梯度方向前进一小步，权重更新计算如下<br>$$<br>w_{ijk} \\leftarrow w_{ijk} + (l)\\frac{\\partial\\ln{P_w(S)}}{\\partial w_{ijk}}<br>$$<br>l为学习速率代表学习步长。通常学习速率为较小的常数。</p>\n</li>\n<li><p>重新规格化权重</p>\n<p>保证wijk的取值在0～1，其和等于1.</p>\n</li>\n</ol>\n</li>\n</ol>\n<h1 id=\"神经网络分类方法\"><a href=\"#神经网络分类方法\" class=\"headerlink\" title=\"神经网络分类方法\"></a>神经网络分类方法</h1><ol>\n<li><p>多层反馈神经网络</p>\n<p>输入同时赋给第一层(输入层)单元，这些单元输出赋予权重，输出给第二层(隐藏层)。</p>\n<p>隐藏层的带权输出又作为输入再馈给另一隐层。</p>\n<p>最后的隐层结点带权输出馈给输出层单元，最终给出相应样本的预测输出。</p>\n<p>该网络是前馈的，即每一个反馈只能发送到前面的输出层或隐含层。</p>\n<p>它也是全连接的，即每一个层中单元均与前面一层的各单元相连接。</p>\n<p>只要中间隐层足够多的话，多层前馈网络中的线性阈值函数，可以充分逼近任何函数。</p>\n</li>\n<li><p>神经网络结构</p>\n<p>确定结构，即：</p>\n<ul>\n<li>输入层的单元数</li>\n<li>隐含层的个数(和层数)</li>\n<li>每个隐含层的单元数目</li>\n<li>输出层单元数目</li>\n</ul>\n<p>对输入值规格化，一般取值在0～1.</p>\n<p>离散数据可以为每一个取值增加一个节点进行编码。例如A={a0,a1,a2}则我们设立三个输入单元I0, I1, I2，每个单元默认为0，若A=a0，则I1置为1.</p>\n<p>此外没有什么特定的规律或规则来指导隐含层的最佳单元数量，它的确定是一个不断试错的过程。</p>\n</li>\n<li><p>后传方法</p>\n<p>后传方法不断地处理一个训练样本集。将处理结果与训练样本已知类别比较所获误差，修改权重，使网络输出与实际类别之间的均方差最小。权重的修改是后传的，即从前向后的。</p>\n<p>其收敛性不能保证。</p>\n<p>流程(伪代码)：</p>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 定义sum函数，以变量x对f(x)求和</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sum</span>(<span class=\"params\">f(<span class=\"params\">x</span>), x</span>):</span></span><br><span class=\"line\">    <span class=\"keyword\">pass</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 初始化</span></span><br><span class=\"line\">init();</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">while</span> !conditions:           <span class=\"comment\"># 条件不满足时</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> X <span class=\"keyword\">in</span> samples:</span><br><span class=\"line\">        <span class=\"keyword\">for</span> each layer:      <span class=\"comment\"># 每个隐含层和输出层</span></span><br><span class=\"line\">            O[j] = <span class=\"number\">1</span> / (<span class=\"number\">1</span> + exp( - I[j]))</span><br><span class=\"line\">            I[j] = <span class=\"built_in\">sum</span>(w[i][j]O[i], i) + theta[j]</span><br><span class=\"line\">        <span class=\"keyword\">for</span> each unit of output layer <span class=\"keyword\">as</span> j:    <span class=\"comment\"># 每个输出层单元j，计算向后传播误差</span></span><br><span class=\"line\">            Err[j] = O[j] * (<span class=\"number\">1</span> - O[j]) * (T[j] - O[j])</span><br><span class=\"line\">        <span class=\"keyword\">for</span> each unit of hidden layer <span class=\"keyword\">as</span> j:    <span class=\"comment\"># 每个隐含层单元j，从最后一层到第一层隐含层</span></span><br><span class=\"line\">            Err[j] = O[j] * (<span class=\"number\">1</span> - O[j]) * <span class=\"built_in\">sum</span>(Err[k] * w[i][j][k], k)</span><br><span class=\"line\">        <span class=\"keyword\">for</span> each w[i][j] <span class=\"keyword\">in</span> the network:       <span class=\"comment\"># network中的权重wij</span></span><br><span class=\"line\">            delta_w[i][j] = (l) * Err[j] * O[i]     <span class=\"comment\"># (l)是学习速率，取值在0～1</span></span><br><span class=\"line\">            w[i][j] += delta_w[i][j]</span><br><span class=\"line\">        <span class=\"keyword\">for</span> each theta[j] <span class=\"keyword\">in</span> the network:      <span class=\"comment\"># network中的偏差thetaij</span></span><br><span class=\"line\">            delta_theta[j] = (l) * Err[j]</span><br><span class=\"line\">            v[i] = theta[j] + delta_theta[j]</span><br><span class=\"line\">        </span><br></pre></td></tr></tbody></table></figure>\n\n<p>看伪代码基本就能明白它的过程了。虽然伪代码可能写的比较迷。。。</p>\n</li>\n</ol>\n<h1 id=\"基于关联的分类方法\"><a href=\"#基于关联的分类方法\" class=\"headerlink\" title=\"基于关联的分类方法\"></a>基于关联的分类方法</h1><p>略，后面会详细说。</p>\n<h1 id=\"其他分类\"><a href=\"#其他分类\" class=\"headerlink\" title=\"其他分类\"></a>其他分类</h1><p>其他分类我也没有细看，纪录一下名字，以后有时间再看。</p>\n<ol>\n<li><p>k-最邻近方法</p>\n<p>这个很简单，就是比较两个点的欧式距离。比较基本的分类方法，略。</p>\n</li>\n<li><p>基于示例推理</p>\n</li>\n<li><p>遗传算法</p>\n</li>\n<li><p>粗糙集方法</p>\n</li>\n<li><p>模糊集合方法</p>\n</li>\n</ol>\n<h1 id=\"预测方法\"><a href=\"#预测方法\" class=\"headerlink\" title=\"预测方法\"></a>预测方法</h1><ol>\n<li><p>线形与多变量回归</p>\n<p>线性代数、概率统计、数值计算方法里都学过。核心是利用最小二乘法。</p>\n</li>\n<li><p>非线性回归</p>\n<p>例如多项式回归<br>$$<br>Y = \\alpha + \\beta_1 X+\\beta_2 X^2+\\beta_3 X^3<br>$$<br>为了将其转化为线性形式，令：<br>$$<br>X_1 = X;X_2 = X^2 ; X_3 = X^3<br>$$<br>接下来就用最小二乘法即可。</p>\n</li>\n<li><p>其他回归模型</p>\n<p>对数回归、泊松回归等</p>\n</li>\n</ol>\n<h1 id=\"分类器准确性\"><a href=\"#分类器准确性\" class=\"headerlink\" title=\"分类器准确性\"></a>分类器准确性</h1><p>分层k次交叉验证方法普遍应用于对分类器预测准确性的评估方面。而bagging方法和boosting方法则通过学习和组合多个(单)分类器来帮助提高整个(数据训练样本所获)分类器的预测准确性。</p>\n","site":{"data":{"recommended_posts":{"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"}],"https://LorrinWWW.github.io/posts/complexity/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/hexo-with-latex/":[{"title":"区块链时代的最正确的打赏姿势——在Hexo博客配置Coinhive网页挖矿","permalink":"http://www.davidfnck.com/blockchain/mine-xmr-in-hexo-by-coinhive.html/"}],"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/":[{"title":"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-statistic/":[{"title":"贝叶斯估计 Bayes estimation","permalink":"https://LorrinWWW.github.io/posts/Bayes-estimation/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"}],"https://LorrinWWW.github.io/posts/[2018.2.5]Nested-LSTMs/":[{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"}],"https://LorrinWWW.github.io/posts/datamining-pretreatment/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Several models for knowledge graph representing and completing 几个知识图谱模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/"}],"https://LorrinWWW.github.io/posts/compression/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/Sobolev-space/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/proba-ch5/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch3/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/proba-ch1/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/ML-CNN/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/EDP-basic-models/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/machine-learning/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/datamining-class-pred/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction/":[],"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/":[{"title":"OS notes","permalink":"https://LorrinWWW.github.io/posts/OS-notes/"}],"https://LorrinWWW.github.io/posts/projet-enjeu-plugin-chrome-101/":[{"title":"Chrome油猴脚本","permalink":"http://gubangzhong.cn/2017/06/13/Chrome油猴脚本/"}],"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/":[{"title":"MongoDB, Docker and Python","permalink":"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/":[{"title":"Hands on Scrapy","permalink":"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/OS-notes/":[{"title":"learning OS and building LorriOS","permalink":"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/"}],"https://LorrinWWW.github.io/posts/Bayes-estimation/":[{"title":"Note of statistic","permalink":"https://LorrinWWW.github.io/posts/Note-of-statistic/"}],"https://LorrinWWW.github.io/posts/management-of-the-firm/":[],"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"}],"https://LorrinWWW.github.io/posts/datamining-qualitative-induction/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-finite-element-method/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/QuadTree/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/proba-ch4/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch2/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/":[{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/Note-of-NLP/":[{"title":"Generative Adversarial Network","permalink":"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/"},{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"}],"https://LorrinWWW.github.io/posts/Note-of-datamining/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"数据挖掘-分类与预测","permalink":"https://LorrinWWW.github.io/posts/datamining-class-pred/"},{"title":"数据挖掘-预处理","permalink":"https://LorrinWWW.github.io/posts/datamining-pretreatment/"}],"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/Note-of-probability/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/participe-present-et-gerondif/":[],"https://LorrinWWW.github.io/posts/Hilbert-space/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/proba-ch6/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}]}}},"excerpt":"","more":"<h1 id=\"基本知识\"><a href=\"#基本知识\" class=\"headerlink\" title=\"基本知识\"></a>基本知识</h1><p>主要分为两个步骤：</p>\n<ol>\n<li><p>建立一个描述已知数据集类别或概念的模型。</p>\n<p>分类规则形式、决策树形式，或数学公式形式。</p>\n</li>\n<li><p>利用所获得的模型进行分类操作。</p>\n<p>首先要对模型分类准确率进行估计。</p>\n<p>若其准确率可以接受，则可以使用该模型进行分类。       </p>\n</li>\n</ol>\n<p>可以根据以下几条标准对各种分类方法进行比较:</p>\n<ol>\n<li>预测准确率，它描述(学习所获)模型能够正确预测未知对象类别或(类别)数值的能力。</li>\n<li>速度，它描述在构造和使用模型时的计算效率。 </li>\n<li>鲁棒性，它描述在数据带有噪声和有数据遗失情况下，(学习所获)模型仍能进行正确预测的能力。 </li>\n<li>可扩展性，它描述对处理大量数据并构造相应学习模型所需要的能力。</li>\n<li>易理解性，它描述学习所获模型表示的可理解程度。 在本章的后面各节，将要陆续介绍上述有关问题的实现方法。</li>\n</ol>\n<h1 id=\"基于决策树的分类\"><a href=\"#基于决策树的分类\" class=\"headerlink\" title=\"基于决策树的分类\"></a>基于决策树的分类</h1><ol>\n<li><p>决策树的生成算法</p>\n<ul>\n<li><p>基本决策树算法</p>\n<p>本质是一个贪心算法，自上而下分而治之。</p>\n</li>\n</ul>\n</li>\n<li><p>属性选择方法</p>\n<p>信息量<br>$$<br>I(s_1,…,s_m) = - \\sum_{i=1}^{m}{p_i \\log (p_i)}<br>$$<br>利用属性A划分当前样本集合所需要的信息可以计算：<br>$$<br>E(A) = \\sum_{j=1}^{v}{\\frac{s_{1j}+s_{2j}+…+s_{mj}}{s}I(s_1,…,s_m) }<br>$$<br>E(A)的值越小，说明其子集划分结果越纯，即越好。而对于一个子集Sj，它的信息为<br>$$<br>I(s_{1j},…,s_{mj}) = - \\sum_{i=1}^{m}{p_{ij} \\log (p_{ij})}<br>$$<br>这样利用属性A对当前分支结点进行相应样本集合划分所获得的信息增益是:<br>$$<br>Gain(A) =I(s_1,…,s_m)- E(A)<br>$$<br>也就是说Gain(A)被认为是利用属性A进行划分后，所获的的信息熵的减少量。</p>\n<p>决策树归纳算法计算每个属性的Gain，选择信息增益最大的属性，作为测试属性并由此产生相应的分支节点。</p>\n</li>\n<li><p>树枝的修剪</p>\n<ol>\n<li><p>事前修剪</p>\n<p>基本原理是设置一个阀值，当某一节点的的样本数少于阀值，则停止细分。难点在于设置一个合理的阀值。</p>\n</li>\n<li><p>事后修剪</p>\n<p>从一个充分生长的树中修建。</p>\n<p>可以使用基于代价成本的方法，也可以使用最短描述长度(MDL)。</p>\n<p>前者计算其分类错误率，若修剪枝导致分类错误率变高，则保留，否则剪枝；后者选择最简单的，无需独立的数据测试</p>\n</li>\n</ol>\n</li>\n<li><p>规则获取</p>\n<p>在已经获得了一个决策树的基础上，可以使用”if…else…”语句描述该决策树。</p>\n</li>\n<li><p>基本决策树方法改进及其扩展性</p>\n</li>\n</ol>\n<h1 id=\"贝叶斯分类方法\"><a href=\"#贝叶斯分类方法\" class=\"headerlink\" title=\"贝叶斯分类方法\"></a>贝叶斯分类方法</h1><p>贝叶斯分类器是一个统计分类器，基于贝叶斯定理。</p>\n<p>简单贝叶斯分类器的分类性能可以与决策树和神经网络相比。</p>\n<p>简单贝叶斯分类器假设一个指定类别中各属性的取值是互相独立的，它可以帮助减少计算量。</p>\n<ol>\n<li><p>贝叶斯定理</p>\n<p>设X为一个类别未知的数据样本，H为某个假设，则：<br>$$<br>P(H|X) = \\frac{P(X|H)P(H)}{P(X)}<br>$$</p>\n</li>\n<li><p>简单贝叶斯分类方法</p>\n<p>步骤说明如下：</p>\n<ol>\n<li><p>每一个数据都是有一个n维特征向量X={x1,…,xn}构成，分别描述其n个属性(A1,…,An)。</p>\n</li>\n<li><p>若有m个不同的类别(C1,…,Cm)，分类器将未知X归属到类别Ci，当仅当<br>$$<br>P(C_i|X) = \\max(P(C_j|X)|1\\le j\\le m)<br>$$<br>所以我们要找到最大的<br>$$<br>P(C_i|X) = \\frac{P(X|C_i)P(C_i)}{P(X)}<br>$$</p>\n</li>\n<li><p>要找到它，只需要P(X|Ci)P(Ci)取最大即可。由于各类别的事前概率是未知的，我们假设P(Ci)是互相相等的，这样，第二步中的式子取最大就转化为了寻找最大的P(X|Ci)</p>\n</li>\n<li><p>由于所含的属性比较多，直接计算P(X|Ci)的计算量还是很大的。由于简单贝叶斯分类器假设各属性独立，则：<br>$$<br>P(X|C_i) = \\prod{P(x_k|C_i)}<br>$$<br>可以根据训练数据样本估算P(xk|Ci)的值。具体如下：</p>\n<ul>\n<li><p>若Ak是符号量<br>$$<br>P(x_k|Ci)=\\frac{s_{ik}}{s_i}<br>$$<br>这里sik为训练样本中类别为Ci且属性Ak取vk的样本数。si为训练样本中类别为Ci的样本数。</p>\n</li>\n<li><p>若Ak是连续量<br>$$<br>P(x_k|Ci)=g(x_k,\\mu_{C_i},\\sigma_{C_i}) = \\frac{1}{\\sqrt{2\\pi}\\sigma_{C_i}}e^{-\\frac{(x-\\mu_{C_i})^2}{2\\sigma^2_{C_i}}}<br>$$<br>其中$g(x_k,\\mu_{C_i},\\sigma_{C_i})$ï为属性为Ak的高斯规范密度函数。</p>\n</li>\n</ul>\n</li>\n<li><p>为了预测X的分类，我们通过上述方法估计各个类别的正确率，将X归属到可能性最高的类别。</p>\n</li>\n</ol>\n</li>\n<li><p>贝叶斯信念网络</p>\n<p>简单贝叶斯假设属性相互独立，从而简化计算，而实际上属性相互依赖的情况比较常见，所以又出现了贝叶斯信念网络，用于描述这种相互关联的概率分布。</p>\n<p>贝叶斯信念网络提供了一个图形模型来描述其中的因果关系。信念网络包括两个部分。</p>\n<ul>\n<li><p>第一部分是有向无环图</p>\n<p>每一个节点代表随机变量。</p>\n<p>每一条弧代表一个概率依赖。</p>\n<p>给定父节点，每个变量有条件的独立于图中非子节点。</p>\n</li>\n<li><p>第二部分是包含所有变量的条件概率表(CPT)</p>\n<p>对于一个变量Z，CPT定义了P(Z|parent(Z))，由此可以得到一个表。</p>\n<p>例如，LunCancer的父节点是FamilyHistory和Smoker，</p>\n</li>\n</ul>\n</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>FH, S</th>\n<th>FH, ~S</th>\n<th>~FH, S</th>\n<th>~FH, ~S</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>LungCancer</td>\n<td>0.8</td>\n<td>0.5</td>\n<td>0.7</td>\n<td>0.1</td>\n</tr>\n<tr>\n<td>~LungCancer</td>\n<td>0.2</td>\n<td>0.5</td>\n<td>0.3</td>\n<td>0.9</td>\n</tr>\n</tbody></table>\n<p>数据对象的联合概率通过如下公式计算。<br>$$<br>P(z_1,…,z_n) = \\prod{P(z_i|parent(z_i))}<br>$$</p>\n<ol start=\"4\">\n<li><p>贝叶斯信念网络的学习</p>\n<p>学习算法步骤如下：</p>\n<ol>\n<li><p>计算下降梯度<br>$$<br>\\frac{\\partial\\ln{P_w(S)}}{\\partial w_{ijk}} = \\sum_{d=1}^{s}{\\frac{P(Y_i=y_{ij}, U_i=u_{ik} |X_d)}{w_{ijk}}}<br>$$<br>​<br>左边是计算训练集合S中每个样本Xd的概率。设这一概率为p。</p>\n<p>由Yi和Ui表示隐含变量，p可通过样本中可观察到的变量以及标准贝叶斯网络推理计算得到。</p>\n</li>\n<li><p>沿梯度方向前进一小步，权重更新计算如下<br>$$<br>w_{ijk} \\leftarrow w_{ijk} + (l)\\frac{\\partial\\ln{P_w(S)}}{\\partial w_{ijk}}<br>$$<br>l为学习速率代表学习步长。通常学习速率为较小的常数。</p>\n</li>\n<li><p>重新规格化权重</p>\n<p>保证wijk的取值在0～1，其和等于1.</p>\n</li>\n</ol>\n</li>\n</ol>\n<h1 id=\"神经网络分类方法\"><a href=\"#神经网络分类方法\" class=\"headerlink\" title=\"神经网络分类方法\"></a>神经网络分类方法</h1><ol>\n<li><p>多层反馈神经网络</p>\n<p>输入同时赋给第一层(输入层)单元，这些单元输出赋予权重，输出给第二层(隐藏层)。</p>\n<p>隐藏层的带权输出又作为输入再馈给另一隐层。</p>\n<p>最后的隐层结点带权输出馈给输出层单元，最终给出相应样本的预测输出。</p>\n<p>该网络是前馈的，即每一个反馈只能发送到前面的输出层或隐含层。</p>\n<p>它也是全连接的，即每一个层中单元均与前面一层的各单元相连接。</p>\n<p>只要中间隐层足够多的话，多层前馈网络中的线性阈值函数，可以充分逼近任何函数。</p>\n</li>\n<li><p>神经网络结构</p>\n<p>确定结构，即：</p>\n<ul>\n<li>输入层的单元数</li>\n<li>隐含层的个数(和层数)</li>\n<li>每个隐含层的单元数目</li>\n<li>输出层单元数目</li>\n</ul>\n<p>对输入值规格化，一般取值在0～1.</p>\n<p>离散数据可以为每一个取值增加一个节点进行编码。例如A={a0,a1,a2}则我们设立三个输入单元I0, I1, I2，每个单元默认为0，若A=a0，则I1置为1.</p>\n<p>此外没有什么特定的规律或规则来指导隐含层的最佳单元数量，它的确定是一个不断试错的过程。</p>\n</li>\n<li><p>后传方法</p>\n<p>后传方法不断地处理一个训练样本集。将处理结果与训练样本已知类别比较所获误差，修改权重，使网络输出与实际类别之间的均方差最小。权重的修改是后传的，即从前向后的。</p>\n<p>其收敛性不能保证。</p>\n<p>流程(伪代码)：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 定义sum函数，以变量x对f(x)求和</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sum</span>(<span class=\"params\">f(<span class=\"params\">x</span>), x</span>):</span></span><br><span class=\"line\">    <span class=\"keyword\">pass</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 初始化</span></span><br><span class=\"line\">init();</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">while</span> !conditions:           <span class=\"comment\"># 条件不满足时</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> X <span class=\"keyword\">in</span> samples:</span><br><span class=\"line\">        <span class=\"keyword\">for</span> each layer:      <span class=\"comment\"># 每个隐含层和输出层</span></span><br><span class=\"line\">            O[j] = <span class=\"number\">1</span> / (<span class=\"number\">1</span> + exp( - I[j]))</span><br><span class=\"line\">            I[j] = <span class=\"built_in\">sum</span>(w[i][j]O[i], i) + theta[j]</span><br><span class=\"line\">        <span class=\"keyword\">for</span> each unit of output layer <span class=\"keyword\">as</span> j:    <span class=\"comment\"># 每个输出层单元j，计算向后传播误差</span></span><br><span class=\"line\">            Err[j] = O[j] * (<span class=\"number\">1</span> - O[j]) * (T[j] - O[j])</span><br><span class=\"line\">        <span class=\"keyword\">for</span> each unit of hidden layer <span class=\"keyword\">as</span> j:    <span class=\"comment\"># 每个隐含层单元j，从最后一层到第一层隐含层</span></span><br><span class=\"line\">            Err[j] = O[j] * (<span class=\"number\">1</span> - O[j]) * <span class=\"built_in\">sum</span>(Err[k] * w[i][j][k], k)</span><br><span class=\"line\">        <span class=\"keyword\">for</span> each w[i][j] <span class=\"keyword\">in</span> the network:       <span class=\"comment\"># network中的权重wij</span></span><br><span class=\"line\">            delta_w[i][j] = (l) * Err[j] * O[i]     <span class=\"comment\"># (l)是学习速率，取值在0～1</span></span><br><span class=\"line\">            w[i][j] += delta_w[i][j]</span><br><span class=\"line\">        <span class=\"keyword\">for</span> each theta[j] <span class=\"keyword\">in</span> the network:      <span class=\"comment\"># network中的偏差thetaij</span></span><br><span class=\"line\">            delta_theta[j] = (l) * Err[j]</span><br><span class=\"line\">            v[i] = theta[j] + delta_theta[j]</span><br><span class=\"line\">        </span><br></pre></td></tr></table></figure>\n\n<p>看伪代码基本就能明白它的过程了。虽然伪代码可能写的比较迷。。。</p>\n</li>\n</ol>\n<h1 id=\"基于关联的分类方法\"><a href=\"#基于关联的分类方法\" class=\"headerlink\" title=\"基于关联的分类方法\"></a>基于关联的分类方法</h1><p>略，后面会详细说。</p>\n<h1 id=\"其他分类\"><a href=\"#其他分类\" class=\"headerlink\" title=\"其他分类\"></a>其他分类</h1><p>其他分类我也没有细看，纪录一下名字，以后有时间再看。</p>\n<ol>\n<li><p>k-最邻近方法</p>\n<p>这个很简单，就是比较两个点的欧式距离。比较基本的分类方法，略。</p>\n</li>\n<li><p>基于示例推理</p>\n</li>\n<li><p>遗传算法</p>\n</li>\n<li><p>粗糙集方法</p>\n</li>\n<li><p>模糊集合方法</p>\n</li>\n</ol>\n<h1 id=\"预测方法\"><a href=\"#预测方法\" class=\"headerlink\" title=\"预测方法\"></a>预测方法</h1><ol>\n<li><p>线形与多变量回归</p>\n<p>线性代数、概率统计、数值计算方法里都学过。核心是利用最小二乘法。</p>\n</li>\n<li><p>非线性回归</p>\n<p>例如多项式回归<br>$$<br>Y = \\alpha + \\beta_1 X+\\beta_2 X^2+\\beta_3 X^3<br>$$<br>为了将其转化为线性形式，令：<br>$$<br>X_1 = X;X_2 = X^2 ; X_3 = X^3<br>$$<br>接下来就用最小二乘法即可。</p>\n</li>\n<li><p>其他回归模型</p>\n<p>对数回归、泊松回归等</p>\n</li>\n</ol>\n<h1 id=\"分类器准确性\"><a href=\"#分类器准确性\" class=\"headerlink\" title=\"分类器准确性\"></a>分类器准确性</h1><p>分层k次交叉验证方法普遍应用于对分类器预测准确性的评估方面。而bagging方法和boosting方法则通过学习和组合多个(单)分类器来帮助提高整个(数据训练样本所获)分类器的预测准确性。</p>\n"},{"title":"数据挖掘-预处理","date":"2016-12-06T14:05:57.000Z","_content":"\n由于数据的快速膨胀，我们获得的数据往往带有大量的噪声，所以我们需要对其进行一定的预处理。\n\n主要包括数据清洗、数据集成、 数据转换和数据消减。\n\n- 数据清洗\n\n  填补遗漏的数据值、平滑有噪声数据、识别或除去异常值，以及解决不一致问题。\n\n- 数据集成\n\n  将来自多个数据源的数据合并到一起。\n\n- 数据转换\n\n  主要是对数据进行规格化操作。\n\n- 数据消减\n\n  缩小所挖掘数据的规模，但却不会影响(或基本不影响)最终的挖掘结果。\n\n  - 数据聚合\n  - 消减维数\n  - 数据压缩\n  - 数据块消减\n\n# 数据清洗\n\n1. 处理空数据\n   - 忽略该数据\n   - 手工填补\n   - 利用缺省填补\n   - 利用均值填补\n   - 利用同类别均值\n   - 利用最可能的值\n     - 回归分析\n     - 贝叶斯计算公式\n\n   ​最后一种是最常用的。\n\n2. 噪声处理\n\n   - Bin方法 — 排序，分组，平滑处理（分组取均值、按边界等）\n   - 聚类方法\n   - 人机结合检查方法\n   - 回归方法 — 借助回归曲线\n\n\n# 数据的集成与转换\n\n1. 数据的集成处理\n\n   几个问题：\n\n   - 模式集成，例如：\"custom_id\", \"cum_num\" 是不是同一个模式？\n\n   - 冗余问题，若一个属性能从其他属性推算出来，那么它是冗余的。\n\n     我们可以通过相关系数的推算来确定：\n     $$\n     r_{A,B} = \\frac{\\sum{A-\\bar{A}}}{(n-1)\\sigma_A \\sigma_B}\n     $$\n\n   - 数据值冲突检测与消除，比如单位不同，语意偏差。\n\n2. 数据的转化处理\n\n   1. 平滑处理，bin、聚类、回归\n\n   2. 合计处理，对数据进行总结合计操作。\n\n   3. 数据泛化处理，例如：年龄映射到老年、中老年、青年等。\n\n   4. 规格化，例如：将成绩(可能总分是10分、20分或100分)折算成4分制绩点。\n\n      - 最大最小规格化方法 — 一种线性规格化，绩点的处理属于这种\n\n      - 零均值规格化方法\n        $$\n        v' = \\frac{v - \\bar{v}}{\\sigma}\n        $$\n\n      - 十基数变换规格化方法\n        $$\n        v' = \\frac{v}{10^j}\n        $$\n\n   5. 属性构造，根据已有的属性构造新的属性，例如根据高宽生成面积。\n\n# 数据消减\n\n数据消减技术正是用于帮助从原有庞大数据集中获得一个精简的数据集合，并使这一精简数据集保持原有数据集的完整性，这样在精简数据集上进行数据挖掘显然效率 更高，并且挖掘出来的结果与使用原有数据集所获得结果基本相同。\n\n1. 数据立方合计\n\n   以三个轴，生成一个数据立方。\n\n   每一层次的数据立方都是对其低一层数据的进一步抽象，因此它是一种有效的数据消减。\n\n2. 维数消减\n\n3. 数据压缩\n\n   - 小波分析\n   - 主要素分析\n\n4. 数据块消减\n\n   - 回归与线性对数模型\n   - 直方图\n   - 聚类\n   - 采样\n\n5. 离散化与概念层次生成\n\n   - bin方法\n   - 直方图\n   - 聚类\n   - 基于熵的离散化\n   - 自然划分分段法\n\n# 自动生成概念层次树\n\n对于数值属性，可以利用划分规则、直方图分析和聚类分析方法对数据进行分段并构造相应的概念层次树；而对于类别属性，则可以利用概念层次树所涉及属性的不同值个数，构造相应的概念层次树。 ","source":"_posts/datamining-pretreatment.md","raw":"---\ntitle: 数据挖掘-预处理\ndate: 2016-12-06 22:05:57\ncategories: programming\ntags: [datamining, programming]\n---\n\n由于数据的快速膨胀，我们获得的数据往往带有大量的噪声，所以我们需要对其进行一定的预处理。\n\n主要包括数据清洗、数据集成、 数据转换和数据消减。\n\n- 数据清洗\n\n  填补遗漏的数据值、平滑有噪声数据、识别或除去异常值，以及解决不一致问题。\n\n- 数据集成\n\n  将来自多个数据源的数据合并到一起。\n\n- 数据转换\n\n  主要是对数据进行规格化操作。\n\n- 数据消减\n\n  缩小所挖掘数据的规模，但却不会影响(或基本不影响)最终的挖掘结果。\n\n  - 数据聚合\n  - 消减维数\n  - 数据压缩\n  - 数据块消减\n\n# 数据清洗\n\n1. 处理空数据\n   - 忽略该数据\n   - 手工填补\n   - 利用缺省填补\n   - 利用均值填补\n   - 利用同类别均值\n   - 利用最可能的值\n     - 回归分析\n     - 贝叶斯计算公式\n\n   ​最后一种是最常用的。\n\n2. 噪声处理\n\n   - Bin方法 — 排序，分组，平滑处理（分组取均值、按边界等）\n   - 聚类方法\n   - 人机结合检查方法\n   - 回归方法 — 借助回归曲线\n\n\n# 数据的集成与转换\n\n1. 数据的集成处理\n\n   几个问题：\n\n   - 模式集成，例如：\"custom_id\", \"cum_num\" 是不是同一个模式？\n\n   - 冗余问题，若一个属性能从其他属性推算出来，那么它是冗余的。\n\n     我们可以通过相关系数的推算来确定：\n     $$\n     r_{A,B} = \\frac{\\sum{A-\\bar{A}}}{(n-1)\\sigma_A \\sigma_B}\n     $$\n\n   - 数据值冲突检测与消除，比如单位不同，语意偏差。\n\n2. 数据的转化处理\n\n   1. 平滑处理，bin、聚类、回归\n\n   2. 合计处理，对数据进行总结合计操作。\n\n   3. 数据泛化处理，例如：年龄映射到老年、中老年、青年等。\n\n   4. 规格化，例如：将成绩(可能总分是10分、20分或100分)折算成4分制绩点。\n\n      - 最大最小规格化方法 — 一种线性规格化，绩点的处理属于这种\n\n      - 零均值规格化方法\n        $$\n        v' = \\frac{v - \\bar{v}}{\\sigma}\n        $$\n\n      - 十基数变换规格化方法\n        $$\n        v' = \\frac{v}{10^j}\n        $$\n\n   5. 属性构造，根据已有的属性构造新的属性，例如根据高宽生成面积。\n\n# 数据消减\n\n数据消减技术正是用于帮助从原有庞大数据集中获得一个精简的数据集合，并使这一精简数据集保持原有数据集的完整性，这样在精简数据集上进行数据挖掘显然效率 更高，并且挖掘出来的结果与使用原有数据集所获得结果基本相同。\n\n1. 数据立方合计\n\n   以三个轴，生成一个数据立方。\n\n   每一层次的数据立方都是对其低一层数据的进一步抽象，因此它是一种有效的数据消减。\n\n2. 维数消减\n\n3. 数据压缩\n\n   - 小波分析\n   - 主要素分析\n\n4. 数据块消减\n\n   - 回归与线性对数模型\n   - 直方图\n   - 聚类\n   - 采样\n\n5. 离散化与概念层次生成\n\n   - bin方法\n   - 直方图\n   - 聚类\n   - 基于熵的离散化\n   - 自然划分分段法\n\n# 自动生成概念层次树\n\n对于数值属性，可以利用划分规则、直方图分析和聚类分析方法对数据进行分段并构造相应的概念层次树；而对于类别属性，则可以利用概念层次树所涉及属性的不同值个数，构造相应的概念层次树。 ","slug":"datamining-pretreatment","published":1,"updated":"2016-12-08T20:29:26.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckw4qufq3003ggwtl84r119e1","content":"<p>由于数据的快速膨胀，我们获得的数据往往带有大量的噪声，所以我们需要对其进行一定的预处理。</p>\n<p>主要包括数据清洗、数据集成、 数据转换和数据消减。</p>\n<ul>\n<li><p>数据清洗</p>\n<p>填补遗漏的数据值、平滑有噪声数据、识别或除去异常值，以及解决不一致问题。</p>\n</li>\n<li><p>数据集成</p>\n<p>将来自多个数据源的数据合并到一起。</p>\n</li>\n<li><p>数据转换</p>\n<p>主要是对数据进行规格化操作。</p>\n</li>\n<li><p>数据消减</p>\n<p>缩小所挖掘数据的规模，但却不会影响(或基本不影响)最终的挖掘结果。</p>\n<ul>\n<li>数据聚合</li>\n<li>消减维数</li>\n<li>数据压缩</li>\n<li>数据块消减</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"数据清洗\"><a href=\"#数据清洗\" class=\"headerlink\" title=\"数据清洗\"></a>数据清洗</h1><ol>\n<li><p>处理空数据</p>\n<ul>\n<li>忽略该数据</li>\n<li>手工填补</li>\n<li>利用缺省填补</li>\n<li>利用均值填补</li>\n<li>利用同类别均值</li>\n<li>利用最可能的值<ul>\n<li>回归分析</li>\n<li>贝叶斯计算公式</li>\n</ul>\n</li>\n</ul>\n<p>​最后一种是最常用的。</p>\n</li>\n<li><p>噪声处理</p>\n<ul>\n<li>Bin方法 — 排序，分组，平滑处理（分组取均值、按边界等）</li>\n<li>聚类方法</li>\n<li>人机结合检查方法</li>\n<li>回归方法 — 借助回归曲线</li>\n</ul>\n</li>\n</ol>\n<h1 id=\"数据的集成与转换\"><a href=\"#数据的集成与转换\" class=\"headerlink\" title=\"数据的集成与转换\"></a>数据的集成与转换</h1><ol>\n<li><p>数据的集成处理</p>\n<p>几个问题：</p>\n<ul>\n<li><p>模式集成，例如：”custom_id”, “cum_num” 是不是同一个模式？</p>\n</li>\n<li><p>冗余问题，若一个属性能从其他属性推算出来，那么它是冗余的。</p>\n<p>我们可以通过相关系数的推算来确定：<br>$$<br>r_{A,B} = \\frac{\\sum{A-\\bar{A}}}{(n-1)\\sigma_A \\sigma_B}<br>$$</p>\n</li>\n<li><p>数据值冲突检测与消除，比如单位不同，语意偏差。</p>\n</li>\n</ul>\n</li>\n<li><p>数据的转化处理</p>\n<ol>\n<li><p>平滑处理，bin、聚类、回归</p>\n</li>\n<li><p>合计处理，对数据进行总结合计操作。</p>\n</li>\n<li><p>数据泛化处理，例如：年龄映射到老年、中老年、青年等。</p>\n</li>\n<li><p>规格化，例如：将成绩(可能总分是10分、20分或100分)折算成4分制绩点。</p>\n<ul>\n<li><p>最大最小规格化方法 — 一种线性规格化，绩点的处理属于这种</p>\n</li>\n<li><p>零均值规格化方法<br>$$<br>v’ = \\frac{v - \\bar{v}}{\\sigma}<br>$$</p>\n</li>\n<li><p>十基数变换规格化方法<br>$$<br>v’ = \\frac{v}{10^j}<br>$$</p>\n</li>\n</ul>\n</li>\n<li><p>属性构造，根据已有的属性构造新的属性，例如根据高宽生成面积。</p>\n</li>\n</ol>\n</li>\n</ol>\n<h1 id=\"数据消减\"><a href=\"#数据消减\" class=\"headerlink\" title=\"数据消减\"></a>数据消减</h1><p>数据消减技术正是用于帮助从原有庞大数据集中获得一个精简的数据集合，并使这一精简数据集保持原有数据集的完整性，这样在精简数据集上进行数据挖掘显然效率 更高，并且挖掘出来的结果与使用原有数据集所获得结果基本相同。</p>\n<ol>\n<li><p>数据立方合计</p>\n<p>以三个轴，生成一个数据立方。</p>\n<p>每一层次的数据立方都是对其低一层数据的进一步抽象，因此它是一种有效的数据消减。</p>\n</li>\n<li><p>维数消减</p>\n</li>\n<li><p>数据压缩</p>\n<ul>\n<li>小波分析</li>\n<li>主要素分析</li>\n</ul>\n</li>\n<li><p>数据块消减</p>\n<ul>\n<li>回归与线性对数模型</li>\n<li>直方图</li>\n<li>聚类</li>\n<li>采样</li>\n</ul>\n</li>\n<li><p>离散化与概念层次生成</p>\n<ul>\n<li>bin方法</li>\n<li>直方图</li>\n<li>聚类</li>\n<li>基于熵的离散化</li>\n<li>自然划分分段法</li>\n</ul>\n</li>\n</ol>\n<h1 id=\"自动生成概念层次树\"><a href=\"#自动生成概念层次树\" class=\"headerlink\" title=\"自动生成概念层次树\"></a>自动生成概念层次树</h1><p>对于数值属性，可以利用划分规则、直方图分析和聚类分析方法对数据进行分段并构造相应的概念层次树；而对于类别属性，则可以利用概念层次树所涉及属性的不同值个数，构造相应的概念层次树。 </p>\n","site":{"data":{"recommended_posts":{"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"}],"https://LorrinWWW.github.io/posts/complexity/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/hexo-with-latex/":[{"title":"区块链时代的最正确的打赏姿势——在Hexo博客配置Coinhive网页挖矿","permalink":"http://www.davidfnck.com/blockchain/mine-xmr-in-hexo-by-coinhive.html/"}],"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/":[{"title":"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-statistic/":[{"title":"贝叶斯估计 Bayes estimation","permalink":"https://LorrinWWW.github.io/posts/Bayes-estimation/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"}],"https://LorrinWWW.github.io/posts/[2018.2.5]Nested-LSTMs/":[{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"}],"https://LorrinWWW.github.io/posts/datamining-pretreatment/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Several models for knowledge graph representing and completing 几个知识图谱模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/"}],"https://LorrinWWW.github.io/posts/compression/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/Sobolev-space/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/proba-ch5/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch3/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/proba-ch1/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/ML-CNN/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/EDP-basic-models/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/machine-learning/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/datamining-class-pred/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction/":[],"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/":[{"title":"OS notes","permalink":"https://LorrinWWW.github.io/posts/OS-notes/"}],"https://LorrinWWW.github.io/posts/projet-enjeu-plugin-chrome-101/":[{"title":"Chrome油猴脚本","permalink":"http://gubangzhong.cn/2017/06/13/Chrome油猴脚本/"}],"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/":[{"title":"MongoDB, Docker and Python","permalink":"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/":[{"title":"Hands on Scrapy","permalink":"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/OS-notes/":[{"title":"learning OS and building LorriOS","permalink":"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/"}],"https://LorrinWWW.github.io/posts/Bayes-estimation/":[{"title":"Note of statistic","permalink":"https://LorrinWWW.github.io/posts/Note-of-statistic/"}],"https://LorrinWWW.github.io/posts/management-of-the-firm/":[],"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"}],"https://LorrinWWW.github.io/posts/datamining-qualitative-induction/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-finite-element-method/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/QuadTree/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/proba-ch4/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch2/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/":[{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/Note-of-NLP/":[{"title":"Generative Adversarial Network","permalink":"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/"},{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"}],"https://LorrinWWW.github.io/posts/Note-of-datamining/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"数据挖掘-分类与预测","permalink":"https://LorrinWWW.github.io/posts/datamining-class-pred/"},{"title":"数据挖掘-预处理","permalink":"https://LorrinWWW.github.io/posts/datamining-pretreatment/"}],"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/Note-of-probability/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/participe-present-et-gerondif/":[],"https://LorrinWWW.github.io/posts/Hilbert-space/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/proba-ch6/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}]}}},"excerpt":"","more":"<p>由于数据的快速膨胀，我们获得的数据往往带有大量的噪声，所以我们需要对其进行一定的预处理。</p>\n<p>主要包括数据清洗、数据集成、 数据转换和数据消减。</p>\n<ul>\n<li><p>数据清洗</p>\n<p>填补遗漏的数据值、平滑有噪声数据、识别或除去异常值，以及解决不一致问题。</p>\n</li>\n<li><p>数据集成</p>\n<p>将来自多个数据源的数据合并到一起。</p>\n</li>\n<li><p>数据转换</p>\n<p>主要是对数据进行规格化操作。</p>\n</li>\n<li><p>数据消减</p>\n<p>缩小所挖掘数据的规模，但却不会影响(或基本不影响)最终的挖掘结果。</p>\n<ul>\n<li>数据聚合</li>\n<li>消减维数</li>\n<li>数据压缩</li>\n<li>数据块消减</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"数据清洗\"><a href=\"#数据清洗\" class=\"headerlink\" title=\"数据清洗\"></a>数据清洗</h1><ol>\n<li><p>处理空数据</p>\n<ul>\n<li>忽略该数据</li>\n<li>手工填补</li>\n<li>利用缺省填补</li>\n<li>利用均值填补</li>\n<li>利用同类别均值</li>\n<li>利用最可能的值<ul>\n<li>回归分析</li>\n<li>贝叶斯计算公式</li>\n</ul>\n</li>\n</ul>\n<p>​最后一种是最常用的。</p>\n</li>\n<li><p>噪声处理</p>\n<ul>\n<li>Bin方法 — 排序，分组，平滑处理（分组取均值、按边界等）</li>\n<li>聚类方法</li>\n<li>人机结合检查方法</li>\n<li>回归方法 — 借助回归曲线</li>\n</ul>\n</li>\n</ol>\n<h1 id=\"数据的集成与转换\"><a href=\"#数据的集成与转换\" class=\"headerlink\" title=\"数据的集成与转换\"></a>数据的集成与转换</h1><ol>\n<li><p>数据的集成处理</p>\n<p>几个问题：</p>\n<ul>\n<li><p>模式集成，例如：”custom_id”, “cum_num” 是不是同一个模式？</p>\n</li>\n<li><p>冗余问题，若一个属性能从其他属性推算出来，那么它是冗余的。</p>\n<p>我们可以通过相关系数的推算来确定：<br>$$<br>r_{A,B} = \\frac{\\sum{A-\\bar{A}}}{(n-1)\\sigma_A \\sigma_B}<br>$$</p>\n</li>\n<li><p>数据值冲突检测与消除，比如单位不同，语意偏差。</p>\n</li>\n</ul>\n</li>\n<li><p>数据的转化处理</p>\n<ol>\n<li><p>平滑处理，bin、聚类、回归</p>\n</li>\n<li><p>合计处理，对数据进行总结合计操作。</p>\n</li>\n<li><p>数据泛化处理，例如：年龄映射到老年、中老年、青年等。</p>\n</li>\n<li><p>规格化，例如：将成绩(可能总分是10分、20分或100分)折算成4分制绩点。</p>\n<ul>\n<li><p>最大最小规格化方法 — 一种线性规格化，绩点的处理属于这种</p>\n</li>\n<li><p>零均值规格化方法<br>$$<br>v’ = \\frac{v - \\bar{v}}{\\sigma}<br>$$</p>\n</li>\n<li><p>十基数变换规格化方法<br>$$<br>v’ = \\frac{v}{10^j}<br>$$</p>\n</li>\n</ul>\n</li>\n<li><p>属性构造，根据已有的属性构造新的属性，例如根据高宽生成面积。</p>\n</li>\n</ol>\n</li>\n</ol>\n<h1 id=\"数据消减\"><a href=\"#数据消减\" class=\"headerlink\" title=\"数据消减\"></a>数据消减</h1><p>数据消减技术正是用于帮助从原有庞大数据集中获得一个精简的数据集合，并使这一精简数据集保持原有数据集的完整性，这样在精简数据集上进行数据挖掘显然效率 更高，并且挖掘出来的结果与使用原有数据集所获得结果基本相同。</p>\n<ol>\n<li><p>数据立方合计</p>\n<p>以三个轴，生成一个数据立方。</p>\n<p>每一层次的数据立方都是对其低一层数据的进一步抽象，因此它是一种有效的数据消减。</p>\n</li>\n<li><p>维数消减</p>\n</li>\n<li><p>数据压缩</p>\n<ul>\n<li>小波分析</li>\n<li>主要素分析</li>\n</ul>\n</li>\n<li><p>数据块消减</p>\n<ul>\n<li>回归与线性对数模型</li>\n<li>直方图</li>\n<li>聚类</li>\n<li>采样</li>\n</ul>\n</li>\n<li><p>离散化与概念层次生成</p>\n<ul>\n<li>bin方法</li>\n<li>直方图</li>\n<li>聚类</li>\n<li>基于熵的离散化</li>\n<li>自然划分分段法</li>\n</ul>\n</li>\n</ol>\n<h1 id=\"自动生成概念层次树\"><a href=\"#自动生成概念层次树\" class=\"headerlink\" title=\"自动生成概念层次树\"></a>自动生成概念层次树</h1><p>对于数值属性，可以利用划分规则、直方图分析和聚类分析方法对数据进行分段并构造相应的概念层次树；而对于类别属性，则可以利用概念层次树所涉及属性的不同值个数，构造相应的概念层次树。 </p>\n"},{"title":"数据挖掘-定性归纳","date":"2016-12-08T13:31:50.000Z","_content":"\n# 数据泛化和概念对比\n\n1.    数据立方方法\n\n      利用数据立方方法进行数据泛化，被分析的数据存放在一个多维数据库(数据立方)中。\n\n      数据立方的维是通过一系列能够形成层次的属性或网格，例如:日期可以包含属性天、周、月、季和年，这些属性构成了维的网格。一个数据立方中存放着预先对部分或所有维(属性)的合计计算结果。 \n\n      roll up: 数据泛化\n\n      drill down: 数据细化\n\n2.    基于属性的归纳方法\n\n      它是一种在线数据分析技术方法。\n\n      基于属性归纳的基本思想就是，首先利用关系数据库查询来收集与任务相关的数据，并通过对任务相关数据集中各属性不同值个数的检查，完成数据泛化操作。数据泛化操作是通过属性消减或属性泛化(又称为概念层次提升)操作来完成的。通过合并(泛化后)相同行并累计它们相应的个数。这就自然减少了泛化后的数据集大小。所获(泛化后)结果以图表和规则等多种不同形式提供给用户。\n\n      涉及的操作有两种：\n\n      1. 属性消减\n\n         例如，有四个属性：街道、城市、省份，国家，那么街道的更高层次是由后面三个属性来表示的，此时取消街道属性相当于泛化操作。\n\n      2. 属性泛化\n\n         它是基于以下规则进行:若一个属性(在初始数据集中) 有许多不同数值，且该属性存在一组泛化操作，则可以选择一个泛化操作对该属性进行处理。 \n\n      这里我们注意到泛化的过程不足，则依然不方便分析；若泛化太过，则会使其失去本身所含有的意义。通常我们通过以下两种方法控制泛化程度。\n\n      1. 属性泛化阀值控制\n\n         将属性泛化至设定的阀值。\n\n      2. 泛化关系阀值控制\n\n      用户都应能调整阀值。\n\n3.    属性关联分析\n\n      从概念上讲，属性关联分析的过程如下\n\n      1. 数据收集\n\n      2. 利用保守AOI方法进行属性相关分析\n\n         一般来说，这里主要是消除数据集中取不同值个数过多的属性或对可泛化属性进行泛化。一般而言，保险起见，这里的阀值一般比较大。\n\n      3. 利用所确定评估标准评估每个初选后的属性。\n\n         如信息增益方法。\n\n      4. 消除无关或弱相关的属性\n\n      5. 利用AOI方法生成概念描述\n\n         采用更严格的属性泛化控制阈值来进行 基于属性的归纳操作。\n\n4.    概念对比\n\n      1. 概念对比方法和实现\n         1. 数据收集\n\n         2. 属性相关分析\n\n            应用分析概念对比方法，以便保留相关程度最高的若干属性(维)供稍后分析处理。\n\n         3. 同步泛化\n         4. 卷上卷下\n         5. 挖掘结果表示\n\n      2. t_weight - 同一行所占比例\n\n      3. d_weight - 同一列所占比例\n\n5.    挖掘数据库\n\n      1. 计算中心趋势\n\n         1. 算数平均\n         2. 加权算术平均\n         3. 中间值\n\n      2. 计算数据分布\n\n         最常用的数据分布度量参数就是五值摘要(四分值)、值间范围和标准偏差。\n\n         四分值的三个分度为Q1,Q2,Q3。\n\n         中间数为M。\n\n         最小数据Minimum，最大数据Maximum。\n\n         - 分值间范围：IQR = Q3 - Q1\n         - 五值摘要：Minimum, Q3, M, Q1, Maximum\n\n         数据变化程度用方差来表示，自由度为n-1.","source":"_posts/datamining-qualitative-induction.md","raw":"---\ntitle: 数据挖掘-定性归纳\ndate: 2016-12-08 21:31:50\ncategories: [programming]\ntags: [datamining, qualitative-induction, programming]\n---\n\n# 数据泛化和概念对比\n\n1.    数据立方方法\n\n      利用数据立方方法进行数据泛化，被分析的数据存放在一个多维数据库(数据立方)中。\n\n      数据立方的维是通过一系列能够形成层次的属性或网格，例如:日期可以包含属性天、周、月、季和年，这些属性构成了维的网格。一个数据立方中存放着预先对部分或所有维(属性)的合计计算结果。 \n\n      roll up: 数据泛化\n\n      drill down: 数据细化\n\n2.    基于属性的归纳方法\n\n      它是一种在线数据分析技术方法。\n\n      基于属性归纳的基本思想就是，首先利用关系数据库查询来收集与任务相关的数据，并通过对任务相关数据集中各属性不同值个数的检查，完成数据泛化操作。数据泛化操作是通过属性消减或属性泛化(又称为概念层次提升)操作来完成的。通过合并(泛化后)相同行并累计它们相应的个数。这就自然减少了泛化后的数据集大小。所获(泛化后)结果以图表和规则等多种不同形式提供给用户。\n\n      涉及的操作有两种：\n\n      1. 属性消减\n\n         例如，有四个属性：街道、城市、省份，国家，那么街道的更高层次是由后面三个属性来表示的，此时取消街道属性相当于泛化操作。\n\n      2. 属性泛化\n\n         它是基于以下规则进行:若一个属性(在初始数据集中) 有许多不同数值，且该属性存在一组泛化操作，则可以选择一个泛化操作对该属性进行处理。 \n\n      这里我们注意到泛化的过程不足，则依然不方便分析；若泛化太过，则会使其失去本身所含有的意义。通常我们通过以下两种方法控制泛化程度。\n\n      1. 属性泛化阀值控制\n\n         将属性泛化至设定的阀值。\n\n      2. 泛化关系阀值控制\n\n      用户都应能调整阀值。\n\n3.    属性关联分析\n\n      从概念上讲，属性关联分析的过程如下\n\n      1. 数据收集\n\n      2. 利用保守AOI方法进行属性相关分析\n\n         一般来说，这里主要是消除数据集中取不同值个数过多的属性或对可泛化属性进行泛化。一般而言，保险起见，这里的阀值一般比较大。\n\n      3. 利用所确定评估标准评估每个初选后的属性。\n\n         如信息增益方法。\n\n      4. 消除无关或弱相关的属性\n\n      5. 利用AOI方法生成概念描述\n\n         采用更严格的属性泛化控制阈值来进行 基于属性的归纳操作。\n\n4.    概念对比\n\n      1. 概念对比方法和实现\n         1. 数据收集\n\n         2. 属性相关分析\n\n            应用分析概念对比方法，以便保留相关程度最高的若干属性(维)供稍后分析处理。\n\n         3. 同步泛化\n         4. 卷上卷下\n         5. 挖掘结果表示\n\n      2. t_weight - 同一行所占比例\n\n      3. d_weight - 同一列所占比例\n\n5.    挖掘数据库\n\n      1. 计算中心趋势\n\n         1. 算数平均\n         2. 加权算术平均\n         3. 中间值\n\n      2. 计算数据分布\n\n         最常用的数据分布度量参数就是五值摘要(四分值)、值间范围和标准偏差。\n\n         四分值的三个分度为Q1,Q2,Q3。\n\n         中间数为M。\n\n         最小数据Minimum，最大数据Maximum。\n\n         - 分值间范围：IQR = Q3 - Q1\n         - 五值摘要：Minimum, Q3, M, Q1, Maximum\n\n         数据变化程度用方差来表示，自由度为n-1.","slug":"datamining-qualitative-induction","published":1,"updated":"2016-12-23T06:55:11.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckw4qufq3003igwtl5rl03yxp","content":"<h1 id=\"数据泛化和概念对比\"><a href=\"#数据泛化和概念对比\" class=\"headerlink\" title=\"数据泛化和概念对比\"></a>数据泛化和概念对比</h1><ol>\n<li><p>数据立方方法</p>\n<p>   利用数据立方方法进行数据泛化，被分析的数据存放在一个多维数据库(数据立方)中。</p>\n<p>   数据立方的维是通过一系列能够形成层次的属性或网格，例如:日期可以包含属性天、周、月、季和年，这些属性构成了维的网格。一个数据立方中存放着预先对部分或所有维(属性)的合计计算结果。 </p>\n<p>   roll up: 数据泛化</p>\n<p>   drill down: 数据细化</p>\n</li>\n<li><p>基于属性的归纳方法</p>\n<p>   它是一种在线数据分析技术方法。</p>\n<p>   基于属性归纳的基本思想就是，首先利用关系数据库查询来收集与任务相关的数据，并通过对任务相关数据集中各属性不同值个数的检查，完成数据泛化操作。数据泛化操作是通过属性消减或属性泛化(又称为概念层次提升)操作来完成的。通过合并(泛化后)相同行并累计它们相应的个数。这就自然减少了泛化后的数据集大小。所获(泛化后)结果以图表和规则等多种不同形式提供给用户。</p>\n<p>   涉及的操作有两种：</p>\n<ol>\n<li><p>属性消减</p>\n<p>例如，有四个属性：街道、城市、省份，国家，那么街道的更高层次是由后面三个属性来表示的，此时取消街道属性相当于泛化操作。</p>\n</li>\n<li><p>属性泛化</p>\n<p>它是基于以下规则进行:若一个属性(在初始数据集中) 有许多不同数值，且该属性存在一组泛化操作，则可以选择一个泛化操作对该属性进行处理。 </p>\n</li>\n</ol>\n<p>   这里我们注意到泛化的过程不足，则依然不方便分析；若泛化太过，则会使其失去本身所含有的意义。通常我们通过以下两种方法控制泛化程度。</p>\n<ol>\n<li><p>属性泛化阀值控制</p>\n<p>将属性泛化至设定的阀值。</p>\n</li>\n<li><p>泛化关系阀值控制</p>\n</li>\n</ol>\n<p>   用户都应能调整阀值。</p>\n</li>\n<li><p>属性关联分析</p>\n<p>   从概念上讲，属性关联分析的过程如下</p>\n<ol>\n<li><p>数据收集</p>\n</li>\n<li><p>利用保守AOI方法进行属性相关分析</p>\n<p>一般来说，这里主要是消除数据集中取不同值个数过多的属性或对可泛化属性进行泛化。一般而言，保险起见，这里的阀值一般比较大。</p>\n</li>\n<li><p>利用所确定评估标准评估每个初选后的属性。</p>\n<p>如信息增益方法。</p>\n</li>\n<li><p>消除无关或弱相关的属性</p>\n</li>\n<li><p>利用AOI方法生成概念描述</p>\n<p>采用更严格的属性泛化控制阈值来进行 基于属性的归纳操作。</p>\n</li>\n</ol>\n</li>\n<li><p>概念对比</p>\n<ol>\n<li><p>概念对比方法和实现</p>\n<ol>\n<li><p>数据收集</p>\n</li>\n<li><p>属性相关分析</p>\n<p>应用分析概念对比方法，以便保留相关程度最高的若干属性(维)供稍后分析处理。</p>\n</li>\n<li><p>同步泛化</p>\n</li>\n<li><p>卷上卷下</p>\n</li>\n<li><p>挖掘结果表示</p>\n</li>\n</ol>\n</li>\n<li><p>t_weight - 同一行所占比例</p>\n</li>\n<li><p>d_weight - 同一列所占比例</p>\n</li>\n</ol>\n</li>\n<li><p>挖掘数据库</p>\n<ol>\n<li><p>计算中心趋势</p>\n<ol>\n<li>算数平均</li>\n<li>加权算术平均</li>\n<li>中间值</li>\n</ol>\n</li>\n<li><p>计算数据分布</p>\n<p>最常用的数据分布度量参数就是五值摘要(四分值)、值间范围和标准偏差。</p>\n<p>四分值的三个分度为Q1,Q2,Q3。</p>\n<p>中间数为M。</p>\n<p>最小数据Minimum，最大数据Maximum。</p>\n<ul>\n<li>分值间范围：IQR = Q3 - Q1</li>\n<li>五值摘要：Minimum, Q3, M, Q1, Maximum</li>\n</ul>\n<p>数据变化程度用方差来表示，自由度为n-1.</p>\n</li>\n</ol>\n</li>\n</ol>\n","site":{"data":{"recommended_posts":{"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"}],"https://LorrinWWW.github.io/posts/complexity/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/hexo-with-latex/":[{"title":"区块链时代的最正确的打赏姿势——在Hexo博客配置Coinhive网页挖矿","permalink":"http://www.davidfnck.com/blockchain/mine-xmr-in-hexo-by-coinhive.html/"}],"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/":[{"title":"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-statistic/":[{"title":"贝叶斯估计 Bayes estimation","permalink":"https://LorrinWWW.github.io/posts/Bayes-estimation/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"}],"https://LorrinWWW.github.io/posts/[2018.2.5]Nested-LSTMs/":[{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"}],"https://LorrinWWW.github.io/posts/datamining-pretreatment/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Several models for knowledge graph representing and completing 几个知识图谱模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/"}],"https://LorrinWWW.github.io/posts/compression/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/Sobolev-space/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/proba-ch5/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch3/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/proba-ch1/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/ML-CNN/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/EDP-basic-models/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/machine-learning/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/datamining-class-pred/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction/":[],"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/":[{"title":"OS notes","permalink":"https://LorrinWWW.github.io/posts/OS-notes/"}],"https://LorrinWWW.github.io/posts/projet-enjeu-plugin-chrome-101/":[{"title":"Chrome油猴脚本","permalink":"http://gubangzhong.cn/2017/06/13/Chrome油猴脚本/"}],"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/":[{"title":"MongoDB, Docker and Python","permalink":"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/":[{"title":"Hands on Scrapy","permalink":"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/OS-notes/":[{"title":"learning OS and building LorriOS","permalink":"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/"}],"https://LorrinWWW.github.io/posts/Bayes-estimation/":[{"title":"Note of statistic","permalink":"https://LorrinWWW.github.io/posts/Note-of-statistic/"}],"https://LorrinWWW.github.io/posts/management-of-the-firm/":[],"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"}],"https://LorrinWWW.github.io/posts/datamining-qualitative-induction/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-finite-element-method/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/QuadTree/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/proba-ch4/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch2/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/":[{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/Note-of-NLP/":[{"title":"Generative Adversarial Network","permalink":"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/"},{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"}],"https://LorrinWWW.github.io/posts/Note-of-datamining/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"数据挖掘-分类与预测","permalink":"https://LorrinWWW.github.io/posts/datamining-class-pred/"},{"title":"数据挖掘-预处理","permalink":"https://LorrinWWW.github.io/posts/datamining-pretreatment/"}],"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/Note-of-probability/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/participe-present-et-gerondif/":[],"https://LorrinWWW.github.io/posts/Hilbert-space/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/proba-ch6/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}]}}},"excerpt":"","more":"<h1 id=\"数据泛化和概念对比\"><a href=\"#数据泛化和概念对比\" class=\"headerlink\" title=\"数据泛化和概念对比\"></a>数据泛化和概念对比</h1><ol>\n<li><p>数据立方方法</p>\n<p>   利用数据立方方法进行数据泛化，被分析的数据存放在一个多维数据库(数据立方)中。</p>\n<p>   数据立方的维是通过一系列能够形成层次的属性或网格，例如:日期可以包含属性天、周、月、季和年，这些属性构成了维的网格。一个数据立方中存放着预先对部分或所有维(属性)的合计计算结果。 </p>\n<p>   roll up: 数据泛化</p>\n<p>   drill down: 数据细化</p>\n</li>\n<li><p>基于属性的归纳方法</p>\n<p>   它是一种在线数据分析技术方法。</p>\n<p>   基于属性归纳的基本思想就是，首先利用关系数据库查询来收集与任务相关的数据，并通过对任务相关数据集中各属性不同值个数的检查，完成数据泛化操作。数据泛化操作是通过属性消减或属性泛化(又称为概念层次提升)操作来完成的。通过合并(泛化后)相同行并累计它们相应的个数。这就自然减少了泛化后的数据集大小。所获(泛化后)结果以图表和规则等多种不同形式提供给用户。</p>\n<p>   涉及的操作有两种：</p>\n<ol>\n<li><p>属性消减</p>\n<p>例如，有四个属性：街道、城市、省份，国家，那么街道的更高层次是由后面三个属性来表示的，此时取消街道属性相当于泛化操作。</p>\n</li>\n<li><p>属性泛化</p>\n<p>它是基于以下规则进行:若一个属性(在初始数据集中) 有许多不同数值，且该属性存在一组泛化操作，则可以选择一个泛化操作对该属性进行处理。 </p>\n</li>\n</ol>\n<p>   这里我们注意到泛化的过程不足，则依然不方便分析；若泛化太过，则会使其失去本身所含有的意义。通常我们通过以下两种方法控制泛化程度。</p>\n<ol>\n<li><p>属性泛化阀值控制</p>\n<p>将属性泛化至设定的阀值。</p>\n</li>\n<li><p>泛化关系阀值控制</p>\n</li>\n</ol>\n<p>   用户都应能调整阀值。</p>\n</li>\n<li><p>属性关联分析</p>\n<p>   从概念上讲，属性关联分析的过程如下</p>\n<ol>\n<li><p>数据收集</p>\n</li>\n<li><p>利用保守AOI方法进行属性相关分析</p>\n<p>一般来说，这里主要是消除数据集中取不同值个数过多的属性或对可泛化属性进行泛化。一般而言，保险起见，这里的阀值一般比较大。</p>\n</li>\n<li><p>利用所确定评估标准评估每个初选后的属性。</p>\n<p>如信息增益方法。</p>\n</li>\n<li><p>消除无关或弱相关的属性</p>\n</li>\n<li><p>利用AOI方法生成概念描述</p>\n<p>采用更严格的属性泛化控制阈值来进行 基于属性的归纳操作。</p>\n</li>\n</ol>\n</li>\n<li><p>概念对比</p>\n<ol>\n<li><p>概念对比方法和实现</p>\n<ol>\n<li><p>数据收集</p>\n</li>\n<li><p>属性相关分析</p>\n<p>应用分析概念对比方法，以便保留相关程度最高的若干属性(维)供稍后分析处理。</p>\n</li>\n<li><p>同步泛化</p>\n</li>\n<li><p>卷上卷下</p>\n</li>\n<li><p>挖掘结果表示</p>\n</li>\n</ol>\n</li>\n<li><p>t_weight - 同一行所占比例</p>\n</li>\n<li><p>d_weight - 同一列所占比例</p>\n</li>\n</ol>\n</li>\n<li><p>挖掘数据库</p>\n<ol>\n<li><p>计算中心趋势</p>\n<ol>\n<li>算数平均</li>\n<li>加权算术平均</li>\n<li>中间值</li>\n</ol>\n</li>\n<li><p>计算数据分布</p>\n<p>最常用的数据分布度量参数就是五值摘要(四分值)、值间范围和标准偏差。</p>\n<p>四分值的三个分度为Q1,Q2,Q3。</p>\n<p>中间数为M。</p>\n<p>最小数据Minimum，最大数据Maximum。</p>\n<ul>\n<li>分值间范围：IQR = Q3 - Q1</li>\n<li>五值摘要：Minimum, Q3, M, Q1, Maximum</li>\n</ul>\n<p>数据变化程度用方差来表示，自由度为n-1.</p>\n</li>\n</ol>\n</li>\n</ol>\n"},{"title":"图论 graph","date":"2016-11-27T06:42:24.000Z","_content":"\n# The symbols\n\nIn graph thery, a graph G = (V, E) is a collection of points.\n\nV, called vertices and lines connecting some subset of them\n\nE, called edges, is contained by V ✖ V\n\nUnion-Find\n\n# Others\n\n以后涉及相关问题再回过头来补充，暂时引用wiki凑数：\n\n[图论](https://zh.wikipedia.org/wiki/%E5%9B%BE%E8%AE%BA)\n\n重要算法：\n\n>- [戴克斯特拉算法](https://zh.wikipedia.org/wiki/%E6%88%B4%E5%85%8B%E6%96%AF%E7%89%B9%E6%8B%89%E7%AE%97%E6%B3%95)(D.A)\n>- [克鲁斯卡尔算法](https://zh.wikipedia.org/wiki/%E5%85%8B%E9%B2%81%E6%96%AF%E5%85%8B%E5%B0%94%E6%BC%94%E7%AE%97%E6%B3%95)(K.A)\n>- [普里姆算法](https://zh.wikipedia.org/wiki/%E6%99%AE%E9%87%8C%E5%A7%86%E7%AE%97%E6%B3%95)(P.A)\n>- [拓扑排序](https://zh.wikipedia.org/wiki/%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F)算法(TSA)\n>- [关键路径](https://zh.wikipedia.org/wiki/%E5%85%B3%E9%94%AE%E8%B7%AF%E5%BE%84)算法(CPA)\n>- [广度优先搜索](https://zh.wikipedia.org/wiki/%E5%B9%BF%E5%BA%A6%E4%BC%98%E5%85%88%E6%90%9C%E7%B4%A2)算法([BFS](https://zh.wikipedia.org/wiki/BFS)'s A)\n>- [深度优先搜索](https://zh.wikipedia.org/wiki/%E6%B7%B1%E5%BA%A6%E4%BC%98%E5%85%88%E6%90%9C%E7%B4%A2)算法([DFS](https://zh.wikipedia.org/wiki/DFS)'s A)\n\n","source":"_posts/graph.md","raw":"---\ntitle: 图论 graph\ndate: 2016-11-27 14:42:24\ncategories: [programming]\ntags: [graph, algo, programming]\n---\n\n# The symbols\n\nIn graph thery, a graph G = (V, E) is a collection of points.\n\nV, called vertices and lines connecting some subset of them\n\nE, called edges, is contained by V ✖ V\n\nUnion-Find\n\n# Others\n\n以后涉及相关问题再回过头来补充，暂时引用wiki凑数：\n\n[图论](https://zh.wikipedia.org/wiki/%E5%9B%BE%E8%AE%BA)\n\n重要算法：\n\n>- [戴克斯特拉算法](https://zh.wikipedia.org/wiki/%E6%88%B4%E5%85%8B%E6%96%AF%E7%89%B9%E6%8B%89%E7%AE%97%E6%B3%95)(D.A)\n>- [克鲁斯卡尔算法](https://zh.wikipedia.org/wiki/%E5%85%8B%E9%B2%81%E6%96%AF%E5%85%8B%E5%B0%94%E6%BC%94%E7%AE%97%E6%B3%95)(K.A)\n>- [普里姆算法](https://zh.wikipedia.org/wiki/%E6%99%AE%E9%87%8C%E5%A7%86%E7%AE%97%E6%B3%95)(P.A)\n>- [拓扑排序](https://zh.wikipedia.org/wiki/%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F)算法(TSA)\n>- [关键路径](https://zh.wikipedia.org/wiki/%E5%85%B3%E9%94%AE%E8%B7%AF%E5%BE%84)算法(CPA)\n>- [广度优先搜索](https://zh.wikipedia.org/wiki/%E5%B9%BF%E5%BA%A6%E4%BC%98%E5%85%88%E6%90%9C%E7%B4%A2)算法([BFS](https://zh.wikipedia.org/wiki/BFS)'s A)\n>- [深度优先搜索](https://zh.wikipedia.org/wiki/%E6%B7%B1%E5%BA%A6%E4%BC%98%E5%85%88%E6%90%9C%E7%B4%A2)算法([DFS](https://zh.wikipedia.org/wiki/DFS)'s A)\n\n","slug":"graph","published":1,"updated":"2017-02-11T12:08:30.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckw4qufq7003mgwtldit7eklh","content":"<h1 id=\"The-symbols\"><a href=\"#The-symbols\" class=\"headerlink\" title=\"The symbols\"></a>The symbols</h1><p>In graph thery, a graph G = (V, E) is a collection of points.</p>\n<p>V, called vertices and lines connecting some subset of them</p>\n<p>E, called edges, is contained by V ✖ V</p>\n<p>Union-Find</p>\n<h1 id=\"Others\"><a href=\"#Others\" class=\"headerlink\" title=\"Others\"></a>Others</h1><p>以后涉及相关问题再回过头来补充，暂时引用wiki凑数：</p>\n<p><a href=\"https://zh.wikipedia.org/wiki/%E5%9B%BE%E8%AE%BA\">图论</a></p>\n<p>重要算法：</p>\n<blockquote>\n<ul>\n<li><a href=\"https://zh.wikipedia.org/wiki/%E6%88%B4%E5%85%8B%E6%96%AF%E7%89%B9%E6%8B%89%E7%AE%97%E6%B3%95\">戴克斯特拉算法</a>(D.A)</li>\n<li><a href=\"https://zh.wikipedia.org/wiki/%E5%85%8B%E9%B2%81%E6%96%AF%E5%85%8B%E5%B0%94%E6%BC%94%E7%AE%97%E6%B3%95\">克鲁斯卡尔算法</a>(K.A)</li>\n<li><a href=\"https://zh.wikipedia.org/wiki/%E6%99%AE%E9%87%8C%E5%A7%86%E7%AE%97%E6%B3%95\">普里姆算法</a>(P.A)</li>\n<li><a href=\"https://zh.wikipedia.org/wiki/%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F\">拓扑排序</a>算法(TSA)</li>\n<li><a href=\"https://zh.wikipedia.org/wiki/%E5%85%B3%E9%94%AE%E8%B7%AF%E5%BE%84\">关键路径</a>算法(CPA)</li>\n<li><a href=\"https://zh.wikipedia.org/wiki/%E5%B9%BF%E5%BA%A6%E4%BC%98%E5%85%88%E6%90%9C%E7%B4%A2\">广度优先搜索</a>算法(<a href=\"https://zh.wikipedia.org/wiki/BFS\">BFS</a>‘s A)</li>\n<li><a href=\"https://zh.wikipedia.org/wiki/%E6%B7%B1%E5%BA%A6%E4%BC%98%E5%85%88%E6%90%9C%E7%B4%A2\">深度优先搜索</a>算法(<a href=\"https://zh.wikipedia.org/wiki/DFS\">DFS</a>‘s A)</li>\n</ul>\n</blockquote>\n","site":{"data":{"recommended_posts":{"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"}],"https://LorrinWWW.github.io/posts/complexity/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/hexo-with-latex/":[{"title":"区块链时代的最正确的打赏姿势——在Hexo博客配置Coinhive网页挖矿","permalink":"http://www.davidfnck.com/blockchain/mine-xmr-in-hexo-by-coinhive.html/"}],"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/":[{"title":"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-statistic/":[{"title":"贝叶斯估计 Bayes estimation","permalink":"https://LorrinWWW.github.io/posts/Bayes-estimation/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"}],"https://LorrinWWW.github.io/posts/[2018.2.5]Nested-LSTMs/":[{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"}],"https://LorrinWWW.github.io/posts/datamining-pretreatment/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Several models for knowledge graph representing and completing 几个知识图谱模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/"}],"https://LorrinWWW.github.io/posts/compression/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/Sobolev-space/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/proba-ch5/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch3/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/proba-ch1/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/ML-CNN/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/EDP-basic-models/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/machine-learning/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/datamining-class-pred/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction/":[],"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/":[{"title":"OS notes","permalink":"https://LorrinWWW.github.io/posts/OS-notes/"}],"https://LorrinWWW.github.io/posts/projet-enjeu-plugin-chrome-101/":[{"title":"Chrome油猴脚本","permalink":"http://gubangzhong.cn/2017/06/13/Chrome油猴脚本/"}],"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/":[{"title":"MongoDB, Docker and Python","permalink":"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/":[{"title":"Hands on Scrapy","permalink":"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/OS-notes/":[{"title":"learning OS and building LorriOS","permalink":"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/"}],"https://LorrinWWW.github.io/posts/Bayes-estimation/":[{"title":"Note of statistic","permalink":"https://LorrinWWW.github.io/posts/Note-of-statistic/"}],"https://LorrinWWW.github.io/posts/management-of-the-firm/":[],"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"}],"https://LorrinWWW.github.io/posts/datamining-qualitative-induction/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-finite-element-method/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/QuadTree/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/proba-ch4/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch2/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/":[{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/Note-of-NLP/":[{"title":"Generative Adversarial Network","permalink":"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/"},{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"}],"https://LorrinWWW.github.io/posts/Note-of-datamining/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"数据挖掘-分类与预测","permalink":"https://LorrinWWW.github.io/posts/datamining-class-pred/"},{"title":"数据挖掘-预处理","permalink":"https://LorrinWWW.github.io/posts/datamining-pretreatment/"}],"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/Note-of-probability/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/participe-present-et-gerondif/":[],"https://LorrinWWW.github.io/posts/Hilbert-space/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/proba-ch6/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}]}}},"excerpt":"","more":"<h1 id=\"The-symbols\"><a href=\"#The-symbols\" class=\"headerlink\" title=\"The symbols\"></a>The symbols</h1><p>In graph thery, a graph G = (V, E) is a collection of points.</p>\n<p>V, called vertices and lines connecting some subset of them</p>\n<p>E, called edges, is contained by V ✖ V</p>\n<p>Union-Find</p>\n<h1 id=\"Others\"><a href=\"#Others\" class=\"headerlink\" title=\"Others\"></a>Others</h1><p>以后涉及相关问题再回过头来补充，暂时引用wiki凑数：</p>\n<p><a href=\"https://zh.wikipedia.org/wiki/%E5%9B%BE%E8%AE%BA\">图论</a></p>\n<p>重要算法：</p>\n<blockquote>\n<ul>\n<li><a href=\"https://zh.wikipedia.org/wiki/%E6%88%B4%E5%85%8B%E6%96%AF%E7%89%B9%E6%8B%89%E7%AE%97%E6%B3%95\">戴克斯特拉算法</a>(D.A)</li>\n<li><a href=\"https://zh.wikipedia.org/wiki/%E5%85%8B%E9%B2%81%E6%96%AF%E5%85%8B%E5%B0%94%E6%BC%94%E7%AE%97%E6%B3%95\">克鲁斯卡尔算法</a>(K.A)</li>\n<li><a href=\"https://zh.wikipedia.org/wiki/%E6%99%AE%E9%87%8C%E5%A7%86%E7%AE%97%E6%B3%95\">普里姆算法</a>(P.A)</li>\n<li><a href=\"https://zh.wikipedia.org/wiki/%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F\">拓扑排序</a>算法(TSA)</li>\n<li><a href=\"https://zh.wikipedia.org/wiki/%E5%85%B3%E9%94%AE%E8%B7%AF%E5%BE%84\">关键路径</a>算法(CPA)</li>\n<li><a href=\"https://zh.wikipedia.org/wiki/%E5%B9%BF%E5%BA%A6%E4%BC%98%E5%85%88%E6%90%9C%E7%B4%A2\">广度优先搜索</a>算法(<a href=\"https://zh.wikipedia.org/wiki/BFS\">BFS</a>‘s A)</li>\n<li><a href=\"https://zh.wikipedia.org/wiki/%E6%B7%B1%E5%BA%A6%E4%BC%98%E5%85%88%E6%90%9C%E7%B4%A2\">深度优先搜索</a>算法(<a href=\"https://zh.wikipedia.org/wiki/DFS\">DFS</a>‘s A)</li>\n</ul>\n</blockquote>\n"},{"title":"解决hexo使用公式冲突问题 hexo with latex","date":"2016-11-30T14:19:02.000Z","_content":"\n在hexo中使用大量公式的同学一定会发现，在hexo很多公式的渲染不正常。这是由于markdown渲染器和latex渲染器冲突的问题（具体说，就是公式中的特殊字符首先被markdown渲染器转义了）。我们可以通过更换Markdown渲染插件来解决这个问题。\n\nGoogle了一些博文，在[如何处理Hexo和MathJax的兼容问题](http://2wildkids.com/2016/10/06/%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86Hexo%E5%92%8CMathJax%E7%9A%84%E5%85%BC%E5%AE%B9%E9%97%AE%E9%A2%98/)这篇文章中发现了一个符合要求的插件：[hexo-renderer-kramed](https://github.com/sun11/hexo-renderer-kramed)。作者fork了 hexo-renderer-marked 项目，并且只对MathJax支持进行了改进，其他特性完全一致。\n\n简单粗暴的讲，卸载原渲染器，安装我们需要的渲染器。\n\n依次运行下列语句即可。\n\n```\n$ npm uninstall hexo-renderer-marked --save\n$ npm install hexo-renderer-kramed --save\n```\n\n现在公式的显示已经正常。\n\n注：传说行内代码渲染仍然有问题，在代码块中工作正常。","source":"_posts/hexo-with-latex.md","raw":"---\ntitle: 解决hexo使用公式冲突问题 hexo with latex\ndate: 2016-11-30 22:19:02\ncategories: other\ntags: [hexo, latex, mathjax, marked]\n---\n\n在hexo中使用大量公式的同学一定会发现，在hexo很多公式的渲染不正常。这是由于markdown渲染器和latex渲染器冲突的问题（具体说，就是公式中的特殊字符首先被markdown渲染器转义了）。我们可以通过更换Markdown渲染插件来解决这个问题。\n\nGoogle了一些博文，在[如何处理Hexo和MathJax的兼容问题](http://2wildkids.com/2016/10/06/%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86Hexo%E5%92%8CMathJax%E7%9A%84%E5%85%BC%E5%AE%B9%E9%97%AE%E9%A2%98/)这篇文章中发现了一个符合要求的插件：[hexo-renderer-kramed](https://github.com/sun11/hexo-renderer-kramed)。作者fork了 hexo-renderer-marked 项目，并且只对MathJax支持进行了改进，其他特性完全一致。\n\n简单粗暴的讲，卸载原渲染器，安装我们需要的渲染器。\n\n依次运行下列语句即可。\n\n```\n$ npm uninstall hexo-renderer-marked --save\n$ npm install hexo-renderer-kramed --save\n```\n\n现在公式的显示已经正常。\n\n注：传说行内代码渲染仍然有问题，在代码块中工作正常。","slug":"hexo-with-latex","published":1,"updated":"2016-11-30T21:45:34.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckw4qufq8003pgwtldso8fm3u","content":"<p>在hexo中使用大量公式的同学一定会发现，在hexo很多公式的渲染不正常。这是由于markdown渲染器和latex渲染器冲突的问题（具体说，就是公式中的特殊字符首先被markdown渲染器转义了）。我们可以通过更换Markdown渲染插件来解决这个问题。</p>\n<p>Google了一些博文，在<a href=\"http://2wildkids.com/2016/10/06/%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86Hexo%E5%92%8CMathJax%E7%9A%84%E5%85%BC%E5%AE%B9%E9%97%AE%E9%A2%98/\">如何处理Hexo和MathJax的兼容问题</a>这篇文章中发现了一个符合要求的插件：<a href=\"https://github.com/sun11/hexo-renderer-kramed\">hexo-renderer-kramed</a>。作者fork了 hexo-renderer-marked 项目，并且只对MathJax支持进行了改进，其他特性完全一致。</p>\n<p>简单粗暴的讲，卸载原渲染器，安装我们需要的渲染器。</p>\n<p>依次运行下列语句即可。</p>\n<figure class=\"highlight plaintext\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ npm uninstall hexo-renderer-marked --save</span><br><span class=\"line\">$ npm install hexo-renderer-kramed --save</span><br></pre></td></tr></tbody></table></figure>\n\n<p>现在公式的显示已经正常。</p>\n<p>注：传说行内代码渲染仍然有问题，在代码块中工作正常。</p>\n","site":{"data":{"recommended_posts":{"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"}],"https://LorrinWWW.github.io/posts/complexity/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/hexo-with-latex/":[{"title":"区块链时代的最正确的打赏姿势——在Hexo博客配置Coinhive网页挖矿","permalink":"http://www.davidfnck.com/blockchain/mine-xmr-in-hexo-by-coinhive.html/"}],"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/":[{"title":"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-statistic/":[{"title":"贝叶斯估计 Bayes estimation","permalink":"https://LorrinWWW.github.io/posts/Bayes-estimation/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"}],"https://LorrinWWW.github.io/posts/[2018.2.5]Nested-LSTMs/":[{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"}],"https://LorrinWWW.github.io/posts/datamining-pretreatment/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Several models for knowledge graph representing and completing 几个知识图谱模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/"}],"https://LorrinWWW.github.io/posts/compression/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/Sobolev-space/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/proba-ch5/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch3/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/proba-ch1/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/ML-CNN/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/EDP-basic-models/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/machine-learning/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/datamining-class-pred/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction/":[],"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/":[{"title":"OS notes","permalink":"https://LorrinWWW.github.io/posts/OS-notes/"}],"https://LorrinWWW.github.io/posts/projet-enjeu-plugin-chrome-101/":[{"title":"Chrome油猴脚本","permalink":"http://gubangzhong.cn/2017/06/13/Chrome油猴脚本/"}],"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/":[{"title":"MongoDB, Docker and Python","permalink":"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/":[{"title":"Hands on Scrapy","permalink":"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/OS-notes/":[{"title":"learning OS and building LorriOS","permalink":"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/"}],"https://LorrinWWW.github.io/posts/Bayes-estimation/":[{"title":"Note of statistic","permalink":"https://LorrinWWW.github.io/posts/Note-of-statistic/"}],"https://LorrinWWW.github.io/posts/management-of-the-firm/":[],"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"}],"https://LorrinWWW.github.io/posts/datamining-qualitative-induction/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-finite-element-method/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/QuadTree/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/proba-ch4/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch2/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/":[{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/Note-of-NLP/":[{"title":"Generative Adversarial Network","permalink":"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/"},{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"}],"https://LorrinWWW.github.io/posts/Note-of-datamining/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"数据挖掘-分类与预测","permalink":"https://LorrinWWW.github.io/posts/datamining-class-pred/"},{"title":"数据挖掘-预处理","permalink":"https://LorrinWWW.github.io/posts/datamining-pretreatment/"}],"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/Note-of-probability/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/participe-present-et-gerondif/":[],"https://LorrinWWW.github.io/posts/Hilbert-space/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/proba-ch6/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}]}}},"excerpt":"","more":"<p>在hexo中使用大量公式的同学一定会发现，在hexo很多公式的渲染不正常。这是由于markdown渲染器和latex渲染器冲突的问题（具体说，就是公式中的特殊字符首先被markdown渲染器转义了）。我们可以通过更换Markdown渲染插件来解决这个问题。</p>\n<p>Google了一些博文，在<a href=\"http://2wildkids.com/2016/10/06/%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86Hexo%E5%92%8CMathJax%E7%9A%84%E5%85%BC%E5%AE%B9%E9%97%AE%E9%A2%98/\">如何处理Hexo和MathJax的兼容问题</a>这篇文章中发现了一个符合要求的插件：<a href=\"https://github.com/sun11/hexo-renderer-kramed\">hexo-renderer-kramed</a>。作者fork了 hexo-renderer-marked 项目，并且只对MathJax支持进行了改进，其他特性完全一致。</p>\n<p>简单粗暴的讲，卸载原渲染器，安装我们需要的渲染器。</p>\n<p>依次运行下列语句即可。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ npm uninstall hexo-renderer-marked --save</span><br><span class=\"line\">$ npm install hexo-renderer-kramed --save</span><br></pre></td></tr></table></figure>\n\n<p>现在公式的显示已经正常。</p>\n<p>注：传说行内代码渲染仍然有问题，在代码块中工作正常。</p>\n"},{"title":"intro-about-KG","date":"2018-07-01T12:59:58.000Z","_content":"\n# 知识图谱\n\n之前其实陆陆续续地看了一些知识图谱相关的论文，对其的理解始终停留在比较浅显的层面，或者说一个非常不全面的状态，以至于，在实习中真的要尝试着手建立一个通用知识图谱的时候，却不知道如何下手。在实习期间打算写这篇综述来整理一下之前看到的一些琐碎的知识。\n\n## 介绍和定义\n\n知识图谱（knowledge graph）最初是又Google提出的概念，目的是确定一种面相知识的存储结构。通常我们将数据存在一个关系型数据库或是key-value型数据库，数据和数据之间的关系是通过表来定义的；当数据的关系比较复杂而且比较灵活的时候，传统数据库的表达能力和响应速度都有较大的局限性，事实上，知识就是一种关系很强的数据，而且知识千变万化，难以用一些简单的规则来预先确定表。由此，知识图谱的想法也就很自然了，知识是一种关系性很强的数据，知识的存储结构必然应该是类似于图的。\n\n通常来说，在实际应用中，我们可以简单的认为知识图谱就是一个多关系图，其中我们用实体（entity）来表示节点，用关系（relation）来表示边，因此知识的表达是通过一个三元组—（实体h-关系r-实体t）来实现。\n\nTODO\n\n## 知识图谱构建\n\n某种程度上来说，知识图谱最困难，最需要人力的部分就是知识图谱的构建。数据来源通常有以下几种\n\n### 结构化数据\n\n这个比较理想，但是信息一定是不完善或是滞后的，可以作为初期的构建，后期还是要自己来维护。这里略过。\n\n### 非结构化数据\n\n大数据时代更多的是这样的数据，需要一定的信息抽取、ner等nlp技术的支持，必要时需要人工进行审核。在构建类似的图谱过程当中，主要涉及以下几个方面的自然语言处理技术：  \n\n- 实体命名识别（Name Entity Recognition）\n- 关系抽取（Relation Extraction）    \n- 实体统一（Entity Resolution）    \n- 指代消解（Coreference Resolution）\n\n## 知识图谱的存储\n\nTODO：\n\nRDF和图数据库等\n\n## 知识图谱的应用","source":"_posts/intro-about-KG.md","raw":"---\ntitle: intro-about-KG\ndate: 2018-07-01 20:59:58\ncategories: [research]\ntags: [knowledge-graph, knowledge-reasoning, deep-learning, machine-learning]\n---\n\n# 知识图谱\n\n之前其实陆陆续续地看了一些知识图谱相关的论文，对其的理解始终停留在比较浅显的层面，或者说一个非常不全面的状态，以至于，在实习中真的要尝试着手建立一个通用知识图谱的时候，却不知道如何下手。在实习期间打算写这篇综述来整理一下之前看到的一些琐碎的知识。\n\n## 介绍和定义\n\n知识图谱（knowledge graph）最初是又Google提出的概念，目的是确定一种面相知识的存储结构。通常我们将数据存在一个关系型数据库或是key-value型数据库，数据和数据之间的关系是通过表来定义的；当数据的关系比较复杂而且比较灵活的时候，传统数据库的表达能力和响应速度都有较大的局限性，事实上，知识就是一种关系很强的数据，而且知识千变万化，难以用一些简单的规则来预先确定表。由此，知识图谱的想法也就很自然了，知识是一种关系性很强的数据，知识的存储结构必然应该是类似于图的。\n\n通常来说，在实际应用中，我们可以简单的认为知识图谱就是一个多关系图，其中我们用实体（entity）来表示节点，用关系（relation）来表示边，因此知识的表达是通过一个三元组—（实体h-关系r-实体t）来实现。\n\nTODO\n\n## 知识图谱构建\n\n某种程度上来说，知识图谱最困难，最需要人力的部分就是知识图谱的构建。数据来源通常有以下几种\n\n### 结构化数据\n\n这个比较理想，但是信息一定是不完善或是滞后的，可以作为初期的构建，后期还是要自己来维护。这里略过。\n\n### 非结构化数据\n\n大数据时代更多的是这样的数据，需要一定的信息抽取、ner等nlp技术的支持，必要时需要人工进行审核。在构建类似的图谱过程当中，主要涉及以下几个方面的自然语言处理技术：  \n\n- 实体命名识别（Name Entity Recognition）\n- 关系抽取（Relation Extraction）    \n- 实体统一（Entity Resolution）    \n- 指代消解（Coreference Resolution）\n\n## 知识图谱的存储\n\nTODO：\n\nRDF和图数据库等\n\n## 知识图谱的应用","slug":"intro-about-KG","published":1,"updated":"2018-07-01T13:27:45.722Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckw4qufqa003ugwtldlh2fq72","content":"<h1 id=\"知识图谱\"><a href=\"#知识图谱\" class=\"headerlink\" title=\"知识图谱\"></a>知识图谱</h1><p>之前其实陆陆续续地看了一些知识图谱相关的论文，对其的理解始终停留在比较浅显的层面，或者说一个非常不全面的状态，以至于，在实习中真的要尝试着手建立一个通用知识图谱的时候，却不知道如何下手。在实习期间打算写这篇综述来整理一下之前看到的一些琐碎的知识。</p>\n<h2 id=\"介绍和定义\"><a href=\"#介绍和定义\" class=\"headerlink\" title=\"介绍和定义\"></a>介绍和定义</h2><p>知识图谱（knowledge graph）最初是又Google提出的概念，目的是确定一种面相知识的存储结构。通常我们将数据存在一个关系型数据库或是key-value型数据库，数据和数据之间的关系是通过表来定义的；当数据的关系比较复杂而且比较灵活的时候，传统数据库的表达能力和响应速度都有较大的局限性，事实上，知识就是一种关系很强的数据，而且知识千变万化，难以用一些简单的规则来预先确定表。由此，知识图谱的想法也就很自然了，知识是一种关系性很强的数据，知识的存储结构必然应该是类似于图的。</p>\n<p>通常来说，在实际应用中，我们可以简单的认为知识图谱就是一个多关系图，其中我们用实体（entity）来表示节点，用关系（relation）来表示边，因此知识的表达是通过一个三元组—（实体h-关系r-实体t）来实现。</p>\n<p>TODO</p>\n<h2 id=\"知识图谱构建\"><a href=\"#知识图谱构建\" class=\"headerlink\" title=\"知识图谱构建\"></a>知识图谱构建</h2><p>某种程度上来说，知识图谱最困难，最需要人力的部分就是知识图谱的构建。数据来源通常有以下几种</p>\n<h3 id=\"结构化数据\"><a href=\"#结构化数据\" class=\"headerlink\" title=\"结构化数据\"></a>结构化数据</h3><p>这个比较理想，但是信息一定是不完善或是滞后的，可以作为初期的构建，后期还是要自己来维护。这里略过。</p>\n<h3 id=\"非结构化数据\"><a href=\"#非结构化数据\" class=\"headerlink\" title=\"非结构化数据\"></a>非结构化数据</h3><p>大数据时代更多的是这样的数据，需要一定的信息抽取、ner等nlp技术的支持，必要时需要人工进行审核。在构建类似的图谱过程当中，主要涉及以下几个方面的自然语言处理技术：  </p>\n<ul>\n<li>实体命名识别（Name Entity Recognition）</li>\n<li>关系抽取（Relation Extraction）    </li>\n<li>实体统一（Entity Resolution）    </li>\n<li>指代消解（Coreference Resolution）</li>\n</ul>\n<h2 id=\"知识图谱的存储\"><a href=\"#知识图谱的存储\" class=\"headerlink\" title=\"知识图谱的存储\"></a>知识图谱的存储</h2><p>TODO：</p>\n<p>RDF和图数据库等</p>\n<h2 id=\"知识图谱的应用\"><a href=\"#知识图谱的应用\" class=\"headerlink\" title=\"知识图谱的应用\"></a>知识图谱的应用</h2>","site":{"data":{"recommended_posts":{"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"}],"https://LorrinWWW.github.io/posts/complexity/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/hexo-with-latex/":[{"title":"区块链时代的最正确的打赏姿势——在Hexo博客配置Coinhive网页挖矿","permalink":"http://www.davidfnck.com/blockchain/mine-xmr-in-hexo-by-coinhive.html/"}],"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/":[{"title":"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-statistic/":[{"title":"贝叶斯估计 Bayes estimation","permalink":"https://LorrinWWW.github.io/posts/Bayes-estimation/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"}],"https://LorrinWWW.github.io/posts/[2018.2.5]Nested-LSTMs/":[{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"}],"https://LorrinWWW.github.io/posts/datamining-pretreatment/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Several models for knowledge graph representing and completing 几个知识图谱模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/"}],"https://LorrinWWW.github.io/posts/compression/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/Sobolev-space/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/proba-ch5/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch3/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/proba-ch1/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/ML-CNN/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/EDP-basic-models/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/machine-learning/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/datamining-class-pred/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction/":[],"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/":[{"title":"OS notes","permalink":"https://LorrinWWW.github.io/posts/OS-notes/"}],"https://LorrinWWW.github.io/posts/projet-enjeu-plugin-chrome-101/":[{"title":"Chrome油猴脚本","permalink":"http://gubangzhong.cn/2017/06/13/Chrome油猴脚本/"}],"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/":[{"title":"MongoDB, Docker and Python","permalink":"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/":[{"title":"Hands on Scrapy","permalink":"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/OS-notes/":[{"title":"learning OS and building LorriOS","permalink":"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/"}],"https://LorrinWWW.github.io/posts/Bayes-estimation/":[{"title":"Note of statistic","permalink":"https://LorrinWWW.github.io/posts/Note-of-statistic/"}],"https://LorrinWWW.github.io/posts/management-of-the-firm/":[],"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"}],"https://LorrinWWW.github.io/posts/datamining-qualitative-induction/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-finite-element-method/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/QuadTree/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/proba-ch4/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch2/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/":[{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/Note-of-NLP/":[{"title":"Generative Adversarial Network","permalink":"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/"},{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"}],"https://LorrinWWW.github.io/posts/Note-of-datamining/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"数据挖掘-分类与预测","permalink":"https://LorrinWWW.github.io/posts/datamining-class-pred/"},{"title":"数据挖掘-预处理","permalink":"https://LorrinWWW.github.io/posts/datamining-pretreatment/"}],"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/Note-of-probability/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/participe-present-et-gerondif/":[],"https://LorrinWWW.github.io/posts/Hilbert-space/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/proba-ch6/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}]}}},"excerpt":"","more":"<h1 id=\"知识图谱\"><a href=\"#知识图谱\" class=\"headerlink\" title=\"知识图谱\"></a>知识图谱</h1><p>之前其实陆陆续续地看了一些知识图谱相关的论文，对其的理解始终停留在比较浅显的层面，或者说一个非常不全面的状态，以至于，在实习中真的要尝试着手建立一个通用知识图谱的时候，却不知道如何下手。在实习期间打算写这篇综述来整理一下之前看到的一些琐碎的知识。</p>\n<h2 id=\"介绍和定义\"><a href=\"#介绍和定义\" class=\"headerlink\" title=\"介绍和定义\"></a>介绍和定义</h2><p>知识图谱（knowledge graph）最初是又Google提出的概念，目的是确定一种面相知识的存储结构。通常我们将数据存在一个关系型数据库或是key-value型数据库，数据和数据之间的关系是通过表来定义的；当数据的关系比较复杂而且比较灵活的时候，传统数据库的表达能力和响应速度都有较大的局限性，事实上，知识就是一种关系很强的数据，而且知识千变万化，难以用一些简单的规则来预先确定表。由此，知识图谱的想法也就很自然了，知识是一种关系性很强的数据，知识的存储结构必然应该是类似于图的。</p>\n<p>通常来说，在实际应用中，我们可以简单的认为知识图谱就是一个多关系图，其中我们用实体（entity）来表示节点，用关系（relation）来表示边，因此知识的表达是通过一个三元组—（实体h-关系r-实体t）来实现。</p>\n<p>TODO</p>\n<h2 id=\"知识图谱构建\"><a href=\"#知识图谱构建\" class=\"headerlink\" title=\"知识图谱构建\"></a>知识图谱构建</h2><p>某种程度上来说，知识图谱最困难，最需要人力的部分就是知识图谱的构建。数据来源通常有以下几种</p>\n<h3 id=\"结构化数据\"><a href=\"#结构化数据\" class=\"headerlink\" title=\"结构化数据\"></a>结构化数据</h3><p>这个比较理想，但是信息一定是不完善或是滞后的，可以作为初期的构建，后期还是要自己来维护。这里略过。</p>\n<h3 id=\"非结构化数据\"><a href=\"#非结构化数据\" class=\"headerlink\" title=\"非结构化数据\"></a>非结构化数据</h3><p>大数据时代更多的是这样的数据，需要一定的信息抽取、ner等nlp技术的支持，必要时需要人工进行审核。在构建类似的图谱过程当中，主要涉及以下几个方面的自然语言处理技术：  </p>\n<ul>\n<li>实体命名识别（Name Entity Recognition）</li>\n<li>关系抽取（Relation Extraction）    </li>\n<li>实体统一（Entity Resolution）    </li>\n<li>指代消解（Coreference Resolution）</li>\n</ul>\n<h2 id=\"知识图谱的存储\"><a href=\"#知识图谱的存储\" class=\"headerlink\" title=\"知识图谱的存储\"></a>知识图谱的存储</h2><p>TODO：</p>\n<p>RDF和图数据库等</p>\n<h2 id=\"知识图谱的应用\"><a href=\"#知识图谱的应用\" class=\"headerlink\" title=\"知识图谱的应用\"></a>知识图谱的应用</h2>"},{"title":"learning OS and building LorriOS","date":"2016-12-28T07:00:08.000Z","_content":"\n暂时用以记录操作系统的学习，以及开发自己的一个toy OS，作为一个项目日志?(如果能成功的话。。)。写这个东西的目的还是给我自己看的（估计其他人也会看的一头雾水吧，笑），毕竟人不太可能一次就找到正确的道路，所以日志写起来可能非常乱糟糟。所以说这只是一个记录，没有什么真正的参考价值。等这个toy os完成的差不多了，我应该会再来总结和记录一下。\n\n另外，下面的DayN并不是真正的天数，毕竟平时也要上课，不可能连续进行。所以大概是按每一次尝试，来进行划分的。\n\n----\n\n刚开始浏览了一下操作系统的基本知识，按考研复习的来吧，然后看的忽然有点跃跃欲试就想能不能写一个简单的OS。\n\n[源代码](https://github.com/codeurJue/lorriOS)\n\n# Day1:\n\n一开始从《30天自制操作系统》开始看，以前有一点汇编基础所以看起来感觉还是可以接受的。\n\n看到第三天就下不去了，倒不是因为内容看不懂，而是环境的问题吧。作者是基于win，用了很多小程序，我用的是os x，很多时候要找一些替代方法。参考网上linux的解决方案，毕竟os x还是不太一样，到下面这一步就是不行，把haribote.bin写到myos.img。\n\n```shell\n# makefile\nmkdir -p /tmp/floppy\nmount -o loop myos.img /tmp/floppy -o fat=12 # 这一条在mac不适用\nsleep 1\ncp haribote.bin /tmp/floppy\nsleep 1\numount /tmp/floppy\n```\n\nOs x 上貌似不能直接mount，当然我也尝试了一些其他的办法，最终失败，感觉不太想再花时间在细枝末节上，打算回法国后在ubuntu下开发，或者和书上保持一样，在win下开发。\n\n\\*后面通过Docker解决\n\n# Day2:\n\n第二天开始阅读《ORANGE'S：一个操作系统的实现》，并打算先阅读，等了解大致情况再动手，不然也只是抄代码，并且在一知半解的情况下容易卡在一些细枝末节的地方。\n\n同时也可以参考xv6，这个是MIT用于教学写的一个类unix的OS，有配套的课程。\n\n两天下来感觉mac用于web开发十分方便，但是内核开发感觉网上的文档、案例偏少，当然主要也是我比较弱，不能够举一反三。一般来说，要不是面向初学者如《30天》而采用win，要不就是使用linux（果然linux才是王道= =）\n\n# Day3:\n\n目前跟着ORANGE，暂时比较顺利，开发环境仍然是mac。基于freedos下成功进入了保护模式。\n\n下面就保护模式做一点笔记。\n\n关于保护模式网上的资料很多，在为什么需要保护模式这个问题上，这里有一个非常直接的要求——我们的系统要运行在32位上，实模式仅支持16位寻址。当然保护模式远远不止如此，但是对于初学者（比如现在的我，囧），它应该是最大的用途之一。\n\n- GDT\n\n   首先要先说一下GDT(Global Descriptor Table)，它是全局描述符表。GDT只有一张，位置任意，通过LGDT指令被存储在GDTR寄存器。\n\n- Selector\n\n   由GDTR访问全局描述符表是通过“段选择子”（实模式下的段寄存器）来完成的。\n\n- LDT\n\n   局部描述符表LDT(Local Descriptor Table)。和GDT类似，直观上与GDT结构相同（其段选择子TI置位），功能上隶属于GDT。GDT只有一张，LDT有多张，每个任务可以有一张。\n\n如果第一次听到上面几个名词，可能还是比较懵逼。我们看看它是怎么运作的。\n\n1. 进行一系列初始化，包括定义GDT，LDT，选择子等\n2. 系统默认进入实模式\n3. 加载GDT，进入保护模式\n4. 加载不同的LDT，以进入不同的子任务\n5. 退出保护模式，反回实模式。\n\n上面只列出了关键步骤，实际上还有很多细节问题，这里只是简单记录其大致思想，所以没有提。\n\n不管怎么说，姑且是跨入了保护模式的大门。\n\n# Day4:\n\n### 特权级\n\n在IA32分段机制中，特权级有4个特权级别，\n\n- LEVEL 0: 内核\n- LEVEL 1, 2: 服务\n- LEVEL 3: 应用程序\n\n处理器通过识别CPL(current privilege level)、DPL(descriptor privilege level)、RPL(requested privilege level)这三个特权级进行特权级检验。\n\nCPL一般被存储在cs和ss的第0位和第1位。CPL等于所在代码段的特权级。当遇到一致代码段时，CPL不改变。（一致代码段能被小于等于它的特权的代码访问）\n\nDPL是段或门的特权级，写在描述符的属性中。\n\nRPL是通过选择子的第0位和第1位表现的。\n\n简单而言，只要CPL和RPL都小于被访问的数据段的DPL就可以了。\n\n### 门\n\n特权级转移可以分为两大类。jmp和call的直接转移；通过描述符间接转移。由于直接转移有诸多的限制，我们常常使用间接转移。\n\n其中，间接转移又分为：\n\n- 指向一个包含目标代码段选择子的门描述符\n- 指向一个包含目标代码段选择子的TSS\n- 指向一个任务门，它又指向一个包含目标代码段选择子的TSS\n\n门描述符分为四种：\n\n- 调用门 Call gate\n- 中断门 Interrupt gate\n- 陷阱门 Trap gate\n- 任务门 Task gate\n\n# Day5:\n\n### 页表\n\n实模式下，int 15h 获得内存信息\n\n页表这一块看的有点心急，感觉理解不是很到位。以后等弄的更清楚了再来总结。\n\n中途调试的时候总是出问题，没想到最后发现一开始32位代码的权限就定义错了。从现在出错的经历来看，os如果崩了的话，一般是权限问题（可能是因为我现在写的内容不多吧）。所以务必小心检查权限。\n\n书上的代码有一点不是很理解，在PagingDemo它使用了大量的push却没有pop。据我的理解的话，push是把数据存到栈中，而无法改变栈外的内存，它也没有使用ret之类的与栈相关的指令，目前不是很理解。\n\n不理解细节也没办法，先试着看看后面的内容吧。\n\n### 中断和异常\n\nIDT，中断描述符表。IDT中的描述符可以是：\n\n- 中断门描述符\n- 陷阱门描述符\n- 任务门描述符\n\n# Day6:\n\n写着是第六天，实际应该是好久之后了= =\n\n期间已经实现了中断和异常的初始化，并能捕获异常，占个位置以后再详细写。\n\n现在着手实现进程。\n\n**进场表、进程体、GDT和TSS的关系：**\n\n1. 进程表和GDT。进程表内的LDT选择子对应一个GDT中的描述符，也就是说每一个进程表对应一个GDT中的描述符。\n2. 进程表和进程。进程表用于描述进程。\n3. GDT和TSS。GDT中有一个描述符对应TSS。\n\n**初始化进场表、进程体、GDT和TSS：**\n\n1. 准备任意进程体。\n\n   任意函数即可。\n\n2. 初始化进程表。\n\n   进程表结构定义在process.h。在global.c中声明一个全局进程表，它是一个进程的数组，包含所有进程。\n\n   在新建进程前，需要初始化进程表。\n\n   填充GDT中进程LDT的描述符，在protect.c。\n\n3. 准备GDT和TSS。\n\n   TSS结构定义在protect.h。\n\n   填充GDT中TSS描述符，在protect.c。\n\n   TSS准备好了，然后加载tr，在kernel.asm。\n\n**从启动时间顺序上，第一个进程的启动过程如下：**\n\n准备进程体\n\n=> 初始化GDT中的TSS和LDT两个描述符，初始化TSS\n\n=> 准备进程表\n\n=> 完成跳转 ring0 -> ring1\n\n### 添加一个任务的总结\n\n按orange书所说，或参考minix，建立了task_table包含所有进程信息，通过遍历它初始化进程。\n\n1. 添加一个进程体，即一个函数，并声明它 (一般在proto.h中声明)\n2. 在task_table中添加一个进程 (global.c)\n3. 添加宏：修改进程数量(++)和定义堆栈空间 (process.h)\n4. 再次编译即可","source":"_posts/learning-OS-and-building-LorriOS.md","raw":"---\ntitle: learning OS and building LorriOS\ndate: 2016-12-28 15:00:08\ncategories: [programming, unfinished]\ntags: [OS, kernel]\n---\n\n暂时用以记录操作系统的学习，以及开发自己的一个toy OS，作为一个项目日志?(如果能成功的话。。)。写这个东西的目的还是给我自己看的（估计其他人也会看的一头雾水吧，笑），毕竟人不太可能一次就找到正确的道路，所以日志写起来可能非常乱糟糟。所以说这只是一个记录，没有什么真正的参考价值。等这个toy os完成的差不多了，我应该会再来总结和记录一下。\n\n另外，下面的DayN并不是真正的天数，毕竟平时也要上课，不可能连续进行。所以大概是按每一次尝试，来进行划分的。\n\n----\n\n刚开始浏览了一下操作系统的基本知识，按考研复习的来吧，然后看的忽然有点跃跃欲试就想能不能写一个简单的OS。\n\n[源代码](https://github.com/codeurJue/lorriOS)\n\n# Day1:\n\n一开始从《30天自制操作系统》开始看，以前有一点汇编基础所以看起来感觉还是可以接受的。\n\n看到第三天就下不去了，倒不是因为内容看不懂，而是环境的问题吧。作者是基于win，用了很多小程序，我用的是os x，很多时候要找一些替代方法。参考网上linux的解决方案，毕竟os x还是不太一样，到下面这一步就是不行，把haribote.bin写到myos.img。\n\n```shell\n# makefile\nmkdir -p /tmp/floppy\nmount -o loop myos.img /tmp/floppy -o fat=12 # 这一条在mac不适用\nsleep 1\ncp haribote.bin /tmp/floppy\nsleep 1\numount /tmp/floppy\n```\n\nOs x 上貌似不能直接mount，当然我也尝试了一些其他的办法，最终失败，感觉不太想再花时间在细枝末节上，打算回法国后在ubuntu下开发，或者和书上保持一样，在win下开发。\n\n\\*后面通过Docker解决\n\n# Day2:\n\n第二天开始阅读《ORANGE'S：一个操作系统的实现》，并打算先阅读，等了解大致情况再动手，不然也只是抄代码，并且在一知半解的情况下容易卡在一些细枝末节的地方。\n\n同时也可以参考xv6，这个是MIT用于教学写的一个类unix的OS，有配套的课程。\n\n两天下来感觉mac用于web开发十分方便，但是内核开发感觉网上的文档、案例偏少，当然主要也是我比较弱，不能够举一反三。一般来说，要不是面向初学者如《30天》而采用win，要不就是使用linux（果然linux才是王道= =）\n\n# Day3:\n\n目前跟着ORANGE，暂时比较顺利，开发环境仍然是mac。基于freedos下成功进入了保护模式。\n\n下面就保护模式做一点笔记。\n\n关于保护模式网上的资料很多，在为什么需要保护模式这个问题上，这里有一个非常直接的要求——我们的系统要运行在32位上，实模式仅支持16位寻址。当然保护模式远远不止如此，但是对于初学者（比如现在的我，囧），它应该是最大的用途之一。\n\n- GDT\n\n   首先要先说一下GDT(Global Descriptor Table)，它是全局描述符表。GDT只有一张，位置任意，通过LGDT指令被存储在GDTR寄存器。\n\n- Selector\n\n   由GDTR访问全局描述符表是通过“段选择子”（实模式下的段寄存器）来完成的。\n\n- LDT\n\n   局部描述符表LDT(Local Descriptor Table)。和GDT类似，直观上与GDT结构相同（其段选择子TI置位），功能上隶属于GDT。GDT只有一张，LDT有多张，每个任务可以有一张。\n\n如果第一次听到上面几个名词，可能还是比较懵逼。我们看看它是怎么运作的。\n\n1. 进行一系列初始化，包括定义GDT，LDT，选择子等\n2. 系统默认进入实模式\n3. 加载GDT，进入保护模式\n4. 加载不同的LDT，以进入不同的子任务\n5. 退出保护模式，反回实模式。\n\n上面只列出了关键步骤，实际上还有很多细节问题，这里只是简单记录其大致思想，所以没有提。\n\n不管怎么说，姑且是跨入了保护模式的大门。\n\n# Day4:\n\n### 特权级\n\n在IA32分段机制中，特权级有4个特权级别，\n\n- LEVEL 0: 内核\n- LEVEL 1, 2: 服务\n- LEVEL 3: 应用程序\n\n处理器通过识别CPL(current privilege level)、DPL(descriptor privilege level)、RPL(requested privilege level)这三个特权级进行特权级检验。\n\nCPL一般被存储在cs和ss的第0位和第1位。CPL等于所在代码段的特权级。当遇到一致代码段时，CPL不改变。（一致代码段能被小于等于它的特权的代码访问）\n\nDPL是段或门的特权级，写在描述符的属性中。\n\nRPL是通过选择子的第0位和第1位表现的。\n\n简单而言，只要CPL和RPL都小于被访问的数据段的DPL就可以了。\n\n### 门\n\n特权级转移可以分为两大类。jmp和call的直接转移；通过描述符间接转移。由于直接转移有诸多的限制，我们常常使用间接转移。\n\n其中，间接转移又分为：\n\n- 指向一个包含目标代码段选择子的门描述符\n- 指向一个包含目标代码段选择子的TSS\n- 指向一个任务门，它又指向一个包含目标代码段选择子的TSS\n\n门描述符分为四种：\n\n- 调用门 Call gate\n- 中断门 Interrupt gate\n- 陷阱门 Trap gate\n- 任务门 Task gate\n\n# Day5:\n\n### 页表\n\n实模式下，int 15h 获得内存信息\n\n页表这一块看的有点心急，感觉理解不是很到位。以后等弄的更清楚了再来总结。\n\n中途调试的时候总是出问题，没想到最后发现一开始32位代码的权限就定义错了。从现在出错的经历来看，os如果崩了的话，一般是权限问题（可能是因为我现在写的内容不多吧）。所以务必小心检查权限。\n\n书上的代码有一点不是很理解，在PagingDemo它使用了大量的push却没有pop。据我的理解的话，push是把数据存到栈中，而无法改变栈外的内存，它也没有使用ret之类的与栈相关的指令，目前不是很理解。\n\n不理解细节也没办法，先试着看看后面的内容吧。\n\n### 中断和异常\n\nIDT，中断描述符表。IDT中的描述符可以是：\n\n- 中断门描述符\n- 陷阱门描述符\n- 任务门描述符\n\n# Day6:\n\n写着是第六天，实际应该是好久之后了= =\n\n期间已经实现了中断和异常的初始化，并能捕获异常，占个位置以后再详细写。\n\n现在着手实现进程。\n\n**进场表、进程体、GDT和TSS的关系：**\n\n1. 进程表和GDT。进程表内的LDT选择子对应一个GDT中的描述符，也就是说每一个进程表对应一个GDT中的描述符。\n2. 进程表和进程。进程表用于描述进程。\n3. GDT和TSS。GDT中有一个描述符对应TSS。\n\n**初始化进场表、进程体、GDT和TSS：**\n\n1. 准备任意进程体。\n\n   任意函数即可。\n\n2. 初始化进程表。\n\n   进程表结构定义在process.h。在global.c中声明一个全局进程表，它是一个进程的数组，包含所有进程。\n\n   在新建进程前，需要初始化进程表。\n\n   填充GDT中进程LDT的描述符，在protect.c。\n\n3. 准备GDT和TSS。\n\n   TSS结构定义在protect.h。\n\n   填充GDT中TSS描述符，在protect.c。\n\n   TSS准备好了，然后加载tr，在kernel.asm。\n\n**从启动时间顺序上，第一个进程的启动过程如下：**\n\n准备进程体\n\n=> 初始化GDT中的TSS和LDT两个描述符，初始化TSS\n\n=> 准备进程表\n\n=> 完成跳转 ring0 -> ring1\n\n### 添加一个任务的总结\n\n按orange书所说，或参考minix，建立了task_table包含所有进程信息，通过遍历它初始化进程。\n\n1. 添加一个进程体，即一个函数，并声明它 (一般在proto.h中声明)\n2. 在task_table中添加一个进程 (global.c)\n3. 添加宏：修改进程数量(++)和定义堆栈空间 (process.h)\n4. 再次编译即可","slug":"learning-OS-and-building-LorriOS","published":1,"updated":"2017-04-17T15:12:41.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckw4qufqa003wgwtl8s2d3e71","content":"<p>暂时用以记录操作系统的学习，以及开发自己的一个toy OS，作为一个项目日志?(如果能成功的话。。)。写这个东西的目的还是给我自己看的（估计其他人也会看的一头雾水吧，笑），毕竟人不太可能一次就找到正确的道路，所以日志写起来可能非常乱糟糟。所以说这只是一个记录，没有什么真正的参考价值。等这个toy os完成的差不多了，我应该会再来总结和记录一下。</p>\n<p>另外，下面的DayN并不是真正的天数，毕竟平时也要上课，不可能连续进行。所以大概是按每一次尝试，来进行划分的。</p>\n<hr>\n<p>刚开始浏览了一下操作系统的基本知识，按考研复习的来吧，然后看的忽然有点跃跃欲试就想能不能写一个简单的OS。</p>\n<p><a href=\"https://github.com/codeurJue/lorriOS\">源代码</a></p>\n<h1 id=\"Day1\"><a href=\"#Day1\" class=\"headerlink\" title=\"Day1:\"></a>Day1:</h1><p>一开始从《30天自制操作系统》开始看，以前有一点汇编基础所以看起来感觉还是可以接受的。</p>\n<p>看到第三天就下不去了，倒不是因为内容看不懂，而是环境的问题吧。作者是基于win，用了很多小程序，我用的是os x，很多时候要找一些替代方法。参考网上linux的解决方案，毕竟os x还是不太一样，到下面这一步就是不行，把haribote.bin写到myos.img。</p>\n<figure class=\"highlight shell\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> makefile</span></span><br><span class=\"line\">mkdir -p /tmp/floppy</span><br><span class=\"line\">mount -o loop myos.img /tmp/floppy -o fat=12 # 这一条在mac不适用</span><br><span class=\"line\">sleep 1</span><br><span class=\"line\">cp haribote.bin /tmp/floppy</span><br><span class=\"line\">sleep 1</span><br><span class=\"line\">umount /tmp/floppy</span><br></pre></td></tr></tbody></table></figure>\n\n<p>Os x 上貌似不能直接mount，当然我也尝试了一些其他的办法，最终失败，感觉不太想再花时间在细枝末节上，打算回法国后在ubuntu下开发，或者和书上保持一样，在win下开发。</p>\n<p>*后面通过Docker解决</p>\n<h1 id=\"Day2\"><a href=\"#Day2\" class=\"headerlink\" title=\"Day2:\"></a>Day2:</h1><p>第二天开始阅读《ORANGE’S：一个操作系统的实现》，并打算先阅读，等了解大致情况再动手，不然也只是抄代码，并且在一知半解的情况下容易卡在一些细枝末节的地方。</p>\n<p>同时也可以参考xv6，这个是MIT用于教学写的一个类unix的OS，有配套的课程。</p>\n<p>两天下来感觉mac用于web开发十分方便，但是内核开发感觉网上的文档、案例偏少，当然主要也是我比较弱，不能够举一反三。一般来说，要不是面向初学者如《30天》而采用win，要不就是使用linux（果然linux才是王道= =）</p>\n<h1 id=\"Day3\"><a href=\"#Day3\" class=\"headerlink\" title=\"Day3:\"></a>Day3:</h1><p>目前跟着ORANGE，暂时比较顺利，开发环境仍然是mac。基于freedos下成功进入了保护模式。</p>\n<p>下面就保护模式做一点笔记。</p>\n<p>关于保护模式网上的资料很多，在为什么需要保护模式这个问题上，这里有一个非常直接的要求——我们的系统要运行在32位上，实模式仅支持16位寻址。当然保护模式远远不止如此，但是对于初学者（比如现在的我，囧），它应该是最大的用途之一。</p>\n<ul>\n<li><p>GDT</p>\n<p> 首先要先说一下GDT(Global Descriptor Table)，它是全局描述符表。GDT只有一张，位置任意，通过LGDT指令被存储在GDTR寄存器。</p>\n</li>\n<li><p>Selector</p>\n<p> 由GDTR访问全局描述符表是通过“段选择子”（实模式下的段寄存器）来完成的。</p>\n</li>\n<li><p>LDT</p>\n<p> 局部描述符表LDT(Local Descriptor Table)。和GDT类似，直观上与GDT结构相同（其段选择子TI置位），功能上隶属于GDT。GDT只有一张，LDT有多张，每个任务可以有一张。</p>\n</li>\n</ul>\n<p>如果第一次听到上面几个名词，可能还是比较懵逼。我们看看它是怎么运作的。</p>\n<ol>\n<li>进行一系列初始化，包括定义GDT，LDT，选择子等</li>\n<li>系统默认进入实模式</li>\n<li>加载GDT，进入保护模式</li>\n<li>加载不同的LDT，以进入不同的子任务</li>\n<li>退出保护模式，反回实模式。</li>\n</ol>\n<p>上面只列出了关键步骤，实际上还有很多细节问题，这里只是简单记录其大致思想，所以没有提。</p>\n<p>不管怎么说，姑且是跨入了保护模式的大门。</p>\n<h1 id=\"Day4\"><a href=\"#Day4\" class=\"headerlink\" title=\"Day4:\"></a>Day4:</h1><h3 id=\"特权级\"><a href=\"#特权级\" class=\"headerlink\" title=\"特权级\"></a>特权级</h3><p>在IA32分段机制中，特权级有4个特权级别，</p>\n<ul>\n<li>LEVEL 0: 内核</li>\n<li>LEVEL 1, 2: 服务</li>\n<li>LEVEL 3: 应用程序</li>\n</ul>\n<p>处理器通过识别CPL(current privilege level)、DPL(descriptor privilege level)、RPL(requested privilege level)这三个特权级进行特权级检验。</p>\n<p>CPL一般被存储在cs和ss的第0位和第1位。CPL等于所在代码段的特权级。当遇到一致代码段时，CPL不改变。（一致代码段能被小于等于它的特权的代码访问）</p>\n<p>DPL是段或门的特权级，写在描述符的属性中。</p>\n<p>RPL是通过选择子的第0位和第1位表现的。</p>\n<p>简单而言，只要CPL和RPL都小于被访问的数据段的DPL就可以了。</p>\n<h3 id=\"门\"><a href=\"#门\" class=\"headerlink\" title=\"门\"></a>门</h3><p>特权级转移可以分为两大类。jmp和call的直接转移；通过描述符间接转移。由于直接转移有诸多的限制，我们常常使用间接转移。</p>\n<p>其中，间接转移又分为：</p>\n<ul>\n<li>指向一个包含目标代码段选择子的门描述符</li>\n<li>指向一个包含目标代码段选择子的TSS</li>\n<li>指向一个任务门，它又指向一个包含目标代码段选择子的TSS</li>\n</ul>\n<p>门描述符分为四种：</p>\n<ul>\n<li>调用门 Call gate</li>\n<li>中断门 Interrupt gate</li>\n<li>陷阱门 Trap gate</li>\n<li>任务门 Task gate</li>\n</ul>\n<h1 id=\"Day5\"><a href=\"#Day5\" class=\"headerlink\" title=\"Day5:\"></a>Day5:</h1><h3 id=\"页表\"><a href=\"#页表\" class=\"headerlink\" title=\"页表\"></a>页表</h3><p>实模式下，int 15h 获得内存信息</p>\n<p>页表这一块看的有点心急，感觉理解不是很到位。以后等弄的更清楚了再来总结。</p>\n<p>中途调试的时候总是出问题，没想到最后发现一开始32位代码的权限就定义错了。从现在出错的经历来看，os如果崩了的话，一般是权限问题（可能是因为我现在写的内容不多吧）。所以务必小心检查权限。</p>\n<p>书上的代码有一点不是很理解，在PagingDemo它使用了大量的push却没有pop。据我的理解的话，push是把数据存到栈中，而无法改变栈外的内存，它也没有使用ret之类的与栈相关的指令，目前不是很理解。</p>\n<p>不理解细节也没办法，先试着看看后面的内容吧。</p>\n<h3 id=\"中断和异常\"><a href=\"#中断和异常\" class=\"headerlink\" title=\"中断和异常\"></a>中断和异常</h3><p>IDT，中断描述符表。IDT中的描述符可以是：</p>\n<ul>\n<li>中断门描述符</li>\n<li>陷阱门描述符</li>\n<li>任务门描述符</li>\n</ul>\n<h1 id=\"Day6\"><a href=\"#Day6\" class=\"headerlink\" title=\"Day6:\"></a>Day6:</h1><p>写着是第六天，实际应该是好久之后了= =</p>\n<p>期间已经实现了中断和异常的初始化，并能捕获异常，占个位置以后再详细写。</p>\n<p>现在着手实现进程。</p>\n<p><strong>进场表、进程体、GDT和TSS的关系：</strong></p>\n<ol>\n<li>进程表和GDT。进程表内的LDT选择子对应一个GDT中的描述符，也就是说每一个进程表对应一个GDT中的描述符。</li>\n<li>进程表和进程。进程表用于描述进程。</li>\n<li>GDT和TSS。GDT中有一个描述符对应TSS。</li>\n</ol>\n<p><strong>初始化进场表、进程体、GDT和TSS：</strong></p>\n<ol>\n<li><p>准备任意进程体。</p>\n<p>任意函数即可。</p>\n</li>\n<li><p>初始化进程表。</p>\n<p>进程表结构定义在process.h。在global.c中声明一个全局进程表，它是一个进程的数组，包含所有进程。</p>\n<p>在新建进程前，需要初始化进程表。</p>\n<p>填充GDT中进程LDT的描述符，在protect.c。</p>\n</li>\n<li><p>准备GDT和TSS。</p>\n<p>TSS结构定义在protect.h。</p>\n<p>填充GDT中TSS描述符，在protect.c。</p>\n<p>TSS准备好了，然后加载tr，在kernel.asm。</p>\n</li>\n</ol>\n<p><strong>从启动时间顺序上，第一个进程的启动过程如下：</strong></p>\n<p>准备进程体</p>\n<p>=&gt; 初始化GDT中的TSS和LDT两个描述符，初始化TSS</p>\n<p>=&gt; 准备进程表</p>\n<p>=&gt; 完成跳转 ring0 -&gt; ring1</p>\n<h3 id=\"添加一个任务的总结\"><a href=\"#添加一个任务的总结\" class=\"headerlink\" title=\"添加一个任务的总结\"></a>添加一个任务的总结</h3><p>按orange书所说，或参考minix，建立了task_table包含所有进程信息，通过遍历它初始化进程。</p>\n<ol>\n<li>添加一个进程体，即一个函数，并声明它 (一般在proto.h中声明)</li>\n<li>在task_table中添加一个进程 (global.c)</li>\n<li>添加宏：修改进程数量(++)和定义堆栈空间 (process.h)</li>\n<li>再次编译即可</li>\n</ol>\n","site":{"data":{"recommended_posts":{"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"}],"https://LorrinWWW.github.io/posts/complexity/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/hexo-with-latex/":[{"title":"区块链时代的最正确的打赏姿势——在Hexo博客配置Coinhive网页挖矿","permalink":"http://www.davidfnck.com/blockchain/mine-xmr-in-hexo-by-coinhive.html/"}],"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/":[{"title":"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-statistic/":[{"title":"贝叶斯估计 Bayes estimation","permalink":"https://LorrinWWW.github.io/posts/Bayes-estimation/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"}],"https://LorrinWWW.github.io/posts/[2018.2.5]Nested-LSTMs/":[{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"}],"https://LorrinWWW.github.io/posts/datamining-pretreatment/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Several models for knowledge graph representing and completing 几个知识图谱模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/"}],"https://LorrinWWW.github.io/posts/compression/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/Sobolev-space/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/proba-ch5/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch3/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/proba-ch1/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/ML-CNN/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/EDP-basic-models/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/machine-learning/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/datamining-class-pred/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction/":[],"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/":[{"title":"OS notes","permalink":"https://LorrinWWW.github.io/posts/OS-notes/"}],"https://LorrinWWW.github.io/posts/projet-enjeu-plugin-chrome-101/":[{"title":"Chrome油猴脚本","permalink":"http://gubangzhong.cn/2017/06/13/Chrome油猴脚本/"}],"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/":[{"title":"MongoDB, Docker and Python","permalink":"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/":[{"title":"Hands on Scrapy","permalink":"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/OS-notes/":[{"title":"learning OS and building LorriOS","permalink":"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/"}],"https://LorrinWWW.github.io/posts/Bayes-estimation/":[{"title":"Note of statistic","permalink":"https://LorrinWWW.github.io/posts/Note-of-statistic/"}],"https://LorrinWWW.github.io/posts/management-of-the-firm/":[],"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"}],"https://LorrinWWW.github.io/posts/datamining-qualitative-induction/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-finite-element-method/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/QuadTree/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/proba-ch4/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch2/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/":[{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/Note-of-NLP/":[{"title":"Generative Adversarial Network","permalink":"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/"},{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"}],"https://LorrinWWW.github.io/posts/Note-of-datamining/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"数据挖掘-分类与预测","permalink":"https://LorrinWWW.github.io/posts/datamining-class-pred/"},{"title":"数据挖掘-预处理","permalink":"https://LorrinWWW.github.io/posts/datamining-pretreatment/"}],"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/Note-of-probability/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/participe-present-et-gerondif/":[],"https://LorrinWWW.github.io/posts/Hilbert-space/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/proba-ch6/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}]}}},"excerpt":"","more":"<p>暂时用以记录操作系统的学习，以及开发自己的一个toy OS，作为一个项目日志?(如果能成功的话。。)。写这个东西的目的还是给我自己看的（估计其他人也会看的一头雾水吧，笑），毕竟人不太可能一次就找到正确的道路，所以日志写起来可能非常乱糟糟。所以说这只是一个记录，没有什么真正的参考价值。等这个toy os完成的差不多了，我应该会再来总结和记录一下。</p>\n<p>另外，下面的DayN并不是真正的天数，毕竟平时也要上课，不可能连续进行。所以大概是按每一次尝试，来进行划分的。</p>\n<hr>\n<p>刚开始浏览了一下操作系统的基本知识，按考研复习的来吧，然后看的忽然有点跃跃欲试就想能不能写一个简单的OS。</p>\n<p><a href=\"https://github.com/codeurJue/lorriOS\">源代码</a></p>\n<h1 id=\"Day1\"><a href=\"#Day1\" class=\"headerlink\" title=\"Day1:\"></a>Day1:</h1><p>一开始从《30天自制操作系统》开始看，以前有一点汇编基础所以看起来感觉还是可以接受的。</p>\n<p>看到第三天就下不去了，倒不是因为内容看不懂，而是环境的问题吧。作者是基于win，用了很多小程序，我用的是os x，很多时候要找一些替代方法。参考网上linux的解决方案，毕竟os x还是不太一样，到下面这一步就是不行，把haribote.bin写到myos.img。</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> makefile</span></span><br><span class=\"line\">mkdir -p /tmp/floppy</span><br><span class=\"line\">mount -o loop myos.img /tmp/floppy -o fat=12 # 这一条在mac不适用</span><br><span class=\"line\">sleep 1</span><br><span class=\"line\">cp haribote.bin /tmp/floppy</span><br><span class=\"line\">sleep 1</span><br><span class=\"line\">umount /tmp/floppy</span><br></pre></td></tr></table></figure>\n\n<p>Os x 上貌似不能直接mount，当然我也尝试了一些其他的办法，最终失败，感觉不太想再花时间在细枝末节上，打算回法国后在ubuntu下开发，或者和书上保持一样，在win下开发。</p>\n<p>*后面通过Docker解决</p>\n<h1 id=\"Day2\"><a href=\"#Day2\" class=\"headerlink\" title=\"Day2:\"></a>Day2:</h1><p>第二天开始阅读《ORANGE’S：一个操作系统的实现》，并打算先阅读，等了解大致情况再动手，不然也只是抄代码，并且在一知半解的情况下容易卡在一些细枝末节的地方。</p>\n<p>同时也可以参考xv6，这个是MIT用于教学写的一个类unix的OS，有配套的课程。</p>\n<p>两天下来感觉mac用于web开发十分方便，但是内核开发感觉网上的文档、案例偏少，当然主要也是我比较弱，不能够举一反三。一般来说，要不是面向初学者如《30天》而采用win，要不就是使用linux（果然linux才是王道= =）</p>\n<h1 id=\"Day3\"><a href=\"#Day3\" class=\"headerlink\" title=\"Day3:\"></a>Day3:</h1><p>目前跟着ORANGE，暂时比较顺利，开发环境仍然是mac。基于freedos下成功进入了保护模式。</p>\n<p>下面就保护模式做一点笔记。</p>\n<p>关于保护模式网上的资料很多，在为什么需要保护模式这个问题上，这里有一个非常直接的要求——我们的系统要运行在32位上，实模式仅支持16位寻址。当然保护模式远远不止如此，但是对于初学者（比如现在的我，囧），它应该是最大的用途之一。</p>\n<ul>\n<li><p>GDT</p>\n<p> 首先要先说一下GDT(Global Descriptor Table)，它是全局描述符表。GDT只有一张，位置任意，通过LGDT指令被存储在GDTR寄存器。</p>\n</li>\n<li><p>Selector</p>\n<p> 由GDTR访问全局描述符表是通过“段选择子”（实模式下的段寄存器）来完成的。</p>\n</li>\n<li><p>LDT</p>\n<p> 局部描述符表LDT(Local Descriptor Table)。和GDT类似，直观上与GDT结构相同（其段选择子TI置位），功能上隶属于GDT。GDT只有一张，LDT有多张，每个任务可以有一张。</p>\n</li>\n</ul>\n<p>如果第一次听到上面几个名词，可能还是比较懵逼。我们看看它是怎么运作的。</p>\n<ol>\n<li>进行一系列初始化，包括定义GDT，LDT，选择子等</li>\n<li>系统默认进入实模式</li>\n<li>加载GDT，进入保护模式</li>\n<li>加载不同的LDT，以进入不同的子任务</li>\n<li>退出保护模式，反回实模式。</li>\n</ol>\n<p>上面只列出了关键步骤，实际上还有很多细节问题，这里只是简单记录其大致思想，所以没有提。</p>\n<p>不管怎么说，姑且是跨入了保护模式的大门。</p>\n<h1 id=\"Day4\"><a href=\"#Day4\" class=\"headerlink\" title=\"Day4:\"></a>Day4:</h1><h3 id=\"特权级\"><a href=\"#特权级\" class=\"headerlink\" title=\"特权级\"></a>特权级</h3><p>在IA32分段机制中，特权级有4个特权级别，</p>\n<ul>\n<li>LEVEL 0: 内核</li>\n<li>LEVEL 1, 2: 服务</li>\n<li>LEVEL 3: 应用程序</li>\n</ul>\n<p>处理器通过识别CPL(current privilege level)、DPL(descriptor privilege level)、RPL(requested privilege level)这三个特权级进行特权级检验。</p>\n<p>CPL一般被存储在cs和ss的第0位和第1位。CPL等于所在代码段的特权级。当遇到一致代码段时，CPL不改变。（一致代码段能被小于等于它的特权的代码访问）</p>\n<p>DPL是段或门的特权级，写在描述符的属性中。</p>\n<p>RPL是通过选择子的第0位和第1位表现的。</p>\n<p>简单而言，只要CPL和RPL都小于被访问的数据段的DPL就可以了。</p>\n<h3 id=\"门\"><a href=\"#门\" class=\"headerlink\" title=\"门\"></a>门</h3><p>特权级转移可以分为两大类。jmp和call的直接转移；通过描述符间接转移。由于直接转移有诸多的限制，我们常常使用间接转移。</p>\n<p>其中，间接转移又分为：</p>\n<ul>\n<li>指向一个包含目标代码段选择子的门描述符</li>\n<li>指向一个包含目标代码段选择子的TSS</li>\n<li>指向一个任务门，它又指向一个包含目标代码段选择子的TSS</li>\n</ul>\n<p>门描述符分为四种：</p>\n<ul>\n<li>调用门 Call gate</li>\n<li>中断门 Interrupt gate</li>\n<li>陷阱门 Trap gate</li>\n<li>任务门 Task gate</li>\n</ul>\n<h1 id=\"Day5\"><a href=\"#Day5\" class=\"headerlink\" title=\"Day5:\"></a>Day5:</h1><h3 id=\"页表\"><a href=\"#页表\" class=\"headerlink\" title=\"页表\"></a>页表</h3><p>实模式下，int 15h 获得内存信息</p>\n<p>页表这一块看的有点心急，感觉理解不是很到位。以后等弄的更清楚了再来总结。</p>\n<p>中途调试的时候总是出问题，没想到最后发现一开始32位代码的权限就定义错了。从现在出错的经历来看，os如果崩了的话，一般是权限问题（可能是因为我现在写的内容不多吧）。所以务必小心检查权限。</p>\n<p>书上的代码有一点不是很理解，在PagingDemo它使用了大量的push却没有pop。据我的理解的话，push是把数据存到栈中，而无法改变栈外的内存，它也没有使用ret之类的与栈相关的指令，目前不是很理解。</p>\n<p>不理解细节也没办法，先试着看看后面的内容吧。</p>\n<h3 id=\"中断和异常\"><a href=\"#中断和异常\" class=\"headerlink\" title=\"中断和异常\"></a>中断和异常</h3><p>IDT，中断描述符表。IDT中的描述符可以是：</p>\n<ul>\n<li>中断门描述符</li>\n<li>陷阱门描述符</li>\n<li>任务门描述符</li>\n</ul>\n<h1 id=\"Day6\"><a href=\"#Day6\" class=\"headerlink\" title=\"Day6:\"></a>Day6:</h1><p>写着是第六天，实际应该是好久之后了= =</p>\n<p>期间已经实现了中断和异常的初始化，并能捕获异常，占个位置以后再详细写。</p>\n<p>现在着手实现进程。</p>\n<p><strong>进场表、进程体、GDT和TSS的关系：</strong></p>\n<ol>\n<li>进程表和GDT。进程表内的LDT选择子对应一个GDT中的描述符，也就是说每一个进程表对应一个GDT中的描述符。</li>\n<li>进程表和进程。进程表用于描述进程。</li>\n<li>GDT和TSS。GDT中有一个描述符对应TSS。</li>\n</ol>\n<p><strong>初始化进场表、进程体、GDT和TSS：</strong></p>\n<ol>\n<li><p>准备任意进程体。</p>\n<p>任意函数即可。</p>\n</li>\n<li><p>初始化进程表。</p>\n<p>进程表结构定义在process.h。在global.c中声明一个全局进程表，它是一个进程的数组，包含所有进程。</p>\n<p>在新建进程前，需要初始化进程表。</p>\n<p>填充GDT中进程LDT的描述符，在protect.c。</p>\n</li>\n<li><p>准备GDT和TSS。</p>\n<p>TSS结构定义在protect.h。</p>\n<p>填充GDT中TSS描述符，在protect.c。</p>\n<p>TSS准备好了，然后加载tr，在kernel.asm。</p>\n</li>\n</ol>\n<p><strong>从启动时间顺序上，第一个进程的启动过程如下：</strong></p>\n<p>准备进程体</p>\n<p>=&gt; 初始化GDT中的TSS和LDT两个描述符，初始化TSS</p>\n<p>=&gt; 准备进程表</p>\n<p>=&gt; 完成跳转 ring0 -&gt; ring1</p>\n<h3 id=\"添加一个任务的总结\"><a href=\"#添加一个任务的总结\" class=\"headerlink\" title=\"添加一个任务的总结\"></a>添加一个任务的总结</h3><p>按orange书所说，或参考minix，建立了task_table包含所有进程信息，通过遍历它初始化进程。</p>\n<ol>\n<li>添加一个进程体，即一个函数，并声明它 (一般在proto.h中声明)</li>\n<li>在task_table中添加一个进程 (global.c)</li>\n<li>添加宏：修改进程数量(++)和定义堆栈空间 (process.h)</li>\n<li>再次编译即可</li>\n</ol>\n"},{"title":"machine learning","date":"2016-12-12T14:04:47.000Z","_content":"\n# 笔记\n\n这里先占个位，等有空来填。\n\n# 参考\n\n[轻松看懂机器学习十大常用算法](http://blog.jobbole.com/108395/?utm_source=blog.jobbole.com&utm_medium=relatedPosts)","source":"_posts/machine-learning.md","raw":"---\ntitle: machine learning\ndate: 2016-12-12 22:04:47\ncategories: [programming, unfinished]\ntags: [machine-learning]\n---\n\n# 笔记\n\n这里先占个位，等有空来填。\n\n# 参考\n\n[轻松看懂机器学习十大常用算法](http://blog.jobbole.com/108395/?utm_source=blog.jobbole.com&utm_medium=relatedPosts)","slug":"machine-learning","published":1,"updated":"2016-12-12T21:07:40.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckw4qufqc0040gwtlcxv3hxw6","content":"<h1 id=\"笔记\"><a href=\"#笔记\" class=\"headerlink\" title=\"笔记\"></a>笔记</h1><p>这里先占个位，等有空来填。</p>\n<h1 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h1><p><a href=\"http://blog.jobbole.com/108395/?utm_source=blog.jobbole.com&amp;utm_medium=relatedPosts\">轻松看懂机器学习十大常用算法</a></p>\n","site":{"data":{"recommended_posts":{"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"}],"https://LorrinWWW.github.io/posts/complexity/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/hexo-with-latex/":[{"title":"区块链时代的最正确的打赏姿势——在Hexo博客配置Coinhive网页挖矿","permalink":"http://www.davidfnck.com/blockchain/mine-xmr-in-hexo-by-coinhive.html/"}],"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/":[{"title":"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-statistic/":[{"title":"贝叶斯估计 Bayes estimation","permalink":"https://LorrinWWW.github.io/posts/Bayes-estimation/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"}],"https://LorrinWWW.github.io/posts/[2018.2.5]Nested-LSTMs/":[{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"}],"https://LorrinWWW.github.io/posts/datamining-pretreatment/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Several models for knowledge graph representing and completing 几个知识图谱模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/"}],"https://LorrinWWW.github.io/posts/compression/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/Sobolev-space/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/proba-ch5/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch3/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/proba-ch1/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/ML-CNN/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/EDP-basic-models/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/machine-learning/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/datamining-class-pred/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction/":[],"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/":[{"title":"OS notes","permalink":"https://LorrinWWW.github.io/posts/OS-notes/"}],"https://LorrinWWW.github.io/posts/projet-enjeu-plugin-chrome-101/":[{"title":"Chrome油猴脚本","permalink":"http://gubangzhong.cn/2017/06/13/Chrome油猴脚本/"}],"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/":[{"title":"MongoDB, Docker and Python","permalink":"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/":[{"title":"Hands on Scrapy","permalink":"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/OS-notes/":[{"title":"learning OS and building LorriOS","permalink":"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/"}],"https://LorrinWWW.github.io/posts/Bayes-estimation/":[{"title":"Note of statistic","permalink":"https://LorrinWWW.github.io/posts/Note-of-statistic/"}],"https://LorrinWWW.github.io/posts/management-of-the-firm/":[],"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"}],"https://LorrinWWW.github.io/posts/datamining-qualitative-induction/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-finite-element-method/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/QuadTree/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/proba-ch4/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch2/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/":[{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/Note-of-NLP/":[{"title":"Generative Adversarial Network","permalink":"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/"},{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"}],"https://LorrinWWW.github.io/posts/Note-of-datamining/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"数据挖掘-分类与预测","permalink":"https://LorrinWWW.github.io/posts/datamining-class-pred/"},{"title":"数据挖掘-预处理","permalink":"https://LorrinWWW.github.io/posts/datamining-pretreatment/"}],"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/Note-of-probability/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/participe-present-et-gerondif/":[],"https://LorrinWWW.github.io/posts/Hilbert-space/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/proba-ch6/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}]}}},"excerpt":"","more":"<h1 id=\"笔记\"><a href=\"#笔记\" class=\"headerlink\" title=\"笔记\"></a>笔记</h1><p>这里先占个位，等有空来填。</p>\n<h1 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h1><p><a href=\"http://blog.jobbole.com/108395/?utm_source=blog.jobbole.com&utm_medium=relatedPosts\">轻松看懂机器学习十大常用算法</a></p>\n"},{"title":"企业管理复习笔记","date":"2016-12-10T09:48:37.000Z","_content":"\nThis is the note of learning \"Management of the firm\", with the book of GESTION (ecole centrale paris).\n\n# The firm and the management\n\n## the firm\n\n1. the definition of the firm\n\n   - The company is an economic entity or a place of creation of value\n   - It designs and / or produces and / or distributes goods and services to meet the demand of CUSTOMERS on MARKETS.\n   - It uses (destroys) resources (mobilized from partners)\n   - it generates positive or negative externalities on its environment\n2. the different types of resources used by the company\n\n   - The work, provided by employees who sell their working time\n   - Financial capital, contributed on a perpetual basis by shareholders or temporary by banks, capital is used to acquire or develop:\n     1. Tangible resources (实体资源)\n     2. Intangible resources such as cognitive resources (knowledge or knowledge, patents or technologies) or brands\n     3. the natural resources (materials and energy) transformed by the company in its process or incorporated in its equipment\n3. the main functional areas (or functions) of a company and their objective (point not covered in the course but seen in case study!)\n\n   - Design or R&D\n   - Manufacturing or production\n   - Marketing / sales\n   - Finance\n\n\n4. the importance of the company's relations with its partners in the framework of contracts and with its stakeholders more generally.\n\n   - The customers to whom its offers are addressed, the people who carry out its activities and the shareholders who bring the capital and hold the company's share capital\n   - With the sectoral communities (suppliers, distributors ...), economic (financiers, prescribers, ...) and social (legislation, populations);\n\n\n## The management\n\n1. the definitions of management\n\n   - Manages a planning / organization function and a control function (animation and evaluation) of the activity.\n   - POCCC: prevoir, organiser, commander, coordonner, controle\n\n2. the difference between the operational mode and the strategic management mode\n\n   - manage operationally :\n\n     to ensure that one does well what one has to do (doing the things right), or it means making good use of resources to reach the objective, or it is to seek efficiency.\n\n   - manage strategically:\n\n     to make sure that you do the right things (doing the right thing)\n\n     That is to say, to choose the assets and the areas where to invest is to build the potential of the company and ensure that it has the relevant resources, this is reflected in the company's balance sheet (stock).\n\n# Marketing\n\n**Things important:**\n\n- PESTEL:\n  - ​Policy, especially government stability and regulations.\n  - Economic, in particular state of the economic situation and economic situation\n  - Sociocultural, particularly demography and changing lifestyles.\n  - Technological, in particular public and private R&D expenditure.\n  - Ecological, in particular legislation on the protection of the environment\n  - Legal, in particular the law of competition and the law of labor.\n\n- SWOT model: \n  - Strengths - Internal\n  - Weaknesses - Internal\n  - Opportunities - External\n  - Threats - External\n\n- Marketing mix (营销组合)\n\n  4P => 4C: \n\n  - Product => Consumer wants and needs\n  - Price => Cost\n  - Promotion => communication\n  - Place (distribution) => Convenience\n\n1. the definition of marketing\n\n   - Aggregate of methods to adapt its offer to changing demand\n     - Assess and anticipate relevant changes \n     - Understand customers' needs and desires\n     - Act on supply and its perception\n   - But also to orient the behavior of various publics (consumers, distributors, public authorities) in a way favorable to the company.\n\n\n2. The difference between marketing and sales\n\n   - The marketing aims to facilitate and accompany the act of sale,\n\n   - The marketing Collects / Synthesizes Customer and Market Information\n\n   - The marketing helps to define offers (products / services) adapted to the customers\n\n   - The sales representative is in charge of the act of sale and the relationship with customers\n\n     Marketing provides elements for sales support.\n\n3. ​The distinction between strategic and operational marketing\n\n   - Strategic marketing is devoted to the conception of the offer: \n\n     It covers the choice of targets, the analysis of needs, the evaluation of competing offers, the generation and the collection of ideas for solutions, the drafting of specifications (Marketing briefs), estimation of forecast volumes and margins, and the launch plan.\n\n   - Operational marketing refers to the activity of preparation and support to the sales effort, once the offer is constituted. Efforts then was given to the choice of distribution channels, on communication, on the construction of sales pitches and support documents, on the definition of price levels, on the accompaniment and monitoring of sales forces.\n\n4. Market segmentation\n\n   The process of dividing markets comprising the heterogeneous needs of many consumers into segments comprising the homogeneous needs of smaller groups\n\n# Strategy\n\n**things important:**\n\n- Tools and Analysis Methods\n\n| Level              | Internal Analysis                        | External Analysis                        |\n| ------------------ | ---------------------------------------- | ---------------------------------------- |\n| Corporate strategy | <p>Management system analysis</p><p>BCG Matrix</p><p>Resources and competence analysis</p> | PESTEL                                   |\n| Business strategy  | <p>Value chain</p><p>General business strategy</p><p>Resources and competence of each domain of activity</p> | <p>Porter’s 5 forces model</p><p>Strategic group mapping</p><p>Product life cycle</p><p>Key success factors</p> |\n\n- **Porter’s 5 forces mode**\n\n  - Suppliers                ->     induxtry competitors\n  - Potential entrants ->\n  - Buyers                     ->\n  - Substitutes             ->\n\n- Strategic group map\n\n  Something like that below:\n\n  ^(high)\n\n  |            O\n\n  |                                                   O\n\n  |(low)———————————————————————>(high)\n\n- Product life circle (PLC)\n\n  1. market development\n  2. growth\n  3. maturity\n  4. decline\n\n- Key success factors\n\n  The factors we must have to compete in a market.\n\n  The rule of the game common to all players\n\n  The necessary conditions to compete in a market.\n\n  1. Segnentation \n  2. DAS (战略分析方法)\n  3. Stracgical activities areas\n\n- **Value chain analysis**\n\n| Support activities     |\n| ---------------------- |\n| Firm infrastructure    |\n| HR                     |\n| Technology Development |\n| Procurement            |\n\n  **Primary activities:**\n\n| Inbound logistics | Outbound logistics | Operations | Marketing | Service | Design |\n| ----------------- | ------------------ | ---------- | --------- | ------- | ------ |\n| ……                | ……                 | ……         | ……        | ……      | ……     |\n\n- BCG matrix\n\n  market growth\n\n  ^(high)\n\n  |                       **Star**                                    **?**\n\n  |\n\n  |                        **Cow**                                   **Dog**(狗带)\n\n  |                                                \n\n  |(low)(high)———————————————————————>(low) market share\n\n1. Definition of strategy\n\n   A firm’s theory about how to excel in the game it is playing\n\n   A firm’s theory about how to create a unique position in the markets and industries within which it is operating\n\n2. Competitive advantage: doing different things\n\n3. Resource-based view (internal analysis)\n\n   - Human \n   - Physical \n   - Financial \n   - Organizational \n\n# Development of the firm\n\nGrowth orientations:\n\n1. Integration — 一体化\n2. Diversification — 多样化\n3. International strategies — 国际战略\n\nModes of growth:\n\n1. Internal = organic growth\n   - Based on own funds\n   - Slower\n2. External\n   - Rapid market share, or competency gain\n   - Accelerator to grow internationally\n\n# Organizational structure\n\n1. Simple structure\n\n   Owner/Director -> Employees\n\n   - Taylorism\n   - Fayol\n\n2. Complex structure\n\n   - Functional\n\n     Ex: Finance, R&D, Communication, IT\n\n     - Advantages:\n\n       – Specialization\n\n       – Accumulation of experience\n\n     - Disadvantages:\n\n       – Coordination and collaboration\n\n   - Divisional\n\n     每个产品都有自己独立的研发销售部门。\n\n     - Advantages\n\n       – Coordination between functions\n\n       – Responsibility of results better defined\n\n     - Disadvantages\n\n       – Problem of reinventing the wheel\n\n       – Internal competition\n\n   - Staff and line\n\n     专门分出一个“staff”来协调各部门。\n\n     - Advantages:\n\n       – Specialized expertise\n\n     - Disadvantages:\n\n       – Conflict between staff and line\n\n   - Matrix \n\n     产品部门和各专业职能垂直构建。\n\n     - Advantages\n\n       – Specialization and coordination are facilitated\n\n     - Disadvantages\n\n       – Each employee has two bosses  \n\n       – Decision making\n","source":"_posts/management-of-the-firm.md","raw":"---\ntitle: 企业管理复习笔记\ndate: 2016-12-10 17:48:37\ncategories: other\ntags: [management, firm]\n---\n\nThis is the note of learning \"Management of the firm\", with the book of GESTION (ecole centrale paris).\n\n# The firm and the management\n\n## the firm\n\n1. the definition of the firm\n\n   - The company is an economic entity or a place of creation of value\n   - It designs and / or produces and / or distributes goods and services to meet the demand of CUSTOMERS on MARKETS.\n   - It uses (destroys) resources (mobilized from partners)\n   - it generates positive or negative externalities on its environment\n2. the different types of resources used by the company\n\n   - The work, provided by employees who sell their working time\n   - Financial capital, contributed on a perpetual basis by shareholders or temporary by banks, capital is used to acquire or develop:\n     1. Tangible resources (实体资源)\n     2. Intangible resources such as cognitive resources (knowledge or knowledge, patents or technologies) or brands\n     3. the natural resources (materials and energy) transformed by the company in its process or incorporated in its equipment\n3. the main functional areas (or functions) of a company and their objective (point not covered in the course but seen in case study!)\n\n   - Design or R&D\n   - Manufacturing or production\n   - Marketing / sales\n   - Finance\n\n\n4. the importance of the company's relations with its partners in the framework of contracts and with its stakeholders more generally.\n\n   - The customers to whom its offers are addressed, the people who carry out its activities and the shareholders who bring the capital and hold the company's share capital\n   - With the sectoral communities (suppliers, distributors ...), economic (financiers, prescribers, ...) and social (legislation, populations);\n\n\n## The management\n\n1. the definitions of management\n\n   - Manages a planning / organization function and a control function (animation and evaluation) of the activity.\n   - POCCC: prevoir, organiser, commander, coordonner, controle\n\n2. the difference between the operational mode and the strategic management mode\n\n   - manage operationally :\n\n     to ensure that one does well what one has to do (doing the things right), or it means making good use of resources to reach the objective, or it is to seek efficiency.\n\n   - manage strategically:\n\n     to make sure that you do the right things (doing the right thing)\n\n     That is to say, to choose the assets and the areas where to invest is to build the potential of the company and ensure that it has the relevant resources, this is reflected in the company's balance sheet (stock).\n\n# Marketing\n\n**Things important:**\n\n- PESTEL:\n  - ​Policy, especially government stability and regulations.\n  - Economic, in particular state of the economic situation and economic situation\n  - Sociocultural, particularly demography and changing lifestyles.\n  - Technological, in particular public and private R&D expenditure.\n  - Ecological, in particular legislation on the protection of the environment\n  - Legal, in particular the law of competition and the law of labor.\n\n- SWOT model: \n  - Strengths - Internal\n  - Weaknesses - Internal\n  - Opportunities - External\n  - Threats - External\n\n- Marketing mix (营销组合)\n\n  4P => 4C: \n\n  - Product => Consumer wants and needs\n  - Price => Cost\n  - Promotion => communication\n  - Place (distribution) => Convenience\n\n1. the definition of marketing\n\n   - Aggregate of methods to adapt its offer to changing demand\n     - Assess and anticipate relevant changes \n     - Understand customers' needs and desires\n     - Act on supply and its perception\n   - But also to orient the behavior of various publics (consumers, distributors, public authorities) in a way favorable to the company.\n\n\n2. The difference between marketing and sales\n\n   - The marketing aims to facilitate and accompany the act of sale,\n\n   - The marketing Collects / Synthesizes Customer and Market Information\n\n   - The marketing helps to define offers (products / services) adapted to the customers\n\n   - The sales representative is in charge of the act of sale and the relationship with customers\n\n     Marketing provides elements for sales support.\n\n3. ​The distinction between strategic and operational marketing\n\n   - Strategic marketing is devoted to the conception of the offer: \n\n     It covers the choice of targets, the analysis of needs, the evaluation of competing offers, the generation and the collection of ideas for solutions, the drafting of specifications (Marketing briefs), estimation of forecast volumes and margins, and the launch plan.\n\n   - Operational marketing refers to the activity of preparation and support to the sales effort, once the offer is constituted. Efforts then was given to the choice of distribution channels, on communication, on the construction of sales pitches and support documents, on the definition of price levels, on the accompaniment and monitoring of sales forces.\n\n4. Market segmentation\n\n   The process of dividing markets comprising the heterogeneous needs of many consumers into segments comprising the homogeneous needs of smaller groups\n\n# Strategy\n\n**things important:**\n\n- Tools and Analysis Methods\n\n| Level              | Internal Analysis                        | External Analysis                        |\n| ------------------ | ---------------------------------------- | ---------------------------------------- |\n| Corporate strategy | <p>Management system analysis</p><p>BCG Matrix</p><p>Resources and competence analysis</p> | PESTEL                                   |\n| Business strategy  | <p>Value chain</p><p>General business strategy</p><p>Resources and competence of each domain of activity</p> | <p>Porter’s 5 forces model</p><p>Strategic group mapping</p><p>Product life cycle</p><p>Key success factors</p> |\n\n- **Porter’s 5 forces mode**\n\n  - Suppliers                ->     induxtry competitors\n  - Potential entrants ->\n  - Buyers                     ->\n  - Substitutes             ->\n\n- Strategic group map\n\n  Something like that below:\n\n  ^(high)\n\n  |            O\n\n  |                                                   O\n\n  |(low)———————————————————————>(high)\n\n- Product life circle (PLC)\n\n  1. market development\n  2. growth\n  3. maturity\n  4. decline\n\n- Key success factors\n\n  The factors we must have to compete in a market.\n\n  The rule of the game common to all players\n\n  The necessary conditions to compete in a market.\n\n  1. Segnentation \n  2. DAS (战略分析方法)\n  3. Stracgical activities areas\n\n- **Value chain analysis**\n\n| Support activities     |\n| ---------------------- |\n| Firm infrastructure    |\n| HR                     |\n| Technology Development |\n| Procurement            |\n\n  **Primary activities:**\n\n| Inbound logistics | Outbound logistics | Operations | Marketing | Service | Design |\n| ----------------- | ------------------ | ---------- | --------- | ------- | ------ |\n| ……                | ……                 | ……         | ……        | ……      | ……     |\n\n- BCG matrix\n\n  market growth\n\n  ^(high)\n\n  |                       **Star**                                    **?**\n\n  |\n\n  |                        **Cow**                                   **Dog**(狗带)\n\n  |                                                \n\n  |(low)(high)———————————————————————>(low) market share\n\n1. Definition of strategy\n\n   A firm’s theory about how to excel in the game it is playing\n\n   A firm’s theory about how to create a unique position in the markets and industries within which it is operating\n\n2. Competitive advantage: doing different things\n\n3. Resource-based view (internal analysis)\n\n   - Human \n   - Physical \n   - Financial \n   - Organizational \n\n# Development of the firm\n\nGrowth orientations:\n\n1. Integration — 一体化\n2. Diversification — 多样化\n3. International strategies — 国际战略\n\nModes of growth:\n\n1. Internal = organic growth\n   - Based on own funds\n   - Slower\n2. External\n   - Rapid market share, or competency gain\n   - Accelerator to grow internationally\n\n# Organizational structure\n\n1. Simple structure\n\n   Owner/Director -> Employees\n\n   - Taylorism\n   - Fayol\n\n2. Complex structure\n\n   - Functional\n\n     Ex: Finance, R&D, Communication, IT\n\n     - Advantages:\n\n       – Specialization\n\n       – Accumulation of experience\n\n     - Disadvantages:\n\n       – Coordination and collaboration\n\n   - Divisional\n\n     每个产品都有自己独立的研发销售部门。\n\n     - Advantages\n\n       – Coordination between functions\n\n       – Responsibility of results better defined\n\n     - Disadvantages\n\n       – Problem of reinventing the wheel\n\n       – Internal competition\n\n   - Staff and line\n\n     专门分出一个“staff”来协调各部门。\n\n     - Advantages:\n\n       – Specialized expertise\n\n     - Disadvantages:\n\n       – Conflict between staff and line\n\n   - Matrix \n\n     产品部门和各专业职能垂直构建。\n\n     - Advantages\n\n       – Specialization and coordination are facilitated\n\n     - Disadvantages\n\n       – Each employee has two bosses  \n\n       – Decision making\n","slug":"management-of-the-firm","published":1,"updated":"2016-12-13T21:26:46.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckw4qufqc0042gwtlfhf76k0h","content":"<p>This is the note of learning “Management of the firm”, with the book of GESTION (ecole centrale paris).</p>\n<h1 id=\"The-firm-and-the-management\"><a href=\"#The-firm-and-the-management\" class=\"headerlink\" title=\"The firm and the management\"></a>The firm and the management</h1><h2 id=\"the-firm\"><a href=\"#the-firm\" class=\"headerlink\" title=\"the firm\"></a>the firm</h2><ol>\n<li><p>the definition of the firm</p>\n<ul>\n<li>The company is an economic entity or a place of creation of value</li>\n<li>It designs and / or produces and / or distributes goods and services to meet the demand of CUSTOMERS on MARKETS.</li>\n<li>It uses (destroys) resources (mobilized from partners)</li>\n<li>it generates positive or negative externalities on its environment</li>\n</ul>\n</li>\n<li><p>the different types of resources used by the company</p>\n<ul>\n<li>The work, provided by employees who sell their working time</li>\n<li>Financial capital, contributed on a perpetual basis by shareholders or temporary by banks, capital is used to acquire or develop:<ol>\n<li>Tangible resources (实体资源)</li>\n<li>Intangible resources such as cognitive resources (knowledge or knowledge, patents or technologies) or brands</li>\n<li>the natural resources (materials and energy) transformed by the company in its process or incorporated in its equipment</li>\n</ol>\n</li>\n</ul>\n</li>\n<li><p>the main functional areas (or functions) of a company and their objective (point not covered in the course but seen in case study!)</p>\n<ul>\n<li>Design or R&amp;D</li>\n<li>Manufacturing or production</li>\n<li>Marketing / sales</li>\n<li>Finance</li>\n</ul>\n</li>\n</ol>\n<ol start=\"4\">\n<li><p>the importance of the company’s relations with its partners in the framework of contracts and with its stakeholders more generally.</p>\n<ul>\n<li>The customers to whom its offers are addressed, the people who carry out its activities and the shareholders who bring the capital and hold the company’s share capital</li>\n<li>With the sectoral communities (suppliers, distributors …), economic (financiers, prescribers, …) and social (legislation, populations);</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"The-management\"><a href=\"#The-management\" class=\"headerlink\" title=\"The management\"></a>The management</h2><ol>\n<li><p>the definitions of management</p>\n<ul>\n<li>Manages a planning / organization function and a control function (animation and evaluation) of the activity.</li>\n<li>POCCC: prevoir, organiser, commander, coordonner, controle</li>\n</ul>\n</li>\n<li><p>the difference between the operational mode and the strategic management mode</p>\n<ul>\n<li><p>manage operationally :</p>\n<p>to ensure that one does well what one has to do (doing the things right), or it means making good use of resources to reach the objective, or it is to seek efficiency.</p>\n</li>\n<li><p>manage strategically:</p>\n<p>to make sure that you do the right things (doing the right thing)</p>\n<p>That is to say, to choose the assets and the areas where to invest is to build the potential of the company and ensure that it has the relevant resources, this is reflected in the company’s balance sheet (stock).</p>\n</li>\n</ul>\n</li>\n</ol>\n<h1 id=\"Marketing\"><a href=\"#Marketing\" class=\"headerlink\" title=\"Marketing\"></a>Marketing</h1><p><strong>Things important:</strong></p>\n<ul>\n<li><p>PESTEL:</p>\n<ul>\n<li>​Policy, especially government stability and regulations.</li>\n<li>Economic, in particular state of the economic situation and economic situation</li>\n<li>Sociocultural, particularly demography and changing lifestyles.</li>\n<li>Technological, in particular public and private R&amp;D expenditure.</li>\n<li>Ecological, in particular legislation on the protection of the environment</li>\n<li>Legal, in particular the law of competition and the law of labor.</li>\n</ul>\n</li>\n<li><p>SWOT model: </p>\n<ul>\n<li>Strengths - Internal</li>\n<li>Weaknesses - Internal</li>\n<li>Opportunities - External</li>\n<li>Threats - External</li>\n</ul>\n</li>\n<li><p>Marketing mix (营销组合)</p>\n<p>4P =&gt; 4C: </p>\n<ul>\n<li>Product =&gt; Consumer wants and needs</li>\n<li>Price =&gt; Cost</li>\n<li>Promotion =&gt; communication</li>\n<li>Place (distribution) =&gt; Convenience</li>\n</ul>\n</li>\n</ul>\n<ol>\n<li><p>the definition of marketing</p>\n<ul>\n<li>Aggregate of methods to adapt its offer to changing demand<ul>\n<li>Assess and anticipate relevant changes </li>\n<li>Understand customers’ needs and desires</li>\n<li>Act on supply and its perception</li>\n</ul>\n</li>\n<li>But also to orient the behavior of various publics (consumers, distributors, public authorities) in a way favorable to the company.</li>\n</ul>\n</li>\n</ol>\n<ol start=\"2\">\n<li><p>The difference between marketing and sales</p>\n<ul>\n<li><p>The marketing aims to facilitate and accompany the act of sale,</p>\n</li>\n<li><p>The marketing Collects / Synthesizes Customer and Market Information</p>\n</li>\n<li><p>The marketing helps to define offers (products / services) adapted to the customers</p>\n</li>\n<li><p>The sales representative is in charge of the act of sale and the relationship with customers</p>\n<p>Marketing provides elements for sales support.</p>\n</li>\n</ul>\n</li>\n<li><p>​The distinction between strategic and operational marketing</p>\n<ul>\n<li><p>Strategic marketing is devoted to the conception of the offer: </p>\n<p>It covers the choice of targets, the analysis of needs, the evaluation of competing offers, the generation and the collection of ideas for solutions, the drafting of specifications (Marketing briefs), estimation of forecast volumes and margins, and the launch plan.</p>\n</li>\n<li><p>Operational marketing refers to the activity of preparation and support to the sales effort, once the offer is constituted. Efforts then was given to the choice of distribution channels, on communication, on the construction of sales pitches and support documents, on the definition of price levels, on the accompaniment and monitoring of sales forces.</p>\n</li>\n</ul>\n</li>\n<li><p>Market segmentation</p>\n<p>The process of dividing markets comprising the heterogeneous needs of many consumers into segments comprising the homogeneous needs of smaller groups</p>\n</li>\n</ol>\n<h1 id=\"Strategy\"><a href=\"#Strategy\" class=\"headerlink\" title=\"Strategy\"></a>Strategy</h1><p><strong>things important:</strong></p>\n<ul>\n<li>Tools and Analysis Methods</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>Level</th>\n<th>Internal Analysis</th>\n<th>External Analysis</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Corporate strategy</td>\n<td><p>Management system analysis</p><p>BCG Matrix</p><p>Resources and competence analysis</p></td>\n<td>PESTEL</td>\n</tr>\n<tr>\n<td>Business strategy</td>\n<td><p>Value chain</p><p>General business strategy</p><p>Resources and competence of each domain of activity</p></td>\n<td><p>Porter’s 5 forces model</p><p>Strategic group mapping</p><p>Product life cycle</p><p>Key success factors</p></td>\n</tr>\n</tbody></table>\n<ul>\n<li><p><strong>Porter’s 5 forces mode</strong></p>\n<ul>\n<li>Suppliers                -&gt;     induxtry competitors</li>\n<li>Potential entrants -&gt;</li>\n<li>Buyers                     -&gt;</li>\n<li>Substitutes             -&gt;</li>\n</ul>\n</li>\n<li><p>Strategic group map</p>\n<p>Something like that below:</p>\n<p>^(high)</p>\n<p>|            O</p>\n<p>|                                                   O</p>\n<p>|(low)———————————————————————&gt;(high)</p>\n</li>\n<li><p>Product life circle (PLC)</p>\n<ol>\n<li>market development</li>\n<li>growth</li>\n<li>maturity</li>\n<li>decline</li>\n</ol>\n</li>\n<li><p>Key success factors</p>\n<p>The factors we must have to compete in a market.</p>\n<p>The rule of the game common to all players</p>\n<p>The necessary conditions to compete in a market.</p>\n<ol>\n<li>Segnentation </li>\n<li>DAS (战略分析方法)</li>\n<li>Stracgical activities areas</li>\n</ol>\n</li>\n<li><p><strong>Value chain analysis</strong></p>\n</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>Support activities</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Firm infrastructure</td>\n</tr>\n<tr>\n<td>HR</td>\n</tr>\n<tr>\n<td>Technology Development</td>\n</tr>\n<tr>\n<td>Procurement</td>\n</tr>\n</tbody></table>\n<p>  <strong>Primary activities:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Inbound logistics</th>\n<th>Outbound logistics</th>\n<th>Operations</th>\n<th>Marketing</th>\n<th>Service</th>\n<th>Design</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>……</td>\n<td>……</td>\n<td>……</td>\n<td>……</td>\n<td>……</td>\n<td>……</td>\n</tr>\n</tbody></table>\n<ul>\n<li><p>BCG matrix</p>\n<p>market growth</p>\n<p>^(high)</p>\n<p>|                       <strong>Star</strong>                                    <strong>?</strong></p>\n<p>|</p>\n<p>|                        <strong>Cow</strong>                                   <strong>Dog</strong>(狗带)</p>\n<p>|                                                </p>\n<p>|(low)(high)———————————————————————&gt;(low) market share</p>\n</li>\n</ul>\n<ol>\n<li><p>Definition of strategy</p>\n<p>A firm’s theory about how to excel in the game it is playing</p>\n<p>A firm’s theory about how to create a unique position in the markets and industries within which it is operating</p>\n</li>\n<li><p>Competitive advantage: doing different things</p>\n</li>\n<li><p>Resource-based view (internal analysis)</p>\n<ul>\n<li>Human </li>\n<li>Physical </li>\n<li>Financial </li>\n<li>Organizational </li>\n</ul>\n</li>\n</ol>\n<h1 id=\"Development-of-the-firm\"><a href=\"#Development-of-the-firm\" class=\"headerlink\" title=\"Development of the firm\"></a>Development of the firm</h1><p>Growth orientations:</p>\n<ol>\n<li>Integration — 一体化</li>\n<li>Diversification — 多样化</li>\n<li>International strategies — 国际战略</li>\n</ol>\n<p>Modes of growth:</p>\n<ol>\n<li>Internal = organic growth<ul>\n<li>Based on own funds</li>\n<li>Slower</li>\n</ul>\n</li>\n<li>External<ul>\n<li>Rapid market share, or competency gain</li>\n<li>Accelerator to grow internationally</li>\n</ul>\n</li>\n</ol>\n<h1 id=\"Organizational-structure\"><a href=\"#Organizational-structure\" class=\"headerlink\" title=\"Organizational structure\"></a>Organizational structure</h1><ol>\n<li><p>Simple structure</p>\n<p>Owner/Director -&gt; Employees</p>\n<ul>\n<li>Taylorism</li>\n<li>Fayol</li>\n</ul>\n</li>\n<li><p>Complex structure</p>\n<ul>\n<li><p>Functional</p>\n<p>Ex: Finance, R&amp;D, Communication, IT</p>\n<ul>\n<li><p>Advantages:</p>\n<p>– Specialization</p>\n<p>– Accumulation of experience</p>\n</li>\n<li><p>Disadvantages:</p>\n<p>– Coordination and collaboration</p>\n</li>\n</ul>\n</li>\n<li><p>Divisional</p>\n<p>每个产品都有自己独立的研发销售部门。</p>\n<ul>\n<li><p>Advantages</p>\n<p>– Coordination between functions</p>\n<p>– Responsibility of results better defined</p>\n</li>\n<li><p>Disadvantages</p>\n<p>– Problem of reinventing the wheel</p>\n<p>– Internal competition</p>\n</li>\n</ul>\n</li>\n<li><p>Staff and line</p>\n<p>专门分出一个“staff”来协调各部门。</p>\n<ul>\n<li><p>Advantages:</p>\n<p>– Specialized expertise</p>\n</li>\n<li><p>Disadvantages:</p>\n<p>– Conflict between staff and line</p>\n</li>\n</ul>\n</li>\n<li><p>Matrix </p>\n<p>产品部门和各专业职能垂直构建。</p>\n<ul>\n<li><p>Advantages</p>\n<p>– Specialization and coordination are facilitated</p>\n</li>\n<li><p>Disadvantages</p>\n<p>– Each employee has two bosses  </p>\n<p>– Decision making</p>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ol>\n","site":{"data":{"recommended_posts":{"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"}],"https://LorrinWWW.github.io/posts/complexity/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/hexo-with-latex/":[{"title":"区块链时代的最正确的打赏姿势——在Hexo博客配置Coinhive网页挖矿","permalink":"http://www.davidfnck.com/blockchain/mine-xmr-in-hexo-by-coinhive.html/"}],"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/":[{"title":"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-statistic/":[{"title":"贝叶斯估计 Bayes estimation","permalink":"https://LorrinWWW.github.io/posts/Bayes-estimation/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"}],"https://LorrinWWW.github.io/posts/[2018.2.5]Nested-LSTMs/":[{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"}],"https://LorrinWWW.github.io/posts/datamining-pretreatment/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Several models for knowledge graph representing and completing 几个知识图谱模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/"}],"https://LorrinWWW.github.io/posts/compression/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/Sobolev-space/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/proba-ch5/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch3/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/proba-ch1/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/ML-CNN/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/EDP-basic-models/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/machine-learning/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/datamining-class-pred/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction/":[],"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/":[{"title":"OS notes","permalink":"https://LorrinWWW.github.io/posts/OS-notes/"}],"https://LorrinWWW.github.io/posts/projet-enjeu-plugin-chrome-101/":[{"title":"Chrome油猴脚本","permalink":"http://gubangzhong.cn/2017/06/13/Chrome油猴脚本/"}],"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/":[{"title":"MongoDB, Docker and Python","permalink":"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/":[{"title":"Hands on Scrapy","permalink":"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/OS-notes/":[{"title":"learning OS and building LorriOS","permalink":"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/"}],"https://LorrinWWW.github.io/posts/Bayes-estimation/":[{"title":"Note of statistic","permalink":"https://LorrinWWW.github.io/posts/Note-of-statistic/"}],"https://LorrinWWW.github.io/posts/management-of-the-firm/":[],"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"}],"https://LorrinWWW.github.io/posts/datamining-qualitative-induction/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-finite-element-method/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/QuadTree/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/proba-ch4/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch2/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/":[{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/Note-of-NLP/":[{"title":"Generative Adversarial Network","permalink":"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/"},{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"}],"https://LorrinWWW.github.io/posts/Note-of-datamining/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"数据挖掘-分类与预测","permalink":"https://LorrinWWW.github.io/posts/datamining-class-pred/"},{"title":"数据挖掘-预处理","permalink":"https://LorrinWWW.github.io/posts/datamining-pretreatment/"}],"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/Note-of-probability/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/participe-present-et-gerondif/":[],"https://LorrinWWW.github.io/posts/Hilbert-space/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/proba-ch6/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}]}}},"excerpt":"","more":"<p>This is the note of learning “Management of the firm”, with the book of GESTION (ecole centrale paris).</p>\n<h1 id=\"The-firm-and-the-management\"><a href=\"#The-firm-and-the-management\" class=\"headerlink\" title=\"The firm and the management\"></a>The firm and the management</h1><h2 id=\"the-firm\"><a href=\"#the-firm\" class=\"headerlink\" title=\"the firm\"></a>the firm</h2><ol>\n<li><p>the definition of the firm</p>\n<ul>\n<li>The company is an economic entity or a place of creation of value</li>\n<li>It designs and / or produces and / or distributes goods and services to meet the demand of CUSTOMERS on MARKETS.</li>\n<li>It uses (destroys) resources (mobilized from partners)</li>\n<li>it generates positive or negative externalities on its environment</li>\n</ul>\n</li>\n<li><p>the different types of resources used by the company</p>\n<ul>\n<li>The work, provided by employees who sell their working time</li>\n<li>Financial capital, contributed on a perpetual basis by shareholders or temporary by banks, capital is used to acquire or develop:<ol>\n<li>Tangible resources (实体资源)</li>\n<li>Intangible resources such as cognitive resources (knowledge or knowledge, patents or technologies) or brands</li>\n<li>the natural resources (materials and energy) transformed by the company in its process or incorporated in its equipment</li>\n</ol>\n</li>\n</ul>\n</li>\n<li><p>the main functional areas (or functions) of a company and their objective (point not covered in the course but seen in case study!)</p>\n<ul>\n<li>Design or R&amp;D</li>\n<li>Manufacturing or production</li>\n<li>Marketing / sales</li>\n<li>Finance</li>\n</ul>\n</li>\n</ol>\n<ol start=\"4\">\n<li><p>the importance of the company’s relations with its partners in the framework of contracts and with its stakeholders more generally.</p>\n<ul>\n<li>The customers to whom its offers are addressed, the people who carry out its activities and the shareholders who bring the capital and hold the company’s share capital</li>\n<li>With the sectoral communities (suppliers, distributors …), economic (financiers, prescribers, …) and social (legislation, populations);</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"The-management\"><a href=\"#The-management\" class=\"headerlink\" title=\"The management\"></a>The management</h2><ol>\n<li><p>the definitions of management</p>\n<ul>\n<li>Manages a planning / organization function and a control function (animation and evaluation) of the activity.</li>\n<li>POCCC: prevoir, organiser, commander, coordonner, controle</li>\n</ul>\n</li>\n<li><p>the difference between the operational mode and the strategic management mode</p>\n<ul>\n<li><p>manage operationally :</p>\n<p>to ensure that one does well what one has to do (doing the things right), or it means making good use of resources to reach the objective, or it is to seek efficiency.</p>\n</li>\n<li><p>manage strategically:</p>\n<p>to make sure that you do the right things (doing the right thing)</p>\n<p>That is to say, to choose the assets and the areas where to invest is to build the potential of the company and ensure that it has the relevant resources, this is reflected in the company’s balance sheet (stock).</p>\n</li>\n</ul>\n</li>\n</ol>\n<h1 id=\"Marketing\"><a href=\"#Marketing\" class=\"headerlink\" title=\"Marketing\"></a>Marketing</h1><p><strong>Things important:</strong></p>\n<ul>\n<li><p>PESTEL:</p>\n<ul>\n<li>​Policy, especially government stability and regulations.</li>\n<li>Economic, in particular state of the economic situation and economic situation</li>\n<li>Sociocultural, particularly demography and changing lifestyles.</li>\n<li>Technological, in particular public and private R&amp;D expenditure.</li>\n<li>Ecological, in particular legislation on the protection of the environment</li>\n<li>Legal, in particular the law of competition and the law of labor.</li>\n</ul>\n</li>\n<li><p>SWOT model: </p>\n<ul>\n<li>Strengths - Internal</li>\n<li>Weaknesses - Internal</li>\n<li>Opportunities - External</li>\n<li>Threats - External</li>\n</ul>\n</li>\n<li><p>Marketing mix (营销组合)</p>\n<p>4P =&gt; 4C: </p>\n<ul>\n<li>Product =&gt; Consumer wants and needs</li>\n<li>Price =&gt; Cost</li>\n<li>Promotion =&gt; communication</li>\n<li>Place (distribution) =&gt; Convenience</li>\n</ul>\n</li>\n</ul>\n<ol>\n<li><p>the definition of marketing</p>\n<ul>\n<li>Aggregate of methods to adapt its offer to changing demand<ul>\n<li>Assess and anticipate relevant changes </li>\n<li>Understand customers’ needs and desires</li>\n<li>Act on supply and its perception</li>\n</ul>\n</li>\n<li>But also to orient the behavior of various publics (consumers, distributors, public authorities) in a way favorable to the company.</li>\n</ul>\n</li>\n</ol>\n<ol start=\"2\">\n<li><p>The difference between marketing and sales</p>\n<ul>\n<li><p>The marketing aims to facilitate and accompany the act of sale,</p>\n</li>\n<li><p>The marketing Collects / Synthesizes Customer and Market Information</p>\n</li>\n<li><p>The marketing helps to define offers (products / services) adapted to the customers</p>\n</li>\n<li><p>The sales representative is in charge of the act of sale and the relationship with customers</p>\n<p>Marketing provides elements for sales support.</p>\n</li>\n</ul>\n</li>\n<li><p>​The distinction between strategic and operational marketing</p>\n<ul>\n<li><p>Strategic marketing is devoted to the conception of the offer: </p>\n<p>It covers the choice of targets, the analysis of needs, the evaluation of competing offers, the generation and the collection of ideas for solutions, the drafting of specifications (Marketing briefs), estimation of forecast volumes and margins, and the launch plan.</p>\n</li>\n<li><p>Operational marketing refers to the activity of preparation and support to the sales effort, once the offer is constituted. Efforts then was given to the choice of distribution channels, on communication, on the construction of sales pitches and support documents, on the definition of price levels, on the accompaniment and monitoring of sales forces.</p>\n</li>\n</ul>\n</li>\n<li><p>Market segmentation</p>\n<p>The process of dividing markets comprising the heterogeneous needs of many consumers into segments comprising the homogeneous needs of smaller groups</p>\n</li>\n</ol>\n<h1 id=\"Strategy\"><a href=\"#Strategy\" class=\"headerlink\" title=\"Strategy\"></a>Strategy</h1><p><strong>things important:</strong></p>\n<ul>\n<li>Tools and Analysis Methods</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>Level</th>\n<th>Internal Analysis</th>\n<th>External Analysis</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Corporate strategy</td>\n<td><p>Management system analysis</p><p>BCG Matrix</p><p>Resources and competence analysis</p></td>\n<td>PESTEL</td>\n</tr>\n<tr>\n<td>Business strategy</td>\n<td><p>Value chain</p><p>General business strategy</p><p>Resources and competence of each domain of activity</p></td>\n<td><p>Porter’s 5 forces model</p><p>Strategic group mapping</p><p>Product life cycle</p><p>Key success factors</p></td>\n</tr>\n</tbody></table>\n<ul>\n<li><p><strong>Porter’s 5 forces mode</strong></p>\n<ul>\n<li>Suppliers                -&gt;     induxtry competitors</li>\n<li>Potential entrants -&gt;</li>\n<li>Buyers                     -&gt;</li>\n<li>Substitutes             -&gt;</li>\n</ul>\n</li>\n<li><p>Strategic group map</p>\n<p>Something like that below:</p>\n<p>^(high)</p>\n<p>|            O</p>\n<p>|                                                   O</p>\n<p>|(low)———————————————————————&gt;(high)</p>\n</li>\n<li><p>Product life circle (PLC)</p>\n<ol>\n<li>market development</li>\n<li>growth</li>\n<li>maturity</li>\n<li>decline</li>\n</ol>\n</li>\n<li><p>Key success factors</p>\n<p>The factors we must have to compete in a market.</p>\n<p>The rule of the game common to all players</p>\n<p>The necessary conditions to compete in a market.</p>\n<ol>\n<li>Segnentation </li>\n<li>DAS (战略分析方法)</li>\n<li>Stracgical activities areas</li>\n</ol>\n</li>\n<li><p><strong>Value chain analysis</strong></p>\n</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>Support activities</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>Firm infrastructure</td>\n</tr>\n<tr>\n<td>HR</td>\n</tr>\n<tr>\n<td>Technology Development</td>\n</tr>\n<tr>\n<td>Procurement</td>\n</tr>\n</tbody></table>\n<p>  <strong>Primary activities:</strong></p>\n<table>\n<thead>\n<tr>\n<th>Inbound logistics</th>\n<th>Outbound logistics</th>\n<th>Operations</th>\n<th>Marketing</th>\n<th>Service</th>\n<th>Design</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>……</td>\n<td>……</td>\n<td>……</td>\n<td>……</td>\n<td>……</td>\n<td>……</td>\n</tr>\n</tbody></table>\n<ul>\n<li><p>BCG matrix</p>\n<p>market growth</p>\n<p>^(high)</p>\n<p>|                       <strong>Star</strong>                                    <strong>?</strong></p>\n<p>|</p>\n<p>|                        <strong>Cow</strong>                                   <strong>Dog</strong>(狗带)</p>\n<p>|                                                </p>\n<p>|(low)(high)———————————————————————&gt;(low) market share</p>\n</li>\n</ul>\n<ol>\n<li><p>Definition of strategy</p>\n<p>A firm’s theory about how to excel in the game it is playing</p>\n<p>A firm’s theory about how to create a unique position in the markets and industries within which it is operating</p>\n</li>\n<li><p>Competitive advantage: doing different things</p>\n</li>\n<li><p>Resource-based view (internal analysis)</p>\n<ul>\n<li>Human </li>\n<li>Physical </li>\n<li>Financial </li>\n<li>Organizational </li>\n</ul>\n</li>\n</ol>\n<h1 id=\"Development-of-the-firm\"><a href=\"#Development-of-the-firm\" class=\"headerlink\" title=\"Development of the firm\"></a>Development of the firm</h1><p>Growth orientations:</p>\n<ol>\n<li>Integration — 一体化</li>\n<li>Diversification — 多样化</li>\n<li>International strategies — 国际战略</li>\n</ol>\n<p>Modes of growth:</p>\n<ol>\n<li>Internal = organic growth<ul>\n<li>Based on own funds</li>\n<li>Slower</li>\n</ul>\n</li>\n<li>External<ul>\n<li>Rapid market share, or competency gain</li>\n<li>Accelerator to grow internationally</li>\n</ul>\n</li>\n</ol>\n<h1 id=\"Organizational-structure\"><a href=\"#Organizational-structure\" class=\"headerlink\" title=\"Organizational structure\"></a>Organizational structure</h1><ol>\n<li><p>Simple structure</p>\n<p>Owner/Director -&gt; Employees</p>\n<ul>\n<li>Taylorism</li>\n<li>Fayol</li>\n</ul>\n</li>\n<li><p>Complex structure</p>\n<ul>\n<li><p>Functional</p>\n<p>Ex: Finance, R&amp;D, Communication, IT</p>\n<ul>\n<li><p>Advantages:</p>\n<p>– Specialization</p>\n<p>– Accumulation of experience</p>\n</li>\n<li><p>Disadvantages:</p>\n<p>– Coordination and collaboration</p>\n</li>\n</ul>\n</li>\n<li><p>Divisional</p>\n<p>每个产品都有自己独立的研发销售部门。</p>\n<ul>\n<li><p>Advantages</p>\n<p>– Coordination between functions</p>\n<p>– Responsibility of results better defined</p>\n</li>\n<li><p>Disadvantages</p>\n<p>– Problem of reinventing the wheel</p>\n<p>– Internal competition</p>\n</li>\n</ul>\n</li>\n<li><p>Staff and line</p>\n<p>专门分出一个“staff”来协调各部门。</p>\n<ul>\n<li><p>Advantages:</p>\n<p>– Specialized expertise</p>\n</li>\n<li><p>Disadvantages:</p>\n<p>– Conflict between staff and line</p>\n</li>\n</ul>\n</li>\n<li><p>Matrix </p>\n<p>产品部门和各专业职能垂直构建。</p>\n<ul>\n<li><p>Advantages</p>\n<p>– Specialization and coordination are facilitated</p>\n</li>\n<li><p>Disadvantages</p>\n<p>– Each employee has two bosses  </p>\n<p>– Decision making</p>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ol>\n"},{"title":"participe présent et gérondif","date":"2016-12-06T04:31:56.000Z","_content":"\n# 现在分词(le participe présent)\n\n## 1. 构成\n现在分词没有人称和数的变化。反身动词的现在分词仍然保留原反身动词的反身代词。\n\n去掉直陈式第一人称复数的词尾-ons，另加-ant \n\n- faire : nous faisons \n\n特殊情况：\n- etre-etant \n- savoir-sachant\n\n## 2. 简述用法\n1. 用作定语，紧接在被修饰词之后，相当于qui引导的从句\n   - L'étranger cherche à trouver quelqu'un **connaissant** (=qui conaisse) à la fois français et l'anglais.\n2. 表原因、时间等，作为状语\n   - **Voyant**(=Comme elle voit) que tout le monde est dejà assis,elle va vite à sa place.\n   - **Ayant**(=Comme il a) mal à la tete,il décide de rester au lit.\n\n## 3. 细分\n### I. 现在分词形容词特性的表现\n许多现在分词已经转化成形容词，有性数变化，可以作为定于或表语。\n1. 动词形容词表示一种状态或性质，后不跟宾语或状语\n   - Ses yeux **brillants** disent la convoitises. 他那双闪光的眼睛流露出他贪婪的本性。\n   - Je l'ai trouvé toute **tremblante**. 我发现她浑身颤抖。\n\n2. 在一些习惯性搭配中表示一些潜在的主语\n\n   - Une seance **payante** (=où l'on paie) 一场要收费的演出。\n   - une collation **soupante** (=si copieuse qu'elle tient lieu de souper) 一顿丰盛的小吃\n\n   这里分词的主语与它的被修饰语不同。\n\n3. 少数已变成了纯粹的形容词，书写有所区别\n\n| 现在分词       | 形容词       |\n| ---------- | --------- |\n| provoquant | provocant |\n| fatiguant  | fatigant  |\n| vaquant    | vacant    |\n| naviguant  | navigant  |\n\n\n### II. 现在分词动词特性的表现\n\n现在分词具有动词的性质，可以有宾语或状语。\n\n1. 代替qui引导的关系从句\n   - C'est un film **captivant** les spectateurs. 这是一部非常吸引观众的影片。\n   - Je le vois **lisant**. 我看见他在念书。\n2. 起状语作用\n   - **Prenant** l'escabeau, il s'offoree d'atteindre le dernier rayon. 他拿了一把小梯子乡尽力够上最后一格。\n   - **Croyant** le bureau vide, il entra. 他以为办公室没人所以就进去了。\n3. 表示并列动作\n   - Le train repartit, **courant** vers le Midi. 火车又开动了，向南方驶去。\n4. 依附于另一主语，构成独立分词从句\n   - Midi **sonnant**, on se met à table. 钟敲十二时，我们都上桌用餐。\n\n\n\n\n# 副动词(le gérondif)\n\n## 1. 构成\n副动词没有人称、性数变化\n\n在现在分词前加en就构成副动词 \n\n- faire : en faisant\n\n注：ayant、étant前不能用en，不能构成副动词。\n\n## 2. 用法\n1. 时间状语，表示动作的同时性\n   - N'oubliez pas de fermer la port **en sortant**.出去时别忘了关门。\n   - Ne lis pas **en mangeant**. 不要以便吃饭以一边看书\n2. 方式状语\n   - Elle arriva **en courant**. 她跑来了\n3. 条件状语\n   - **En se levant** plus tot le matin, il n'arrivera pas en retard. 如果早上起的早点，他就不会迟到了。\n4. 让步状语\n   - **En voyant** critiquant, il n'avait nulle intention de nous décourager. 他尽管批评了我们，但毫无使我们气馁之意。\n5. 原因状语\n   - **En voyant** son embarras, l'agent se fit plus aimable. 警察见他不知所措，就变得更加和蔼了。\n\n**\\*副动词表示的动作的延续**\n\n副动词可以和aller合用，表示延续发展的动作，介词en可以省略。\n\n- Sa vue va **en s'affaiblissant**. 他的视力日益衰退。\n\n## 3. 副动词的强化\n\n1. Tous + 副动词表示强化，这种结构常见于书面。\n2. Rien que + 副动词表示“只要…”\n\n\n\n\n\n# 现在分词和副动词的异同\n\n## 共同点\n\n它们在作状语时应和主体谓语共一个主语，并和主体谓语发生的时间同步。\n\n## 不同点\n\n1. 现在分词动作的施动者不一定是该句的主语，而副动词一定是所在句子的主语。\n\n\\*在一些从古法语流传迄今的成语、谚语等之中，副动词也可能拥有不同的主语，而这种主语是不明显的。\n\n- La fortune vient **en dormant**. 飞来横财。\n\n\n2. 见具体用法\n\n","source":"_posts/participe-present-et-gerondif.md","raw":"---\ntitle: participe présent et gérondif\ndate: 2016-12-06 12:31:56\ncategories: francais\ntags: [francais, language]\n---\n\n# 现在分词(le participe présent)\n\n## 1. 构成\n现在分词没有人称和数的变化。反身动词的现在分词仍然保留原反身动词的反身代词。\n\n去掉直陈式第一人称复数的词尾-ons，另加-ant \n\n- faire : nous faisons \n\n特殊情况：\n- etre-etant \n- savoir-sachant\n\n## 2. 简述用法\n1. 用作定语，紧接在被修饰词之后，相当于qui引导的从句\n   - L'étranger cherche à trouver quelqu'un **connaissant** (=qui conaisse) à la fois français et l'anglais.\n2. 表原因、时间等，作为状语\n   - **Voyant**(=Comme elle voit) que tout le monde est dejà assis,elle va vite à sa place.\n   - **Ayant**(=Comme il a) mal à la tete,il décide de rester au lit.\n\n## 3. 细分\n### I. 现在分词形容词特性的表现\n许多现在分词已经转化成形容词，有性数变化，可以作为定于或表语。\n1. 动词形容词表示一种状态或性质，后不跟宾语或状语\n   - Ses yeux **brillants** disent la convoitises. 他那双闪光的眼睛流露出他贪婪的本性。\n   - Je l'ai trouvé toute **tremblante**. 我发现她浑身颤抖。\n\n2. 在一些习惯性搭配中表示一些潜在的主语\n\n   - Une seance **payante** (=où l'on paie) 一场要收费的演出。\n   - une collation **soupante** (=si copieuse qu'elle tient lieu de souper) 一顿丰盛的小吃\n\n   这里分词的主语与它的被修饰语不同。\n\n3. 少数已变成了纯粹的形容词，书写有所区别\n\n| 现在分词       | 形容词       |\n| ---------- | --------- |\n| provoquant | provocant |\n| fatiguant  | fatigant  |\n| vaquant    | vacant    |\n| naviguant  | navigant  |\n\n\n### II. 现在分词动词特性的表现\n\n现在分词具有动词的性质，可以有宾语或状语。\n\n1. 代替qui引导的关系从句\n   - C'est un film **captivant** les spectateurs. 这是一部非常吸引观众的影片。\n   - Je le vois **lisant**. 我看见他在念书。\n2. 起状语作用\n   - **Prenant** l'escabeau, il s'offoree d'atteindre le dernier rayon. 他拿了一把小梯子乡尽力够上最后一格。\n   - **Croyant** le bureau vide, il entra. 他以为办公室没人所以就进去了。\n3. 表示并列动作\n   - Le train repartit, **courant** vers le Midi. 火车又开动了，向南方驶去。\n4. 依附于另一主语，构成独立分词从句\n   - Midi **sonnant**, on se met à table. 钟敲十二时，我们都上桌用餐。\n\n\n\n\n# 副动词(le gérondif)\n\n## 1. 构成\n副动词没有人称、性数变化\n\n在现在分词前加en就构成副动词 \n\n- faire : en faisant\n\n注：ayant、étant前不能用en，不能构成副动词。\n\n## 2. 用法\n1. 时间状语，表示动作的同时性\n   - N'oubliez pas de fermer la port **en sortant**.出去时别忘了关门。\n   - Ne lis pas **en mangeant**. 不要以便吃饭以一边看书\n2. 方式状语\n   - Elle arriva **en courant**. 她跑来了\n3. 条件状语\n   - **En se levant** plus tot le matin, il n'arrivera pas en retard. 如果早上起的早点，他就不会迟到了。\n4. 让步状语\n   - **En voyant** critiquant, il n'avait nulle intention de nous décourager. 他尽管批评了我们，但毫无使我们气馁之意。\n5. 原因状语\n   - **En voyant** son embarras, l'agent se fit plus aimable. 警察见他不知所措，就变得更加和蔼了。\n\n**\\*副动词表示的动作的延续**\n\n副动词可以和aller合用，表示延续发展的动作，介词en可以省略。\n\n- Sa vue va **en s'affaiblissant**. 他的视力日益衰退。\n\n## 3. 副动词的强化\n\n1. Tous + 副动词表示强化，这种结构常见于书面。\n2. Rien que + 副动词表示“只要…”\n\n\n\n\n\n# 现在分词和副动词的异同\n\n## 共同点\n\n它们在作状语时应和主体谓语共一个主语，并和主体谓语发生的时间同步。\n\n## 不同点\n\n1. 现在分词动作的施动者不一定是该句的主语，而副动词一定是所在句子的主语。\n\n\\*在一些从古法语流传迄今的成语、谚语等之中，副动词也可能拥有不同的主语，而这种主语是不明显的。\n\n- La fortune vient **en dormant**. 飞来横财。\n\n\n2. 见具体用法\n\n","slug":"participe-present-et-gerondif","published":1,"updated":"2016-12-13T21:28:32.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckw4qufqd0046gwtl0t66d1fb","content":"<h1 id=\"现在分词-le-participe-present\"><a href=\"#现在分词-le-participe-present\" class=\"headerlink\" title=\"现在分词(le participe présent)\"></a>现在分词(le participe présent)</h1><h2 id=\"1-构成\"><a href=\"#1-构成\" class=\"headerlink\" title=\"1. 构成\"></a>1. 构成</h2><p>现在分词没有人称和数的变化。反身动词的现在分词仍然保留原反身动词的反身代词。</p>\n<p>去掉直陈式第一人称复数的词尾-ons，另加-ant </p>\n<ul>\n<li>faire : nous faisons </li>\n</ul>\n<p>特殊情况：</p>\n<ul>\n<li>etre-etant </li>\n<li>savoir-sachant</li>\n</ul>\n<h2 id=\"2-简述用法\"><a href=\"#2-简述用法\" class=\"headerlink\" title=\"2. 简述用法\"></a>2. 简述用法</h2><ol>\n<li>用作定语，紧接在被修饰词之后，相当于qui引导的从句<ul>\n<li>L’étranger cherche à trouver quelqu’un <strong>connaissant</strong> (=qui conaisse) à la fois français et l’anglais.</li>\n</ul>\n</li>\n<li>表原因、时间等，作为状语<ul>\n<li><strong>Voyant</strong>(=Comme elle voit) que tout le monde est dejà assis,elle va vite à sa place.</li>\n<li><strong>Ayant</strong>(=Comme il a) mal à la tete,il décide de rester au lit.</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"3-细分\"><a href=\"#3-细分\" class=\"headerlink\" title=\"3. 细分\"></a>3. 细分</h2><h3 id=\"I-现在分词形容词特性的表现\"><a href=\"#I-现在分词形容词特性的表现\" class=\"headerlink\" title=\"I. 现在分词形容词特性的表现\"></a>I. 现在分词形容词特性的表现</h3><p>许多现在分词已经转化成形容词，有性数变化，可以作为定于或表语。</p>\n<ol>\n<li><p>动词形容词表示一种状态或性质，后不跟宾语或状语</p>\n<ul>\n<li>Ses yeux <strong>brillants</strong> disent la convoitises. 他那双闪光的眼睛流露出他贪婪的本性。</li>\n<li>Je l’ai trouvé toute <strong>tremblante</strong>. 我发现她浑身颤抖。</li>\n</ul>\n</li>\n<li><p>在一些习惯性搭配中表示一些潜在的主语</p>\n<ul>\n<li>Une seance <strong>payante</strong> (=où l’on paie) 一场要收费的演出。</li>\n<li>une collation <strong>soupante</strong> (=si copieuse qu’elle tient lieu de souper) 一顿丰盛的小吃</li>\n</ul>\n<p>这里分词的主语与它的被修饰语不同。</p>\n</li>\n<li><p>少数已变成了纯粹的形容词，书写有所区别</p>\n</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>现在分词</th>\n<th>形容词</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>provoquant</td>\n<td>provocant</td>\n</tr>\n<tr>\n<td>fatiguant</td>\n<td>fatigant</td>\n</tr>\n<tr>\n<td>vaquant</td>\n<td>vacant</td>\n</tr>\n<tr>\n<td>naviguant</td>\n<td>navigant</td>\n</tr>\n</tbody></table>\n<h3 id=\"II-现在分词动词特性的表现\"><a href=\"#II-现在分词动词特性的表现\" class=\"headerlink\" title=\"II. 现在分词动词特性的表现\"></a>II. 现在分词动词特性的表现</h3><p>现在分词具有动词的性质，可以有宾语或状语。</p>\n<ol>\n<li>代替qui引导的关系从句<ul>\n<li>C’est un film <strong>captivant</strong> les spectateurs. 这是一部非常吸引观众的影片。</li>\n<li>Je le vois <strong>lisant</strong>. 我看见他在念书。</li>\n</ul>\n</li>\n<li>起状语作用<ul>\n<li><strong>Prenant</strong> l’escabeau, il s’offoree d’atteindre le dernier rayon. 他拿了一把小梯子乡尽力够上最后一格。</li>\n<li><strong>Croyant</strong> le bureau vide, il entra. 他以为办公室没人所以就进去了。</li>\n</ul>\n</li>\n<li>表示并列动作<ul>\n<li>Le train repartit, <strong>courant</strong> vers le Midi. 火车又开动了，向南方驶去。</li>\n</ul>\n</li>\n<li>依附于另一主语，构成独立分词从句<ul>\n<li>Midi <strong>sonnant</strong>, on se met à table. 钟敲十二时，我们都上桌用餐。</li>\n</ul>\n</li>\n</ol>\n<h1 id=\"副动词-le-gerondif\"><a href=\"#副动词-le-gerondif\" class=\"headerlink\" title=\"副动词(le gérondif)\"></a>副动词(le gérondif)</h1><h2 id=\"1-构成-1\"><a href=\"#1-构成-1\" class=\"headerlink\" title=\"1. 构成\"></a>1. 构成</h2><p>副动词没有人称、性数变化</p>\n<p>在现在分词前加en就构成副动词 </p>\n<ul>\n<li>faire : en faisant</li>\n</ul>\n<p>注：ayant、étant前不能用en，不能构成副动词。</p>\n<h2 id=\"2-用法\"><a href=\"#2-用法\" class=\"headerlink\" title=\"2. 用法\"></a>2. 用法</h2><ol>\n<li>时间状语，表示动作的同时性<ul>\n<li>N’oubliez pas de fermer la port <strong>en sortant</strong>.出去时别忘了关门。</li>\n<li>Ne lis pas <strong>en mangeant</strong>. 不要以便吃饭以一边看书</li>\n</ul>\n</li>\n<li>方式状语<ul>\n<li>Elle arriva <strong>en courant</strong>. 她跑来了</li>\n</ul>\n</li>\n<li>条件状语<ul>\n<li><strong>En se levant</strong> plus tot le matin, il n’arrivera pas en retard. 如果早上起的早点，他就不会迟到了。</li>\n</ul>\n</li>\n<li>让步状语<ul>\n<li><strong>En voyant</strong> critiquant, il n’avait nulle intention de nous décourager. 他尽管批评了我们，但毫无使我们气馁之意。</li>\n</ul>\n</li>\n<li>原因状语<ul>\n<li><strong>En voyant</strong> son embarras, l’agent se fit plus aimable. 警察见他不知所措，就变得更加和蔼了。</li>\n</ul>\n</li>\n</ol>\n<p><strong>*副动词表示的动作的延续</strong></p>\n<p>副动词可以和aller合用，表示延续发展的动作，介词en可以省略。</p>\n<ul>\n<li>Sa vue va <strong>en s’affaiblissant</strong>. 他的视力日益衰退。</li>\n</ul>\n<h2 id=\"3-副动词的强化\"><a href=\"#3-副动词的强化\" class=\"headerlink\" title=\"3. 副动词的强化\"></a>3. 副动词的强化</h2><ol>\n<li>Tous + 副动词表示强化，这种结构常见于书面。</li>\n<li>Rien que + 副动词表示“只要…”</li>\n</ol>\n<h1 id=\"现在分词和副动词的异同\"><a href=\"#现在分词和副动词的异同\" class=\"headerlink\" title=\"现在分词和副动词的异同\"></a>现在分词和副动词的异同</h1><h2 id=\"共同点\"><a href=\"#共同点\" class=\"headerlink\" title=\"共同点\"></a>共同点</h2><p>它们在作状语时应和主体谓语共一个主语，并和主体谓语发生的时间同步。</p>\n<h2 id=\"不同点\"><a href=\"#不同点\" class=\"headerlink\" title=\"不同点\"></a>不同点</h2><ol>\n<li>现在分词动作的施动者不一定是该句的主语，而副动词一定是所在句子的主语。</li>\n</ol>\n<p>*在一些从古法语流传迄今的成语、谚语等之中，副动词也可能拥有不同的主语，而这种主语是不明显的。</p>\n<ul>\n<li>La fortune vient <strong>en dormant</strong>. 飞来横财。</li>\n</ul>\n<ol start=\"2\">\n<li>见具体用法</li>\n</ol>\n","site":{"data":{"recommended_posts":{"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"}],"https://LorrinWWW.github.io/posts/complexity/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/hexo-with-latex/":[{"title":"区块链时代的最正确的打赏姿势——在Hexo博客配置Coinhive网页挖矿","permalink":"http://www.davidfnck.com/blockchain/mine-xmr-in-hexo-by-coinhive.html/"}],"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/":[{"title":"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-statistic/":[{"title":"贝叶斯估计 Bayes estimation","permalink":"https://LorrinWWW.github.io/posts/Bayes-estimation/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"}],"https://LorrinWWW.github.io/posts/[2018.2.5]Nested-LSTMs/":[{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"}],"https://LorrinWWW.github.io/posts/datamining-pretreatment/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Several models for knowledge graph representing and completing 几个知识图谱模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/"}],"https://LorrinWWW.github.io/posts/compression/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/Sobolev-space/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/proba-ch5/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch3/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/proba-ch1/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/ML-CNN/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/EDP-basic-models/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/machine-learning/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/datamining-class-pred/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction/":[],"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/":[{"title":"OS notes","permalink":"https://LorrinWWW.github.io/posts/OS-notes/"}],"https://LorrinWWW.github.io/posts/projet-enjeu-plugin-chrome-101/":[{"title":"Chrome油猴脚本","permalink":"http://gubangzhong.cn/2017/06/13/Chrome油猴脚本/"}],"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/":[{"title":"MongoDB, Docker and Python","permalink":"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/":[{"title":"Hands on Scrapy","permalink":"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/OS-notes/":[{"title":"learning OS and building LorriOS","permalink":"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/"}],"https://LorrinWWW.github.io/posts/Bayes-estimation/":[{"title":"Note of statistic","permalink":"https://LorrinWWW.github.io/posts/Note-of-statistic/"}],"https://LorrinWWW.github.io/posts/management-of-the-firm/":[],"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"}],"https://LorrinWWW.github.io/posts/datamining-qualitative-induction/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-finite-element-method/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/QuadTree/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/proba-ch4/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch2/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/":[{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/Note-of-NLP/":[{"title":"Generative Adversarial Network","permalink":"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/"},{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"}],"https://LorrinWWW.github.io/posts/Note-of-datamining/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"数据挖掘-分类与预测","permalink":"https://LorrinWWW.github.io/posts/datamining-class-pred/"},{"title":"数据挖掘-预处理","permalink":"https://LorrinWWW.github.io/posts/datamining-pretreatment/"}],"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/Note-of-probability/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/participe-present-et-gerondif/":[],"https://LorrinWWW.github.io/posts/Hilbert-space/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/proba-ch6/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}]}}},"excerpt":"","more":"<h1 id=\"现在分词-le-participe-present\"><a href=\"#现在分词-le-participe-present\" class=\"headerlink\" title=\"现在分词(le participe présent)\"></a>现在分词(le participe présent)</h1><h2 id=\"1-构成\"><a href=\"#1-构成\" class=\"headerlink\" title=\"1. 构成\"></a>1. 构成</h2><p>现在分词没有人称和数的变化。反身动词的现在分词仍然保留原反身动词的反身代词。</p>\n<p>去掉直陈式第一人称复数的词尾-ons，另加-ant </p>\n<ul>\n<li>faire : nous faisons </li>\n</ul>\n<p>特殊情况：</p>\n<ul>\n<li>etre-etant </li>\n<li>savoir-sachant</li>\n</ul>\n<h2 id=\"2-简述用法\"><a href=\"#2-简述用法\" class=\"headerlink\" title=\"2. 简述用法\"></a>2. 简述用法</h2><ol>\n<li>用作定语，紧接在被修饰词之后，相当于qui引导的从句<ul>\n<li>L’étranger cherche à trouver quelqu’un <strong>connaissant</strong> (=qui conaisse) à la fois français et l’anglais.</li>\n</ul>\n</li>\n<li>表原因、时间等，作为状语<ul>\n<li><strong>Voyant</strong>(=Comme elle voit) que tout le monde est dejà assis,elle va vite à sa place.</li>\n<li><strong>Ayant</strong>(=Comme il a) mal à la tete,il décide de rester au lit.</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"3-细分\"><a href=\"#3-细分\" class=\"headerlink\" title=\"3. 细分\"></a>3. 细分</h2><h3 id=\"I-现在分词形容词特性的表现\"><a href=\"#I-现在分词形容词特性的表现\" class=\"headerlink\" title=\"I. 现在分词形容词特性的表现\"></a>I. 现在分词形容词特性的表现</h3><p>许多现在分词已经转化成形容词，有性数变化，可以作为定于或表语。</p>\n<ol>\n<li><p>动词形容词表示一种状态或性质，后不跟宾语或状语</p>\n<ul>\n<li>Ses yeux <strong>brillants</strong> disent la convoitises. 他那双闪光的眼睛流露出他贪婪的本性。</li>\n<li>Je l’ai trouvé toute <strong>tremblante</strong>. 我发现她浑身颤抖。</li>\n</ul>\n</li>\n<li><p>在一些习惯性搭配中表示一些潜在的主语</p>\n<ul>\n<li>Une seance <strong>payante</strong> (=où l’on paie) 一场要收费的演出。</li>\n<li>une collation <strong>soupante</strong> (=si copieuse qu’elle tient lieu de souper) 一顿丰盛的小吃</li>\n</ul>\n<p>这里分词的主语与它的被修饰语不同。</p>\n</li>\n<li><p>少数已变成了纯粹的形容词，书写有所区别</p>\n</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>现在分词</th>\n<th>形容词</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>provoquant</td>\n<td>provocant</td>\n</tr>\n<tr>\n<td>fatiguant</td>\n<td>fatigant</td>\n</tr>\n<tr>\n<td>vaquant</td>\n<td>vacant</td>\n</tr>\n<tr>\n<td>naviguant</td>\n<td>navigant</td>\n</tr>\n</tbody></table>\n<h3 id=\"II-现在分词动词特性的表现\"><a href=\"#II-现在分词动词特性的表现\" class=\"headerlink\" title=\"II. 现在分词动词特性的表现\"></a>II. 现在分词动词特性的表现</h3><p>现在分词具有动词的性质，可以有宾语或状语。</p>\n<ol>\n<li>代替qui引导的关系从句<ul>\n<li>C’est un film <strong>captivant</strong> les spectateurs. 这是一部非常吸引观众的影片。</li>\n<li>Je le vois <strong>lisant</strong>. 我看见他在念书。</li>\n</ul>\n</li>\n<li>起状语作用<ul>\n<li><strong>Prenant</strong> l’escabeau, il s’offoree d’atteindre le dernier rayon. 他拿了一把小梯子乡尽力够上最后一格。</li>\n<li><strong>Croyant</strong> le bureau vide, il entra. 他以为办公室没人所以就进去了。</li>\n</ul>\n</li>\n<li>表示并列动作<ul>\n<li>Le train repartit, <strong>courant</strong> vers le Midi. 火车又开动了，向南方驶去。</li>\n</ul>\n</li>\n<li>依附于另一主语，构成独立分词从句<ul>\n<li>Midi <strong>sonnant</strong>, on se met à table. 钟敲十二时，我们都上桌用餐。</li>\n</ul>\n</li>\n</ol>\n<h1 id=\"副动词-le-gerondif\"><a href=\"#副动词-le-gerondif\" class=\"headerlink\" title=\"副动词(le gérondif)\"></a>副动词(le gérondif)</h1><h2 id=\"1-构成-1\"><a href=\"#1-构成-1\" class=\"headerlink\" title=\"1. 构成\"></a>1. 构成</h2><p>副动词没有人称、性数变化</p>\n<p>在现在分词前加en就构成副动词 </p>\n<ul>\n<li>faire : en faisant</li>\n</ul>\n<p>注：ayant、étant前不能用en，不能构成副动词。</p>\n<h2 id=\"2-用法\"><a href=\"#2-用法\" class=\"headerlink\" title=\"2. 用法\"></a>2. 用法</h2><ol>\n<li>时间状语，表示动作的同时性<ul>\n<li>N’oubliez pas de fermer la port <strong>en sortant</strong>.出去时别忘了关门。</li>\n<li>Ne lis pas <strong>en mangeant</strong>. 不要以便吃饭以一边看书</li>\n</ul>\n</li>\n<li>方式状语<ul>\n<li>Elle arriva <strong>en courant</strong>. 她跑来了</li>\n</ul>\n</li>\n<li>条件状语<ul>\n<li><strong>En se levant</strong> plus tot le matin, il n’arrivera pas en retard. 如果早上起的早点，他就不会迟到了。</li>\n</ul>\n</li>\n<li>让步状语<ul>\n<li><strong>En voyant</strong> critiquant, il n’avait nulle intention de nous décourager. 他尽管批评了我们，但毫无使我们气馁之意。</li>\n</ul>\n</li>\n<li>原因状语<ul>\n<li><strong>En voyant</strong> son embarras, l’agent se fit plus aimable. 警察见他不知所措，就变得更加和蔼了。</li>\n</ul>\n</li>\n</ol>\n<p><strong>*副动词表示的动作的延续</strong></p>\n<p>副动词可以和aller合用，表示延续发展的动作，介词en可以省略。</p>\n<ul>\n<li>Sa vue va <strong>en s’affaiblissant</strong>. 他的视力日益衰退。</li>\n</ul>\n<h2 id=\"3-副动词的强化\"><a href=\"#3-副动词的强化\" class=\"headerlink\" title=\"3. 副动词的强化\"></a>3. 副动词的强化</h2><ol>\n<li>Tous + 副动词表示强化，这种结构常见于书面。</li>\n<li>Rien que + 副动词表示“只要…”</li>\n</ol>\n<h1 id=\"现在分词和副动词的异同\"><a href=\"#现在分词和副动词的异同\" class=\"headerlink\" title=\"现在分词和副动词的异同\"></a>现在分词和副动词的异同</h1><h2 id=\"共同点\"><a href=\"#共同点\" class=\"headerlink\" title=\"共同点\"></a>共同点</h2><p>它们在作状语时应和主体谓语共一个主语，并和主体谓语发生的时间同步。</p>\n<h2 id=\"不同点\"><a href=\"#不同点\" class=\"headerlink\" title=\"不同点\"></a>不同点</h2><ol>\n<li>现在分词动作的施动者不一定是该句的主语，而副动词一定是所在句子的主语。</li>\n</ol>\n<p>*在一些从古法语流传迄今的成语、谚语等之中，副动词也可能拥有不同的主语，而这种主语是不明显的。</p>\n<ul>\n<li>La fortune vient <strong>en dormant</strong>. 飞来横财。</li>\n</ul>\n<ol start=\"2\">\n<li>见具体用法</li>\n</ol>\n"},{"title":"proba-ch1 概率空间","date":"2016-12-01T04:17:29.000Z","_content":"\n- 一个基本的不等式，当\n\n\n$$\nP(\\cup(A_n)) \\leq \\sum_{n \\in N}^{}{P(A_n)}\n$$\n\n- 如果An递增\n\n$$\nP(\\cup(A_n)) = \\lim_{n -> + \\inf}{P(A_n)}\n$$\n\n- 如果An递减\n\n$$\nP(\\cap(A_n)) = \\lim_{n -> + \\inf}{P(A_n)}\n$$\n\n- 条件概率定义，略\n- 性质\n\n$$\nP({A_1}\\cap{...}\\cap{A_{n-1}}) = P(A_1)P(A_2|A_1)...P(A_n|A_1\\cap{...}\\cap{A_{n-1}})\n$$\n\n- 定理(Equation de partition)\n\n$$\nP(A) = \\sum_{n}{P(A|E_n)P(E_n)}\n$$\n\n- 定理(de Bay)\n\n$$\nP(E_n|A) = \\frac{P(A|E_n)P(E_n)}{\\sum_{m}{P(A|E_m)PE_m}}\n$$\n\n- 两个独立事件概率，略\n\n- 定理\n\n  对一可数或有限事件集，P({wn}) = pn，若对pn求和，则P存在且唯一。\n\n- 均值定义，\n  $$\n  E[X] = \\sum_{\\omega \\in{\\Omega}}{X(\\omega) P(\\omega)}\n  $$\n\n- 方差定义\n\n$$\nVar(X) = E[(X - E(X))^2] = E[X^2] - (E(X)^2)\n$$\n\n- 几种分布\n\n  - Loi discrete uniforme，\n\n  - $$\n    \\forall k \\in \\{1,...,n\\}, P(X=k) = \\frac{1}{n}\n    $$\n\n    $$\n    E[X] = \\frac{n+1}{2}\n    $$\n\n  - Loi de Bernoulli，\n\n    $$\n    P(X=1) = p, t P(X=0) = 1-p\n    $$\n    $$\n    E[N] = p, Var(N) = p(1-p)\n    $$\n\n  - Loi binomiale 二项分布，略\n    $$\n    E[N] = np, Var(N) = np(1-p)\n    $$\n\n  - Loi geometrique \n    $$\n    P(N=n) = P^n(1-p),$$$$\n    E[N] = np, Var(N) = np(1-p)\n    $$\n\n  - Distribution de Poisson\n    $$\n    P(X=n)=\\frac{\\lambda^n}{n!}e^{-\\lambda},\n    $$\n    $$\n    E[X] = \\lambda, Var(X) = \\lambda\n    $$\n","source":"_posts/proba-ch1.md","raw":"---\ntitle: proba-ch1 概率空间\ndate: 2016-12-01 12:17:29\ncategories: math\ntags: [probability, math]\n---\n\n- 一个基本的不等式，当\n\n\n$$\nP(\\cup(A_n)) \\leq \\sum_{n \\in N}^{}{P(A_n)}\n$$\n\n- 如果An递增\n\n$$\nP(\\cup(A_n)) = \\lim_{n -> + \\inf}{P(A_n)}\n$$\n\n- 如果An递减\n\n$$\nP(\\cap(A_n)) = \\lim_{n -> + \\inf}{P(A_n)}\n$$\n\n- 条件概率定义，略\n- 性质\n\n$$\nP({A_1}\\cap{...}\\cap{A_{n-1}}) = P(A_1)P(A_2|A_1)...P(A_n|A_1\\cap{...}\\cap{A_{n-1}})\n$$\n\n- 定理(Equation de partition)\n\n$$\nP(A) = \\sum_{n}{P(A|E_n)P(E_n)}\n$$\n\n- 定理(de Bay)\n\n$$\nP(E_n|A) = \\frac{P(A|E_n)P(E_n)}{\\sum_{m}{P(A|E_m)PE_m}}\n$$\n\n- 两个独立事件概率，略\n\n- 定理\n\n  对一可数或有限事件集，P({wn}) = pn，若对pn求和，则P存在且唯一。\n\n- 均值定义，\n  $$\n  E[X] = \\sum_{\\omega \\in{\\Omega}}{X(\\omega) P(\\omega)}\n  $$\n\n- 方差定义\n\n$$\nVar(X) = E[(X - E(X))^2] = E[X^2] - (E(X)^2)\n$$\n\n- 几种分布\n\n  - Loi discrete uniforme，\n\n  - $$\n    \\forall k \\in \\{1,...,n\\}, P(X=k) = \\frac{1}{n}\n    $$\n\n    $$\n    E[X] = \\frac{n+1}{2}\n    $$\n\n  - Loi de Bernoulli，\n\n    $$\n    P(X=1) = p, t P(X=0) = 1-p\n    $$\n    $$\n    E[N] = p, Var(N) = p(1-p)\n    $$\n\n  - Loi binomiale 二项分布，略\n    $$\n    E[N] = np, Var(N) = np(1-p)\n    $$\n\n  - Loi geometrique \n    $$\n    P(N=n) = P^n(1-p),$$$$\n    E[N] = np, Var(N) = np(1-p)\n    $$\n\n  - Distribution de Poisson\n    $$\n    P(X=n)=\\frac{\\lambda^n}{n!}e^{-\\lambda},\n    $$\n    $$\n    E[X] = \\lambda, Var(X) = \\lambda\n    $$\n","slug":"proba-ch1","published":1,"updated":"2016-12-01T11:19:38.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckw4qufqe0049gwtlhlxk2arj","content":"<ul>\n<li>一个基本的不等式，当</li>\n</ul>\n<p>$$<br>P(\\cup(A_n)) \\leq \\sum_{n \\in N}^{}{P(A_n)}<br>$$</p>\n<ul>\n<li>如果An递增</li>\n</ul>\n<p>$$<br>P(\\cup(A_n)) = \\lim_{n -&gt; + \\inf}{P(A_n)}<br>$$</p>\n<ul>\n<li>如果An递减</li>\n</ul>\n<p>$$<br>P(\\cap(A_n)) = \\lim_{n -&gt; + \\inf}{P(A_n)}<br>$$</p>\n<ul>\n<li>条件概率定义，略</li>\n<li>性质</li>\n</ul>\n<p>$$<br>P({A_1}\\cap{…}\\cap{A_{n-1}}) = P(A_1)P(A_2|A_1)…P(A_n|A_1\\cap{…}\\cap{A_{n-1}})<br>$$</p>\n<ul>\n<li>定理(Equation de partition)</li>\n</ul>\n<p>$$<br>P(A) = \\sum_{n}{P(A|E_n)P(E_n)}<br>$$</p>\n<ul>\n<li>定理(de Bay)</li>\n</ul>\n<p>$$<br>P(E_n|A) = \\frac{P(A|E_n)P(E_n)}{\\sum_{m}{P(A|E_m)PE_m}}<br>$$</p>\n<ul>\n<li><p>两个独立事件概率，略</p>\n</li>\n<li><p>定理</p>\n<p>对一可数或有限事件集，P({wn}) = pn，若对pn求和，则P存在且唯一。</p>\n</li>\n<li><p>均值定义，<br>$$<br>E[X] = \\sum_{\\omega \\in{\\Omega}}{X(\\omega) P(\\omega)}<br>$$</p>\n</li>\n<li><p>方差定义</p>\n</li>\n</ul>\n<p>$$<br>Var(X) = E[(X - E(X))^2] = E[X^2] - (E(X)^2)<br>$$</p>\n<ul>\n<li><p>几种分布</p>\n<ul>\n<li><p>Loi discrete uniforme，</p>\n</li>\n<li><p>$$<br>\\forall k \\in {1,…,n}, P(X=k) = \\frac{1}{n}<br>$$</p>\n<p>$$<br>E[X] = \\frac{n+1}{2}<br>$$</p>\n</li>\n<li><p>Loi de Bernoulli，</p>\n<p>$$<br>P(X=1) = p, t P(X=0) = 1-p<br>$$<br>$$<br>E[N] = p, Var(N) = p(1-p)<br>$$</p>\n</li>\n<li><p>Loi binomiale 二项分布，略<br>$$<br>E[N] = np, Var(N) = np(1-p)<br>$$</p>\n</li>\n<li><p>Loi geometrique<br>$$<br>P(N=n) = P^n(1-p),$$$$<br>E[N] = np, Var(N) = np(1-p)<br>$$</p>\n</li>\n<li><p>Distribution de Poisson<br>$$<br>P(X=n)=\\frac{\\lambda^n}{n!}e^{-\\lambda},<br>$$<br>$$<br>E[X] = \\lambda, Var(X) = \\lambda<br>$$</p>\n</li>\n</ul>\n</li>\n</ul>\n","site":{"data":{"recommended_posts":{"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"}],"https://LorrinWWW.github.io/posts/complexity/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/hexo-with-latex/":[{"title":"区块链时代的最正确的打赏姿势——在Hexo博客配置Coinhive网页挖矿","permalink":"http://www.davidfnck.com/blockchain/mine-xmr-in-hexo-by-coinhive.html/"}],"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/":[{"title":"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-statistic/":[{"title":"贝叶斯估计 Bayes estimation","permalink":"https://LorrinWWW.github.io/posts/Bayes-estimation/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"}],"https://LorrinWWW.github.io/posts/[2018.2.5]Nested-LSTMs/":[{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"}],"https://LorrinWWW.github.io/posts/datamining-pretreatment/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Several models for knowledge graph representing and completing 几个知识图谱模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/"}],"https://LorrinWWW.github.io/posts/compression/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/Sobolev-space/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/proba-ch5/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch3/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/proba-ch1/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/ML-CNN/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/EDP-basic-models/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/machine-learning/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/datamining-class-pred/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction/":[],"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/":[{"title":"OS notes","permalink":"https://LorrinWWW.github.io/posts/OS-notes/"}],"https://LorrinWWW.github.io/posts/projet-enjeu-plugin-chrome-101/":[{"title":"Chrome油猴脚本","permalink":"http://gubangzhong.cn/2017/06/13/Chrome油猴脚本/"}],"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/":[{"title":"MongoDB, Docker and Python","permalink":"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/":[{"title":"Hands on Scrapy","permalink":"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/OS-notes/":[{"title":"learning OS and building LorriOS","permalink":"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/"}],"https://LorrinWWW.github.io/posts/Bayes-estimation/":[{"title":"Note of statistic","permalink":"https://LorrinWWW.github.io/posts/Note-of-statistic/"}],"https://LorrinWWW.github.io/posts/management-of-the-firm/":[],"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"}],"https://LorrinWWW.github.io/posts/datamining-qualitative-induction/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-finite-element-method/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/QuadTree/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/proba-ch4/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch2/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/":[{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/Note-of-NLP/":[{"title":"Generative Adversarial Network","permalink":"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/"},{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"}],"https://LorrinWWW.github.io/posts/Note-of-datamining/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"数据挖掘-分类与预测","permalink":"https://LorrinWWW.github.io/posts/datamining-class-pred/"},{"title":"数据挖掘-预处理","permalink":"https://LorrinWWW.github.io/posts/datamining-pretreatment/"}],"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/Note-of-probability/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/participe-present-et-gerondif/":[],"https://LorrinWWW.github.io/posts/Hilbert-space/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/proba-ch6/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}]}}},"excerpt":"","more":"<ul>\n<li>一个基本的不等式，当</li>\n</ul>\n<p>$$<br>P(\\cup(A_n)) \\leq \\sum_{n \\in N}^{}{P(A_n)}<br>$$</p>\n<ul>\n<li>如果An递增</li>\n</ul>\n<p>$$<br>P(\\cup(A_n)) = \\lim_{n -&gt; + \\inf}{P(A_n)}<br>$$</p>\n<ul>\n<li>如果An递减</li>\n</ul>\n<p>$$<br>P(\\cap(A_n)) = \\lim_{n -&gt; + \\inf}{P(A_n)}<br>$$</p>\n<ul>\n<li>条件概率定义，略</li>\n<li>性质</li>\n</ul>\n<p>$$<br>P({A_1}\\cap{…}\\cap{A_{n-1}}) = P(A_1)P(A_2|A_1)…P(A_n|A_1\\cap{…}\\cap{A_{n-1}})<br>$$</p>\n<ul>\n<li>定理(Equation de partition)</li>\n</ul>\n<p>$$<br>P(A) = \\sum_{n}{P(A|E_n)P(E_n)}<br>$$</p>\n<ul>\n<li>定理(de Bay)</li>\n</ul>\n<p>$$<br>P(E_n|A) = \\frac{P(A|E_n)P(E_n)}{\\sum_{m}{P(A|E_m)PE_m}}<br>$$</p>\n<ul>\n<li><p>两个独立事件概率，略</p>\n</li>\n<li><p>定理</p>\n<p>对一可数或有限事件集，P({wn}) = pn，若对pn求和，则P存在且唯一。</p>\n</li>\n<li><p>均值定义，<br>$$<br>E[X] = \\sum_{\\omega \\in{\\Omega}}{X(\\omega) P(\\omega)}<br>$$</p>\n</li>\n<li><p>方差定义</p>\n</li>\n</ul>\n<p>$$<br>Var(X) = E[(X - E(X))^2] = E[X^2] - (E(X)^2)<br>$$</p>\n<ul>\n<li><p>几种分布</p>\n<ul>\n<li><p>Loi discrete uniforme，</p>\n</li>\n<li><p>$$<br>\\forall k \\in {1,…,n}, P(X=k) = \\frac{1}{n}<br>$$</p>\n<p>$$<br>E[X] = \\frac{n+1}{2}<br>$$</p>\n</li>\n<li><p>Loi de Bernoulli，</p>\n<p>$$<br>P(X=1) = p, t P(X=0) = 1-p<br>$$<br>$$<br>E[N] = p, Var(N) = p(1-p)<br>$$</p>\n</li>\n<li><p>Loi binomiale 二项分布，略<br>$$<br>E[N] = np, Var(N) = np(1-p)<br>$$</p>\n</li>\n<li><p>Loi geometrique<br>$$<br>P(N=n) = P^n(1-p),$$$$<br>E[N] = np, Var(N) = np(1-p)<br>$$</p>\n</li>\n<li><p>Distribution de Poisson<br>$$<br>P(X=n)=\\frac{\\lambda^n}{n!}e^{-\\lambda},<br>$$<br>$$<br>E[X] = \\lambda, Var(X) = \\lambda<br>$$</p>\n</li>\n</ul>\n</li>\n</ul>\n"},{"title":"proba-ch2 随机变量","date":"2016-12-01T04:17:33.000Z","_content":"\n- ​\n\n$$\n\\pi-system\n$$\n\n- R上的概率测度\n\n  - 分布函数 la fonction de repartition\n    $$\n    F:x\\rightarrow F(x) = P(] - \\infty,x] ) \\\\ F:x\\xrightarrow{}F(x) = \\int_{ ]-\\infty,x] } {f(t).\\lambda(dt)}\n    $$\n\n  - 定理\n\n    如果F是分布函数，则当且仅当满足以下三个条件。\n    $$\n    (i) F 递增\\\\(ii)F 右连续\\\\(iii) \\lim_{n \\rightarrow - \\infty}{F(x)=0}, \\lim_{n \\rightarrow+\\infty}{F(x)=1}\n    $$\n\n  - 性质\n    $$\n    P(\\{x\\}) = F(x) - F(x-)\n    $$\n\n- R^N上的概率测度\n\n  -  定义\n  $$\n    F:(x_1,...,x_N)\\xrightarrow{}F(x_1,...,x_N) = P(\\prod_{i=1}^{N}] - \\infty,x_i] )\n  $$\n\n  - 定义\n    $$\n    P(dx_1,...,dx_N) = f(x_1,...,x_N)dx_1...dx_N \\\\ where||f||_{L^2}\n    $$\n\n- 随机变量\n\n  - 定义\n    $$\n    Soit(\\Omega,{\\cal F})\\rightarrow (E, {\\cal E}); X:\\Omega \\rightarrow E \\\\ \\forall A\\in {\\cal E}; X^{-1}(A) \\in {\\cal F}\n    $$\n    X就是一个随机变量\n\n  - 定义 tribu engendree par X\n    $$\n    X^{-1}({\\cal E}) = \\{X^{-1}(A); A \\in {\\cal E}\\} \\\\ \\sigma(X)\n    $$\n\n- 随机变量的loi\n\n  - 定义\n    $$\n    \\forall A \\in {\\cal E}; P_X(A) = P(\\{\\omega: X(\\omega) \\in A\\}) = P(X^{-1}(A))\n    $$\n\n- 积分\n\n  - 数学期望定义\n    $$\n    E[X] = \\int_{\\Omega}{X(\\omega).P(d\\omega)}= \\int_{\\Omega}{X.dP}\n    $$\n\n  - 定理\n    $$\n    (X_n)_{n\\in N} 是随机变量\n    $$\n\n    - 单调趋向性：若Xn非负且趋于X，则\n      $$\n      \\lim_{n \\rightarrow \\infty}{E[X_n]} = E[X]\n      $$\n\n    - Fatou引理：若Xn非负\n      $$\n      E[\\lim_{n \\rightarrow \\infty}{\\inf{X_n}}] \\leq \\lim_{n \\rightarrow \\infty}{\\inf{E[X_n]}}\n      $$\n\n    - convergence dominee: 若limXn = X p.s 若存在Z in L1，且|Xn|<=Z，则\n      $$\n      \\lim_{n \\rightarrow \\infty}{E[X_n]} = E[X]\n      $$\n\n  - 性质(Inegalite de Markov)\n\n    若X是随机变量并admettant un moment d'ordre 1, 对于实数a>0\n    $$\n    P(|X| \\geq a) \\leq \\frac{E[|X|]}{a}\n    $$\n\n  - 转化定理\n    $$\n    E[h(X)] = \\int_E{h(x)P_X(dx)}\n    $$\n\n  - 定义\n  $$\n  \\text{un moment d'ordre n}: \\int_\\Omega{|X|^ndP} < \\infty\n  $$\n\n  - 性质，若0<p<q，可以得到Lq被Lp包含。\n\n  - 定义方差，前面说过了\n\n    - $$\n      Var(aX+b) = a^2 Var(X) \\\\ P(|X-E[X]| \\geq a) \\leq \\frac{Var(X)}{a^2}\n      $$\n\n    - 若X几乎处处等于同一个常数，当仅当Var(X) = 0\n\n  - 定理，与离散相似的\n    $$\n    E[h(X)] = \\int_E{h(x)f_X(dx)}\n    $$\n    特别的\n    $$\n    P(X \\in A) = E[1_{ \\{X\\in A\\} } ] = \\int_A{f_X(dx)}\n    $$\n\n- Vecteurs aleatoires\n\n  X = (X1,…,XN) 与多元离散随机变量类似。\n\n  - 协方差\n    $$\n    Cov(X,Y) = E[(X-E[X])(Y-E[Y])] = E[XY] - E[X]E[Y]\n    $$\n\n    - 性质\n      $$\n      Cov(X,X) =Var(X) \\\\ Cov(X,Y) = Cov(Y,X) \\\\ Var(X+Y) = Var(X) +Var(Y) +2Cov(X,Y)\n      $$\n\n  - 相关系数\n    $$\n    \\rho(X,Y) = \\frac{Cov(X,Y)}{\\sigma(X)\\sigma(Y)}, \\sigma(X) = \\sqrt{Var(X)}, -1 \\leq \\rho \\leq 1\n    $$\n\n  - 改变变量\n    $$\n    \\forall y \\in R^N; f_Y(y) = \\frac{f_X(h^{-1}(y))}{|det(Jh(h^{-1}))|} 1_D(y)\n    $$\n\n  - 边际概率，顾名思义，略\n\n- 独立变量\n\n  定义略\n\n  - 定理，满足下列条件之一，则XY独立\n\n    1. 对于所有A，B\n    $$\n      P(X \\in A, Y\\in B) = P(X\\in A)\n    $$\n\n    2. 对于所有f，g\n    $$\n      E[f(X)g(Y)] = E[f(X)] E[g(Y)]\n    $$\n\n    3. 对于所有f，g，f(X)和g(Y)是独立的\n\n  - 性质\n    $$\n    \\text{Soient (X, Y) un couple de variables aleatoires a valeurs dans } E\\otimes F \\text{ muni de la tribu produit } \\mathcal{E} \\otimes \\tilde{\\mathcal{E}}. \\\\ \\text{ X et Y sont independantes si et seulement si la lor jointe du couple (X, Y) est egale a la mesure produit } P_X \\otimes P_Y.\n    $$\n    $$\n    \\text{注：对于XY相互独立，} P((X,Y)\\in A \\times B) = P_X(A)P_Y(B) \\\\\n    P_X \\otimes P_Y(A \\times B) = P_X(A)P_Y(B)\n    $$\n\n  - 定理，对于随机变量XY，他们相互独立，当且仅当\n    $$\n    \\forall (x,y) \\in R^2; F_{(X,Y)}(x,y) = F_X(x)F_Y(y)\n    $$\n\n  - 定理，对于(X,Y)在lebesgue测度上绝对连续，XY相互独立当且仅当\n    $$\n    \\forall (x,y) \\in R^2; f_{(X,Y)}(x,y) = f_X(x)f_Y(y)\n    $$\n\n  - 性质，XY admettant un moment d'ordre 1, 相互独立，则\n\n  - $$\n    E[XY]=E[X]E[Y]\n    $$\n\n  - 性质，XY admettant un moment d'ordre 2, Cov(X,Y)=0, 则\n\n  - $$\n    Var(X+Y) =Var(X) + Var(Y)\n    $$\n\n    ​\n","source":"_posts/proba-ch2.md","raw":"---\ntitle: proba-ch2 随机变量\ndate: 2016-12-01 12:17:33\ncategories: math\ntags: [probability, math, aleatoire]\n---\n\n- ​\n\n$$\n\\pi-system\n$$\n\n- R上的概率测度\n\n  - 分布函数 la fonction de repartition\n    $$\n    F:x\\rightarrow F(x) = P(] - \\infty,x] ) \\\\ F:x\\xrightarrow{}F(x) = \\int_{ ]-\\infty,x] } {f(t).\\lambda(dt)}\n    $$\n\n  - 定理\n\n    如果F是分布函数，则当且仅当满足以下三个条件。\n    $$\n    (i) F 递增\\\\(ii)F 右连续\\\\(iii) \\lim_{n \\rightarrow - \\infty}{F(x)=0}, \\lim_{n \\rightarrow+\\infty}{F(x)=1}\n    $$\n\n  - 性质\n    $$\n    P(\\{x\\}) = F(x) - F(x-)\n    $$\n\n- R^N上的概率测度\n\n  -  定义\n  $$\n    F:(x_1,...,x_N)\\xrightarrow{}F(x_1,...,x_N) = P(\\prod_{i=1}^{N}] - \\infty,x_i] )\n  $$\n\n  - 定义\n    $$\n    P(dx_1,...,dx_N) = f(x_1,...,x_N)dx_1...dx_N \\\\ where||f||_{L^2}\n    $$\n\n- 随机变量\n\n  - 定义\n    $$\n    Soit(\\Omega,{\\cal F})\\rightarrow (E, {\\cal E}); X:\\Omega \\rightarrow E \\\\ \\forall A\\in {\\cal E}; X^{-1}(A) \\in {\\cal F}\n    $$\n    X就是一个随机变量\n\n  - 定义 tribu engendree par X\n    $$\n    X^{-1}({\\cal E}) = \\{X^{-1}(A); A \\in {\\cal E}\\} \\\\ \\sigma(X)\n    $$\n\n- 随机变量的loi\n\n  - 定义\n    $$\n    \\forall A \\in {\\cal E}; P_X(A) = P(\\{\\omega: X(\\omega) \\in A\\}) = P(X^{-1}(A))\n    $$\n\n- 积分\n\n  - 数学期望定义\n    $$\n    E[X] = \\int_{\\Omega}{X(\\omega).P(d\\omega)}= \\int_{\\Omega}{X.dP}\n    $$\n\n  - 定理\n    $$\n    (X_n)_{n\\in N} 是随机变量\n    $$\n\n    - 单调趋向性：若Xn非负且趋于X，则\n      $$\n      \\lim_{n \\rightarrow \\infty}{E[X_n]} = E[X]\n      $$\n\n    - Fatou引理：若Xn非负\n      $$\n      E[\\lim_{n \\rightarrow \\infty}{\\inf{X_n}}] \\leq \\lim_{n \\rightarrow \\infty}{\\inf{E[X_n]}}\n      $$\n\n    - convergence dominee: 若limXn = X p.s 若存在Z in L1，且|Xn|<=Z，则\n      $$\n      \\lim_{n \\rightarrow \\infty}{E[X_n]} = E[X]\n      $$\n\n  - 性质(Inegalite de Markov)\n\n    若X是随机变量并admettant un moment d'ordre 1, 对于实数a>0\n    $$\n    P(|X| \\geq a) \\leq \\frac{E[|X|]}{a}\n    $$\n\n  - 转化定理\n    $$\n    E[h(X)] = \\int_E{h(x)P_X(dx)}\n    $$\n\n  - 定义\n  $$\n  \\text{un moment d'ordre n}: \\int_\\Omega{|X|^ndP} < \\infty\n  $$\n\n  - 性质，若0<p<q，可以得到Lq被Lp包含。\n\n  - 定义方差，前面说过了\n\n    - $$\n      Var(aX+b) = a^2 Var(X) \\\\ P(|X-E[X]| \\geq a) \\leq \\frac{Var(X)}{a^2}\n      $$\n\n    - 若X几乎处处等于同一个常数，当仅当Var(X) = 0\n\n  - 定理，与离散相似的\n    $$\n    E[h(X)] = \\int_E{h(x)f_X(dx)}\n    $$\n    特别的\n    $$\n    P(X \\in A) = E[1_{ \\{X\\in A\\} } ] = \\int_A{f_X(dx)}\n    $$\n\n- Vecteurs aleatoires\n\n  X = (X1,…,XN) 与多元离散随机变量类似。\n\n  - 协方差\n    $$\n    Cov(X,Y) = E[(X-E[X])(Y-E[Y])] = E[XY] - E[X]E[Y]\n    $$\n\n    - 性质\n      $$\n      Cov(X,X) =Var(X) \\\\ Cov(X,Y) = Cov(Y,X) \\\\ Var(X+Y) = Var(X) +Var(Y) +2Cov(X,Y)\n      $$\n\n  - 相关系数\n    $$\n    \\rho(X,Y) = \\frac{Cov(X,Y)}{\\sigma(X)\\sigma(Y)}, \\sigma(X) = \\sqrt{Var(X)}, -1 \\leq \\rho \\leq 1\n    $$\n\n  - 改变变量\n    $$\n    \\forall y \\in R^N; f_Y(y) = \\frac{f_X(h^{-1}(y))}{|det(Jh(h^{-1}))|} 1_D(y)\n    $$\n\n  - 边际概率，顾名思义，略\n\n- 独立变量\n\n  定义略\n\n  - 定理，满足下列条件之一，则XY独立\n\n    1. 对于所有A，B\n    $$\n      P(X \\in A, Y\\in B) = P(X\\in A)\n    $$\n\n    2. 对于所有f，g\n    $$\n      E[f(X)g(Y)] = E[f(X)] E[g(Y)]\n    $$\n\n    3. 对于所有f，g，f(X)和g(Y)是独立的\n\n  - 性质\n    $$\n    \\text{Soient (X, Y) un couple de variables aleatoires a valeurs dans } E\\otimes F \\text{ muni de la tribu produit } \\mathcal{E} \\otimes \\tilde{\\mathcal{E}}. \\\\ \\text{ X et Y sont independantes si et seulement si la lor jointe du couple (X, Y) est egale a la mesure produit } P_X \\otimes P_Y.\n    $$\n    $$\n    \\text{注：对于XY相互独立，} P((X,Y)\\in A \\times B) = P_X(A)P_Y(B) \\\\\n    P_X \\otimes P_Y(A \\times B) = P_X(A)P_Y(B)\n    $$\n\n  - 定理，对于随机变量XY，他们相互独立，当且仅当\n    $$\n    \\forall (x,y) \\in R^2; F_{(X,Y)}(x,y) = F_X(x)F_Y(y)\n    $$\n\n  - 定理，对于(X,Y)在lebesgue测度上绝对连续，XY相互独立当且仅当\n    $$\n    \\forall (x,y) \\in R^2; f_{(X,Y)}(x,y) = f_X(x)f_Y(y)\n    $$\n\n  - 性质，XY admettant un moment d'ordre 1, 相互独立，则\n\n  - $$\n    E[XY]=E[X]E[Y]\n    $$\n\n  - 性质，XY admettant un moment d'ordre 2, Cov(X,Y)=0, 则\n\n  - $$\n    Var(X+Y) =Var(X) + Var(Y)\n    $$\n\n    ​\n","slug":"proba-ch2","published":1,"updated":"2018-03-13T20:35:29.109Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckw4qufqf004cgwtl34hx7wsh","content":"<ul>\n<li>​</li>\n</ul>\n<p>$$<br>\\pi-system<br>$$</p>\n<ul>\n<li><p>R上的概率测度</p>\n<ul>\n<li><p>分布函数 la fonction de repartition<br>$$<br>F:x\\rightarrow F(x) = P(] - \\infty,x] ) \\ F:x\\xrightarrow{}F(x) = \\int_{ ]-\\infty,x] } {f(t).\\lambda(dt)}<br>$$</p>\n</li>\n<li><p>定理</p>\n<p>如果F是分布函数，则当且仅当满足以下三个条件。<br>$$<br>(i) F 递增\\(ii)F 右连续\\(iii) \\lim_{n \\rightarrow - \\infty}{F(x)=0}, \\lim_{n \\rightarrow+\\infty}{F(x)=1}<br>$$</p>\n</li>\n<li><p>性质<br>$$<br>P({x}) = F(x) - F(x-)<br>$$</p>\n</li>\n</ul>\n</li>\n<li><p>R^N上的概率测度</p>\n<ul>\n<li><p>&nbsp;定义<br>$$<br>F:(x_1,…,x_N)\\xrightarrow{}F(x_1,…,x_N) = P(\\prod_{i=1}^{N}] - \\infty,x_i] )<br>$$</p>\n</li>\n<li><p>定义<br>$$<br>P(dx_1,…,dx_N) = f(x_1,…,x_N)dx_1…dx_N \\ where||f||_{L^2}<br>$$</p>\n</li>\n</ul>\n</li>\n<li><p>随机变量</p>\n<ul>\n<li><p>定义<br>$$<br>Soit(\\Omega,{\\cal F})\\rightarrow (E, {\\cal E}); X:\\Omega \\rightarrow E \\ \\forall A\\in {\\cal E}; X^{-1}(A) \\in {\\cal F}<br>$$<br>X就是一个随机变量</p>\n</li>\n<li><p>定义 tribu engendree par X<br>$$<br>X^{-1}({\\cal E}) = {X^{-1}(A); A \\in {\\cal E}} \\ \\sigma(X)<br>$$</p>\n</li>\n</ul>\n</li>\n<li><p>随机变量的loi</p>\n<ul>\n<li>定义<br>$$<br>\\forall A \\in {\\cal E}; P_X(A) = P({\\omega: X(\\omega) \\in A}) = P(X^{-1}(A))<br>$$</li>\n</ul>\n</li>\n<li><p>积分</p>\n<ul>\n<li><p>数学期望定义<br>$$<br>E[X] = \\int_{\\Omega}{X(\\omega).P(d\\omega)}= \\int_{\\Omega}{X.dP}<br>$$</p>\n</li>\n<li><p>定理<br>$$<br>(X_n)_{n\\in N} 是随机变量<br>$$</p>\n<ul>\n<li><p>单调趋向性：若Xn非负且趋于X，则<br>$$<br>\\lim_{n \\rightarrow \\infty}{E[X_n]} = E[X]<br>$$</p>\n</li>\n<li><p>Fatou引理：若Xn非负<br>$$<br>E[\\lim_{n \\rightarrow \\infty}{\\inf{X_n}}] \\leq \\lim_{n \\rightarrow \\infty}{\\inf{E[X_n]}}<br>$$</p>\n</li>\n<li><p>convergence dominee: 若limXn = X p.s 若存在Z in L1，且|Xn|&lt;=Z，则<br>$$<br>\\lim_{n \\rightarrow \\infty}{E[X_n]} = E[X]<br>$$</p>\n</li>\n</ul>\n</li>\n<li><p>性质(Inegalite de Markov)</p>\n<p>若X是随机变量并admettant un moment d’ordre 1, 对于实数a&gt;0<br>$$<br>P(|X| \\geq a) \\leq \\frac{E[|X|]}{a}<br>$$</p>\n</li>\n<li><p>转化定理<br>$$<br>E[h(X)] = \\int_E{h(x)P_X(dx)}<br>$$</p>\n</li>\n<li><p>定义<br>$$<br>\\text{un moment d’ordre n}: \\int_\\Omega{|X|^ndP} &lt; \\infty<br>$$</p>\n</li>\n<li><p>性质，若0&lt;p&lt;q，可以得到Lq被Lp包含。</p>\n</li>\n<li><p>定义方差，前面说过了</p>\n<ul>\n<li><p>$$<br>Var(aX+b) = a^2 Var(X) \\ P(|X-E[X]| \\geq a) \\leq \\frac{Var(X)}{a^2}<br>$$</p>\n</li>\n<li><p>若X几乎处处等于同一个常数，当仅当Var(X) = 0</p>\n</li>\n</ul>\n</li>\n<li><p>定理，与离散相似的<br>$$<br>E[h(X)] = \\int_E{h(x)f_X(dx)}<br>$$<br>特别的<br>$$<br>P(X \\in A) = E[1_{ {X\\in A} } ] = \\int_A{f_X(dx)}<br>$$</p>\n</li>\n</ul>\n</li>\n<li><p>Vecteurs aleatoires</p>\n<p>X = (X1,…,XN) 与多元离散随机变量类似。</p>\n<ul>\n<li><p>协方差<br>$$<br>Cov(X,Y) = E[(X-E[X])(Y-E[Y])] = E[XY] - E[X]E[Y]<br>$$</p>\n<ul>\n<li>性质<br>$$<br>Cov(X,X) =Var(X) \\ Cov(X,Y) = Cov(Y,X) \\ Var(X+Y) = Var(X) +Var(Y) +2Cov(X,Y)<br>$$</li>\n</ul>\n</li>\n<li><p>相关系数<br>$$<br>\\rho(X,Y) = \\frac{Cov(X,Y)}{\\sigma(X)\\sigma(Y)}, \\sigma(X) = \\sqrt{Var(X)}, -1 \\leq \\rho \\leq 1<br>$$</p>\n</li>\n<li><p>改变变量<br>$$<br>\\forall y \\in R^N; f_Y(y) = \\frac{f_X(h^{-1}(y))}{|det(Jh(h^{-1}))|} 1_D(y)<br>$$</p>\n</li>\n<li><p>边际概率，顾名思义，略</p>\n</li>\n</ul>\n</li>\n<li><p>独立变量</p>\n<p>定义略</p>\n<ul>\n<li><p>定理，满足下列条件之一，则XY独立</p>\n<ol>\n<li><p>对于所有A，B<br>$$<br>P(X \\in A, Y\\in B) = P(X\\in A)<br>$$</p>\n</li>\n<li><p>对于所有f，g<br>$$<br>E[f(X)g(Y)] = E[f(X)] E[g(Y)]<br>$$</p>\n</li>\n<li><p>对于所有f，g，f(X)和g(Y)是独立的</p>\n</li>\n</ol>\n</li>\n<li><p>性质<br>$$<br>\\text{Soient (X, Y) un couple de variables aleatoires a valeurs dans } E\\otimes F \\text{ muni de la tribu produit } \\mathcal{E} \\otimes \\tilde{\\mathcal{E}}. \\ \\text{ X et Y sont independantes si et seulement si la lor jointe du couple (X, Y) est egale a la mesure produit } P_X \\otimes P_Y.<br>$$<br>$$<br>\\text{注：对于XY相互独立，} P((X,Y)\\in A \\times B) = P_X(A)P_Y(B) \\<br>P_X \\otimes P_Y(A \\times B) = P_X(A)P_Y(B)<br>$$</p>\n</li>\n<li><p>定理，对于随机变量XY，他们相互独立，当且仅当<br>$$<br>\\forall (x,y) \\in R^2; F_{(X,Y)}(x,y) = F_X(x)F_Y(y)<br>$$</p>\n</li>\n<li><p>定理，对于(X,Y)在lebesgue测度上绝对连续，XY相互独立当且仅当<br>$$<br>\\forall (x,y) \\in R^2; f_{(X,Y)}(x,y) = f_X(x)f_Y(y)<br>$$</p>\n</li>\n<li><p>性质，XY admettant un moment d’ordre 1, 相互独立，则</p>\n</li>\n<li><p>$$<br>E[XY]=E[X]E[Y]<br>$$</p>\n</li>\n<li><p>性质，XY admettant un moment d’ordre 2, Cov(X,Y)=0, 则</p>\n</li>\n<li><p>$$<br>Var(X+Y) =Var(X) + Var(Y)<br>$$</p>\n<p>​</p>\n</li>\n</ul>\n</li>\n</ul>\n","site":{"data":{"recommended_posts":{"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"}],"https://LorrinWWW.github.io/posts/complexity/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/hexo-with-latex/":[{"title":"区块链时代的最正确的打赏姿势——在Hexo博客配置Coinhive网页挖矿","permalink":"http://www.davidfnck.com/blockchain/mine-xmr-in-hexo-by-coinhive.html/"}],"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/":[{"title":"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-statistic/":[{"title":"贝叶斯估计 Bayes estimation","permalink":"https://LorrinWWW.github.io/posts/Bayes-estimation/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"}],"https://LorrinWWW.github.io/posts/[2018.2.5]Nested-LSTMs/":[{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"}],"https://LorrinWWW.github.io/posts/datamining-pretreatment/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Several models for knowledge graph representing and completing 几个知识图谱模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/"}],"https://LorrinWWW.github.io/posts/compression/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/Sobolev-space/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/proba-ch5/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch3/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/proba-ch1/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/ML-CNN/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/EDP-basic-models/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/machine-learning/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/datamining-class-pred/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction/":[],"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/":[{"title":"OS notes","permalink":"https://LorrinWWW.github.io/posts/OS-notes/"}],"https://LorrinWWW.github.io/posts/projet-enjeu-plugin-chrome-101/":[{"title":"Chrome油猴脚本","permalink":"http://gubangzhong.cn/2017/06/13/Chrome油猴脚本/"}],"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/":[{"title":"MongoDB, Docker and Python","permalink":"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/":[{"title":"Hands on Scrapy","permalink":"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/OS-notes/":[{"title":"learning OS and building LorriOS","permalink":"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/"}],"https://LorrinWWW.github.io/posts/Bayes-estimation/":[{"title":"Note of statistic","permalink":"https://LorrinWWW.github.io/posts/Note-of-statistic/"}],"https://LorrinWWW.github.io/posts/management-of-the-firm/":[],"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"}],"https://LorrinWWW.github.io/posts/datamining-qualitative-induction/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-finite-element-method/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/QuadTree/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/proba-ch4/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch2/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/":[{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/Note-of-NLP/":[{"title":"Generative Adversarial Network","permalink":"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/"},{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"}],"https://LorrinWWW.github.io/posts/Note-of-datamining/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"数据挖掘-分类与预测","permalink":"https://LorrinWWW.github.io/posts/datamining-class-pred/"},{"title":"数据挖掘-预处理","permalink":"https://LorrinWWW.github.io/posts/datamining-pretreatment/"}],"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/Note-of-probability/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/participe-present-et-gerondif/":[],"https://LorrinWWW.github.io/posts/Hilbert-space/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/proba-ch6/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}]}}},"excerpt":"","more":"<ul>\n<li>​</li>\n</ul>\n<p>$$<br>\\pi-system<br>$$</p>\n<ul>\n<li><p>R上的概率测度</p>\n<ul>\n<li><p>分布函数 la fonction de repartition<br>$$<br>F:x\\rightarrow F(x) = P(] - \\infty,x] ) \\ F:x\\xrightarrow{}F(x) = \\int_{ ]-\\infty,x] } {f(t).\\lambda(dt)}<br>$$</p>\n</li>\n<li><p>定理</p>\n<p>如果F是分布函数，则当且仅当满足以下三个条件。<br>$$<br>(i) F 递增\\(ii)F 右连续\\(iii) \\lim_{n \\rightarrow - \\infty}{F(x)=0}, \\lim_{n \\rightarrow+\\infty}{F(x)=1}<br>$$</p>\n</li>\n<li><p>性质<br>$$<br>P({x}) = F(x) - F(x-)<br>$$</p>\n</li>\n</ul>\n</li>\n<li><p>R^N上的概率测度</p>\n<ul>\n<li><p> 定义<br>$$<br>F:(x_1,…,x_N)\\xrightarrow{}F(x_1,…,x_N) = P(\\prod_{i=1}^{N}] - \\infty,x_i] )<br>$$</p>\n</li>\n<li><p>定义<br>$$<br>P(dx_1,…,dx_N) = f(x_1,…,x_N)dx_1…dx_N \\ where||f||_{L^2}<br>$$</p>\n</li>\n</ul>\n</li>\n<li><p>随机变量</p>\n<ul>\n<li><p>定义<br>$$<br>Soit(\\Omega,{\\cal F})\\rightarrow (E, {\\cal E}); X:\\Omega \\rightarrow E \\ \\forall A\\in {\\cal E}; X^{-1}(A) \\in {\\cal F}<br>$$<br>X就是一个随机变量</p>\n</li>\n<li><p>定义 tribu engendree par X<br>$$<br>X^{-1}({\\cal E}) = {X^{-1}(A); A \\in {\\cal E}} \\ \\sigma(X)<br>$$</p>\n</li>\n</ul>\n</li>\n<li><p>随机变量的loi</p>\n<ul>\n<li>定义<br>$$<br>\\forall A \\in {\\cal E}; P_X(A) = P({\\omega: X(\\omega) \\in A}) = P(X^{-1}(A))<br>$$</li>\n</ul>\n</li>\n<li><p>积分</p>\n<ul>\n<li><p>数学期望定义<br>$$<br>E[X] = \\int_{\\Omega}{X(\\omega).P(d\\omega)}= \\int_{\\Omega}{X.dP}<br>$$</p>\n</li>\n<li><p>定理<br>$$<br>(X_n)_{n\\in N} 是随机变量<br>$$</p>\n<ul>\n<li><p>单调趋向性：若Xn非负且趋于X，则<br>$$<br>\\lim_{n \\rightarrow \\infty}{E[X_n]} = E[X]<br>$$</p>\n</li>\n<li><p>Fatou引理：若Xn非负<br>$$<br>E[\\lim_{n \\rightarrow \\infty}{\\inf{X_n}}] \\leq \\lim_{n \\rightarrow \\infty}{\\inf{E[X_n]}}<br>$$</p>\n</li>\n<li><p>convergence dominee: 若limXn = X p.s 若存在Z in L1，且|Xn|&lt;=Z，则<br>$$<br>\\lim_{n \\rightarrow \\infty}{E[X_n]} = E[X]<br>$$</p>\n</li>\n</ul>\n</li>\n<li><p>性质(Inegalite de Markov)</p>\n<p>若X是随机变量并admettant un moment d’ordre 1, 对于实数a&gt;0<br>$$<br>P(|X| \\geq a) \\leq \\frac{E[|X|]}{a}<br>$$</p>\n</li>\n<li><p>转化定理<br>$$<br>E[h(X)] = \\int_E{h(x)P_X(dx)}<br>$$</p>\n</li>\n<li><p>定义<br>$$<br>\\text{un moment d’ordre n}: \\int_\\Omega{|X|^ndP} &lt; \\infty<br>$$</p>\n</li>\n<li><p>性质，若0&lt;p&lt;q，可以得到Lq被Lp包含。</p>\n</li>\n<li><p>定义方差，前面说过了</p>\n<ul>\n<li><p>$$<br>Var(aX+b) = a^2 Var(X) \\ P(|X-E[X]| \\geq a) \\leq \\frac{Var(X)}{a^2}<br>$$</p>\n</li>\n<li><p>若X几乎处处等于同一个常数，当仅当Var(X) = 0</p>\n</li>\n</ul>\n</li>\n<li><p>定理，与离散相似的<br>$$<br>E[h(X)] = \\int_E{h(x)f_X(dx)}<br>$$<br>特别的<br>$$<br>P(X \\in A) = E[1_{ {X\\in A} } ] = \\int_A{f_X(dx)}<br>$$</p>\n</li>\n</ul>\n</li>\n<li><p>Vecteurs aleatoires</p>\n<p>X = (X1,…,XN) 与多元离散随机变量类似。</p>\n<ul>\n<li><p>协方差<br>$$<br>Cov(X,Y) = E[(X-E[X])(Y-E[Y])] = E[XY] - E[X]E[Y]<br>$$</p>\n<ul>\n<li>性质<br>$$<br>Cov(X,X) =Var(X) \\ Cov(X,Y) = Cov(Y,X) \\ Var(X+Y) = Var(X) +Var(Y) +2Cov(X,Y)<br>$$</li>\n</ul>\n</li>\n<li><p>相关系数<br>$$<br>\\rho(X,Y) = \\frac{Cov(X,Y)}{\\sigma(X)\\sigma(Y)}, \\sigma(X) = \\sqrt{Var(X)}, -1 \\leq \\rho \\leq 1<br>$$</p>\n</li>\n<li><p>改变变量<br>$$<br>\\forall y \\in R^N; f_Y(y) = \\frac{f_X(h^{-1}(y))}{|det(Jh(h^{-1}))|} 1_D(y)<br>$$</p>\n</li>\n<li><p>边际概率，顾名思义，略</p>\n</li>\n</ul>\n</li>\n<li><p>独立变量</p>\n<p>定义略</p>\n<ul>\n<li><p>定理，满足下列条件之一，则XY独立</p>\n<ol>\n<li><p>对于所有A，B<br>$$<br>P(X \\in A, Y\\in B) = P(X\\in A)<br>$$</p>\n</li>\n<li><p>对于所有f，g<br>$$<br>E[f(X)g(Y)] = E[f(X)] E[g(Y)]<br>$$</p>\n</li>\n<li><p>对于所有f，g，f(X)和g(Y)是独立的</p>\n</li>\n</ol>\n</li>\n<li><p>性质<br>$$<br>\\text{Soient (X, Y) un couple de variables aleatoires a valeurs dans } E\\otimes F \\text{ muni de la tribu produit } \\mathcal{E} \\otimes \\tilde{\\mathcal{E}}. \\ \\text{ X et Y sont independantes si et seulement si la lor jointe du couple (X, Y) est egale a la mesure produit } P_X \\otimes P_Y.<br>$$<br>$$<br>\\text{注：对于XY相互独立，} P((X,Y)\\in A \\times B) = P_X(A)P_Y(B) \\<br>P_X \\otimes P_Y(A \\times B) = P_X(A)P_Y(B)<br>$$</p>\n</li>\n<li><p>定理，对于随机变量XY，他们相互独立，当且仅当<br>$$<br>\\forall (x,y) \\in R^2; F_{(X,Y)}(x,y) = F_X(x)F_Y(y)<br>$$</p>\n</li>\n<li><p>定理，对于(X,Y)在lebesgue测度上绝对连续，XY相互独立当且仅当<br>$$<br>\\forall (x,y) \\in R^2; f_{(X,Y)}(x,y) = f_X(x)f_Y(y)<br>$$</p>\n</li>\n<li><p>性质，XY admettant un moment d’ordre 1, 相互独立，则</p>\n</li>\n<li><p>$$<br>E[XY]=E[X]E[Y]<br>$$</p>\n</li>\n<li><p>性质，XY admettant un moment d’ordre 2, Cov(X,Y)=0, 则</p>\n</li>\n<li><p>$$<br>Var(X+Y) =Var(X) + Var(Y)<br>$$</p>\n<p>​</p>\n</li>\n</ul>\n</li>\n</ul>\n"},{"title":"proba-ch3 实域概率和特征方程","date":"2016-12-01T06:54:10.000Z","_content":"\n1. 几种常见分布\n\n    若不在区间内，f=0\n\n   1. loi uniforme\n      $$\n      f(x)=\\frac{1}{b-a} si\\ x \\in [a,b]\n      $$\n\n   2. loi exponentielle\n      $$\n      f(x) = \\lambda e^{-\\lambda x} si\\ x \\ge 0 \\\\\n      E[X] = \\frac{1}{\\lambda}, Var(X) = \\lambda^2\n      $$\n\n   3. Loi de weibull\n      $$\n      f(x) = \\alpha \\lambda^\\alpha x^{\\alpha-1}e^{-(\\lambda x)^\\alpha} si\\ x \\ge 0\\\\\n      E[X] = \\frac{\\Gamma(1+1/\\alpha)}{\\lambda}, Var(X) = \\frac{\\Gamma(1+2/\\alpha)}{\\lambda^2}\n      $$\n\n   4. loi gamma\n      $$\n      f(x) = \\frac{\\lambda}{\\Gamma(p)}(\\lambda x)^{p-1}e^{-\\lambda x}\\\\\n      E[X] = \\frac{p}{\\lambda}, Var(X) = \\frac{p}{\\lambda^2}\n      $$\n\n   5. loi normale\n      $$\n      f(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma}exp[-\\frac{(x-m)^2}{2\\sigma^2}], x \\in R\\\\\n      E[X] = m, Var(X) = \\sigma^2\n      $$\n\n   6. Loi lognormale\n      $$\n      f(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma x}exp[-\\frac{(\\ln x-m)^2}{2\\sigma^2}], x \\ge 0 \\\\\n      E[X] = e^{m+\\sigma^2/2}, Var(X) = e^{2m+\\sigma^2}(e^{\\sigma^@}-1)\n      $$\n\n   7. Loi du $ \\chi^2 $\n      $$\n      X = U_1^2+...+U_n^2 \\\\\n      U_i\\ est\\ de\\ loi\\ \\mathcal{N}(0,1) \\\\\n      E[X] = n, Var(X) = 2n\n      $$\n\n   8. Loi de Student\n      $$\n      X = \\frac{U}{\\sqrt{\\frac{Z}{n}}} \\\\\n      U\\ suit\\ la\\ loi\\ normale\\ \\mathcal{N}(0,1) \\\\\n      Z\\text{ est independante de U et suit la loi du }\\chi^2\\text{ a n degres de liberte}\n      $$\n\n2. 特征函数\n   $$\n   \\varphi_X: \\mathbb{R^N} \\rightarrow \\mathbb{C} \\\\\n   t \\rightarrow \\varphi_X(t) = E[e^{i<t,X>}] = \\int_{\\mathbb{R}^N}{e^{i<t,X>}P_X(dx)}\n   $$\n   我们说特征函数是对X进行基于PX的傅立叶变换 = =好绕\n\n   若PX存在概率密度 $f \\in L^1$\n   $$\n   \\varphi_X(t) = \\int_{\\mathbb{R}^N}{e^{i<t,X>}f(x)dx}\n   $$\n\n   - 性质\n\n     - $$\n       \\forall t \\in \\mathbb{R}^N, |\\varphi_X(t)| \\le 1=> \\varphi_X(0) = 1\n       $$\n\n       $$\n       \\varphi_{\\lambda X+a}(t) = e^{iat}\\varphi_X(\\lambda t)\n       $$\n\n       $$\n       \\varphi_X 是一个半正函数，即 \\\\\n       \\forall z_1,...,z_n\\in \\mathbb{C}, \\sum_{1 \\le j, k\\le n}{z_j \\varphi_X(t_j-t_k)\\bar{z_k}} \\ge 0\n       $$\n\n   - 其他性质\n\n     - 特征函数连续\n\n     - PX以lebesgue测度绝对连续，则\n\n     - $$\n       \\lim_{|t|\\rightarrow \\infty}{\\varphi_X(t)} = 0\n       $$\n\n     - XY，它们拥有相同的P，当且仅当\n\n     - $$\n       \\varphi_X=\\varphi_Y\n       $$\n\n     - 逆变换\n\n     - $$\n       \\forall x \\in \\mathbb{R}^N, f(x)=\\frac{1}{(2\\pi)^N}\\int_{\\mathbb{R}^N}{e^{-i<t,X>}\\varphi_X(t)}dt\n       $$\n\n   - 特征方程和独立性\n\n     - 定理\n       $$\n       X_1,...,X_N \\text{ 是独立的，当且仅当它们的特征方程满足：} \\\\\n       \\forall t = (t_1,...,t_N) \\in \\mathbb{R}^N; \\varphi_X(t) = \\prod_{k=1}^{N}{\\varphi_{X_k}(t_k) } \\\\\n       where\\ X = (X_1,...,X_N)\n       $$\n\n     - 性质\n       $$\n       X_1,...,X_n independants, P_{X_1},...,P_{X_N}. \\\\\n       \\text{La loi de } \\sum{X_i} \\text{est le produit de concolution } \\prod{P_{X_i}} \\\\\n       \\text{Pour fonction caracteristique } \\sum{\\varphi_{X_i}} \\text{definie par} \\\\\n       \\forall t \\in \\mathbb{R}^N; \\varphi_{X+...+X_n}(t) = \\prod_{i=1}^{n}{\\varphi_{X_i}(t) }\n       $$\n\n\n","source":"_posts/proba-ch3.md","raw":"---\ntitle: proba-ch3 实域概率和特征方程\ndate: 2016-12-01 14:54:10\ncategories: math\ntags: [probability, math]\n---\n\n1. 几种常见分布\n\n    若不在区间内，f=0\n\n   1. loi uniforme\n      $$\n      f(x)=\\frac{1}{b-a} si\\ x \\in [a,b]\n      $$\n\n   2. loi exponentielle\n      $$\n      f(x) = \\lambda e^{-\\lambda x} si\\ x \\ge 0 \\\\\n      E[X] = \\frac{1}{\\lambda}, Var(X) = \\lambda^2\n      $$\n\n   3. Loi de weibull\n      $$\n      f(x) = \\alpha \\lambda^\\alpha x^{\\alpha-1}e^{-(\\lambda x)^\\alpha} si\\ x \\ge 0\\\\\n      E[X] = \\frac{\\Gamma(1+1/\\alpha)}{\\lambda}, Var(X) = \\frac{\\Gamma(1+2/\\alpha)}{\\lambda^2}\n      $$\n\n   4. loi gamma\n      $$\n      f(x) = \\frac{\\lambda}{\\Gamma(p)}(\\lambda x)^{p-1}e^{-\\lambda x}\\\\\n      E[X] = \\frac{p}{\\lambda}, Var(X) = \\frac{p}{\\lambda^2}\n      $$\n\n   5. loi normale\n      $$\n      f(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma}exp[-\\frac{(x-m)^2}{2\\sigma^2}], x \\in R\\\\\n      E[X] = m, Var(X) = \\sigma^2\n      $$\n\n   6. Loi lognormale\n      $$\n      f(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma x}exp[-\\frac{(\\ln x-m)^2}{2\\sigma^2}], x \\ge 0 \\\\\n      E[X] = e^{m+\\sigma^2/2}, Var(X) = e^{2m+\\sigma^2}(e^{\\sigma^@}-1)\n      $$\n\n   7. Loi du $ \\chi^2 $\n      $$\n      X = U_1^2+...+U_n^2 \\\\\n      U_i\\ est\\ de\\ loi\\ \\mathcal{N}(0,1) \\\\\n      E[X] = n, Var(X) = 2n\n      $$\n\n   8. Loi de Student\n      $$\n      X = \\frac{U}{\\sqrt{\\frac{Z}{n}}} \\\\\n      U\\ suit\\ la\\ loi\\ normale\\ \\mathcal{N}(0,1) \\\\\n      Z\\text{ est independante de U et suit la loi du }\\chi^2\\text{ a n degres de liberte}\n      $$\n\n2. 特征函数\n   $$\n   \\varphi_X: \\mathbb{R^N} \\rightarrow \\mathbb{C} \\\\\n   t \\rightarrow \\varphi_X(t) = E[e^{i<t,X>}] = \\int_{\\mathbb{R}^N}{e^{i<t,X>}P_X(dx)}\n   $$\n   我们说特征函数是对X进行基于PX的傅立叶变换 = =好绕\n\n   若PX存在概率密度 $f \\in L^1$\n   $$\n   \\varphi_X(t) = \\int_{\\mathbb{R}^N}{e^{i<t,X>}f(x)dx}\n   $$\n\n   - 性质\n\n     - $$\n       \\forall t \\in \\mathbb{R}^N, |\\varphi_X(t)| \\le 1=> \\varphi_X(0) = 1\n       $$\n\n       $$\n       \\varphi_{\\lambda X+a}(t) = e^{iat}\\varphi_X(\\lambda t)\n       $$\n\n       $$\n       \\varphi_X 是一个半正函数，即 \\\\\n       \\forall z_1,...,z_n\\in \\mathbb{C}, \\sum_{1 \\le j, k\\le n}{z_j \\varphi_X(t_j-t_k)\\bar{z_k}} \\ge 0\n       $$\n\n   - 其他性质\n\n     - 特征函数连续\n\n     - PX以lebesgue测度绝对连续，则\n\n     - $$\n       \\lim_{|t|\\rightarrow \\infty}{\\varphi_X(t)} = 0\n       $$\n\n     - XY，它们拥有相同的P，当且仅当\n\n     - $$\n       \\varphi_X=\\varphi_Y\n       $$\n\n     - 逆变换\n\n     - $$\n       \\forall x \\in \\mathbb{R}^N, f(x)=\\frac{1}{(2\\pi)^N}\\int_{\\mathbb{R}^N}{e^{-i<t,X>}\\varphi_X(t)}dt\n       $$\n\n   - 特征方程和独立性\n\n     - 定理\n       $$\n       X_1,...,X_N \\text{ 是独立的，当且仅当它们的特征方程满足：} \\\\\n       \\forall t = (t_1,...,t_N) \\in \\mathbb{R}^N; \\varphi_X(t) = \\prod_{k=1}^{N}{\\varphi_{X_k}(t_k) } \\\\\n       where\\ X = (X_1,...,X_N)\n       $$\n\n     - 性质\n       $$\n       X_1,...,X_n independants, P_{X_1},...,P_{X_N}. \\\\\n       \\text{La loi de } \\sum{X_i} \\text{est le produit de concolution } \\prod{P_{X_i}} \\\\\n       \\text{Pour fonction caracteristique } \\sum{\\varphi_{X_i}} \\text{definie par} \\\\\n       \\forall t \\in \\mathbb{R}^N; \\varphi_{X+...+X_n}(t) = \\prod_{i=1}^{n}{\\varphi_{X_i}(t) }\n       $$\n\n\n","slug":"proba-ch3","published":1,"updated":"2016-12-01T15:28:37.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckw4qufqh004fgwtl3ik43tc5","content":"<ol>\n<li><p>几种常见分布</p>\n<p> 若不在区间内，f=0</p>\n<ol>\n<li><p>loi uniforme<br>$$<br>f(x)=\\frac{1}{b-a} si\\ x \\in [a,b]<br>$$</p>\n</li>\n<li><p>loi exponentielle<br>$$<br>f(x) = \\lambda e^{-\\lambda x} si\\ x \\ge 0 \\<br>E[X] = \\frac{1}{\\lambda}, Var(X) = \\lambda^2<br>$$</p>\n</li>\n<li><p>Loi de weibull<br>$$<br>f(x) = \\alpha \\lambda^\\alpha x^{\\alpha-1}e^{-(\\lambda x)^\\alpha} si\\ x \\ge 0\\<br>E[X] = \\frac{\\Gamma(1+1/\\alpha)}{\\lambda}, Var(X) = \\frac{\\Gamma(1+2/\\alpha)}{\\lambda^2}<br>$$</p>\n</li>\n<li><p>loi gamma<br>$$<br>f(x) = \\frac{\\lambda}{\\Gamma(p)}(\\lambda x)^{p-1}e^{-\\lambda x}\\<br>E[X] = \\frac{p}{\\lambda}, Var(X) = \\frac{p}{\\lambda^2}<br>$$</p>\n</li>\n<li><p>loi normale<br>$$<br>f(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma}exp[-\\frac{(x-m)^2}{2\\sigma^2}], x \\in R\\<br>E[X] = m, Var(X) = \\sigma^2<br>$$</p>\n</li>\n<li><p>Loi lognormale<br>$$<br>f(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma x}exp[-\\frac{(\\ln x-m)^2}{2\\sigma^2}], x \\ge 0 \\<br>E[X] = e^{m+\\sigma^2/2}, Var(X) = e^{2m+\\sigma^2}(e^{\\sigma^@}-1)<br>$$</p>\n</li>\n<li><p>Loi du $ \\chi^2 $<br>$$<br>X = U_1^2+…+U_n^2 \\<br>U_i\\ est\\ de\\ loi\\ \\mathcal{N}(0,1) \\<br>E[X] = n, Var(X) = 2n<br>$$</p>\n</li>\n<li><p>Loi de Student<br>$$<br>X = \\frac{U}{\\sqrt{\\frac{Z}{n}}} \\<br>U\\ suit\\ la\\ loi\\ normale\\ \\mathcal{N}(0,1) \\<br>Z\\text{ est independante de U et suit la loi du }\\chi^2\\text{ a n degres de liberte}<br>$$</p>\n</li>\n</ol>\n</li>\n<li><p>特征函数<br>$$<br>\\varphi_X: \\mathbb{R^N} \\rightarrow \\mathbb{C} \\<br>t \\rightarrow \\varphi_X(t) = E[e^{i&lt;t,X&gt;}] = \\int_{\\mathbb{R}^N}{e^{i&lt;t,X&gt;}P_X(dx)}<br>$$<br>我们说特征函数是对X进行基于PX的傅立叶变换 = =好绕</p>\n<p>若PX存在概率密度 $f \\in L^1$<br>$$<br>\\varphi_X(t) = \\int_{\\mathbb{R}^N}{e^{i&lt;t,X&gt;}f(x)dx}<br>$$</p>\n<ul>\n<li><p>性质</p>\n<ul>\n<li><p>$$<br>\\forall t \\in \\mathbb{R}^N, |\\varphi_X(t)| \\le 1=&gt; \\varphi_X(0) = 1<br>$$</p>\n<p>$$<br>\\varphi_{\\lambda X+a}(t) = e^{iat}\\varphi_X(\\lambda t)<br>$$</p>\n<p>$$<br>\\varphi_X 是一个半正函数，即 \\<br>\\forall z_1,…,z_n\\in \\mathbb{C}, \\sum_{1 \\le j, k\\le n}{z_j \\varphi_X(t_j-t_k)\\bar{z_k}} \\ge 0<br>$$</p>\n</li>\n</ul>\n</li>\n<li><p>其他性质</p>\n<ul>\n<li><p>特征函数连续</p>\n</li>\n<li><p>PX以lebesgue测度绝对连续，则</p>\n</li>\n<li><p>$$<br>\\lim_{|t|\\rightarrow \\infty}{\\varphi_X(t)} = 0<br>$$</p>\n</li>\n<li><p>XY，它们拥有相同的P，当且仅当</p>\n</li>\n<li><p>$$<br>\\varphi_X=\\varphi_Y<br>$$</p>\n</li>\n<li><p>逆变换</p>\n</li>\n<li><p>$$<br>\\forall x \\in \\mathbb{R}^N, f(x)=\\frac{1}{(2\\pi)^N}\\int_{\\mathbb{R}^N}{e^{-i&lt;t,X&gt;}\\varphi_X(t)}dt<br>$$</p>\n</li>\n</ul>\n</li>\n<li><p>特征方程和独立性</p>\n<ul>\n<li><p>定理<br>$$<br>X_1,…,X_N \\text{ 是独立的，当且仅当它们的特征方程满足：} \\<br>\\forall t = (t_1,…,t_N) \\in \\mathbb{R}^N; \\varphi_X(t) = \\prod_{k=1}^{N}{\\varphi_{X_k}(t_k) } \\<br>where\\ X = (X_1,…,X_N)<br>$$</p>\n</li>\n<li><p>性质<br>$$<br>X_1,…,X_n independants, P_{X_1},…,P_{X_N}. \\<br>\\text{La loi de } \\sum{X_i} \\text{est le produit de concolution } \\prod{P_{X_i}} \\<br>\\text{Pour fonction caracteristique } \\sum{\\varphi_{X_i}} \\text{definie par} \\<br>\\forall t \\in \\mathbb{R}^N; \\varphi_{X+…+X_n}(t) = \\prod_{i=1}^{n}{\\varphi_{X_i}(t) }<br>$$</p>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ol>\n","site":{"data":{"recommended_posts":{"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"}],"https://LorrinWWW.github.io/posts/complexity/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/hexo-with-latex/":[{"title":"区块链时代的最正确的打赏姿势——在Hexo博客配置Coinhive网页挖矿","permalink":"http://www.davidfnck.com/blockchain/mine-xmr-in-hexo-by-coinhive.html/"}],"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/":[{"title":"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-statistic/":[{"title":"贝叶斯估计 Bayes estimation","permalink":"https://LorrinWWW.github.io/posts/Bayes-estimation/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"}],"https://LorrinWWW.github.io/posts/[2018.2.5]Nested-LSTMs/":[{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"}],"https://LorrinWWW.github.io/posts/datamining-pretreatment/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Several models for knowledge graph representing and completing 几个知识图谱模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/"}],"https://LorrinWWW.github.io/posts/compression/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/Sobolev-space/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/proba-ch5/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch3/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/proba-ch1/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/ML-CNN/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/EDP-basic-models/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/machine-learning/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/datamining-class-pred/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction/":[],"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/":[{"title":"OS notes","permalink":"https://LorrinWWW.github.io/posts/OS-notes/"}],"https://LorrinWWW.github.io/posts/projet-enjeu-plugin-chrome-101/":[{"title":"Chrome油猴脚本","permalink":"http://gubangzhong.cn/2017/06/13/Chrome油猴脚本/"}],"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/":[{"title":"MongoDB, Docker and Python","permalink":"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/":[{"title":"Hands on Scrapy","permalink":"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/OS-notes/":[{"title":"learning OS and building LorriOS","permalink":"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/"}],"https://LorrinWWW.github.io/posts/Bayes-estimation/":[{"title":"Note of statistic","permalink":"https://LorrinWWW.github.io/posts/Note-of-statistic/"}],"https://LorrinWWW.github.io/posts/management-of-the-firm/":[],"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"}],"https://LorrinWWW.github.io/posts/datamining-qualitative-induction/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-finite-element-method/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/QuadTree/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/proba-ch4/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch2/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/":[{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/Note-of-NLP/":[{"title":"Generative Adversarial Network","permalink":"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/"},{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"}],"https://LorrinWWW.github.io/posts/Note-of-datamining/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"数据挖掘-分类与预测","permalink":"https://LorrinWWW.github.io/posts/datamining-class-pred/"},{"title":"数据挖掘-预处理","permalink":"https://LorrinWWW.github.io/posts/datamining-pretreatment/"}],"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/Note-of-probability/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/participe-present-et-gerondif/":[],"https://LorrinWWW.github.io/posts/Hilbert-space/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/proba-ch6/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}]}}},"excerpt":"","more":"<ol>\n<li><p>几种常见分布</p>\n<p> 若不在区间内，f=0</p>\n<ol>\n<li><p>loi uniforme<br>$$<br>f(x)=\\frac{1}{b-a} si\\ x \\in [a,b]<br>$$</p>\n</li>\n<li><p>loi exponentielle<br>$$<br>f(x) = \\lambda e^{-\\lambda x} si\\ x \\ge 0 \\<br>E[X] = \\frac{1}{\\lambda}, Var(X) = \\lambda^2<br>$$</p>\n</li>\n<li><p>Loi de weibull<br>$$<br>f(x) = \\alpha \\lambda^\\alpha x^{\\alpha-1}e^{-(\\lambda x)^\\alpha} si\\ x \\ge 0\\<br>E[X] = \\frac{\\Gamma(1+1/\\alpha)}{\\lambda}, Var(X) = \\frac{\\Gamma(1+2/\\alpha)}{\\lambda^2}<br>$$</p>\n</li>\n<li><p>loi gamma<br>$$<br>f(x) = \\frac{\\lambda}{\\Gamma(p)}(\\lambda x)^{p-1}e^{-\\lambda x}\\<br>E[X] = \\frac{p}{\\lambda}, Var(X) = \\frac{p}{\\lambda^2}<br>$$</p>\n</li>\n<li><p>loi normale<br>$$<br>f(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma}exp[-\\frac{(x-m)^2}{2\\sigma^2}], x \\in R\\<br>E[X] = m, Var(X) = \\sigma^2<br>$$</p>\n</li>\n<li><p>Loi lognormale<br>$$<br>f(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma x}exp[-\\frac{(\\ln x-m)^2}{2\\sigma^2}], x \\ge 0 \\<br>E[X] = e^{m+\\sigma^2/2}, Var(X) = e^{2m+\\sigma^2}(e^{\\sigma^@}-1)<br>$$</p>\n</li>\n<li><p>Loi du $ \\chi^2 $<br>$$<br>X = U_1^2+…+U_n^2 \\<br>U_i\\ est\\ de\\ loi\\ \\mathcal{N}(0,1) \\<br>E[X] = n, Var(X) = 2n<br>$$</p>\n</li>\n<li><p>Loi de Student<br>$$<br>X = \\frac{U}{\\sqrt{\\frac{Z}{n}}} \\<br>U\\ suit\\ la\\ loi\\ normale\\ \\mathcal{N}(0,1) \\<br>Z\\text{ est independante de U et suit la loi du }\\chi^2\\text{ a n degres de liberte}<br>$$</p>\n</li>\n</ol>\n</li>\n<li><p>特征函数<br>$$<br>\\varphi_X: \\mathbb{R^N} \\rightarrow \\mathbb{C} \\<br>t \\rightarrow \\varphi_X(t) = E[e^{i&lt;t,X&gt;}] = \\int_{\\mathbb{R}^N}{e^{i&lt;t,X&gt;}P_X(dx)}<br>$$<br>我们说特征函数是对X进行基于PX的傅立叶变换 = =好绕</p>\n<p>若PX存在概率密度 $f \\in L^1$<br>$$<br>\\varphi_X(t) = \\int_{\\mathbb{R}^N}{e^{i&lt;t,X&gt;}f(x)dx}<br>$$</p>\n<ul>\n<li><p>性质</p>\n<ul>\n<li><p>$$<br>\\forall t \\in \\mathbb{R}^N, |\\varphi_X(t)| \\le 1=&gt; \\varphi_X(0) = 1<br>$$</p>\n<p>$$<br>\\varphi_{\\lambda X+a}(t) = e^{iat}\\varphi_X(\\lambda t)<br>$$</p>\n<p>$$<br>\\varphi_X 是一个半正函数，即 \\<br>\\forall z_1,…,z_n\\in \\mathbb{C}, \\sum_{1 \\le j, k\\le n}{z_j \\varphi_X(t_j-t_k)\\bar{z_k}} \\ge 0<br>$$</p>\n</li>\n</ul>\n</li>\n<li><p>其他性质</p>\n<ul>\n<li><p>特征函数连续</p>\n</li>\n<li><p>PX以lebesgue测度绝对连续，则</p>\n</li>\n<li><p>$$<br>\\lim_{|t|\\rightarrow \\infty}{\\varphi_X(t)} = 0<br>$$</p>\n</li>\n<li><p>XY，它们拥有相同的P，当且仅当</p>\n</li>\n<li><p>$$<br>\\varphi_X=\\varphi_Y<br>$$</p>\n</li>\n<li><p>逆变换</p>\n</li>\n<li><p>$$<br>\\forall x \\in \\mathbb{R}^N, f(x)=\\frac{1}{(2\\pi)^N}\\int_{\\mathbb{R}^N}{e^{-i&lt;t,X&gt;}\\varphi_X(t)}dt<br>$$</p>\n</li>\n</ul>\n</li>\n<li><p>特征方程和独立性</p>\n<ul>\n<li><p>定理<br>$$<br>X_1,…,X_N \\text{ 是独立的，当且仅当它们的特征方程满足：} \\<br>\\forall t = (t_1,…,t_N) \\in \\mathbb{R}^N; \\varphi_X(t) = \\prod_{k=1}^{N}{\\varphi_{X_k}(t_k) } \\<br>where\\ X = (X_1,…,X_N)<br>$$</p>\n</li>\n<li><p>性质<br>$$<br>X_1,…,X_n independants, P_{X_1},…,P_{X_N}. \\<br>\\text{La loi de } \\sum{X_i} \\text{est le produit de concolution } \\prod{P_{X_i}} \\<br>\\text{Pour fonction caracteristique } \\sum{\\varphi_{X_i}} \\text{definie par} \\<br>\\forall t \\in \\mathbb{R}^N; \\varphi_{X+…+X_n}(t) = \\prod_{i=1}^{n}{\\varphi_{X_i}(t) }<br>$$</p>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ol>\n"},{"title":"proba-ch4 高斯向量","date":"2016-12-01T08:28:52.000Z","_content":"\n定义\n\nUn vecteur aléatoire est dit gaussien si toute combinaison linéaire de ses composantes suit une loi gaussienne.\n\n也就是：\n$$\n\\forall \\lambda_1,...,\\lambda_N \\in \\mathbb{R} , \\sum_{j=1}^{N}{\\lambda_jX_j \\text{ suit une loi normale.}}\n$$\n\n- 特征函数\n  $$\n  \\varphi_X: t \\rightarrow e^{i < \\mu, t> -\\frac{1}{2}<t,Dt>} \\\\\n  => \\varphi_X = e^{i\\sum_{j=1}^{N}{\\mu_j t_j}- \\frac{1}{2}\\sum_{1\\le j,k\\le N}{t_j D_{j,k}t_k}}\n  $$\n  $\\mu$, le vecteur moyenne, 平均向量\n\n  D, la matrice de covariances de X, 协方差矩阵\n\n- 理论，X=(X1,…,XN) 是高斯向量，则Xj相互独立当仅当其协方差矩阵D为对角线矩阵。（也就是只有自己和自己的协方差不为零）\n\n- Densite de la loi d'un vecteur gaussien\n  $$\n  D \\ne 0 \\\\\n  \\mathcal{N}(\\mu,D) \\text{在Lebesgue测度下，在}\\mathbb{R}^N\\text{上绝对连续} \\\\\n  x \\longmapsto \\frac{1}{(2\\pi)^{N/2}\\sqrt{\\det D}}e^{-\\frac{1}{2}<x-\\mu,D^{-1}(x-\\mu)>}\n  $$\n\n","source":"_posts/proba-ch4.md","raw":"---\ntitle: proba-ch4 高斯向量\ndate: 2016-12-01 16:28:52\ncategories: math\ntags: [probability, math, vector]\n---\n\n定义\n\nUn vecteur aléatoire est dit gaussien si toute combinaison linéaire de ses composantes suit une loi gaussienne.\n\n也就是：\n$$\n\\forall \\lambda_1,...,\\lambda_N \\in \\mathbb{R} , \\sum_{j=1}^{N}{\\lambda_jX_j \\text{ suit une loi normale.}}\n$$\n\n- 特征函数\n  $$\n  \\varphi_X: t \\rightarrow e^{i < \\mu, t> -\\frac{1}{2}<t,Dt>} \\\\\n  => \\varphi_X = e^{i\\sum_{j=1}^{N}{\\mu_j t_j}- \\frac{1}{2}\\sum_{1\\le j,k\\le N}{t_j D_{j,k}t_k}}\n  $$\n  $\\mu$, le vecteur moyenne, 平均向量\n\n  D, la matrice de covariances de X, 协方差矩阵\n\n- 理论，X=(X1,…,XN) 是高斯向量，则Xj相互独立当仅当其协方差矩阵D为对角线矩阵。（也就是只有自己和自己的协方差不为零）\n\n- Densite de la loi d'un vecteur gaussien\n  $$\n  D \\ne 0 \\\\\n  \\mathcal{N}(\\mu,D) \\text{在Lebesgue测度下，在}\\mathbb{R}^N\\text{上绝对连续} \\\\\n  x \\longmapsto \\frac{1}{(2\\pi)^{N/2}\\sqrt{\\det D}}e^{-\\frac{1}{2}<x-\\mu,D^{-1}(x-\\mu)>}\n  $$\n\n","slug":"proba-ch4","published":1,"updated":"2016-12-19T13:11:37.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckw4qufqi004jgwtl1whs0az0","content":"<p>定义</p>\n<p>Un vecteur aléatoire est dit gaussien si toute combinaison linéaire de ses composantes suit une loi gaussienne.</p>\n<p>也就是：<br>$$<br>\\forall \\lambda_1,…,\\lambda_N \\in \\mathbb{R} , \\sum_{j=1}^{N}{\\lambda_jX_j \\text{ suit une loi normale.}}<br>$$</p>\n<ul>\n<li><p>特征函数<br>$$<br>\\varphi_X: t \\rightarrow e^{i &lt; \\mu, t&gt; -\\frac{1}{2}&lt;t,Dt&gt;} \\<br>=&gt; \\varphi_X = e^{i\\sum_{j=1}^{N}{\\mu_j t_j}- \\frac{1}{2}\\sum_{1\\le j,k\\le N}{t_j D_{j,k}t_k}}<br>$$<br>$\\mu$, le vecteur moyenne, 平均向量</p>\n<p>D, la matrice de covariances de X, 协方差矩阵</p>\n</li>\n<li><p>理论，X=(X1,…,XN) 是高斯向量，则Xj相互独立当仅当其协方差矩阵D为对角线矩阵。（也就是只有自己和自己的协方差不为零）</p>\n</li>\n<li><p>Densite de la loi d’un vecteur gaussien<br>$$<br>D \\ne 0 \\<br>\\mathcal{N}(\\mu,D) \\text{在Lebesgue测度下，在}\\mathbb{R}^N\\text{上绝对连续} \\<br>x \\longmapsto \\frac{1}{(2\\pi)^{N/2}\\sqrt{\\det D}}e^{-\\frac{1}{2}&lt;x-\\mu,D^{-1}(x-\\mu)&gt;}<br>$$</p>\n</li>\n</ul>\n","site":{"data":{"recommended_posts":{"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"}],"https://LorrinWWW.github.io/posts/complexity/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/hexo-with-latex/":[{"title":"区块链时代的最正确的打赏姿势——在Hexo博客配置Coinhive网页挖矿","permalink":"http://www.davidfnck.com/blockchain/mine-xmr-in-hexo-by-coinhive.html/"}],"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/":[{"title":"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-statistic/":[{"title":"贝叶斯估计 Bayes estimation","permalink":"https://LorrinWWW.github.io/posts/Bayes-estimation/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"}],"https://LorrinWWW.github.io/posts/[2018.2.5]Nested-LSTMs/":[{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"}],"https://LorrinWWW.github.io/posts/datamining-pretreatment/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Several models for knowledge graph representing and completing 几个知识图谱模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/"}],"https://LorrinWWW.github.io/posts/compression/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/Sobolev-space/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/proba-ch5/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch3/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/proba-ch1/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/ML-CNN/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/EDP-basic-models/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/machine-learning/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/datamining-class-pred/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction/":[],"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/":[{"title":"OS notes","permalink":"https://LorrinWWW.github.io/posts/OS-notes/"}],"https://LorrinWWW.github.io/posts/projet-enjeu-plugin-chrome-101/":[{"title":"Chrome油猴脚本","permalink":"http://gubangzhong.cn/2017/06/13/Chrome油猴脚本/"}],"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/":[{"title":"MongoDB, Docker and Python","permalink":"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/":[{"title":"Hands on Scrapy","permalink":"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/OS-notes/":[{"title":"learning OS and building LorriOS","permalink":"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/"}],"https://LorrinWWW.github.io/posts/Bayes-estimation/":[{"title":"Note of statistic","permalink":"https://LorrinWWW.github.io/posts/Note-of-statistic/"}],"https://LorrinWWW.github.io/posts/management-of-the-firm/":[],"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"}],"https://LorrinWWW.github.io/posts/datamining-qualitative-induction/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-finite-element-method/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/QuadTree/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/proba-ch4/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch2/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/":[{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/Note-of-NLP/":[{"title":"Generative Adversarial Network","permalink":"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/"},{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"}],"https://LorrinWWW.github.io/posts/Note-of-datamining/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"数据挖掘-分类与预测","permalink":"https://LorrinWWW.github.io/posts/datamining-class-pred/"},{"title":"数据挖掘-预处理","permalink":"https://LorrinWWW.github.io/posts/datamining-pretreatment/"}],"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/Note-of-probability/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/participe-present-et-gerondif/":[],"https://LorrinWWW.github.io/posts/Hilbert-space/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/proba-ch6/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}]}}},"excerpt":"","more":"<p>定义</p>\n<p>Un vecteur aléatoire est dit gaussien si toute combinaison linéaire de ses composantes suit une loi gaussienne.</p>\n<p>也就是：<br>$$<br>\\forall \\lambda_1,…,\\lambda_N \\in \\mathbb{R} , \\sum_{j=1}^{N}{\\lambda_jX_j \\text{ suit une loi normale.}}<br>$$</p>\n<ul>\n<li><p>特征函数<br>$$<br>\\varphi_X: t \\rightarrow e^{i &lt; \\mu, t&gt; -\\frac{1}{2}&lt;t,Dt&gt;} \\<br>=&gt; \\varphi_X = e^{i\\sum_{j=1}^{N}{\\mu_j t_j}- \\frac{1}{2}\\sum_{1\\le j,k\\le N}{t_j D_{j,k}t_k}}<br>$$<br>$\\mu$, le vecteur moyenne, 平均向量</p>\n<p>D, la matrice de covariances de X, 协方差矩阵</p>\n</li>\n<li><p>理论，X=(X1,…,XN) 是高斯向量，则Xj相互独立当仅当其协方差矩阵D为对角线矩阵。（也就是只有自己和自己的协方差不为零）</p>\n</li>\n<li><p>Densite de la loi d’un vecteur gaussien<br>$$<br>D \\ne 0 \\<br>\\mathcal{N}(\\mu,D) \\text{在Lebesgue测度下，在}\\mathbb{R}^N\\text{上绝对连续} \\<br>x \\longmapsto \\frac{1}{(2\\pi)^{N/2}\\sqrt{\\det D}}e^{-\\frac{1}{2}&lt;x-\\mu,D^{-1}(x-\\mu)&gt;}<br>$$</p>\n</li>\n</ul>\n"},{"title":"proba-ch5 数列和随机变量系列","date":"2016-12-01T15:14:28.000Z","_content":"\n# 数列和随机变量系列\n\n1. 零一律\n\n   > **零一律**是概率论中的一个定律，它是安德雷·柯尔莫哥洛夫发现的，因此有时也叫柯尔莫哥洛夫零一律。其内容是：有些事件发生的概率不是几乎一（几乎肯定发生），就是几乎零（几乎肯定不发生）。这样的事件被称为“尾事件”。 — wiki\n\n   $$\n   (X_n)_{n \\in \\mathbb{N}} v.a. \\\\\n   \\mathcal{T}_n = \\sigma(X_k; k \\ge n) \\\\\n   \\mathcal{T}_{\\infty} = \\cap_{n \\in \\mathbb{N}}{\\mathcal{T}_n} \\text{ est   appelee tribu de queue de la suite } (X_n)_{n \\in \\mathbb{N}}\n   $$\n   - 定理(loi de zero-un)\n\n     如果存在一个事件A属于$\\mathcal{T}_{\\infty}$，则P(A)等于0或1。\n\n2. 不同定义\n\n   - convergence presque sure\n\n     定义，当存在一个事件满足以下条件\n     $$\n     \\exists \\Omega^*, \\forall \\omega \\in \\Omega^*, \\lim_{n \\rightarrow \\infty}{X_n(\\omega)} = X(\\omega)\n     $$\n     我们认为一个随机变量序列趋向(p.s.)一个随机变量。即\n     $$\n     P\\{\\omega \\in \\Omega^*: \\lim_{n \\rightarrow \\infty}{X_n(\\omega)} = X(\\omega)\\} = 1\n     $$\n\n   - Convergence dans Lp\n\n     定义。首先Xn和X都在Lp空间下，并且其差值的绝对值的p次方的期望的极限等于零。- -|||\n\n     也就是\n     $$\n     \\lim_{n \\rightarrow \\infty}{E[|X_n-X|^p]} = 0\n     $$\n\n   - convergence en probabilite\n\n     定义，满足以下条件，称依概率收敛。\n     $$\n     \\forall \\varepsilon, \\lim_{n \\to \\infty}{P\\{\\omega:X_n(\\omega)-X(\\omega)>\\varepsilon}\\}=0 \\\\\n     ou\\  \\lim_{n \\to \\infty}{P\\{X_n-X>\\varepsilon}\\}=0\n     $$\n     定理，\n\n     1. 如果Xn趋向X(p.s.)，则fXn趋向fX(p.s.)\n     2. 如果Xn趋向X(P)，则fXn趋向fX(P)\n\n     定理，\n\n     Xn是实随机变量，以下关系等价\n     $$\n     X_n \\xrightarrow{P} X \\Leftrightarrow \\lim_{n \\to \\infty} E(\\frac{|X_n-X|}{1+|X_n-X|}) = 0\n     $$\n     定理，\n\n     Xn是实随机变量，则\n     $$\n     若X_n \\xrightarrow{L^P}X, X_n \\xrightarrow{P}{X} \\\\\n     若X_n \\to X p.s., X_n \\xrightarrow{P}{X} \\\\\n     $$\n     定理，\n\n     Xn是实随机变量，若Xn依概率收敛于X，我们能找到一个序列\n     $$\n     (X_{n_k})_{k \\in \\mathbb{N}}\n     $$\n     使得\n     $$\n     X_n \\to X, p.s.\n     $$\n     定理，\n\n     Xn是实随机变量，若Xn依概率收敛于X，并存在一个$Y \\in L^p, |X_n| \\le Y$，则\n     $$\n     X \\in L^p并且 X_n \\xrightarrow{L^p}X\n     $$\n\n\n\n\n\n3. 波莱尔－坎泰利引理\n\n   > 大致上，波莱尔－坎泰利引理说明了，如果有无穷个概率事件，它们发生的概率之和是有限的，那么其中的无限多个事件一同发生的概率是零。这个定理实际上是测度论的结论在概率论中的应用。 —wiki\n\n   1. 如果有无穷个概率事件，它们发生的概率之和是有限的，那么其中的无限多个事件一同发生的概率是零。\n   2. 如果有无穷个概率事件，无限多个事件一同发生的概率是零，那么它们发生的概率之和是有限的。\n\n4. 大数定律\n\n   > 在数学与统计学中，大数定律又称大数法则、大数律，是描述相当多次数重复实验的结果的定律。根据这个定律知道，样本数量越多，则其平均就越趋近期望值。 — wiki\n\n   - 强大数定律\n     $$\n     X_n \\in L^2 \\text{若它们独立同分布且在同一个概率空间，则} \\\\\n     lim_{n \\to \\infty}{\\frac{\\sum_{i=1}^{n}{X_i}}{n}} = \\mu \\ p.s.\n     $$\n\n   - 切比雪夫大数定律\n     $$\n     X_n  \\text{若它们独立同分布且在同一个概率空间，则} \\\\\n     lim_{n \\to \\infty}{\\frac{\\sum_{i=1}^{n}{X_i}}{n}} = \\mu \\ p.s. \\text{ 当且仅当} E[X_i] \\text{对于所有i存在}\n     $$\n\n5. Convergence en loi\n\n   弱趋向：\n   $$\n   \\int_{\\mathbb{R}^N}{f(x)\\nu_n(dx)} \\xrightarrow{n \\to \\infty} \\int_{\\mathbb{R}^N}{f(x)\\nu(dx)}\n   $$\n   定义，若PXn弱趋向于PX，则Xn是 convergence en loi vers X。记为：\n   $$\n   X_n \\xrightarrow{\\mathcal{D}} X\n   $$\n   定理，\n   $$\n   X_n \\xrightarrow{\\mathcal{D}} X \\Leftrightarrow \\lim_{n \\to \\infty}E[f(X_n)] = E[f(X)]\n   $$\n   定理，\n   $$\n   X_n \\xrightarrow{P} X \\Rightarrow X_n \\xrightarrow{\\mathcal{D}} X\n   $$\n   定理，si Xn converge en loi vers une v.a. constante presque surement, alors elle converge en probabilite.\n\n   - 离散变量的convergence en loi\n   - 分布函数的convergence en loi\n   - 特征函数的convergence en loi\n\n   均略=_=\n\n6. 中心极限定理\n   $$\n   若Var(X_n) < \\infty \\\\\n   记S_n = \\sum{Xi} \\\\\n   \\frac{S_n-n\\mu}{\\sigma\\sqrt{n}} \\xrightarrow{\\mathcal{D}} \\mathcal{N}(0,1)\n   $$\n   ​\n\n","source":"_posts/proba-ch5.md","raw":"---\ntitle: proba-ch5 数列和随机变量系列\ncategories: math\ntags:\n  - math\n  - probability\ndate: 2016-12-01 23:14:28\n---\n\n# 数列和随机变量系列\n\n1. 零一律\n\n   > **零一律**是概率论中的一个定律，它是安德雷·柯尔莫哥洛夫发现的，因此有时也叫柯尔莫哥洛夫零一律。其内容是：有些事件发生的概率不是几乎一（几乎肯定发生），就是几乎零（几乎肯定不发生）。这样的事件被称为“尾事件”。 — wiki\n\n   $$\n   (X_n)_{n \\in \\mathbb{N}} v.a. \\\\\n   \\mathcal{T}_n = \\sigma(X_k; k \\ge n) \\\\\n   \\mathcal{T}_{\\infty} = \\cap_{n \\in \\mathbb{N}}{\\mathcal{T}_n} \\text{ est   appelee tribu de queue de la suite } (X_n)_{n \\in \\mathbb{N}}\n   $$\n   - 定理(loi de zero-un)\n\n     如果存在一个事件A属于$\\mathcal{T}_{\\infty}$，则P(A)等于0或1。\n\n2. 不同定义\n\n   - convergence presque sure\n\n     定义，当存在一个事件满足以下条件\n     $$\n     \\exists \\Omega^*, \\forall \\omega \\in \\Omega^*, \\lim_{n \\rightarrow \\infty}{X_n(\\omega)} = X(\\omega)\n     $$\n     我们认为一个随机变量序列趋向(p.s.)一个随机变量。即\n     $$\n     P\\{\\omega \\in \\Omega^*: \\lim_{n \\rightarrow \\infty}{X_n(\\omega)} = X(\\omega)\\} = 1\n     $$\n\n   - Convergence dans Lp\n\n     定义。首先Xn和X都在Lp空间下，并且其差值的绝对值的p次方的期望的极限等于零。- -|||\n\n     也就是\n     $$\n     \\lim_{n \\rightarrow \\infty}{E[|X_n-X|^p]} = 0\n     $$\n\n   - convergence en probabilite\n\n     定义，满足以下条件，称依概率收敛。\n     $$\n     \\forall \\varepsilon, \\lim_{n \\to \\infty}{P\\{\\omega:X_n(\\omega)-X(\\omega)>\\varepsilon}\\}=0 \\\\\n     ou\\  \\lim_{n \\to \\infty}{P\\{X_n-X>\\varepsilon}\\}=0\n     $$\n     定理，\n\n     1. 如果Xn趋向X(p.s.)，则fXn趋向fX(p.s.)\n     2. 如果Xn趋向X(P)，则fXn趋向fX(P)\n\n     定理，\n\n     Xn是实随机变量，以下关系等价\n     $$\n     X_n \\xrightarrow{P} X \\Leftrightarrow \\lim_{n \\to \\infty} E(\\frac{|X_n-X|}{1+|X_n-X|}) = 0\n     $$\n     定理，\n\n     Xn是实随机变量，则\n     $$\n     若X_n \\xrightarrow{L^P}X, X_n \\xrightarrow{P}{X} \\\\\n     若X_n \\to X p.s., X_n \\xrightarrow{P}{X} \\\\\n     $$\n     定理，\n\n     Xn是实随机变量，若Xn依概率收敛于X，我们能找到一个序列\n     $$\n     (X_{n_k})_{k \\in \\mathbb{N}}\n     $$\n     使得\n     $$\n     X_n \\to X, p.s.\n     $$\n     定理，\n\n     Xn是实随机变量，若Xn依概率收敛于X，并存在一个$Y \\in L^p, |X_n| \\le Y$，则\n     $$\n     X \\in L^p并且 X_n \\xrightarrow{L^p}X\n     $$\n\n\n\n\n\n3. 波莱尔－坎泰利引理\n\n   > 大致上，波莱尔－坎泰利引理说明了，如果有无穷个概率事件，它们发生的概率之和是有限的，那么其中的无限多个事件一同发生的概率是零。这个定理实际上是测度论的结论在概率论中的应用。 —wiki\n\n   1. 如果有无穷个概率事件，它们发生的概率之和是有限的，那么其中的无限多个事件一同发生的概率是零。\n   2. 如果有无穷个概率事件，无限多个事件一同发生的概率是零，那么它们发生的概率之和是有限的。\n\n4. 大数定律\n\n   > 在数学与统计学中，大数定律又称大数法则、大数律，是描述相当多次数重复实验的结果的定律。根据这个定律知道，样本数量越多，则其平均就越趋近期望值。 — wiki\n\n   - 强大数定律\n     $$\n     X_n \\in L^2 \\text{若它们独立同分布且在同一个概率空间，则} \\\\\n     lim_{n \\to \\infty}{\\frac{\\sum_{i=1}^{n}{X_i}}{n}} = \\mu \\ p.s.\n     $$\n\n   - 切比雪夫大数定律\n     $$\n     X_n  \\text{若它们独立同分布且在同一个概率空间，则} \\\\\n     lim_{n \\to \\infty}{\\frac{\\sum_{i=1}^{n}{X_i}}{n}} = \\mu \\ p.s. \\text{ 当且仅当} E[X_i] \\text{对于所有i存在}\n     $$\n\n5. Convergence en loi\n\n   弱趋向：\n   $$\n   \\int_{\\mathbb{R}^N}{f(x)\\nu_n(dx)} \\xrightarrow{n \\to \\infty} \\int_{\\mathbb{R}^N}{f(x)\\nu(dx)}\n   $$\n   定义，若PXn弱趋向于PX，则Xn是 convergence en loi vers X。记为：\n   $$\n   X_n \\xrightarrow{\\mathcal{D}} X\n   $$\n   定理，\n   $$\n   X_n \\xrightarrow{\\mathcal{D}} X \\Leftrightarrow \\lim_{n \\to \\infty}E[f(X_n)] = E[f(X)]\n   $$\n   定理，\n   $$\n   X_n \\xrightarrow{P} X \\Rightarrow X_n \\xrightarrow{\\mathcal{D}} X\n   $$\n   定理，si Xn converge en loi vers une v.a. constante presque surement, alors elle converge en probabilite.\n\n   - 离散变量的convergence en loi\n   - 分布函数的convergence en loi\n   - 特征函数的convergence en loi\n\n   均略=_=\n\n6. 中心极限定理\n   $$\n   若Var(X_n) < \\infty \\\\\n   记S_n = \\sum{Xi} \\\\\n   \\frac{S_n-n\\mu}{\\sigma\\sqrt{n}} \\xrightarrow{\\mathcal{D}} \\mathcal{N}(0,1)\n   $$\n   ​\n\n","slug":"proba-ch5","published":1,"updated":"2016-12-11T11:05:17.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckw4qufqj004ngwtl69iddlyd","content":"<h1 id=\"数列和随机变量系列\"><a href=\"#数列和随机变量系列\" class=\"headerlink\" title=\"数列和随机变量系列\"></a>数列和随机变量系列</h1><ol>\n<li><p>零一律</p>\n<blockquote>\n<p><strong>零一律</strong>是概率论中的一个定律，它是安德雷·柯尔莫哥洛夫发现的，因此有时也叫柯尔莫哥洛夫零一律。其内容是：有些事件发生的概率不是几乎一（几乎肯定发生），就是几乎零（几乎肯定不发生）。这样的事件被称为“尾事件”。 — wiki</p>\n</blockquote>\n<p>$$<br>(X_n)<em>{n \\in \\mathbb{N}} v.a. \\<br>\\mathcal{T}<em>n = \\sigma(X_k; k \\ge n) \\<br>\\mathcal{T}</em>{\\infty} = \\cap</em>{n \\in \\mathbb{N}}{\\mathcal{T}<em>n} \\text{ est   appelee tribu de queue de la suite } (X_n)</em>{n \\in \\mathbb{N}}<br>$$</p>\n<ul>\n<li><p>定理(loi de zero-un)</p>\n<p>如果存在一个事件A属于$\\mathcal{T}_{\\infty}$，则P(A)等于0或1。</p>\n</li>\n</ul>\n</li>\n<li><p>不同定义</p>\n<ul>\n<li><p>convergence presque sure</p>\n<p>定义，当存在一个事件满足以下条件<br>$$<br>\\exists \\Omega^*, \\forall \\omega \\in \\Omega^*, \\lim_{n \\rightarrow \\infty}{X_n(\\omega)} = X(\\omega)<br>$$<br>我们认为一个随机变量序列趋向(p.s.)一个随机变量。即<br>$$<br>P{\\omega \\in \\Omega^*: \\lim_{n \\rightarrow \\infty}{X_n(\\omega)} = X(\\omega)} = 1<br>$$</p>\n</li>\n<li><p>Convergence dans Lp</p>\n<p>定义。首先Xn和X都在Lp空间下，并且其差值的绝对值的p次方的期望的极限等于零。- -|||</p>\n<p>也就是<br>$$<br>\\lim_{n \\rightarrow \\infty}{E[|X_n-X|^p]} = 0<br>$$</p>\n</li>\n<li><p>convergence en probabilite</p>\n<p>定义，满足以下条件，称依概率收敛。<br>$$<br>\\forall \\varepsilon, \\lim_{n \\to \\infty}{P{\\omega:X_n(\\omega)-X(\\omega)&gt;\\varepsilon}}=0 \\<br>ou\\  \\lim_{n \\to \\infty}{P{X_n-X&gt;\\varepsilon}}=0<br>$$<br>定理，</p>\n<ol>\n<li>如果Xn趋向X(p.s.)，则fXn趋向fX(p.s.)</li>\n<li>如果Xn趋向X(P)，则fXn趋向fX(P)</li>\n</ol>\n<p>定理，</p>\n<p>Xn是实随机变量，以下关系等价<br>$$<br>X_n \\xrightarrow{P} X \\Leftrightarrow \\lim_{n \\to \\infty} E(\\frac{|X_n-X|}{1+|X_n-X|}) = 0<br>$$<br>定理，</p>\n<p>Xn是实随机变量，则<br>$$<br>若X_n \\xrightarrow{L^P}X, X_n \\xrightarrow{P}{X} \\<br>若X_n \\to X p.s., X_n \\xrightarrow{P}{X} \\<br>$$<br>定理，</p>\n<p>Xn是实随机变量，若Xn依概率收敛于X，我们能找到一个序列<br>$$<br>(X_{n_k})_{k \\in \\mathbb{N}}<br>$$<br>使得<br>$$<br>X_n \\to X, p.s.<br>$$<br>定理，</p>\n<p>Xn是实随机变量，若Xn依概率收敛于X，并存在一个$Y \\in L^p, |X_n| \\le Y$，则<br>$$<br>X \\in L^p并且 X_n \\xrightarrow{L^p}X<br>$$</p>\n</li>\n</ul>\n</li>\n</ol>\n<ol start=\"3\">\n<li><p>波莱尔－坎泰利引理</p>\n<blockquote>\n<p>大致上，波莱尔－坎泰利引理说明了，如果有无穷个概率事件，它们发生的概率之和是有限的，那么其中的无限多个事件一同发生的概率是零。这个定理实际上是测度论的结论在概率论中的应用。 —wiki</p>\n</blockquote>\n<ol>\n<li>如果有无穷个概率事件，它们发生的概率之和是有限的，那么其中的无限多个事件一同发生的概率是零。</li>\n<li>如果有无穷个概率事件，无限多个事件一同发生的概率是零，那么它们发生的概率之和是有限的。</li>\n</ol>\n</li>\n<li><p>大数定律</p>\n<blockquote>\n<p>在数学与统计学中，大数定律又称大数法则、大数律，是描述相当多次数重复实验的结果的定律。根据这个定律知道，样本数量越多，则其平均就越趋近期望值。 — wiki</p>\n</blockquote>\n<ul>\n<li><p>强大数定律<br>$$<br>X_n \\in L^2 \\text{若它们独立同分布且在同一个概率空间，则} \\<br>lim_{n \\to \\infty}{\\frac{\\sum_{i=1}^{n}{X_i}}{n}} = \\mu \\ p.s.<br>$$</p>\n</li>\n<li><p>切比雪夫大数定律<br>$$<br>X_n  \\text{若它们独立同分布且在同一个概率空间，则} \\<br>lim_{n \\to \\infty}{\\frac{\\sum_{i=1}^{n}{X_i}}{n}} = \\mu \\ p.s. \\text{ 当且仅当} E[X_i] \\text{对于所有i存在}<br>$$</p>\n</li>\n</ul>\n</li>\n<li><p>Convergence en loi</p>\n<p>弱趋向：<br>$$<br>\\int_{\\mathbb{R}^N}{f(x)\\nu_n(dx)} \\xrightarrow{n \\to \\infty} \\int_{\\mathbb{R}^N}{f(x)\\nu(dx)}<br>$$<br>定义，若PXn弱趋向于PX，则Xn是 convergence en loi vers X。记为：<br>$$<br>X_n \\xrightarrow{\\mathcal{D}} X<br>$$<br>定理，<br>$$<br>X_n \\xrightarrow{\\mathcal{D}} X \\Leftrightarrow \\lim_{n \\to \\infty}E[f(X_n)] = E[f(X)]<br>$$<br>定理，<br>$$<br>X_n \\xrightarrow{P} X \\Rightarrow X_n \\xrightarrow{\\mathcal{D}} X<br>$$<br>定理，si Xn converge en loi vers une v.a. constante presque surement, alors elle converge en probabilite.</p>\n<ul>\n<li>离散变量的convergence en loi</li>\n<li>分布函数的convergence en loi</li>\n<li>特征函数的convergence en loi</li>\n</ul>\n<p>均略=_=</p>\n</li>\n<li><p>中心极限定理<br>$$<br>若Var(X_n) &lt; \\infty \\<br>记S_n = \\sum{Xi} \\<br>\\frac{S_n-n\\mu}{\\sigma\\sqrt{n}} \\xrightarrow{\\mathcal{D}} \\mathcal{N}(0,1)<br>$$<br>​</p>\n</li>\n</ol>\n","site":{"data":{"recommended_posts":{"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"}],"https://LorrinWWW.github.io/posts/complexity/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/hexo-with-latex/":[{"title":"区块链时代的最正确的打赏姿势——在Hexo博客配置Coinhive网页挖矿","permalink":"http://www.davidfnck.com/blockchain/mine-xmr-in-hexo-by-coinhive.html/"}],"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/":[{"title":"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-statistic/":[{"title":"贝叶斯估计 Bayes estimation","permalink":"https://LorrinWWW.github.io/posts/Bayes-estimation/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"}],"https://LorrinWWW.github.io/posts/[2018.2.5]Nested-LSTMs/":[{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"}],"https://LorrinWWW.github.io/posts/datamining-pretreatment/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Several models for knowledge graph representing and completing 几个知识图谱模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/"}],"https://LorrinWWW.github.io/posts/compression/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/Sobolev-space/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/proba-ch5/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch3/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/proba-ch1/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/ML-CNN/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/EDP-basic-models/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/machine-learning/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/datamining-class-pred/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction/":[],"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/":[{"title":"OS notes","permalink":"https://LorrinWWW.github.io/posts/OS-notes/"}],"https://LorrinWWW.github.io/posts/projet-enjeu-plugin-chrome-101/":[{"title":"Chrome油猴脚本","permalink":"http://gubangzhong.cn/2017/06/13/Chrome油猴脚本/"}],"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/":[{"title":"MongoDB, Docker and Python","permalink":"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/":[{"title":"Hands on Scrapy","permalink":"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/OS-notes/":[{"title":"learning OS and building LorriOS","permalink":"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/"}],"https://LorrinWWW.github.io/posts/Bayes-estimation/":[{"title":"Note of statistic","permalink":"https://LorrinWWW.github.io/posts/Note-of-statistic/"}],"https://LorrinWWW.github.io/posts/management-of-the-firm/":[],"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"}],"https://LorrinWWW.github.io/posts/datamining-qualitative-induction/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-finite-element-method/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/QuadTree/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/proba-ch4/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch2/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/":[{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/Note-of-NLP/":[{"title":"Generative Adversarial Network","permalink":"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/"},{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"}],"https://LorrinWWW.github.io/posts/Note-of-datamining/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"数据挖掘-分类与预测","permalink":"https://LorrinWWW.github.io/posts/datamining-class-pred/"},{"title":"数据挖掘-预处理","permalink":"https://LorrinWWW.github.io/posts/datamining-pretreatment/"}],"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/Note-of-probability/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/participe-present-et-gerondif/":[],"https://LorrinWWW.github.io/posts/Hilbert-space/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/proba-ch6/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}]}}},"excerpt":"","more":"<h1 id=\"数列和随机变量系列\"><a href=\"#数列和随机变量系列\" class=\"headerlink\" title=\"数列和随机变量系列\"></a>数列和随机变量系列</h1><ol>\n<li><p>零一律</p>\n<blockquote>\n<p><strong>零一律</strong>是概率论中的一个定律，它是安德雷·柯尔莫哥洛夫发现的，因此有时也叫柯尔莫哥洛夫零一律。其内容是：有些事件发生的概率不是几乎一（几乎肯定发生），就是几乎零（几乎肯定不发生）。这样的事件被称为“尾事件”。 — wiki</p>\n</blockquote>\n<p>$$<br>(X_n)<em>{n \\in \\mathbb{N}} v.a. \\<br>\\mathcal{T}<em>n = \\sigma(X_k; k \\ge n) \\<br>\\mathcal{T}</em>{\\infty} = \\cap</em>{n \\in \\mathbb{N}}{\\mathcal{T}<em>n} \\text{ est   appelee tribu de queue de la suite } (X_n)</em>{n \\in \\mathbb{N}}<br>$$</p>\n<ul>\n<li><p>定理(loi de zero-un)</p>\n<p>如果存在一个事件A属于$\\mathcal{T}_{\\infty}$，则P(A)等于0或1。</p>\n</li>\n</ul>\n</li>\n<li><p>不同定义</p>\n<ul>\n<li><p>convergence presque sure</p>\n<p>定义，当存在一个事件满足以下条件<br>$$<br>\\exists \\Omega^*, \\forall \\omega \\in \\Omega^*, \\lim_{n \\rightarrow \\infty}{X_n(\\omega)} = X(\\omega)<br>$$<br>我们认为一个随机变量序列趋向(p.s.)一个随机变量。即<br>$$<br>P{\\omega \\in \\Omega^*: \\lim_{n \\rightarrow \\infty}{X_n(\\omega)} = X(\\omega)} = 1<br>$$</p>\n</li>\n<li><p>Convergence dans Lp</p>\n<p>定义。首先Xn和X都在Lp空间下，并且其差值的绝对值的p次方的期望的极限等于零。- -|||</p>\n<p>也就是<br>$$<br>\\lim_{n \\rightarrow \\infty}{E[|X_n-X|^p]} = 0<br>$$</p>\n</li>\n<li><p>convergence en probabilite</p>\n<p>定义，满足以下条件，称依概率收敛。<br>$$<br>\\forall \\varepsilon, \\lim_{n \\to \\infty}{P{\\omega:X_n(\\omega)-X(\\omega)&gt;\\varepsilon}}=0 \\<br>ou\\  \\lim_{n \\to \\infty}{P{X_n-X&gt;\\varepsilon}}=0<br>$$<br>定理，</p>\n<ol>\n<li>如果Xn趋向X(p.s.)，则fXn趋向fX(p.s.)</li>\n<li>如果Xn趋向X(P)，则fXn趋向fX(P)</li>\n</ol>\n<p>定理，</p>\n<p>Xn是实随机变量，以下关系等价<br>$$<br>X_n \\xrightarrow{P} X \\Leftrightarrow \\lim_{n \\to \\infty} E(\\frac{|X_n-X|}{1+|X_n-X|}) = 0<br>$$<br>定理，</p>\n<p>Xn是实随机变量，则<br>$$<br>若X_n \\xrightarrow{L^P}X, X_n \\xrightarrow{P}{X} \\<br>若X_n \\to X p.s., X_n \\xrightarrow{P}{X} \\<br>$$<br>定理，</p>\n<p>Xn是实随机变量，若Xn依概率收敛于X，我们能找到一个序列<br>$$<br>(X_{n_k})_{k \\in \\mathbb{N}}<br>$$<br>使得<br>$$<br>X_n \\to X, p.s.<br>$$<br>定理，</p>\n<p>Xn是实随机变量，若Xn依概率收敛于X，并存在一个$Y \\in L^p, |X_n| \\le Y$，则<br>$$<br>X \\in L^p并且 X_n \\xrightarrow{L^p}X<br>$$</p>\n</li>\n</ul>\n</li>\n</ol>\n<ol start=\"3\">\n<li><p>波莱尔－坎泰利引理</p>\n<blockquote>\n<p>大致上，波莱尔－坎泰利引理说明了，如果有无穷个概率事件，它们发生的概率之和是有限的，那么其中的无限多个事件一同发生的概率是零。这个定理实际上是测度论的结论在概率论中的应用。 —wiki</p>\n</blockquote>\n<ol>\n<li>如果有无穷个概率事件，它们发生的概率之和是有限的，那么其中的无限多个事件一同发生的概率是零。</li>\n<li>如果有无穷个概率事件，无限多个事件一同发生的概率是零，那么它们发生的概率之和是有限的。</li>\n</ol>\n</li>\n<li><p>大数定律</p>\n<blockquote>\n<p>在数学与统计学中，大数定律又称大数法则、大数律，是描述相当多次数重复实验的结果的定律。根据这个定律知道，样本数量越多，则其平均就越趋近期望值。 — wiki</p>\n</blockquote>\n<ul>\n<li><p>强大数定律<br>$$<br>X_n \\in L^2 \\text{若它们独立同分布且在同一个概率空间，则} \\<br>lim_{n \\to \\infty}{\\frac{\\sum_{i=1}^{n}{X_i}}{n}} = \\mu \\ p.s.<br>$$</p>\n</li>\n<li><p>切比雪夫大数定律<br>$$<br>X_n  \\text{若它们独立同分布且在同一个概率空间，则} \\<br>lim_{n \\to \\infty}{\\frac{\\sum_{i=1}^{n}{X_i}}{n}} = \\mu \\ p.s. \\text{ 当且仅当} E[X_i] \\text{对于所有i存在}<br>$$</p>\n</li>\n</ul>\n</li>\n<li><p>Convergence en loi</p>\n<p>弱趋向：<br>$$<br>\\int_{\\mathbb{R}^N}{f(x)\\nu_n(dx)} \\xrightarrow{n \\to \\infty} \\int_{\\mathbb{R}^N}{f(x)\\nu(dx)}<br>$$<br>定义，若PXn弱趋向于PX，则Xn是 convergence en loi vers X。记为：<br>$$<br>X_n \\xrightarrow{\\mathcal{D}} X<br>$$<br>定理，<br>$$<br>X_n \\xrightarrow{\\mathcal{D}} X \\Leftrightarrow \\lim_{n \\to \\infty}E[f(X_n)] = E[f(X)]<br>$$<br>定理，<br>$$<br>X_n \\xrightarrow{P} X \\Rightarrow X_n \\xrightarrow{\\mathcal{D}} X<br>$$<br>定理，si Xn converge en loi vers une v.a. constante presque surement, alors elle converge en probabilite.</p>\n<ul>\n<li>离散变量的convergence en loi</li>\n<li>分布函数的convergence en loi</li>\n<li>特征函数的convergence en loi</li>\n</ul>\n<p>均略=_=</p>\n</li>\n<li><p>中心极限定理<br>$$<br>若Var(X_n) &lt; \\infty \\<br>记S_n = \\sum{Xi} \\<br>\\frac{S_n-n\\mu}{\\sigma\\sqrt{n}} \\xrightarrow{\\mathcal{D}} \\mathcal{N}(0,1)<br>$$<br>​</p>\n</li>\n</ol>\n"},{"title":"proba-ch6 条件期望","date":"2016-12-02T13:32:30.000Z","_content":"\n$$\nP(A|B) = \\frac{P(A\\cap B)}{P(B)}\n$$\n\n$$\nE[X|Y=y] = E_Q[X] = \\sum_{x \\in \\tilde E}{xP(X=x|Y=y)}\n$$\n\n$$\n\\psi : y \\mapsto E[X|Y=y] \\text{ if } P(Y=y) > 0 \\\\\notherwise\\ 0\n$$\n\n$$\nE[X|Y] = \\psi(Y)\n$$\n\n- 定义\n  $$\n  \\mathcal{G} 是一个sous-tribu。\\\\\n  E[X|\\mathcal{G}] 是一个随机变量Y,满足 \\\\\n  Y \\in L^1, \\forall A \\in \\mathcal{G}, \\int_A X dP = \\int_AYdP\n  $$\n\n- 定义 L2\n  $$\n  X \\in L^2, \\mathcal{G} 是一个sous-tribu \\\\\n  E[X|\\mathcal{G}] 是一个随机变量Y,满足 \\\\\n  \\forall Z \\in L^2, E[XZ] = E[YZ]\n  $$\n\n  - 性质\n    $$\n    E[E[X|\\mathcal{G}]] = E[X]\n    $$\n\n  - 性质\n    $$\n    若X \\in L^1, E[X|\\mathcal{G}] = E[X] \\Leftrightarrow X是\\mathcal{G}可测的。\n    $$\n\n  - 性质\n    $$\n    X是\\mathcal{G}可测的, X,Y,XY \\in L^1 \\\\\n    \\Rightarrow E[XY|\\mathcal{G}] = XE[Y|\\mathcal{G}]\n    $$\n\n- 定义\n  $$\n  E[X|Y] = E[X|\\sigma(Y)]\n  $$\n\n  - 性质 若XY是实随机变量，则存在一个函数h\n    $$\n    E[X|Y] = h(Y)\n    $$\n","source":"_posts/proba-ch6.md","raw":"---\ntitle: proba-ch6 条件期望\ncategories: math\ntags:\n  - math\n  - probability\ndate: 2016-12-02 21:32:30\n---\n\n$$\nP(A|B) = \\frac{P(A\\cap B)}{P(B)}\n$$\n\n$$\nE[X|Y=y] = E_Q[X] = \\sum_{x \\in \\tilde E}{xP(X=x|Y=y)}\n$$\n\n$$\n\\psi : y \\mapsto E[X|Y=y] \\text{ if } P(Y=y) > 0 \\\\\notherwise\\ 0\n$$\n\n$$\nE[X|Y] = \\psi(Y)\n$$\n\n- 定义\n  $$\n  \\mathcal{G} 是一个sous-tribu。\\\\\n  E[X|\\mathcal{G}] 是一个随机变量Y,满足 \\\\\n  Y \\in L^1, \\forall A \\in \\mathcal{G}, \\int_A X dP = \\int_AYdP\n  $$\n\n- 定义 L2\n  $$\n  X \\in L^2, \\mathcal{G} 是一个sous-tribu \\\\\n  E[X|\\mathcal{G}] 是一个随机变量Y,满足 \\\\\n  \\forall Z \\in L^2, E[XZ] = E[YZ]\n  $$\n\n  - 性质\n    $$\n    E[E[X|\\mathcal{G}]] = E[X]\n    $$\n\n  - 性质\n    $$\n    若X \\in L^1, E[X|\\mathcal{G}] = E[X] \\Leftrightarrow X是\\mathcal{G}可测的。\n    $$\n\n  - 性质\n    $$\n    X是\\mathcal{G}可测的, X,Y,XY \\in L^1 \\\\\n    \\Rightarrow E[XY|\\mathcal{G}] = XE[Y|\\mathcal{G}]\n    $$\n\n- 定义\n  $$\n  E[X|Y] = E[X|\\sigma(Y)]\n  $$\n\n  - 性质 若XY是实随机变量，则存在一个函数h\n    $$\n    E[X|Y] = h(Y)\n    $$\n","slug":"proba-ch6","published":1,"updated":"2016-12-02T21:09:54.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckw4qufqk004rgwtlgijj1k8k","content":"<p>$$<br>P(A|B) = \\frac{P(A\\cap B)}{P(B)}<br>$$</p>\n<p>$$<br>E[X|Y=y] = E_Q[X] = \\sum_{x \\in \\tilde E}{xP(X=x|Y=y)}<br>$$</p>\n<p>$$<br>\\psi : y \\mapsto E[X|Y=y] \\text{ if } P(Y=y) &gt; 0 \\<br>otherwise\\ 0<br>$$</p>\n<p>$$<br>E[X|Y] = \\psi(Y)<br>$$</p>\n<ul>\n<li><p>定义<br>$$<br>\\mathcal{G} 是一个sous-tribu。\\<br>E[X|\\mathcal{G}] 是一个随机变量Y,满足 \\<br>Y \\in L^1, \\forall A \\in \\mathcal{G}, \\int_A X dP = \\int_AYdP<br>$$</p>\n</li>\n<li><p>定义 L2<br>$$<br>X \\in L^2, \\mathcal{G} 是一个sous-tribu \\<br>E[X|\\mathcal{G}] 是一个随机变量Y,满足 \\<br>\\forall Z \\in L^2, E[XZ] = E[YZ]<br>$$</p>\n<ul>\n<li><p>性质<br>$$<br>E[E[X|\\mathcal{G}]] = E[X]<br>$$</p>\n</li>\n<li><p>性质<br>$$<br>若X \\in L^1, E[X|\\mathcal{G}] = E[X] \\Leftrightarrow X是\\mathcal{G}可测的。<br>$$</p>\n</li>\n<li><p>性质<br>$$<br>X是\\mathcal{G}可测的, X,Y,XY \\in L^1 \\<br>\\Rightarrow E[XY|\\mathcal{G}] = XE[Y|\\mathcal{G}]<br>$$</p>\n</li>\n</ul>\n</li>\n<li><p>定义<br>$$<br>E[X|Y] = E[X|\\sigma(Y)]<br>$$</p>\n<ul>\n<li>性质 若XY是实随机变量，则存在一个函数h<br>$$<br>E[X|Y] = h(Y)<br>$$</li>\n</ul>\n</li>\n</ul>\n","site":{"data":{"recommended_posts":{"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"}],"https://LorrinWWW.github.io/posts/complexity/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/hexo-with-latex/":[{"title":"区块链时代的最正确的打赏姿势——在Hexo博客配置Coinhive网页挖矿","permalink":"http://www.davidfnck.com/blockchain/mine-xmr-in-hexo-by-coinhive.html/"}],"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/":[{"title":"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-statistic/":[{"title":"贝叶斯估计 Bayes estimation","permalink":"https://LorrinWWW.github.io/posts/Bayes-estimation/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"}],"https://LorrinWWW.github.io/posts/[2018.2.5]Nested-LSTMs/":[{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"}],"https://LorrinWWW.github.io/posts/datamining-pretreatment/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Several models for knowledge graph representing and completing 几个知识图谱模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/"}],"https://LorrinWWW.github.io/posts/compression/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/Sobolev-space/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/proba-ch5/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch3/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/proba-ch1/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/ML-CNN/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/EDP-basic-models/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/machine-learning/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/datamining-class-pred/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction/":[],"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/":[{"title":"OS notes","permalink":"https://LorrinWWW.github.io/posts/OS-notes/"}],"https://LorrinWWW.github.io/posts/projet-enjeu-plugin-chrome-101/":[{"title":"Chrome油猴脚本","permalink":"http://gubangzhong.cn/2017/06/13/Chrome油猴脚本/"}],"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/":[{"title":"MongoDB, Docker and Python","permalink":"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/":[{"title":"Hands on Scrapy","permalink":"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/OS-notes/":[{"title":"learning OS and building LorriOS","permalink":"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/"}],"https://LorrinWWW.github.io/posts/Bayes-estimation/":[{"title":"Note of statistic","permalink":"https://LorrinWWW.github.io/posts/Note-of-statistic/"}],"https://LorrinWWW.github.io/posts/management-of-the-firm/":[],"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"}],"https://LorrinWWW.github.io/posts/datamining-qualitative-induction/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-finite-element-method/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/QuadTree/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/proba-ch4/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch2/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/":[{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/Note-of-NLP/":[{"title":"Generative Adversarial Network","permalink":"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/"},{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"}],"https://LorrinWWW.github.io/posts/Note-of-datamining/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"数据挖掘-分类与预测","permalink":"https://LorrinWWW.github.io/posts/datamining-class-pred/"},{"title":"数据挖掘-预处理","permalink":"https://LorrinWWW.github.io/posts/datamining-pretreatment/"}],"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/Note-of-probability/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/participe-present-et-gerondif/":[],"https://LorrinWWW.github.io/posts/Hilbert-space/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/proba-ch6/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}]}}},"excerpt":"","more":"<p>$$<br>P(A|B) = \\frac{P(A\\cap B)}{P(B)}<br>$$</p>\n<p>$$<br>E[X|Y=y] = E_Q[X] = \\sum_{x \\in \\tilde E}{xP(X=x|Y=y)}<br>$$</p>\n<p>$$<br>\\psi : y \\mapsto E[X|Y=y] \\text{ if } P(Y=y) &gt; 0 \\<br>otherwise\\ 0<br>$$</p>\n<p>$$<br>E[X|Y] = \\psi(Y)<br>$$</p>\n<ul>\n<li><p>定义<br>$$<br>\\mathcal{G} 是一个sous-tribu。\\<br>E[X|\\mathcal{G}] 是一个随机变量Y,满足 \\<br>Y \\in L^1, \\forall A \\in \\mathcal{G}, \\int_A X dP = \\int_AYdP<br>$$</p>\n</li>\n<li><p>定义 L2<br>$$<br>X \\in L^2, \\mathcal{G} 是一个sous-tribu \\<br>E[X|\\mathcal{G}] 是一个随机变量Y,满足 \\<br>\\forall Z \\in L^2, E[XZ] = E[YZ]<br>$$</p>\n<ul>\n<li><p>性质<br>$$<br>E[E[X|\\mathcal{G}]] = E[X]<br>$$</p>\n</li>\n<li><p>性质<br>$$<br>若X \\in L^1, E[X|\\mathcal{G}] = E[X] \\Leftrightarrow X是\\mathcal{G}可测的。<br>$$</p>\n</li>\n<li><p>性质<br>$$<br>X是\\mathcal{G}可测的, X,Y,XY \\in L^1 \\<br>\\Rightarrow E[XY|\\mathcal{G}] = XE[Y|\\mathcal{G}]<br>$$</p>\n</li>\n</ul>\n</li>\n<li><p>定义<br>$$<br>E[X|Y] = E[X|\\sigma(Y)]<br>$$</p>\n<ul>\n<li>性质 若XY是实随机变量，则存在一个函数h<br>$$<br>E[X|Y] = h(Y)<br>$$</li>\n</ul>\n</li>\n</ul>\n"},{"title":"Chrome插件开发 - Hello world","date":"2016-11-30T04:01:24.000Z","_content":"\n在学校选的projet是关于chrome插件开发的，这里记录一下。\n\n# Hello world!\n\n凡事先从hello world开始。\n\n1. 阅读chrome的开发手册，新建一个项目文件夹\n\n2. 我们需要manifest.json文件，告诉chrome我们的配置，去哪里找我们文件。\n\n   下面的写的是我们目前的设置，只写hello world的话只需要配置基本的设置以及default_popup。\n\n   ```json\n   {\n     \"manifest_version\": 2,\n     \"name\": \"TrelloGement\",\n     \"description\": \"Organiser ses recherches d'appartement sur Paris grâce à Trello!\",\n     \"version\": \"0.2.1\",\n     \"browser_action\": {\n       \"default_icon\": \"icon.png\",\n       \"default_popup\": \"popup.html\"\n     },\n     \"background\": {\n       \"scripts\": [\"background.js\", \"jquery-3.1.1.min.js\", \"client.js\"]\n     },\n     \"permissions\": [\"activeTab\", \"storage\", \"tabs\", \"https://api.trello.com/*\", \"https://trello.com/*\"]\n   }\n   ```\n\n3. 新建popup.html\n\n   ```html\n   <!DOCTYPE html>\n   <html lang=\"en\">\n   <head>\n   \t<meta charset=\"UTF-8\">\n   \t<title>Trellogement</title>\n   </head>\n   <body>\n     <h1> Hello, world! </h1>\n   </body>\n   </html>\n   ```\n\n4. 一个简单的Hello world就实现了，在chrome加载这个文件夹作为未打包的插件，在popup的位置点击可以看到“Hello, world!”\n\n","source":"_posts/projet-enjeu-plugin-chrome-101.md","raw":"---\ntitle: Chrome插件开发 - Hello world\ndate: 2016-11-30 12:01:24\ncategories: programming\ntags: [web, chrome]\n---\n\n在学校选的projet是关于chrome插件开发的，这里记录一下。\n\n# Hello world!\n\n凡事先从hello world开始。\n\n1. 阅读chrome的开发手册，新建一个项目文件夹\n\n2. 我们需要manifest.json文件，告诉chrome我们的配置，去哪里找我们文件。\n\n   下面的写的是我们目前的设置，只写hello world的话只需要配置基本的设置以及default_popup。\n\n   ```json\n   {\n     \"manifest_version\": 2,\n     \"name\": \"TrelloGement\",\n     \"description\": \"Organiser ses recherches d'appartement sur Paris grâce à Trello!\",\n     \"version\": \"0.2.1\",\n     \"browser_action\": {\n       \"default_icon\": \"icon.png\",\n       \"default_popup\": \"popup.html\"\n     },\n     \"background\": {\n       \"scripts\": [\"background.js\", \"jquery-3.1.1.min.js\", \"client.js\"]\n     },\n     \"permissions\": [\"activeTab\", \"storage\", \"tabs\", \"https://api.trello.com/*\", \"https://trello.com/*\"]\n   }\n   ```\n\n3. 新建popup.html\n\n   ```html\n   <!DOCTYPE html>\n   <html lang=\"en\">\n   <head>\n   \t<meta charset=\"UTF-8\">\n   \t<title>Trellogement</title>\n   </head>\n   <body>\n     <h1> Hello, world! </h1>\n   </body>\n   </html>\n   ```\n\n4. 一个简单的Hello world就实现了，在chrome加载这个文件夹作为未打包的插件，在popup的位置点击可以看到“Hello, world!”\n\n","slug":"projet-enjeu-plugin-chrome-101","published":1,"updated":"2016-11-30T11:19:02.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckw4qufql004vgwtl2fyf5s0m","content":"<p>在学校选的projet是关于chrome插件开发的，这里记录一下。</p>\n<h1 id=\"Hello-world\"><a href=\"#Hello-world\" class=\"headerlink\" title=\"Hello world!\"></a>Hello world!</h1><p>凡事先从hello world开始。</p>\n<ol>\n<li><p>阅读chrome的开发手册，新建一个项目文件夹</p>\n</li>\n<li><p>我们需要manifest.json文件，告诉chrome我们的配置，去哪里找我们文件。</p>\n<p>下面的写的是我们目前的设置，只写hello world的话只需要配置基本的设置以及default_popup。</p>\n<figure class=\"highlight json\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">{</span><br><span class=\"line\">  <span class=\"attr\">\"manifest_version\"</span>: <span class=\"number\">2</span>,</span><br><span class=\"line\">  <span class=\"attr\">\"name\"</span>: <span class=\"string\">\"TrelloGement\"</span>,</span><br><span class=\"line\">  <span class=\"attr\">\"description\"</span>: <span class=\"string\">\"Organiser ses recherches d'appartement sur Paris grâce à Trello!\"</span>,</span><br><span class=\"line\">  <span class=\"attr\">\"version\"</span>: <span class=\"string\">\"0.2.1\"</span>,</span><br><span class=\"line\">  <span class=\"attr\">\"browser_action\"</span>: {</span><br><span class=\"line\">    <span class=\"attr\">\"default_icon\"</span>: <span class=\"string\">\"icon.png\"</span>,</span><br><span class=\"line\">    <span class=\"attr\">\"default_popup\"</span>: <span class=\"string\">\"popup.html\"</span></span><br><span class=\"line\">  },</span><br><span class=\"line\">  <span class=\"attr\">\"background\"</span>: {</span><br><span class=\"line\">    <span class=\"attr\">\"scripts\"</span>: [<span class=\"string\">\"background.js\"</span>, <span class=\"string\">\"jquery-3.1.1.min.js\"</span>, <span class=\"string\">\"client.js\"</span>]</span><br><span class=\"line\">  },</span><br><span class=\"line\">  <span class=\"attr\">\"permissions\"</span>: [<span class=\"string\">\"activeTab\"</span>, <span class=\"string\">\"storage\"</span>, <span class=\"string\">\"tabs\"</span>, <span class=\"string\">\"https://api.trello.com/*\"</span>, <span class=\"string\">\"https://trello.com/*\"</span>]</span><br><span class=\"line\">}</span><br></pre></td></tr></tbody></table></figure></li>\n<li><p>新建popup.html</p>\n<figure class=\"highlight html\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&lt;!DOCTYPE <span class=\"meta-keyword\">html</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">html</span> <span class=\"attr\">lang</span>=<span class=\"string\">\"en\"</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">head</span>&gt;</span></span><br><span class=\"line\">\t<span class=\"tag\">&lt;<span class=\"name\">meta</span> <span class=\"attr\">charset</span>=<span class=\"string\">\"UTF-8\"</span>&gt;</span></span><br><span class=\"line\">\t<span class=\"tag\">&lt;<span class=\"name\">title</span>&gt;</span>Trellogement<span class=\"tag\">&lt;/<span class=\"name\">title</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">head</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">body</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">h1</span>&gt;</span> Hello, world! <span class=\"tag\">&lt;/<span class=\"name\">h1</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">body</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">html</span>&gt;</span></span><br></pre></td></tr></tbody></table></figure></li>\n<li><p>一个简单的Hello world就实现了，在chrome加载这个文件夹作为未打包的插件，在popup的位置点击可以看到“Hello, world!”</p>\n</li>\n</ol>\n","site":{"data":{"recommended_posts":{"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"}],"https://LorrinWWW.github.io/posts/complexity/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/hexo-with-latex/":[{"title":"区块链时代的最正确的打赏姿势——在Hexo博客配置Coinhive网页挖矿","permalink":"http://www.davidfnck.com/blockchain/mine-xmr-in-hexo-by-coinhive.html/"}],"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/":[{"title":"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-statistic/":[{"title":"贝叶斯估计 Bayes estimation","permalink":"https://LorrinWWW.github.io/posts/Bayes-estimation/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"}],"https://LorrinWWW.github.io/posts/[2018.2.5]Nested-LSTMs/":[{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"}],"https://LorrinWWW.github.io/posts/datamining-pretreatment/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Several models for knowledge graph representing and completing 几个知识图谱模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/"}],"https://LorrinWWW.github.io/posts/compression/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/Sobolev-space/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/proba-ch5/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch3/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/proba-ch1/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/ML-CNN/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/EDP-basic-models/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/machine-learning/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/datamining-class-pred/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction/":[],"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/":[{"title":"OS notes","permalink":"https://LorrinWWW.github.io/posts/OS-notes/"}],"https://LorrinWWW.github.io/posts/projet-enjeu-plugin-chrome-101/":[{"title":"Chrome油猴脚本","permalink":"http://gubangzhong.cn/2017/06/13/Chrome油猴脚本/"}],"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/":[{"title":"MongoDB, Docker and Python","permalink":"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/":[{"title":"Hands on Scrapy","permalink":"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/OS-notes/":[{"title":"learning OS and building LorriOS","permalink":"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/"}],"https://LorrinWWW.github.io/posts/Bayes-estimation/":[{"title":"Note of statistic","permalink":"https://LorrinWWW.github.io/posts/Note-of-statistic/"}],"https://LorrinWWW.github.io/posts/management-of-the-firm/":[],"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"}],"https://LorrinWWW.github.io/posts/datamining-qualitative-induction/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-finite-element-method/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/QuadTree/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/proba-ch4/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch2/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/":[{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/Note-of-NLP/":[{"title":"Generative Adversarial Network","permalink":"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/"},{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"}],"https://LorrinWWW.github.io/posts/Note-of-datamining/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"数据挖掘-分类与预测","permalink":"https://LorrinWWW.github.io/posts/datamining-class-pred/"},{"title":"数据挖掘-预处理","permalink":"https://LorrinWWW.github.io/posts/datamining-pretreatment/"}],"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/Note-of-probability/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/participe-present-et-gerondif/":[],"https://LorrinWWW.github.io/posts/Hilbert-space/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/proba-ch6/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}]}}},"excerpt":"","more":"<p>在学校选的projet是关于chrome插件开发的，这里记录一下。</p>\n<h1 id=\"Hello-world\"><a href=\"#Hello-world\" class=\"headerlink\" title=\"Hello world!\"></a>Hello world!</h1><p>凡事先从hello world开始。</p>\n<ol>\n<li><p>阅读chrome的开发手册，新建一个项目文件夹</p>\n</li>\n<li><p>我们需要manifest.json文件，告诉chrome我们的配置，去哪里找我们文件。</p>\n<p>下面的写的是我们目前的设置，只写hello world的话只需要配置基本的设置以及default_popup。</p>\n<figure class=\"highlight json\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">  <span class=\"attr\">&quot;manifest_version&quot;</span>: <span class=\"number\">2</span>,</span><br><span class=\"line\">  <span class=\"attr\">&quot;name&quot;</span>: <span class=\"string\">&quot;TrelloGement&quot;</span>,</span><br><span class=\"line\">  <span class=\"attr\">&quot;description&quot;</span>: <span class=\"string\">&quot;Organiser ses recherches d&#x27;appartement sur Paris grâce à Trello!&quot;</span>,</span><br><span class=\"line\">  <span class=\"attr\">&quot;version&quot;</span>: <span class=\"string\">&quot;0.2.1&quot;</span>,</span><br><span class=\"line\">  <span class=\"attr\">&quot;browser_action&quot;</span>: &#123;</span><br><span class=\"line\">    <span class=\"attr\">&quot;default_icon&quot;</span>: <span class=\"string\">&quot;icon.png&quot;</span>,</span><br><span class=\"line\">    <span class=\"attr\">&quot;default_popup&quot;</span>: <span class=\"string\">&quot;popup.html&quot;</span></span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">  <span class=\"attr\">&quot;background&quot;</span>: &#123;</span><br><span class=\"line\">    <span class=\"attr\">&quot;scripts&quot;</span>: [<span class=\"string\">&quot;background.js&quot;</span>, <span class=\"string\">&quot;jquery-3.1.1.min.js&quot;</span>, <span class=\"string\">&quot;client.js&quot;</span>]</span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">  <span class=\"attr\">&quot;permissions&quot;</span>: [<span class=\"string\">&quot;activeTab&quot;</span>, <span class=\"string\">&quot;storage&quot;</span>, <span class=\"string\">&quot;tabs&quot;</span>, <span class=\"string\">&quot;https://api.trello.com/*&quot;</span>, <span class=\"string\">&quot;https://trello.com/*&quot;</span>]</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></li>\n<li><p>新建popup.html</p>\n<figure class=\"highlight html\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&lt;!DOCTYPE <span class=\"meta-keyword\">html</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">html</span> <span class=\"attr\">lang</span>=<span class=\"string\">&quot;en&quot;</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">head</span>&gt;</span></span><br><span class=\"line\">\t<span class=\"tag\">&lt;<span class=\"name\">meta</span> <span class=\"attr\">charset</span>=<span class=\"string\">&quot;UTF-8&quot;</span>&gt;</span></span><br><span class=\"line\">\t<span class=\"tag\">&lt;<span class=\"name\">title</span>&gt;</span>Trellogement<span class=\"tag\">&lt;/<span class=\"name\">title</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">head</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">body</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">h1</span>&gt;</span> Hello, world! <span class=\"tag\">&lt;/<span class=\"name\">h1</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">body</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">html</span>&gt;</span></span><br></pre></td></tr></table></figure></li>\n<li><p>一个简单的Hello world就实现了，在chrome加载这个文件夹作为未打包的插件，在popup的位置点击可以看到“Hello, world!”</p>\n</li>\n</ol>\n"},{"title":"Set up the RSS feed","date":"2018-05-12T04:57:31.000Z","_content":"\nThis post is both declaration of the update and a test whether the rss works well.\n\nThis RSS feed is generated by [hexo-generator-feed](https://github.com/hexojs/hexo-generator-feed) plugin.\n\n1. Install the plugin\n\n   ```Shell\n   $ npm install hexo-generator-feed --save\n   ```\n\n2. update the theme configuration (_config.yml).\n\n   Enable the plugin:\n\n   ```yaml\n   plugins:\n      ...\n      hexo-generator-feed: true\n\n   feed:\n   \ttype: atom  \n   \tpath: atom.xml  \n   \tlimit: 20\n   ```\n\n   Show the RSS link on the website, this step depends on the theme we use, it should be something like:\n\n   ```yaml\n   links:\n   \trss: /atom.xml # this should be the path you assigned above.\n   ```\n\n   ​\n\n3. And voilà, now you can subscribe to the RSS feed you created.\n\n   <img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-05-12-image-201805121311408.png\" width=\"20%\">\n\n","source":"_posts/update-rss.md","raw":"---\ntitle: Set up the RSS feed\ndate: 2018-05-12 12:57:31\ncategories: [other]\ntags: [hexo, rss]\n---\n\nThis post is both declaration of the update and a test whether the rss works well.\n\nThis RSS feed is generated by [hexo-generator-feed](https://github.com/hexojs/hexo-generator-feed) plugin.\n\n1. Install the plugin\n\n   ```Shell\n   $ npm install hexo-generator-feed --save\n   ```\n\n2. update the theme configuration (_config.yml).\n\n   Enable the plugin:\n\n   ```yaml\n   plugins:\n      ...\n      hexo-generator-feed: true\n\n   feed:\n   \ttype: atom  \n   \tpath: atom.xml  \n   \tlimit: 20\n   ```\n\n   Show the RSS link on the website, this step depends on the theme we use, it should be something like:\n\n   ```yaml\n   links:\n   \trss: /atom.xml # this should be the path you assigned above.\n   ```\n\n   ​\n\n3. And voilà, now you can subscribe to the RSS feed you created.\n\n   <img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-05-12-image-201805121311408.png\" width=\"20%\">\n\n","slug":"update-rss","published":1,"updated":"2020-11-03T03:26:00.854Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckw4qufqm004zgwtlg4182z10","content":"<p>This post is both declaration of the update and a test whether the rss works well.</p>\n<p>This RSS feed is generated by <a href=\"https://github.com/hexojs/hexo-generator-feed\">hexo-generator-feed</a> plugin.</p>\n<ol>\n<li><p>Install the plugin</p>\n<figure class=\"highlight shell\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">$</span><span class=\"bash\"> npm install hexo-generator-feed --save</span></span><br></pre></td></tr></tbody></table></figure></li>\n<li><p>update the theme configuration (_config.yml).</p>\n<p>Enable the plugin:</p>\n<figure class=\"highlight yaml\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">plugins:</span></span><br><span class=\"line\">   <span class=\"string\">...</span></span><br><span class=\"line\">   <span class=\"attr\">hexo-generator-feed:</span> <span class=\"literal\">true</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"attr\">feed:</span></span><br><span class=\"line\">\t<span class=\"attr\">type:</span> <span class=\"string\">atom</span>  </span><br><span class=\"line\">\t<span class=\"attr\">path:</span> <span class=\"string\">atom.xml</span>  </span><br><span class=\"line\">\t<span class=\"attr\">limit:</span> <span class=\"number\">20</span></span><br></pre></td></tr></tbody></table></figure>\n\n<p>Show the RSS link on the website, this step depends on the theme we use, it should be something like:</p>\n<figure class=\"highlight yaml\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">links:</span></span><br><span class=\"line\">\t<span class=\"attr\">rss:</span> <span class=\"string\">/atom.xml</span> <span class=\"comment\"># this should be the path you assigned above.</span></span><br></pre></td></tr></tbody></table></figure>\n\n<p>​</p>\n</li>\n<li><p>And voilà, now you can subscribe to the RSS feed you created.</p>\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-05-12-image-201805121311408.png\" width=\"20%\"></li>\n</ol>\n","site":{"data":{"recommended_posts":{"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"}],"https://LorrinWWW.github.io/posts/complexity/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/hexo-with-latex/":[{"title":"区块链时代的最正确的打赏姿势——在Hexo博客配置Coinhive网页挖矿","permalink":"http://www.davidfnck.com/blockchain/mine-xmr-in-hexo-by-coinhive.html/"}],"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/":[{"title":"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-statistic/":[{"title":"贝叶斯估计 Bayes estimation","permalink":"https://LorrinWWW.github.io/posts/Bayes-estimation/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"}],"https://LorrinWWW.github.io/posts/[2018.2.5]Nested-LSTMs/":[{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"}],"https://LorrinWWW.github.io/posts/datamining-pretreatment/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Several models for knowledge graph representing and completing 几个知识图谱模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/"}],"https://LorrinWWW.github.io/posts/compression/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/Sobolev-space/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/proba-ch5/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch3/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/proba-ch1/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/ML-CNN/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/EDP-basic-models/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/machine-learning/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/datamining-class-pred/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction/":[],"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/":[{"title":"OS notes","permalink":"https://LorrinWWW.github.io/posts/OS-notes/"}],"https://LorrinWWW.github.io/posts/projet-enjeu-plugin-chrome-101/":[{"title":"Chrome油猴脚本","permalink":"http://gubangzhong.cn/2017/06/13/Chrome油猴脚本/"}],"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/":[{"title":"MongoDB, Docker and Python","permalink":"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/":[{"title":"Hands on Scrapy","permalink":"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/OS-notes/":[{"title":"learning OS and building LorriOS","permalink":"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/"}],"https://LorrinWWW.github.io/posts/Bayes-estimation/":[{"title":"Note of statistic","permalink":"https://LorrinWWW.github.io/posts/Note-of-statistic/"}],"https://LorrinWWW.github.io/posts/management-of-the-firm/":[],"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"}],"https://LorrinWWW.github.io/posts/datamining-qualitative-induction/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-finite-element-method/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/QuadTree/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/proba-ch4/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch2/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/":[{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/Note-of-NLP/":[{"title":"Generative Adversarial Network","permalink":"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/"},{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"}],"https://LorrinWWW.github.io/posts/Note-of-datamining/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"数据挖掘-分类与预测","permalink":"https://LorrinWWW.github.io/posts/datamining-class-pred/"},{"title":"数据挖掘-预处理","permalink":"https://LorrinWWW.github.io/posts/datamining-pretreatment/"}],"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/Note-of-probability/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/participe-present-et-gerondif/":[],"https://LorrinWWW.github.io/posts/Hilbert-space/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/proba-ch6/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}]}}},"excerpt":"","more":"<p>This post is both declaration of the update and a test whether the rss works well.</p>\n<p>This RSS feed is generated by <a href=\"https://github.com/hexojs/hexo-generator-feed\">hexo-generator-feed</a> plugin.</p>\n<ol>\n<li><p>Install the plugin</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">$</span><span class=\"bash\"> npm install hexo-generator-feed --save</span></span><br></pre></td></tr></table></figure></li>\n<li><p>update the theme configuration (_config.yml).</p>\n<p>Enable the plugin:</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">plugins:</span></span><br><span class=\"line\">   <span class=\"string\">...</span></span><br><span class=\"line\">   <span class=\"attr\">hexo-generator-feed:</span> <span class=\"literal\">true</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"attr\">feed:</span></span><br><span class=\"line\">\t<span class=\"attr\">type:</span> <span class=\"string\">atom</span>  </span><br><span class=\"line\">\t<span class=\"attr\">path:</span> <span class=\"string\">atom.xml</span>  </span><br><span class=\"line\">\t<span class=\"attr\">limit:</span> <span class=\"number\">20</span></span><br></pre></td></tr></table></figure>\n\n<p>Show the RSS link on the website, this step depends on the theme we use, it should be something like:</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">links:</span></span><br><span class=\"line\">\t<span class=\"attr\">rss:</span> <span class=\"string\">/atom.xml</span> <span class=\"comment\"># this should be the path you assigned above.</span></span><br></pre></td></tr></table></figure>\n\n<p>​</p>\n</li>\n<li><p>And voilà, now you can subscribe to the RSS feed you created.</p>\n<img src=\"https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-05-12-image-201805121311408.png\" width=\"20%\"></li>\n</ol>\n"},{"title":"CheatSheet for Setting up New VPS","date":"2020-02-09T08:18:11.000Z","_content":"\n# Basics\n\n## Create New Users\n\nfirst login in the root account.\n\nto add a user:\n\n```bash\nadduser [UserName]\n```\n\nif want it to have sudo privilege, then:\n\n~~~bash\nusermod -aG sudo [UserName]\n~~~\n\n## Set Up SSH Config\n\nadd these to ~/.ssh/config\n\n```\nHost [ShortNameForTheVPS]\nHostName [ItsIPOrDomain]\nUser [TheUserToLogin]\nIdentitiesOnly yes\n```\n\nif you want to ssh via proxy, for MacOS users:\n\n```\nHost [ShortNameForTheVPS]\nHostName [ItsIPOrDomain]\nProxyCommand nc -X 5 -x 127.0.0.1:1082 %h %p\nUser [TheUserToLogin]\nIdentitiesOnly yes\n```\n\n-X 5 means using socks5; -x specify proxy server ip and port.\n\nthen ssh-copy-id, so we don't have to type password every time we ssh:\n\n```bash\nssh-copy-id [ShortNameForTheVPS]\n```\n\nAs the result, we only need to type to login:\n\n```bash\nssh [ShortNameForTheVPS]\n```\n\n## Powerful Vim\n\nLife is short, I use spf13.\n\n```bash\ncurl https://j.mp/spf13-vim3 -L > spf13-vim.sh && sh spf13-vim.sh\n```\n\nif every thing goes fine, it will usually take within 5 minnutes.\n\n## Powerful Zsh\n\nBash is good but Zsh is better. For ubuntu users:\n\n```bash\nsudo apt install zsh\n```\n\nAgain, life is short, I use oh-my-zsh.\n\n```bash\nsh -c \"$(curl -fsSL https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\"\n```\n\nit is installed instantly.\n\n# Network Related\n\nUsually it is suggested to expose ports as few as possible, and only preserve 80(HTTP) and 443(HTTPS).\n\nWe can use nginx/caddy/... to do HTTP(S) reverse proxy.\n\n## Caddy\n\ninstall go\n\n```bash\nsudo apt install golang\n```\n\ninstall caddy. we choose caddy 1 here.\n\n```bash\ncurl https://getcaddy.com | bash -s personal\n```\n\nrun caddy, and it should run as a simple server on port 2015. it should return 404 when visit.\n\n```bash\ncaddy\n```\n\nto run caddy as server, follow instructions [here](https://github.com/caddyserver/caddy/tree/master/dist/init/linux-systemd).\n\nto use CloudFlare CDN, follow instuctions [here(chinese)](https://melty.land/blog/caddy-and-cloudflare)\n\n# Scientific Purpose\n\ne.g. interactive demo.\n\n## Python Environment\n\ndownload anaconda, the latest address can be found [here](https://www.anaconda.com/distribution/).\n\n```bash\nwget https://repo.anaconda.com/archive/Anaconda3-2019.10-Linux-x86_64.sh\n```\n\ninstall it.\n\n```bash\nbash Anaconda3-2019.10-Linux-x86_64.sh\n```\n\nif use zsh, the install script won't add initialization script to .zshrc, so we need to copy from .bashrc\n\nre-login, so the initialization script will work.\n\ninstall pytorch will be another long story, so we won't go in deep here.\n\n","source":"_posts/vps-cheatsheet.md","raw":"---\ntitle: CheatSheet for Setting up New VPS\ndate: 2020-02-09 16:18:11\ncategories: other\ntags: [vps]\n---\n\n# Basics\n\n## Create New Users\n\nfirst login in the root account.\n\nto add a user:\n\n```bash\nadduser [UserName]\n```\n\nif want it to have sudo privilege, then:\n\n~~~bash\nusermod -aG sudo [UserName]\n~~~\n\n## Set Up SSH Config\n\nadd these to ~/.ssh/config\n\n```\nHost [ShortNameForTheVPS]\nHostName [ItsIPOrDomain]\nUser [TheUserToLogin]\nIdentitiesOnly yes\n```\n\nif you want to ssh via proxy, for MacOS users:\n\n```\nHost [ShortNameForTheVPS]\nHostName [ItsIPOrDomain]\nProxyCommand nc -X 5 -x 127.0.0.1:1082 %h %p\nUser [TheUserToLogin]\nIdentitiesOnly yes\n```\n\n-X 5 means using socks5; -x specify proxy server ip and port.\n\nthen ssh-copy-id, so we don't have to type password every time we ssh:\n\n```bash\nssh-copy-id [ShortNameForTheVPS]\n```\n\nAs the result, we only need to type to login:\n\n```bash\nssh [ShortNameForTheVPS]\n```\n\n## Powerful Vim\n\nLife is short, I use spf13.\n\n```bash\ncurl https://j.mp/spf13-vim3 -L > spf13-vim.sh && sh spf13-vim.sh\n```\n\nif every thing goes fine, it will usually take within 5 minnutes.\n\n## Powerful Zsh\n\nBash is good but Zsh is better. For ubuntu users:\n\n```bash\nsudo apt install zsh\n```\n\nAgain, life is short, I use oh-my-zsh.\n\n```bash\nsh -c \"$(curl -fsSL https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\"\n```\n\nit is installed instantly.\n\n# Network Related\n\nUsually it is suggested to expose ports as few as possible, and only preserve 80(HTTP) and 443(HTTPS).\n\nWe can use nginx/caddy/... to do HTTP(S) reverse proxy.\n\n## Caddy\n\ninstall go\n\n```bash\nsudo apt install golang\n```\n\ninstall caddy. we choose caddy 1 here.\n\n```bash\ncurl https://getcaddy.com | bash -s personal\n```\n\nrun caddy, and it should run as a simple server on port 2015. it should return 404 when visit.\n\n```bash\ncaddy\n```\n\nto run caddy as server, follow instructions [here](https://github.com/caddyserver/caddy/tree/master/dist/init/linux-systemd).\n\nto use CloudFlare CDN, follow instuctions [here(chinese)](https://melty.land/blog/caddy-and-cloudflare)\n\n# Scientific Purpose\n\ne.g. interactive demo.\n\n## Python Environment\n\ndownload anaconda, the latest address can be found [here](https://www.anaconda.com/distribution/).\n\n```bash\nwget https://repo.anaconda.com/archive/Anaconda3-2019.10-Linux-x86_64.sh\n```\n\ninstall it.\n\n```bash\nbash Anaconda3-2019.10-Linux-x86_64.sh\n```\n\nif use zsh, the install script won't add initialization script to .zshrc, so we need to copy from .bashrc\n\nre-login, so the initialization script will work.\n\ninstall pytorch will be another long story, so we won't go in deep here.\n\n","slug":"vps-cheatsheet","published":1,"updated":"2020-02-09T13:43:52.811Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckw4qufqp0053gwtl5crh54nc","content":"<h1 id=\"Basics\"><a href=\"#Basics\" class=\"headerlink\" title=\"Basics\"></a>Basics</h1><h2 id=\"Create-New-Users\"><a href=\"#Create-New-Users\" class=\"headerlink\" title=\"Create New Users\"></a>Create New Users</h2><p>first login in the root account.</p>\n<p>to add a user:</p>\n<figure class=\"highlight bash\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">adduser [UserName]</span><br></pre></td></tr></tbody></table></figure>\n\n<p>if want it to have sudo privilege, then:</p>\n<figure class=\"highlight bash\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">usermod -aG sudo [UserName]</span><br></pre></td></tr></tbody></table></figure>\n\n<h2 id=\"Set-Up-SSH-Config\"><a href=\"#Set-Up-SSH-Config\" class=\"headerlink\" title=\"Set Up SSH Config\"></a>Set Up SSH Config</h2><p>add these to ~/.ssh/config</p>\n<figure class=\"highlight plaintext\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Host [ShortNameForTheVPS]</span><br><span class=\"line\">HostName [ItsIPOrDomain]</span><br><span class=\"line\">User [TheUserToLogin]</span><br><span class=\"line\">IdentitiesOnly yes</span><br></pre></td></tr></tbody></table></figure>\n\n<p>if you want to ssh via proxy, for MacOS users:</p>\n<figure class=\"highlight plaintext\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Host [ShortNameForTheVPS]</span><br><span class=\"line\">HostName [ItsIPOrDomain]</span><br><span class=\"line\">ProxyCommand nc -X 5 -x 127.0.0.1:1082 %h %p</span><br><span class=\"line\">User [TheUserToLogin]</span><br><span class=\"line\">IdentitiesOnly yes</span><br></pre></td></tr></tbody></table></figure>\n\n<p>-X 5 means using socks5; -x specify proxy server ip and port.</p>\n<p>then ssh-copy-id, so we don’t have to type password every time we ssh:</p>\n<figure class=\"highlight bash\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ssh-copy-id [ShortNameForTheVPS]</span><br></pre></td></tr></tbody></table></figure>\n\n<p>As the result, we only need to type to login:</p>\n<figure class=\"highlight bash\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ssh [ShortNameForTheVPS]</span><br></pre></td></tr></tbody></table></figure>\n\n<h2 id=\"Powerful-Vim\"><a href=\"#Powerful-Vim\" class=\"headerlink\" title=\"Powerful Vim\"></a>Powerful Vim</h2><p>Life is short, I use spf13.</p>\n<figure class=\"highlight bash\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl https://j.mp/spf13-vim3 -L &gt; spf13-vim.sh &amp;&amp; sh spf13-vim.sh</span><br></pre></td></tr></tbody></table></figure>\n\n<p>if every thing goes fine, it will usually take within 5 minnutes.</p>\n<h2 id=\"Powerful-Zsh\"><a href=\"#Powerful-Zsh\" class=\"headerlink\" title=\"Powerful Zsh\"></a>Powerful Zsh</h2><p>Bash is good but Zsh is better. For ubuntu users:</p>\n<figure class=\"highlight bash\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt install zsh</span><br></pre></td></tr></tbody></table></figure>\n\n<p>Again, life is short, I use oh-my-zsh.</p>\n<figure class=\"highlight bash\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sh -c <span class=\"string\">\"<span class=\"subst\">$(curl -fsSL https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh)</span>\"</span></span><br></pre></td></tr></tbody></table></figure>\n\n<p>it is installed instantly.</p>\n<h1 id=\"Network-Related\"><a href=\"#Network-Related\" class=\"headerlink\" title=\"Network Related\"></a>Network Related</h1><p>Usually it is suggested to expose ports as few as possible, and only preserve 80(HTTP) and 443(HTTPS).</p>\n<p>We can use nginx/caddy/… to do HTTP(S) reverse proxy.</p>\n<h2 id=\"Caddy\"><a href=\"#Caddy\" class=\"headerlink\" title=\"Caddy\"></a>Caddy</h2><p>install go</p>\n<figure class=\"highlight bash\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt install golang</span><br></pre></td></tr></tbody></table></figure>\n\n<p>install caddy. we choose caddy 1 here.</p>\n<figure class=\"highlight bash\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl https://getcaddy.com | bash -s personal</span><br></pre></td></tr></tbody></table></figure>\n\n<p>run caddy, and it should run as a simple server on port 2015. it should return 404 when visit.</p>\n<figure class=\"highlight bash\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">caddy</span><br></pre></td></tr></tbody></table></figure>\n\n<p>to run caddy as server, follow instructions <a href=\"https://github.com/caddyserver/caddy/tree/master/dist/init/linux-systemd\">here</a>.</p>\n<p>to use CloudFlare CDN, follow instuctions <a href=\"https://melty.land/blog/caddy-and-cloudflare\">here(chinese)</a></p>\n<h1 id=\"Scientific-Purpose\"><a href=\"#Scientific-Purpose\" class=\"headerlink\" title=\"Scientific Purpose\"></a>Scientific Purpose</h1><p>e.g. interactive demo.</p>\n<h2 id=\"Python-Environment\"><a href=\"#Python-Environment\" class=\"headerlink\" title=\"Python Environment\"></a>Python Environment</h2><p>download anaconda, the latest address can be found <a href=\"https://www.anaconda.com/distribution/\">here</a>.</p>\n<figure class=\"highlight bash\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget https://repo.anaconda.com/archive/Anaconda3-2019.10-Linux-x86_64.sh</span><br></pre></td></tr></tbody></table></figure>\n\n<p>install it.</p>\n<figure class=\"highlight bash\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bash Anaconda3-2019.10-Linux-x86_64.sh</span><br></pre></td></tr></tbody></table></figure>\n\n<p>if use zsh, the install script won’t add initialization script to .zshrc, so we need to copy from .bashrc</p>\n<p>re-login, so the initialization script will work.</p>\n<p>install pytorch will be another long story, so we won’t go in deep here.</p>\n","site":{"data":{"recommended_posts":{"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"},{"title":"Open-World Knowledge Graph Completion 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/"}],"https://LorrinWWW.github.io/posts/complexity/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/hexo-with-latex/":[{"title":"区块链时代的最正确的打赏姿势——在Hexo博客配置Coinhive网页挖矿","permalink":"http://www.davidfnck.com/blockchain/mine-xmr-in-hexo-by-coinhive.html/"}],"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/":[{"title":"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/Note-of-statistic/":[{"title":"贝叶斯估计 Bayes estimation","permalink":"https://LorrinWWW.github.io/posts/Bayes-estimation/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"}],"https://LorrinWWW.github.io/posts/[2018.2.5]Nested-LSTMs/":[{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"}],"https://LorrinWWW.github.io/posts/datamining-pretreatment/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/[2018.2.26]Open-World-Knowledge-Graph-Completion/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"Several models for knowledge graph representing and completing 几个知识图谱模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/"}],"https://LorrinWWW.github.io/posts/compression/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/Sobolev-space/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/proba-ch5/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch3/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"}],"https://LorrinWWW.github.io/posts/proba-ch1/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/ML-CNN/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"machine learning","permalink":"https://LorrinWWW.github.io/posts/machine-learning/"}],"https://LorrinWWW.github.io/posts/EDP-basic-models/":[{"title":"EDP基础：矩阵的复习","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"},{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"}],"https://LorrinWWW.github.io/posts/machine-learning/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/datamining-class-pred/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-basic-matrix-review/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Sobolev space","permalink":"https://LorrinWWW.github.io/posts/Sobolev-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction/":[],"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/":[{"title":"OS notes","permalink":"https://LorrinWWW.github.io/posts/OS-notes/"}],"https://LorrinWWW.github.io/posts/projet-enjeu-plugin-chrome-101/":[{"title":"Chrome油猴脚本","permalink":"http://gubangzhong.cn/2017/06/13/Chrome油猴脚本/"}],"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/":[{"title":"MongoDB, Docker and Python","permalink":"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/MongoDB-Docker-and-Python/":[{"title":"Hands on Scrapy","permalink":"https://LorrinWWW.github.io/posts/Hands-on-Scrapy/"},{"title":"Python 安装指南","permalink":"http://www.davidfnck.com/python/python-tutorial-01-install.html/"}],"https://LorrinWWW.github.io/posts/OS-notes/":[{"title":"learning OS and building LorriOS","permalink":"https://LorrinWWW.github.io/posts/learning-OS-and-building-LorriOS/"}],"https://LorrinWWW.github.io/posts/Bayes-estimation/":[{"title":"Note of statistic","permalink":"https://LorrinWWW.github.io/posts/Note-of-statistic/"}],"https://LorrinWWW.github.io/posts/management-of-the-firm/":[],"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/":[{"title":"Event detection and co-reference with minimal supervision 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/[2018.4.14]Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","permalink":"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/"}],"https://LorrinWWW.github.io/posts/datamining-qualitative-induction/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/graph/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/EDP-finite-element-method/":[{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/QuadTree/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"}],"https://LorrinWWW.github.io/posts/proba-ch4/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/proba-ch2/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/[2018.1.21]Event-detection-and-co-referentce/":[{"title":"Event detection 的几个神经网络模型","permalink":"https://LorrinWWW.github.io/posts/[2018.3.20]Event-detection/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.4.4]A-Neural-Model-for-Joint-Event-Detection-and-Summarization/"}],"https://LorrinWWW.github.io/posts/Note-of-NLP/":[{"title":"Generative Adversarial Network","permalink":"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/"},{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"}],"https://LorrinWWW.github.io/posts/[2017.12.10]Entity-resolution/":[{"title":"Relation Classification via Attention Model 笔记","permalink":"https://LorrinWWW.github.io/posts/[2017.12.17]Relation-Classification-via-Attention-Model/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","permalink":"https://LorrinWWW.github.io/posts/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/"},{"title":"几个 relation extraction 远程监督模型","permalink":"https://LorrinWWW.github.io/posts/[2018.1.14]Models-for-relation-extraction/"},{"title":"RNN生成古诗","permalink":"http://gubangzhong.cn/2018/01/07/RNN生成古诗/"}],"https://LorrinWWW.github.io/posts/Generative-Adversarial-Network/":[{"title":"Note of NLP","permalink":"https://LorrinWWW.github.io/posts/Note-of-NLP/"}],"https://LorrinWWW.github.io/posts/Note-of-datamining/":[{"title":"Note of knowledge graph","permalink":"https://LorrinWWW.github.io/posts/Note-of-knowledge-graph/"},{"title":"数据挖掘-分类与预测","permalink":"https://LorrinWWW.github.io/posts/datamining-class-pred/"},{"title":"数据挖掘-预处理","permalink":"https://LorrinWWW.github.io/posts/datamining-pretreatment/"}],"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"算法笔记 Note of learning Algo","permalink":"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/Note-of-probability/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}],"https://LorrinWWW.github.io/posts/participe-present-et-gerondif/":[],"https://LorrinWWW.github.io/posts/Hilbert-space/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"概率论笔记 Note of probability","permalink":"https://LorrinWWW.github.io/posts/Note-of-probability/"}],"https://LorrinWWW.github.io/posts/Note-of-learning-Algo/":[{"title":"ML CNN","permalink":"https://LorrinWWW.github.io/posts/ML-CNN/"},{"title":"面向考试常用编程思想 Method of programming facing to exams","permalink":"https://LorrinWWW.github.io/posts/Method-of-programming-facing-to-exams/"},{"title":"编码解码 compression","permalink":"https://LorrinWWW.github.io/posts/compression/"}],"https://LorrinWWW.github.io/posts/proba-ch6/":[{"title":"EDP finite element method","permalink":"https://LorrinWWW.github.io/posts/EDP-finite-element-method/"},{"title":"EDP basic models","permalink":"https://LorrinWWW.github.io/posts/EDP-basic-models/"},{"title":"Hilbert space","permalink":"https://LorrinWWW.github.io/posts/Hilbert-space/"}]}}},"excerpt":"","more":"<h1 id=\"Basics\"><a href=\"#Basics\" class=\"headerlink\" title=\"Basics\"></a>Basics</h1><h2 id=\"Create-New-Users\"><a href=\"#Create-New-Users\" class=\"headerlink\" title=\"Create New Users\"></a>Create New Users</h2><p>first login in the root account.</p>\n<p>to add a user:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">adduser [UserName]</span><br></pre></td></tr></table></figure>\n\n<p>if want it to have sudo privilege, then:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">usermod -aG sudo [UserName]</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"Set-Up-SSH-Config\"><a href=\"#Set-Up-SSH-Config\" class=\"headerlink\" title=\"Set Up SSH Config\"></a>Set Up SSH Config</h2><p>add these to ~/.ssh/config</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Host [ShortNameForTheVPS]</span><br><span class=\"line\">HostName [ItsIPOrDomain]</span><br><span class=\"line\">User [TheUserToLogin]</span><br><span class=\"line\">IdentitiesOnly yes</span><br></pre></td></tr></table></figure>\n\n<p>if you want to ssh via proxy, for MacOS users:</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Host [ShortNameForTheVPS]</span><br><span class=\"line\">HostName [ItsIPOrDomain]</span><br><span class=\"line\">ProxyCommand nc -X 5 -x 127.0.0.1:1082 %h %p</span><br><span class=\"line\">User [TheUserToLogin]</span><br><span class=\"line\">IdentitiesOnly yes</span><br></pre></td></tr></table></figure>\n\n<p>-X 5 means using socks5; -x specify proxy server ip and port.</p>\n<p>then ssh-copy-id, so we don’t have to type password every time we ssh:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ssh-copy-id [ShortNameForTheVPS]</span><br></pre></td></tr></table></figure>\n\n<p>As the result, we only need to type to login:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ssh [ShortNameForTheVPS]</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"Powerful-Vim\"><a href=\"#Powerful-Vim\" class=\"headerlink\" title=\"Powerful Vim\"></a>Powerful Vim</h2><p>Life is short, I use spf13.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl https://j.mp/spf13-vim3 -L &gt; spf13-vim.sh &amp;&amp; sh spf13-vim.sh</span><br></pre></td></tr></table></figure>\n\n<p>if every thing goes fine, it will usually take within 5 minnutes.</p>\n<h2 id=\"Powerful-Zsh\"><a href=\"#Powerful-Zsh\" class=\"headerlink\" title=\"Powerful Zsh\"></a>Powerful Zsh</h2><p>Bash is good but Zsh is better. For ubuntu users:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt install zsh</span><br></pre></td></tr></table></figure>\n\n<p>Again, life is short, I use oh-my-zsh.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sh -c <span class=\"string\">&quot;<span class=\"subst\">$(curl -fsSL https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh)</span>&quot;</span></span><br></pre></td></tr></table></figure>\n\n<p>it is installed instantly.</p>\n<h1 id=\"Network-Related\"><a href=\"#Network-Related\" class=\"headerlink\" title=\"Network Related\"></a>Network Related</h1><p>Usually it is suggested to expose ports as few as possible, and only preserve 80(HTTP) and 443(HTTPS).</p>\n<p>We can use nginx/caddy/… to do HTTP(S) reverse proxy.</p>\n<h2 id=\"Caddy\"><a href=\"#Caddy\" class=\"headerlink\" title=\"Caddy\"></a>Caddy</h2><p>install go</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt install golang</span><br></pre></td></tr></table></figure>\n\n<p>install caddy. we choose caddy 1 here.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl https://getcaddy.com | bash -s personal</span><br></pre></td></tr></table></figure>\n\n<p>run caddy, and it should run as a simple server on port 2015. it should return 404 when visit.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">caddy</span><br></pre></td></tr></table></figure>\n\n<p>to run caddy as server, follow instructions <a href=\"https://github.com/caddyserver/caddy/tree/master/dist/init/linux-systemd\">here</a>.</p>\n<p>to use CloudFlare CDN, follow instuctions <a href=\"https://melty.land/blog/caddy-and-cloudflare\">here(chinese)</a></p>\n<h1 id=\"Scientific-Purpose\"><a href=\"#Scientific-Purpose\" class=\"headerlink\" title=\"Scientific Purpose\"></a>Scientific Purpose</h1><p>e.g. interactive demo.</p>\n<h2 id=\"Python-Environment\"><a href=\"#Python-Environment\" class=\"headerlink\" title=\"Python Environment\"></a>Python Environment</h2><p>download anaconda, the latest address can be found <a href=\"https://www.anaconda.com/distribution/\">here</a>.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget https://repo.anaconda.com/archive/Anaconda3-2019.10-Linux-x86_64.sh</span><br></pre></td></tr></table></figure>\n\n<p>install it.</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bash Anaconda3-2019.10-Linux-x86_64.sh</span><br></pre></td></tr></table></figure>\n\n<p>if use zsh, the install script won’t add initialization script to .zshrc, so we need to copy from .bashrc</p>\n<p>re-login, so the initialization script will work.</p>\n<p>install pytorch will be another long story, so we won’t go in deep here.</p>\n"}],"PostAsset":[],"PostCategory":[{"post_id":"ckw4qufom0001gwtl4ehk83to","category_id":"ckw4qufor0004gwtldwdudn8l","_id":"ckw4qufoz000ggwtl7el8ec12"},{"post_id":"ckw4qufox000dgwtldq2v1h0p","category_id":"ckw4qufor0004gwtldwdudn8l","_id":"ckw4qufp2000lgwtlhutybxnv"},{"post_id":"ckw4qufop0003gwtl3dk5e0gb","category_id":"ckw4qufor0004gwtldwdudn8l","_id":"ckw4qufp4000qgwtl4q9c2ohm"},{"post_id":"ckw4qufot0007gwtldi2mbss0","category_id":"ckw4qufor0004gwtldwdudn8l","_id":"ckw4qufp5000sgwtl7vlpfr2y"},{"post_id":"ckw4qufou0009gwtlab327nnk","category_id":"ckw4qufor0004gwtldwdudn8l","_id":"ckw4qufp8000ygwtl2tnvfwgs"},{"post_id":"ckw4qufou0009gwtlab327nnk","category_id":"ckw4qufp2000mgwtl0radcg7s","_id":"ckw4qufp90013gwtl71ff0src"},{"post_id":"ckw4qufov000agwtleg6gd9g5","category_id":"ckw4qufp5000tgwtl6ywt3c4w","_id":"ckw4qufpa0016gwtlestb0zkj"},{"post_id":"ckw4qufp7000xgwtl0rnhe8o8","category_id":"ckw4qufp5000tgwtl6ywt3c4w","_id":"ckw4qufpb0019gwtleaz0dzhv"},{"post_id":"ckw4qufp80011gwtl9j3b7d15","category_id":"ckw4qufp5000tgwtl6ywt3c4w","_id":"ckw4qufpc001dgwtl9067337k"},{"post_id":"ckw4qufoy000egwtlcr1j4206","category_id":"ckw4qufp5000tgwtl6ywt3c4w","_id":"ckw4qufpe001hgwtl0gcu24m1"},{"post_id":"ckw4qufp90014gwtlg8dt7wef","category_id":"ckw4qufor0004gwtldwdudn8l","_id":"ckw4qufpe001lgwtla5izgkx9"},{"post_id":"ckw4qufpa0017gwtl2o49btu3","category_id":"ckw4qufor0004gwtldwdudn8l","_id":"ckw4qufph001pgwtlb1jj904m"},{"post_id":"ckw4qufpa0017gwtl2o49btu3","category_id":"ckw4qufp2000mgwtl0radcg7s","_id":"ckw4qufpi001tgwtl8lw1duge"},{"post_id":"ckw4qufpd001fgwtlc4g6f3la","category_id":"ckw4qufp5000tgwtl6ywt3c4w","_id":"ckw4qufpm001wgwtlea3t86sm"},{"post_id":"ckw4qufp2000kgwtl01ad8zyn","category_id":"ckw4qufp5000tgwtl6ywt3c4w","_id":"ckw4qufpn0020gwtl5a9p02v2"},{"post_id":"ckw4qufpe001jgwtl4fjl5y7q","category_id":"ckw4qufor0004gwtldwdudn8l","_id":"ckw4qufpo0022gwtlhxudbxeg"},{"post_id":"ckw4qufpe001jgwtl4fjl5y7q","category_id":"ckw4qufp2000mgwtl0radcg7s","_id":"ckw4qufpp0025gwtle6aq5914"},{"post_id":"ckw4qufp3000pgwtl2fv8cajr","category_id":"ckw4qufp5000tgwtl6ywt3c4w","_id":"ckw4qufpq0028gwtl0b5b4qhc"},{"post_id":"ckw4qufp4000rgwtlg65o8sme","category_id":"ckw4qufp5000tgwtl6ywt3c4w","_id":"ckw4qufpr002bgwtl0dppb4e4"},{"post_id":"ckw4qufp6000vgwtl7kika36r","category_id":"ckw4qufp5000tgwtl6ywt3c4w","_id":"ckw4qufps002egwtldb2j3qbs"},{"post_id":"ckw4qufp6000vgwtl7kika36r","category_id":"ckw4qufpn001zgwtlf76efbqh","_id":"ckw4qufpt002hgwtlhrv10rf3"},{"post_id":"ckw4qufp0000igwtlg2ur61jm","category_id":"ckw4qufp5000tgwtl6ywt3c4w","_id":"ckw4qufpu002lgwtl7oikbi2i"},{"post_id":"ckw4qufp0000igwtlg2ur61jm","category_id":"ckw4qufpn001zgwtlf76efbqh","_id":"ckw4qufpv002ogwtlg58a883u"},{"post_id":"ckw4qufpc001bgwtl809wau5y","category_id":"ckw4qufp5000tgwtl6ywt3c4w","_id":"ckw4qufpw002tgwtl8rl4cgkm"},{"post_id":"ckw4qufpc001bgwtl809wau5y","category_id":"ckw4qufpn001zgwtlf76efbqh","_id":"ckw4qufpx002wgwtlg1qa7nst"},{"post_id":"ckw4qufpu002mgwtl1ggb971o","category_id":"ckw4qufpt002igwtl4r5aa33y","_id":"ckw4qufpy0030gwtl606a5vw4"},{"post_id":"ckw4qufpf001ngwtlek9x1j7n","category_id":"ckw4qufpt002igwtl4r5aa33y","_id":"ckw4qufpz0032gwtl10h4erod"},{"post_id":"ckw4qufpv002qgwtl5hcyhnfp","category_id":"ckw4qufpt002igwtl4r5aa33y","_id":"ckw4qufq00036gwtlh0d6200d"},{"post_id":"ckw4qufpw002ugwtl2aagaeeh","category_id":"ckw4qufpt002igwtl4r5aa33y","_id":"ckw4qufq10039gwtld3l6faxm"},{"post_id":"ckw4qufph001qgwtldfrq166l","category_id":"ckw4qufpt002igwtl4r5aa33y","_id":"ckw4qufq2003egwtle2ja9696"},{"post_id":"ckw4qufpx002ygwtlfb0shkrr","category_id":"ckw4qufpt002igwtl4r5aa33y","_id":"ckw4qufq3003hgwtl7ulcca8h"},{"post_id":"ckw4qufpi001ugwtlfxsu6m4s","category_id":"ckw4qufpt002igwtl4r5aa33y","_id":"ckw4qufq7003lgwtlex260sb2"},{"post_id":"ckw4qufpz0034gwtl8swk33xy","category_id":"ckw4qufp5000tgwtl6ywt3c4w","_id":"ckw4qufq8003ogwtl5zto17bb"},{"post_id":"ckw4qufq10038gwtle9ta0x74","category_id":"ckw4qufp5000tgwtl6ywt3c4w","_id":"ckw4qufq9003sgwtl3i751eo2"},{"post_id":"ckw4qufpm001ygwtl9ur9czfz","category_id":"ckw4qufpt002igwtl4r5aa33y","_id":"ckw4qufqa003vgwtla9zifiu3"},{"post_id":"ckw4qufq1003bgwtlfgnpfpxn","category_id":"ckw4qufp5000tgwtl6ywt3c4w","_id":"ckw4qufqb003ygwtleb9eg1bd"},{"post_id":"ckw4qufq3003ggwtl84r119e1","category_id":"ckw4qufp5000tgwtl6ywt3c4w","_id":"ckw4qufqc0041gwtl17vg938s"},{"post_id":"ckw4qufpn0021gwtl3zwebh66","category_id":"ckw4qufpt002igwtl4r5aa33y","_id":"ckw4qufqd0044gwtlhbj62tig"},{"post_id":"ckw4qufq3003igwtl5rl03yxp","category_id":"ckw4qufp5000tgwtl6ywt3c4w","_id":"ckw4qufqe0048gwtle1owakv6"},{"post_id":"ckw4qufq7003mgwtldit7eklh","category_id":"ckw4qufp5000tgwtl6ywt3c4w","_id":"ckw4qufqf004bgwtl32h47jdz"},{"post_id":"ckw4qufpo0024gwtlezik90yy","category_id":"ckw4qufpt002igwtl4r5aa33y","_id":"ckw4qufqg004egwtl78zzd4rd"},{"post_id":"ckw4qufqa003ugwtldlh2fq72","category_id":"ckw4qufpt002igwtl4r5aa33y","_id":"ckw4qufqh004igwtl7ak1gvi7"},{"post_id":"ckw4qufpp0027gwtl4fief2tn","category_id":"ckw4qufpt002igwtl4r5aa33y","_id":"ckw4qufqi004mgwtld9ovft1e"},{"post_id":"ckw4qufqa003wgwtl8s2d3e71","category_id":"ckw4qufp5000tgwtl6ywt3c4w","_id":"ckw4qufqk004qgwtl7l5h1h2j"},{"post_id":"ckw4qufqa003wgwtl8s2d3e71","category_id":"ckw4qufpn001zgwtlf76efbqh","_id":"ckw4qufql004ugwtlg7s64jm7"},{"post_id":"ckw4qufpq0029gwtlb95sfef3","category_id":"ckw4qufpt002igwtl4r5aa33y","_id":"ckw4qufqm004ygwtl10nv6ozh"},{"post_id":"ckw4qufqc0040gwtlcxv3hxw6","category_id":"ckw4qufp5000tgwtl6ywt3c4w","_id":"ckw4qufqn0051gwtlce8729qy"},{"post_id":"ckw4qufqc0040gwtlcxv3hxw6","category_id":"ckw4qufpn001zgwtlf76efbqh","_id":"ckw4qufqq0055gwtld10n5fim"},{"post_id":"ckw4qufpr002dgwtl3nxy0i7w","category_id":"ckw4qufpt002igwtl4r5aa33y","_id":"ckw4qufqr0056gwtl8j1i638s"},{"post_id":"ckw4qufqe0049gwtlhlxk2arj","category_id":"ckw4qufor0004gwtldwdudn8l","_id":"ckw4qufqr0058gwtl4qol2g0u"},{"post_id":"ckw4qufqf004cgwtl34hx7wsh","category_id":"ckw4qufor0004gwtldwdudn8l","_id":"ckw4qufqr0059gwtl2x82cels"},{"post_id":"ckw4qufps002fgwtl1uppb39d","category_id":"ckw4qufpt002igwtl4r5aa33y","_id":"ckw4qufqr005bgwtlb6u1en1j"},{"post_id":"ckw4qufqh004fgwtl3ik43tc5","category_id":"ckw4qufor0004gwtldwdudn8l","_id":"ckw4qufqr005cgwtl4yf23wyn"},{"post_id":"ckw4qufqi004jgwtl1whs0az0","category_id":"ckw4qufor0004gwtldwdudn8l","_id":"ckw4qufqs005egwtl74aeej4n"},{"post_id":"ckw4qufpt002jgwtl6mih547q","category_id":"ckw4qufpt002igwtl4r5aa33y","_id":"ckw4qufqs005ggwtl4b438cfz"},{"post_id":"ckw4qufqj004ngwtl69iddlyd","category_id":"ckw4qufor0004gwtldwdudn8l","_id":"ckw4qufqs005jgwtl3obz51ps"},{"post_id":"ckw4qufqk004rgwtlgijj1k8k","category_id":"ckw4qufor0004gwtldwdudn8l","_id":"ckw4qufqt005lgwtl8b18g9w2"},{"post_id":"ckw4qufq8003pgwtldso8fm3u","category_id":"ckw4qufqj004ogwtlejyf8alu","_id":"ckw4qufqt005ogwtlb39yd0ii"},{"post_id":"ckw4qufql004vgwtl2fyf5s0m","category_id":"ckw4qufp5000tgwtl6ywt3c4w","_id":"ckw4qufqt005pgwtlh04743xl"},{"post_id":"ckw4qufqm004zgwtlg4182z10","category_id":"ckw4qufqj004ogwtlejyf8alu","_id":"ckw4qufqt005rgwtla1bn025c"},{"post_id":"ckw4qufqc0042gwtlfhf76k0h","category_id":"ckw4qufqj004ogwtlejyf8alu","_id":"ckw4qufqt005tgwtl9huw3gjr"},{"post_id":"ckw4qufqp0053gwtl5crh54nc","category_id":"ckw4qufqj004ogwtlejyf8alu","_id":"ckw4qufqu005vgwtl0tb76ibb"},{"post_id":"ckw4qufqd0046gwtl0t66d1fb","category_id":"ckw4qufqq0054gwtl0f4z5t7h","_id":"ckw4qufqu005ygwtla1eid0ts"}],"PostTag":[{"post_id":"ckw4qufom0001gwtl4ehk83to","tag_id":"ckw4qufos0005gwtlay367zy2","_id":"ckw4qufp1000jgwtle2x7hrvj"},{"post_id":"ckw4qufom0001gwtl4ehk83to","tag_id":"ckw4qufow000cgwtl78jdaw7h","_id":"ckw4qufp2000ngwtl7f80fl9d"},{"post_id":"ckw4qufop0003gwtl3dk5e0gb","tag_id":"ckw4qufoz000hgwtladbd29db","_id":"ckw4qufp7000wgwtl9oy6cdtd"},{"post_id":"ckw4qufop0003gwtl3dk5e0gb","tag_id":"ckw4qufp3000ogwtl4wno0fr5","_id":"ckw4qufp80010gwtlbks65pd0"},{"post_id":"ckw4qufot0007gwtldi2mbss0","tag_id":"ckw4qufp6000ugwtlcfuj7k5r","_id":"ckw4qufpc001agwtlbm8b6jla"},{"post_id":"ckw4qufot0007gwtldi2mbss0","tag_id":"ckw4qufoz000hgwtladbd29db","_id":"ckw4qufpd001egwtlb0bgey8p"},{"post_id":"ckw4qufpa0017gwtl2o49btu3","tag_id":"ckw4qufow000cgwtl78jdaw7h","_id":"ckw4qufpe001igwtlbeo45xx9"},{"post_id":"ckw4qufpa0017gwtl2o49btu3","tag_id":"ckw4qufp6000ugwtlcfuj7k5r","_id":"ckw4qufpf001mgwtl3zsadegj"},{"post_id":"ckw4qufou0009gwtlab327nnk","tag_id":"ckw4qufp6000ugwtlcfuj7k5r","_id":"ckw4qufpi001rgwtlcdqaho89"},{"post_id":"ckw4qufou0009gwtlab327nnk","tag_id":"ckw4qufpd001ggwtl17bbbsh6","_id":"ckw4qufpl001vgwtlf6yx5wpi"},{"post_id":"ckw4qufov000agwtleg6gd9g5","tag_id":"ckw4qufpf001ogwtl6llwh6ul","_id":"ckw4qufpu002kgwtla96hd0qj"},{"post_id":"ckw4qufov000agwtleg6gd9g5","tag_id":"ckw4qufpm001xgwtlfvsdd6rw","_id":"ckw4qufpv002ngwtlfl270wnx"},{"post_id":"ckw4qufov000agwtleg6gd9g5","tag_id":"ckw4qufpo0023gwtl7yn08xnj","_id":"ckw4qufpw002sgwtla37f7ta6"},{"post_id":"ckw4qufov000agwtleg6gd9g5","tag_id":"ckw4qufpr002agwtl809ldyee","_id":"ckw4qufpx002vgwtlcfjj46ux"},{"post_id":"ckw4qufox000dgwtldq2v1h0p","tag_id":"ckw4qufpt002ggwtlgvx77lg6","_id":"ckw4qufq00037gwtlgu8uar7w"},{"post_id":"ckw4qufox000dgwtldq2v1h0p","tag_id":"ckw4qufp6000ugwtlcfuj7k5r","_id":"ckw4qufq1003agwtl3womhkkx"},{"post_id":"ckw4qufox000dgwtldq2v1h0p","tag_id":"ckw4qufpx002xgwtl6wts79f8","_id":"ckw4qufq2003fgwtl5vey9ge3"},{"post_id":"ckw4qufoy000egwtlcr1j4206","tag_id":"ckw4qufpz0033gwtlb6c01n88","_id":"ckw4qufq8003ngwtl6sae1px6"},{"post_id":"ckw4qufoy000egwtlcr1j4206","tag_id":"ckw4qufq2003dgwtlb0scc2ss","_id":"ckw4qufq9003rgwtl8dfs6r8g"},{"post_id":"ckw4qufqc0040gwtlcxv3hxw6","tag_id":"ckw4qufq4003kgwtl2xhq1q32","_id":"ckw4qufqd0045gwtle07yc085"},{"post_id":"ckw4qufp0000igwtlg2ur61jm","tag_id":"ckw4qufq4003kgwtl2xhq1q32","_id":"ckw4qufqh004hgwtl68155dhl"},{"post_id":"ckw4qufp0000igwtlg2ur61jm","tag_id":"ckw4qufq9003tgwtl25z0hfyj","_id":"ckw4qufqi004kgwtlfn7f43bz"},{"post_id":"ckw4qufp0000igwtlg2ur61jm","tag_id":"ckw4qufqb003zgwtl9kdj3chy","_id":"ckw4qufqj004pgwtl258xdtbw"},{"post_id":"ckw4qufp0000igwtlg2ur61jm","tag_id":"ckw4qufqe0047gwtl42ptggfv","_id":"ckw4qufqk004sgwtl71ychyoc"},{"post_id":"ckw4qufp2000kgwtl01ad8zyn","tag_id":"ckw4qufqb003zgwtl9kdj3chy","_id":"ckw4qufqm004wgwtlcz1b6abj"},{"post_id":"ckw4qufp2000kgwtl01ad8zyn","tag_id":"ckw4qufq9003tgwtl25z0hfyj","_id":"ckw4qufqn0050gwtlf2y7djny"},{"post_id":"ckw4qufp3000pgwtl2fv8cajr","tag_id":"ckw4qufqk004tgwtl9vcfe3a0","_id":"ckw4qufqs005fgwtl6mla6gfm"},{"post_id":"ckw4qufp3000pgwtl2fv8cajr","tag_id":"ckw4qufqn0052gwtlgk31blmp","_id":"ckw4qufqs005hgwtl5lnncijw"},{"post_id":"ckw4qufp3000pgwtl2fv8cajr","tag_id":"ckw4qufqr0057gwtl0xrpgerg","_id":"ckw4qufqs005kgwtl5kyj1v4g"},{"post_id":"ckw4qufp3000pgwtl2fv8cajr","tag_id":"ckw4qufpm001xgwtlfvsdd6rw","_id":"ckw4qufqt005mgwtl9wrggo4c"},{"post_id":"ckw4qufp4000rgwtlg65o8sme","tag_id":"ckw4qufqs005dgwtlbupwc4nw","_id":"ckw4qufqt005sgwtl97mpcjfe"},{"post_id":"ckw4qufp4000rgwtlg65o8sme","tag_id":"ckw4qufq4003kgwtl2xhq1q32","_id":"ckw4qufqt005ugwtl4yvj09s4"},{"post_id":"ckw4qufp4000rgwtlg65o8sme","tag_id":"ckw4qufq2003dgwtlb0scc2ss","_id":"ckw4qufqu005xgwtl5fga9hti"},{"post_id":"ckw4qufp6000vgwtl7kika36r","tag_id":"ckw4qufqt005qgwtl96ktd0qf","_id":"ckw4qufqu005zgwtlducwazrq"},{"post_id":"ckw4qufp7000xgwtl0rnhe8o8","tag_id":"ckw4qufqu005wgwtleerbfs64","_id":"ckw4qufqv0063gwtl4xbvdgzi"},{"post_id":"ckw4qufp7000xgwtl0rnhe8o8","tag_id":"ckw4qufq4003kgwtl2xhq1q32","_id":"ckw4qufqv0064gwtldbzgeaw6"},{"post_id":"ckw4qufp7000xgwtl0rnhe8o8","tag_id":"ckw4qufqt005qgwtl96ktd0qf","_id":"ckw4qufqv0066gwtl9z1vcl25"},{"post_id":"ckw4qufp80011gwtl9j3b7d15","tag_id":"ckw4qufqb003zgwtl9kdj3chy","_id":"ckw4qufqw0068gwtl4wh4dnpn"},{"post_id":"ckw4qufp80011gwtl9j3b7d15","tag_id":"ckw4qufq9003tgwtl25z0hfyj","_id":"ckw4qufqw0069gwtl9v5xcgta"},{"post_id":"ckw4qufp90014gwtlg8dt7wef","tag_id":"ckw4qufp6000ugwtlcfuj7k5r","_id":"ckw4qufqw006bgwtl8wgd8ktn"},{"post_id":"ckw4qufp90014gwtlg8dt7wef","tag_id":"ckw4qufqv0067gwtl8ky8ctym","_id":"ckw4qufqw006cgwtl7lku4mah"},{"post_id":"ckw4qufpc001bgwtl809wau5y","tag_id":"ckw4qufqw006agwtl2hqlbnfg","_id":"ckw4qufqw006egwtl2uyccvh3"},{"post_id":"ckw4qufpd001fgwtlc4g6f3la","tag_id":"ckw4qufqb003zgwtl9kdj3chy","_id":"ckw4qufqx006hgwtlbc9zhq18"},{"post_id":"ckw4qufpd001fgwtlc4g6f3la","tag_id":"ckw4qufqw006fgwtlfkjk4o9m","_id":"ckw4qufqx006igwtl8hnoclr3"},{"post_id":"ckw4qufpe001jgwtl4fjl5y7q","tag_id":"ckw4qufqx006ggwtlelps9xrq","_id":"ckw4qufqx006lgwtl8sznhxxd"},{"post_id":"ckw4qufpe001jgwtl4fjl5y7q","tag_id":"ckw4qufpx002xgwtl6wts79f8","_id":"ckw4qufqx006mgwtl8pu82zna"},{"post_id":"ckw4qufpe001jgwtl4fjl5y7q","tag_id":"ckw4qufp6000ugwtlcfuj7k5r","_id":"ckw4qufqy006ogwtlfnvl8op6"},{"post_id":"ckw4qufpe001jgwtl4fjl5y7q","tag_id":"ckw4qufoz000hgwtladbd29db","_id":"ckw4qufqy006pgwtld3uheqk6"},{"post_id":"ckw4qufpf001ngwtlek9x1j7n","tag_id":"ckw4qufqx006kgwtlamww4ut7","_id":"ckw4qufqz006ugwtl5iqf4myj"},{"post_id":"ckw4qufpf001ngwtlek9x1j7n","tag_id":"ckw4qufqy006ngwtlgoqr8nef","_id":"ckw4qufqz006vgwtlg3vkf15t"},{"post_id":"ckw4qufpf001ngwtlek9x1j7n","tag_id":"ckw4qufqy006qgwtla8bhcj4b","_id":"ckw4qufr0006xgwtlh50g2qsd"},{"post_id":"ckw4qufpf001ngwtlek9x1j7n","tag_id":"ckw4qufqy006rgwtl5bhweofz","_id":"ckw4qufr0006ygwtl8fyucrvv"},{"post_id":"ckw4qufpf001ngwtlek9x1j7n","tag_id":"ckw4qufqy006sgwtlh7it7du6","_id":"ckw4qufr10070gwtlaq973ia8"},{"post_id":"ckw4qufph001qgwtldfrq166l","tag_id":"ckw4qufqy006qgwtla8bhcj4b","_id":"ckw4qufr10071gwtlezf7hyl6"},{"post_id":"ckw4qufph001qgwtldfrq166l","tag_id":"ckw4qufqz006wgwtlahaagyw9","_id":"ckw4qufr10073gwtld36ign60"},{"post_id":"ckw4qufpi001ugwtlfxsu6m4s","tag_id":"ckw4qufr1006zgwtleaty1ja3","_id":"ckw4qufr20076gwtl15fp2z6r"},{"post_id":"ckw4qufpi001ugwtlfxsu6m4s","tag_id":"ckw4qufr10072gwtlazka3t5f","_id":"ckw4qufr20077gwtl9rgs4gpk"},{"post_id":"ckw4qufpi001ugwtlfxsu6m4s","tag_id":"ckw4qufqy006qgwtla8bhcj4b","_id":"ckw4qufr20079gwtl8u9t8o3m"},{"post_id":"ckw4qufpm001ygwtl9ur9czfz","tag_id":"ckw4qufr20075gwtlcb7ecevi","_id":"ckw4qufr3007bgwtl75tp0ghl"},{"post_id":"ckw4qufpm001ygwtl9ur9czfz","tag_id":"ckw4qufr20078gwtl10u3g04z","_id":"ckw4qufr3007cgwtlfltsb0p9"},{"post_id":"ckw4qufpn0021gwtl3zwebh66","tag_id":"ckw4qufr2007agwtl4mmggk72","_id":"ckw4qufr4007ggwtlhok12092"},{"post_id":"ckw4qufpn0021gwtl3zwebh66","tag_id":"ckw4qufr3007dgwtle4oz7slw","_id":"ckw4qufr4007hgwtlfivdfqx9"},{"post_id":"ckw4qufpn0021gwtl3zwebh66","tag_id":"ckw4qufr3007egwtl6216fyl4","_id":"ckw4qufr4007jgwtl3al5ek06"},{"post_id":"ckw4qufpo0024gwtlezik90yy","tag_id":"ckw4qufqy006qgwtla8bhcj4b","_id":"ckw4qufr4007mgwtl1f9d8mfs"},{"post_id":"ckw4qufpo0024gwtlezik90yy","tag_id":"ckw4qufr4007igwtl3x2uh3e2","_id":"ckw4qufr5007ngwtl6ua2b6a0"},{"post_id":"ckw4qufpo0024gwtlezik90yy","tag_id":"ckw4qufr4007kgwtlfwe1csr0","_id":"ckw4qufr5007pgwtl58ksae3h"},{"post_id":"ckw4qufpp0027gwtl4fief2tn","tag_id":"ckw4qufr4007lgwtla60aaaqr","_id":"ckw4qufr6007sgwtlee6yg74n"},{"post_id":"ckw4qufpp0027gwtl4fief2tn","tag_id":"ckw4qufqe0047gwtl42ptggfv","_id":"ckw4qufr6007tgwtlcdww0a5p"},{"post_id":"ckw4qufpp0027gwtl4fief2tn","tag_id":"ckw4qufqu005wgwtleerbfs64","_id":"ckw4qufr6007vgwtlaxl0hoqn"},{"post_id":"ckw4qufpq0029gwtlb95sfef3","tag_id":"ckw4qufqy006rgwtl5bhweofz","_id":"ckw4qufr6007xgwtld6sf9qtd"},{"post_id":"ckw4qufpq0029gwtlb95sfef3","tag_id":"ckw4qufqy006sgwtlh7it7du6","_id":"ckw4qufr6007ygwtl9yupf5ze"},{"post_id":"ckw4qufpr002dgwtl3nxy0i7w","tag_id":"ckw4qufr4007lgwtla60aaaqr","_id":"ckw4qufr70081gwtl3ejugxxf"},{"post_id":"ckw4qufpr002dgwtl3nxy0i7w","tag_id":"ckw4qufqu005wgwtleerbfs64","_id":"ckw4qufr70082gwtl8dzw9moa"},{"post_id":"ckw4qufps002fgwtl1uppb39d","tag_id":"ckw4qufr20075gwtlcb7ecevi","_id":"ckw4qufr70085gwtlfh2uecd0"},{"post_id":"ckw4qufps002fgwtl1uppb39d","tag_id":"ckw4qufr70083gwtl2coo77sh","_id":"ckw4qufr70086gwtl2a549a4d"},{"post_id":"ckw4qufpt002jgwtl6mih547q","tag_id":"ckw4qufr1006zgwtleaty1ja3","_id":"ckw4qufr8008agwtl7b7p2veh"},{"post_id":"ckw4qufpt002jgwtl6mih547q","tag_id":"ckw4qufqy006qgwtla8bhcj4b","_id":"ckw4qufr8008bgwtl3j1qgl3f"},{"post_id":"ckw4qufpt002jgwtl6mih547q","tag_id":"ckw4qufr80088gwtl9eoa50pt","_id":"ckw4qufr8008dgwtl2o1l7ucn"},{"post_id":"ckw4qufpu002mgwtl1ggb971o","tag_id":"ckw4qufr20075gwtlcb7ecevi","_id":"ckw4qufr9008fgwtl6w7906gk"},{"post_id":"ckw4qufpu002mgwtl1ggb971o","tag_id":"ckw4qufr8008cgwtlaw9lfdkt","_id":"ckw4qufr9008ggwtlcl7f2qvc"},{"post_id":"ckw4qufpv002qgwtl5hcyhnfp","tag_id":"ckw4qufr70083gwtl2coo77sh","_id":"ckw4qufra008kgwtl275mav8y"},{"post_id":"ckw4qufpv002qgwtl5hcyhnfp","tag_id":"ckw4qufr9008hgwtl0xq75ksk","_id":"ckw4qufra008lgwtl11gcfkdq"},{"post_id":"ckw4qufpv002qgwtl5hcyhnfp","tag_id":"ckw4qufqu005wgwtleerbfs64","_id":"ckw4qufra008ngwtl7lbc9m30"},{"post_id":"ckw4qufpw002ugwtl2aagaeeh","tag_id":"ckw4qufr70083gwtl2coo77sh","_id":"ckw4qufra008pgwtl5qgv7l3d"},{"post_id":"ckw4qufpw002ugwtl2aagaeeh","tag_id":"ckw4qufra008mgwtlhrfsam25","_id":"ckw4qufrb008qgwtl5999hz1x"},{"post_id":"ckw4qufpx002ygwtlfb0shkrr","tag_id":"ckw4qufra008ogwtlg6nq85d8","_id":"ckw4qufrb008tgwtl2geb6sif"},{"post_id":"ckw4qufpx002ygwtlfb0shkrr","tag_id":"ckw4qufqs005dgwtlbupwc4nw","_id":"ckw4qufrb008ugwtlgkmd27cy"},{"post_id":"ckw4qufpz0034gwtl8swk33xy","tag_id":"ckw4qufqb003zgwtl9kdj3chy","_id":"ckw4qufrc008ygwtlcqeg8pvh"},{"post_id":"ckw4qufpz0034gwtl8swk33xy","tag_id":"ckw4qufq9003tgwtl25z0hfyj","_id":"ckw4qufrc008zgwtl4ftgai29"},{"post_id":"ckw4qufpz0034gwtl8swk33xy","tag_id":"ckw4qufrb008wgwtleawtfqdr","_id":"ckw4qufrc0091gwtl34ix1jm3"},{"post_id":"ckw4qufq10038gwtle9ta0x74","tag_id":"ckw4qufqb003zgwtl9kdj3chy","_id":"ckw4qufrd0094gwtlebtufgim"},{"post_id":"ckw4qufq10038gwtle9ta0x74","tag_id":"ckw4qufrc0090gwtl2e4lgg7e","_id":"ckw4qufrd0095gwtl1raybmmq"},{"post_id":"ckw4qufq10038gwtle9ta0x74","tag_id":"ckw4qufq9003tgwtl25z0hfyj","_id":"ckw4qufrd0097gwtlexwz59dk"},{"post_id":"ckw4qufq1003bgwtlfgnpfpxn","tag_id":"ckw4qufqt005qgwtl96ktd0qf","_id":"ckw4qufre009bgwtl0c06ar1j"},{"post_id":"ckw4qufq1003bgwtlfgnpfpxn","tag_id":"ckw4qufq9003tgwtl25z0hfyj","_id":"ckw4qufre009cgwtle7l96uzi"},{"post_id":"ckw4qufq1003bgwtlfgnpfpxn","tag_id":"ckw4qufrd0098gwtlf4fy9xm5","_id":"ckw4qufre009egwtla2vddi9e"},{"post_id":"ckw4qufq1003bgwtlfgnpfpxn","tag_id":"ckw4qufre0099gwtldqai8mu3","_id":"ckw4qufre009fgwtl8mbfhbp3"},{"post_id":"ckw4qufq3003ggwtl84r119e1","tag_id":"ckw4qufqt005qgwtl96ktd0qf","_id":"ckw4qufrg009hgwtl423k9j76"},{"post_id":"ckw4qufq3003ggwtl84r119e1","tag_id":"ckw4qufq9003tgwtl25z0hfyj","_id":"ckw4qufrg009igwtl3lpq8m8j"},{"post_id":"ckw4qufq3003igwtl5rl03yxp","tag_id":"ckw4qufqt005qgwtl96ktd0qf","_id":"ckw4qufrh009mgwtlal95c0vp"},{"post_id":"ckw4qufq3003igwtl5rl03yxp","tag_id":"ckw4qufrg009jgwtlbfpt2enm","_id":"ckw4qufrh009ngwtl1see0ww4"},{"post_id":"ckw4qufq3003igwtl5rl03yxp","tag_id":"ckw4qufq9003tgwtl25z0hfyj","_id":"ckw4qufri009pgwtlg3cmcodc"},{"post_id":"ckw4qufq7003mgwtldit7eklh","tag_id":"ckw4qufrh009lgwtlf9zi1l9s","_id":"ckw4qufrj009sgwtl1hgj01q1"},{"post_id":"ckw4qufq7003mgwtldit7eklh","tag_id":"ckw4qufqb003zgwtl9kdj3chy","_id":"ckw4qufrj009tgwtlcbzn7ndj"},{"post_id":"ckw4qufq7003mgwtldit7eklh","tag_id":"ckw4qufq9003tgwtl25z0hfyj","_id":"ckw4qufrj009vgwtl2em15yi9"},{"post_id":"ckw4qufq8003pgwtldso8fm3u","tag_id":"ckw4qufri009rgwtl1nh08i1n","_id":"ckw4qufrk009zgwtldcnpb47d"},{"post_id":"ckw4qufq8003pgwtldso8fm3u","tag_id":"ckw4qufrj009ugwtl54s44vev","_id":"ckw4qufrk00a0gwtl5oczf5h9"},{"post_id":"ckw4qufq8003pgwtldso8fm3u","tag_id":"ckw4qufrj009wgwtl3gm3dlla","_id":"ckw4qufrk00a2gwtl0ho38xvo"},{"post_id":"ckw4qufq8003pgwtldso8fm3u","tag_id":"ckw4qufrj009xgwtlcwxf4ov8","_id":"ckw4qufrk00a3gwtlelvsb2gu"},{"post_id":"ckw4qufqa003ugwtldlh2fq72","tag_id":"ckw4qufqu005wgwtleerbfs64","_id":"ckw4qufrl00a5gwtl2ce3bxoj"},{"post_id":"ckw4qufqa003ugwtldlh2fq72","tag_id":"ckw4qufrk00a1gwtl3hqhg8jg","_id":"ckw4qufrl00a6gwtl07a2eb9s"},{"post_id":"ckw4qufqa003ugwtldlh2fq72","tag_id":"ckw4qufq2003dgwtlb0scc2ss","_id":"ckw4qufrl00a8gwtl0qjnhj5w"},{"post_id":"ckw4qufqa003ugwtldlh2fq72","tag_id":"ckw4qufq4003kgwtl2xhq1q32","_id":"ckw4qufrl00a9gwtlgsw756ti"},{"post_id":"ckw4qufqa003wgwtl8s2d3e71","tag_id":"ckw4qufqw006agwtl2hqlbnfg","_id":"ckw4qufrl00abgwtl8aewg26a"},{"post_id":"ckw4qufqa003wgwtl8s2d3e71","tag_id":"ckw4qufrl00a7gwtl2mfuh5ko","_id":"ckw4qufrl00acgwtl055m0xtt"},{"post_id":"ckw4qufqc0042gwtlfhf76k0h","tag_id":"ckw4qufrl00aagwtlcqe6giyt","_id":"ckw4qufrm00afgwtlav7gbw0z"},{"post_id":"ckw4qufqc0042gwtlfhf76k0h","tag_id":"ckw4qufrl00adgwtl0kcgakna","_id":"ckw4qufrm00aggwtl256dcw7m"},{"post_id":"ckw4qufqd0046gwtl0t66d1fb","tag_id":"ckw4qufrm00aegwtl2pxhcric","_id":"ckw4qufrm00ajgwtl0sodgwvr"},{"post_id":"ckw4qufqd0046gwtl0t66d1fb","tag_id":"ckw4qufrm00ahgwtl1ij0h0np","_id":"ckw4qufrm00akgwtl2xai4ery"},{"post_id":"ckw4qufqe0049gwtlhlxk2arj","tag_id":"ckw4qufqv0067gwtl8ky8ctym","_id":"ckw4qufrn00amgwtlhbj0fkq0"},{"post_id":"ckw4qufqe0049gwtlhlxk2arj","tag_id":"ckw4qufp6000ugwtlcfuj7k5r","_id":"ckw4qufrn00angwtl5wdc3k7j"},{"post_id":"ckw4qufqf004cgwtl34hx7wsh","tag_id":"ckw4qufqv0067gwtl8ky8ctym","_id":"ckw4qufrn00aqgwtlf121bf9w"},{"post_id":"ckw4qufqf004cgwtl34hx7wsh","tag_id":"ckw4qufp6000ugwtlcfuj7k5r","_id":"ckw4qufrn00argwtl5pyvfjus"},{"post_id":"ckw4qufqf004cgwtl34hx7wsh","tag_id":"ckw4qufrn00aogwtl2dw1ebq7","_id":"ckw4qufro00atgwtlhvnbhrjj"},{"post_id":"ckw4qufqh004fgwtl3ik43tc5","tag_id":"ckw4qufqv0067gwtl8ky8ctym","_id":"ckw4qufro00augwtl5vsp7fpv"},{"post_id":"ckw4qufqh004fgwtl3ik43tc5","tag_id":"ckw4qufp6000ugwtlcfuj7k5r","_id":"ckw4qufro00awgwtlbsrl2iom"},{"post_id":"ckw4qufqi004jgwtl1whs0az0","tag_id":"ckw4qufqv0067gwtl8ky8ctym","_id":"ckw4qufro00aygwtlcj1u83rh"},{"post_id":"ckw4qufqi004jgwtl1whs0az0","tag_id":"ckw4qufp6000ugwtlcfuj7k5r","_id":"ckw4qufro00azgwtldwnf37ar"},{"post_id":"ckw4qufqi004jgwtl1whs0az0","tag_id":"ckw4qufro00avgwtl3fkqd7s7","_id":"ckw4qufrp00b1gwtlcap636ty"},{"post_id":"ckw4qufqj004ngwtl69iddlyd","tag_id":"ckw4qufp6000ugwtlcfuj7k5r","_id":"ckw4qufrp00b2gwtl1c817o1b"},{"post_id":"ckw4qufqj004ngwtl69iddlyd","tag_id":"ckw4qufqv0067gwtl8ky8ctym","_id":"ckw4qufrp00b4gwtl50qi4ziq"},{"post_id":"ckw4qufqk004rgwtlgijj1k8k","tag_id":"ckw4qufp6000ugwtlcfuj7k5r","_id":"ckw4qufrp00b5gwtlaq6a70lk"},{"post_id":"ckw4qufqk004rgwtlgijj1k8k","tag_id":"ckw4qufqv0067gwtl8ky8ctym","_id":"ckw4qufrq00b7gwtl3hgo4w9g"},{"post_id":"ckw4qufql004vgwtl2fyf5s0m","tag_id":"ckw4qufrp00b3gwtld7jtf556","_id":"ckw4qufrq00b9gwtlbkhu4nim"},{"post_id":"ckw4qufql004vgwtl2fyf5s0m","tag_id":"ckw4qufrp00b6gwtl8bd1cveu","_id":"ckw4qufrq00bagwtlfbq9docm"},{"post_id":"ckw4qufqm004zgwtlg4182z10","tag_id":"ckw4qufri009rgwtl1nh08i1n","_id":"ckw4qufrq00bdgwtl8wix6x79"},{"post_id":"ckw4qufqm004zgwtlg4182z10","tag_id":"ckw4qufrq00bbgwtl3q933e1g","_id":"ckw4qufrr00begwtla3gq3n4d"},{"post_id":"ckw4qufqp0053gwtl5crh54nc","tag_id":"ckw4qufrq00bcgwtl66wsgsew","_id":"ckw4qufrr00bfgwtlernte4u7"}],"Tag":[{"name":"Bayes","_id":"ckw4qufos0005gwtlay367zy2"},{"name":"statistic","_id":"ckw4qufow000cgwtl78jdaw7h"},{"name":"EDP","_id":"ckw4qufoz000hgwtladbd29db"},{"name":"matrix","_id":"ckw4qufp3000ogwtl4wno0fr5"},{"name":"math","_id":"ckw4qufp6000ugwtlcfuj7k5r"},{"name":"FEM","_id":"ckw4qufpd001ggwtl17bbbsh6"},{"name":"scrapy","_id":"ckw4qufpf001ogwtl6llwh6ul"},{"name":"python","_id":"ckw4qufpm001xgwtlfvsdd6rw"},{"name":"spider","_id":"ckw4qufpo0023gwtl7yn08xnj"},{"name":"crawl","_id":"ckw4qufpr002agwtl809ldyee"},{"name":"Hilbert","_id":"ckw4qufpt002ggwtlgvx77lg6"},{"name":"analyse","_id":"ckw4qufpx002xgwtl6wts79f8"},{"name":"GAN","_id":"ckw4qufpz0033gwtlb6c01n88"},{"name":"deep-learning","_id":"ckw4qufq2003dgwtlb0scc2ss"},{"name":"machine-learning","_id":"ckw4qufq4003kgwtl2xhq1q32"},{"name":"programming","_id":"ckw4qufq9003tgwtl25z0hfyj"},{"name":"algo","_id":"ckw4qufqb003zgwtl9kdj3chy"},{"name":"CNN","_id":"ckw4qufqe0047gwtl42ptggfv"},{"name":"mongo","_id":"ckw4qufqk004tgwtl9vcfe3a0"},{"name":"mongodb","_id":"ckw4qufqn0052gwtlgk31blmp"},{"name":"docker","_id":"ckw4qufqr0057gwtl0xrpgerg"},{"name":"nlp","_id":"ckw4qufqs005dgwtlbupwc4nw"},{"name":"datamining","_id":"ckw4qufqt005qgwtl96ktd0qf"},{"name":"knowledge-graph","_id":"ckw4qufqu005wgwtleerbfs64"},{"name":"probability","_id":"ckw4qufqv0067gwtl8ky8ctym"},{"name":"OS","_id":"ckw4qufqw006agwtl2hqlbnfg"},{"name":"data-structure","_id":"ckw4qufqw006fgwtlfkjk4o9m"},{"name":"Sobolev","_id":"ckw4qufqx006ggwtlelps9xrq"},{"name":"entity-resolution","_id":"ckw4qufqx006kgwtlamww4ut7"},{"name":"sequence-labeling","_id":"ckw4qufqy006ngwtlgoqr8nef"},{"name":"relation-extraction","_id":"ckw4qufqy006qgwtla8bhcj4b"},{"name":"LSTM","_id":"ckw4qufqy006rgwtl5bhweofz"},{"name":"RNN","_id":"ckw4qufqy006sgwtlh7it7du6"},{"name":"distant-supervision","_id":"ckw4qufqz006wgwtlahaagyw9"},{"name":"relation-classification","_id":"ckw4qufr1006zgwtleaty1ja3"},{"name":"attention","_id":"ckw4qufr10072gwtlazka3t5f"},{"name":"event-detection","_id":"ckw4qufr20075gwtlcb7ecevi"},{"name":"co-reference","_id":"ckw4qufr20078gwtl10u3g04z"},{"name":"convolution","_id":"ckw4qufr2007agwtl4mmggk72"},{"name":"BiLSTM","_id":"ckw4qufr3007dgwtle4oz7slw"},{"name":"event-extraction","_id":"ckw4qufr3007egwtl6216fyl4"},{"name":"limited-supervision","_id":"ckw4qufr4007igwtl3x2uh3e2"},{"name":"weak-supervision","_id":"ckw4qufr4007kgwtlfwe1csr0"},{"name":"KGC","_id":"ckw4qufr4007lgwtla60aaaqr"},{"name":"neural-network","_id":"ckw4qufr70083gwtl2coo77sh"},{"name":"reinforcement-learning","_id":"ckw4qufr80088gwtl9eoa50pt"},{"name":"summarization","_id":"ckw4qufr8008cgwtlaw9lfdkt"},{"name":"NLP","_id":"ckw4qufr9008hgwtl0xq75ksk"},{"name":"regular-expression","_id":"ckw4qufra008mgwtlhrfsam25"},{"name":"review","_id":"ckw4qufra008ogwtlg6nq85d8"},{"name":"complexity","_id":"ckw4qufrb008wgwtleawtfqdr"},{"name":"compression","_id":"ckw4qufrc0090gwtl2e4lgg7e"},{"name":"classification","_id":"ckw4qufrd0098gwtlf4fy9xm5"},{"name":"prediction","_id":"ckw4qufre0099gwtldqai8mu3"},{"name":"qualitative-induction","_id":"ckw4qufrg009jgwtlbfpt2enm"},{"name":"graph","_id":"ckw4qufrh009lgwtlf9zi1l9s"},{"name":"hexo","_id":"ckw4qufri009rgwtl1nh08i1n"},{"name":"latex","_id":"ckw4qufrj009ugwtl54s44vev"},{"name":"mathjax","_id":"ckw4qufrj009wgwtl3gm3dlla"},{"name":"marked","_id":"ckw4qufrj009xgwtlcwxf4ov8"},{"name":"knowledge-reasoning","_id":"ckw4qufrk00a1gwtl3hqhg8jg"},{"name":"kernel","_id":"ckw4qufrl00a7gwtl2mfuh5ko"},{"name":"management","_id":"ckw4qufrl00aagwtlcqe6giyt"},{"name":"firm","_id":"ckw4qufrl00adgwtl0kcgakna"},{"name":"francais","_id":"ckw4qufrm00aegwtl2pxhcric"},{"name":"language","_id":"ckw4qufrm00ahgwtl1ij0h0np"},{"name":"aleatoire","_id":"ckw4qufrn00aogwtl2dw1ebq7"},{"name":"vector","_id":"ckw4qufro00avgwtl3fkqd7s7"},{"name":"web","_id":"ckw4qufrp00b3gwtld7jtf556"},{"name":"chrome","_id":"ckw4qufrp00b6gwtl8bd1cveu"},{"name":"rss","_id":"ckw4qufrq00bbgwtl3q933e1g"},{"name":"vps","_id":"ckw4qufrq00bcgwtl66wsgsew"}]}}