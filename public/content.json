{"meta":{"title":"Lorrin's Blog","subtitle":"半路出家的码农 | 法国留学中","description":"分享一些想法，每周更一篇论文阅读","author":"Lorrin","url":"https://LorrinWWW.github.io"},"pages":[{"title":"categories","date":"2016-11-27T12:13:12.000Z","updated":"2018-03-10T21:47:49.441Z","comments":true,"path":"categories/index.html","permalink":"https://LorrinWWW.github.io/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2016-11-27T12:15:12.000Z","updated":"2018-03-10T21:47:27.795Z","comments":true,"path":"tags/index.html","permalink":"https://LorrinWWW.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Several models for knowledge graph representing and completing 几个知识图谱模型","slug":"[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing","date":"2018-03-10T07:00:00.000Z","updated":"2018-03-12T20:23:34.399Z","comments":true,"path":"[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/","link":"","permalink":"https://LorrinWWW.github.io/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/","excerpt":"","text":"2018.3.10Several models for knowledge graph representing and completing摘要：上次看到的ConMask在开放领域knowledge graph completion有着不错的表现，这次我们不考虑开放领域，介绍几个经典的模型。 1. Series of Trans1.1 TransE [1]TransE [1] 可能是最为常用也最为基础的方法是一种基于强化学习(RL)的模型. 它有一个简单实用的假设： h+r = t其中h是head entity的向量，t是tail entity的向量，r是关系向量。 TransE定义了loss function： \\mathcal{L(T)} = \\sum_{\\in T} [\\gamma + E() - E(","categories":[{"name":"research","slug":"research","permalink":"https://LorrinWWW.github.io/categories/research/"}],"tags":[{"name":"knowledge-graph","slug":"knowledge-graph","permalink":"https://LorrinWWW.github.io/tags/knowledge-graph/"},{"name":"KGC","slug":"KGC","permalink":"https://LorrinWWW.github.io/tags/KGC/"}]},{"title":"Open-World Knowledge Graph Completion 笔记","slug":"[2018.2.26]Open-World-Knowledge-Graph-Completion","date":"2018-02-26T07:00:00.000Z","updated":"2018-03-12T20:23:16.727Z","comments":true,"path":"[2018.2.26]Open-World-Knowledge-Graph-Completion/","link":"","permalink":"https://LorrinWWW.github.io/[2018.2.26]Open-World-Knowledge-Graph-Completion/","excerpt":"","text":"Open-World Knowledge Graph Completion摘要：[1]文首先讨论了Closed-World KGC，它无法处理从 KG 外部加入的新实体，并严重依赖已有KG连接的，不能对弱连接有好的预测。为此定义了 Open-World KGC，可以接收 新的实体并链接到 KG；并依此提出了ConMask模型，在给定关系和实体名、实体描述的前提下，利用attention机制通过关系定位实体描述中最相关的词，再以这些词和实体得到要链接的实体。 1. Introduction知识图谱（KG）是一种信息网络，它用三元组 $(h,r,t)$ 来表示知识（h: head entity, t: tail entity, r: relation），目前比较出名的KG有 DBPedia，ConceptNet 等，目前的大多数KG都有噪音且不完整，比如基于Wikipedia的DBPedia有460万个实体，但其中一半实体拥有少于5个的关系。 这说明了大部分的知识图谱仍然是非常不完善的，我们必须从一开始就要考虑系统的修改、补充完善的可能性。这项任务被定义为Knowledge Graph Completion (KGC)。 Closed-World KGC给定一个不完整的KG $G=(E,R,T)$ 其中 $E,R,T$ 分别表示实体集，关系集以及三元组集，Closed-World KGC的任务就是通过找到一系列丢失的三元组 $ T’ = { \\langle h,r,t \\rangle|h \\in E, r \\in R, t \\in E, \\langle h,r,t \\rangle \\notin T }$ 来补充现有的 $G$. 一个很重要的地方在于，Closed-World KGC 假定了新的实体、关系都被原有的 $G$ 包含，对于不在 $G$ 中的实体则一筹莫展。 目前的Closed-World KGC方法很多往往使用TranE或者低维特征表示模型，前者的核心思想就是 $h+r=t$ ，后者则指 Embedding 等。 该方法仅对固定的或者缓慢更新的KG有效，对于快速变更的KG则效果一般。 Open-World KGC给定一个不完整的KG $G=(E,R,T)$ 其中 $E,R,T$ 分别表示实体集，关系集以及三元组集，Open-World KGC 的任务就是找到 $G$ 中没有的三元组集，$T’ ={|h\\in E^i,r\\in R, t\\in E^i,\\notin T}$ 其中 $E^i$ 是G的实体超集。 Closed-World方法就是根据知识图谱的拓扑结构更新一个随机的向量作为实体和关系的embedding，但对于不在网络中的实体，这个方法就失效了，这个时候就需要用别的特征来代替这个用网络拓扑结构得到的特征。 一般直觉就是用实体的描述（entity description），根据实体的描述来得到特征，但从非结构化文本中学习向量表示比在网络的拓扑结构中要难得多，原因如下： 在Closed-world KGC模型中，每个实体都有一个embedding (从与它相连的实体上学得的)，但Open-World KGC模型则需要从实体描述的word embedding中得到entity embedding。而无论实体之间的联系情况是什么，word embedding的更新都会导致有相同词的entities的更新。 因为使用了非结构化文本，所以Open-World KGC模型可能会引入噪音或者冗余信息。 2. Closed-World KGC在 Closed-World KGC 中，最为常用也最为基础的方法是一种给予强化学习(RL)的模型，被称为TransE [2]. 它有一个简单实用的假设： h+r = t其中h是head entity的向量，t是tail entity的向量，r是关系向量。 TransE定义了loss function： \\mathcal{L(T)} = \\sum_{\\in T} [\\gamma + E(\\langle h,r,t \\rangle) - E(\\langle h',r',t' \\rangle)]_+其中 $T$ 代表一个三元组的集合；$E(\\langle h,r,t \\rangle) = ||h+r-t||_{L_n}$是energy function；$\\langle h,r,t \\rangle$是G中的一个三元组；$h’,\\langle r’,t’ \\rangle$代表一个不存在于 $T$ 的三元组，通过随机替换一部分$\\langle h,r,t \\rangle$来得到。 这里还略去了很多TransE的变体等其他模型，但它们都是基于Closed-World KGC来做的。 3. ConMask for Open-World KGC首先通过一个例子来说明： 任务：填补三元组 $\\langle \\text{Ameen Sayani, residence, ?}\\rangle$，其中KG中并没有Ameen Sayani这个实体。 描述：“… Ameen Sayani was introduced to All India Radio, Bombay, by his brother Hamid Sayani. Ameen participated in English programmes there for ten years …” ， 目标预测实体：Bombay (or Mumbai) 为了找到Ameen Sayani的住址，在处理这个任务的过程中，我们不会从头看到尾，而是找到相关的关键词比如家庭或工作相关的词。这里，我们发现Ameen Sayani的工作地点All India Radio在Bombay，因此我们推测Ameen Sayani也住在Bombay（Bombay就是现在的Mumbai）。 这个过程也可以被归纳为： 定位与该任务相关的信息。 根据上下文和相关文本推断。 根据相关文本推出正确目标实体。 仿照这个过程，ConMask的工作方式被设计为： Relationship-dependent content masking — 标记那些与任务相关的词语。 Target fusion — 从相关文本中抽取目标实体的embedding。 Target entity resolution — 通过计算KG中的候选目标实体，2中抽取出的实体embedding以及其它文本特征之间的相似度来选定目标实体。 ConMask模型总体结构如上，ConMask通过选择与给定关系相关的词来避免引入不相关的和有噪音的词。对于相关的文本，ConMask通过全连接卷积神经网络（FCN）来提取word-embedding。最后它将提取的embedding于KG中存在的实体进行比较，从而获得一系列目标实体。 3.1 Relationship-dependent content maskingConMask根据给定的关系预处理输入文本，来选择一些相关的小片段，从而屏蔽掉无关文本。content-masking这一灵感来源于基于attention机制的RNN网络[3]，关于attention之前的笔记也有学习过。 基于相似度得到选择最相关的词，具体公式如下： \\tau(\\phi(e), \\psi(r)) = W_{\\phi(e)} \\circ f_w(W_{\\phi(e)}, W_{\\psi(r)})其中 $e$ 是一个实体，$r$ 是某个关系, $\\phi$ 是description function并返回一个向量用于表示对一个实体或关系的描述，$\\psi$ 是name mapping function并返回一个向量用于表示一个实体或关系的名字， $ W{\\phi{(e)}} \\in \\mathbb{R}^{|\\phi(r)|\\times k} $ 是一个描述矩阵每一行表示一个k维的描述中的word-embedding， $W{\\phi{(e)}} \\in \\mathbb{R}^{|\\phi(r)|\\times k} $ 是一个名字矩阵每一行表示一个k维的实体名字word-embedding，$\\circ$ 是row-wise product，$f_w$ 用于计算的每一行的屏蔽比重。 作者给了一个简单的$f_w$ ，Maximal Word-Relationship Weights(MWRW)，就是计算实体描述中每个词向量与关系名称的每个词向量的最大cos相似度: f_w^{MWRW}(W_{\\phi(e)}, W_{\\psi(r)})_{[i]} = max_j(\\frac{\\sum_m^k{W_{\\phi(e)[i,m]} W_{\\psi(r)[j,m]}}}{\\sqrt{\\sum_m^k{W^2_{\\phi(e)[i,m]}}}\\sqrt{\\sum_m^k{W^2_{\\psi(e)[j,m]}}}})这个公式会给与给定关系无关的词更小的权重，与关系语义接近的词更大的权重，但权重最高的词一般不是目标实体，如下图所示，给定关系spouse，得到最大权重的是married，虽然married与spouse在语义上接近，但它并不是目标实体，因此作者称这种有着最大MWRW权重的词为指示词（indicator word），因为正确的词一般就在该词附近，在下图例子中可以发现目标实体barack obama就在married后面。 为了给目标实体word正确的权重，作者改进了这个公式，具体公式如下，这个公式就是每个词的权重不会小于之前 $k_m$ 称为 Maximal Context-Relationship Weights (MCRW)： f_w^{MCRW}(W_{\\phi(e)}, W_{\\psi(r)})_{[i]} = max(f_w^{MWRW}(W_{\\phi(e)}, W_{\\psi(r)})_{[i-k_m:i]}) 3.2 Target Fusion这一步骤用于输出基于词的实体embedding，这个过程记为$\\xi$，使用Conetent Masking $\\tau$ 的输出。它使用全连接卷积网络，其结构如下： Semantic Averaging 我们可以对所有实体进行embedding，但是这会产生大量的参数，使计算变得非常复杂。事实上，因为Target fusion函数用于抽取，所以对不需要抽取的实体名字使用target fusion就会显得很奇怪也很没有必要。 这里作者提出了一个简单的语义平均法来计算这些实体的embedding：$\\eta(W) = \\frac{1}{k_l}\\sum_i^{k_i}W_i$ 3.3 Loss function为了加速训练，我们参考 list-wise ranking loss function (Shi and Weninger 2017)，并设计 partial list-wise ranking loss function，拥有正负目标采样。正样本就是训练集的标注内容，记为$E^+$；负样本就是替换正样本的head entity或tail entity所得到的，记为$E^-$ 。 \\mathcal{L}(h, r, t) = \\begin{cases} \\sum_{h_+\\in E^+}{-\\frac{log(S(h_+,r,t,E^+\\cup E^-))}{|E^+|}}, & \\text{if }p_c > 0.5; \\\\ \\sum_{h_+\\in E^+}{-\\frac{log(S(h,r,t_+,E^+\\cup E^-))}{|E^+|}}, & \\text{if }p_c \\le 0.5; . \\end{cases}$p_c$ 服从$[0,1]$的均匀分布，大于0.5时，把输入实体作为tail entity，小于0.5的时候就是作为head entity，表示替换head entity和tail entity的概率各为50%。另有$S$, 即 softmax normalized output of ConMask： S(h,r,t,E^+) = \\begin{cases} \\sum_{e \\in E^\\pm}^{exp(ConMask(h,r,t))}{exp(ConMask(e,r,t))} & \\text{if } p_c > 0.5 \\\\ \\sum_{e \\in E^\\pm}^{exp(ConMask(e,r,t))}{exp(ConMask(h,r,t))} & \\text{if } p_c \\le 0.5 \\\\ \\end{cases}4. Results从结果上看，对比其他模型，在开放领域，ConMask获得了最佳的效果；在Closed-World中，尽管ConMask不是为此设计的，但是对比TransE和TransR依然不逊色，结果相仿。 目前而言，ConMask模型只能预测在实体描述中表达的关系，将来还应考虑扩展它，使其能够发现新的或隐含的关系。 Bibliographies笔记参考：https://zhuanlan.zhihu.com/p/33026043，http://blog.csdn.net/TgqDT3gGaMdkHasLZv/article/details/79224178 代码实现：https://github.com/bxshi/ConMask [1] Shi, Baoxu, and Tim Weninger. “Open-World Knowledge Graph Completion.” arXiv preprint arXiv:1711.03438 (2017). [2] Bordes, A., Usunier, N., Garcia-Duran, A., Weston, J., &amp; Yakhnenko, O. (2013). Translating embeddings for modeling multi-relational data. In Advances in neural information processing systems (pp. 2787-2795). [3] Chorowski, J. K., Bahdanau, D., Serdyuk, D., Cho, K., &amp; Bengio, Y. (2015). Attention-based models for speech recognition. In Advances in neural information processing systems (pp. 577-585).","categories":[{"name":"research","slug":"research","permalink":"https://LorrinWWW.github.io/categories/research/"}],"tags":[{"name":"CNN","slug":"CNN","permalink":"https://LorrinWWW.github.io/tags/CNN/"},{"name":"knowledge-graph","slug":"knowledge-graph","permalink":"https://LorrinWWW.github.io/tags/knowledge-graph/"},{"name":"KGC","slug":"KGC","permalink":"https://LorrinWWW.github.io/tags/KGC/"}]},{"title":"Nested LSTMs 笔记","slug":"[2018.2.5]Nested-LSTMs","date":"2018-02-05T07:00:00.000Z","updated":"2018-03-12T20:22:57.676Z","comments":true,"path":"[2018.2.5]Nested-LSTMs/","link":"","permalink":"https://LorrinWWW.github.io/[2018.2.5]Nested-LSTMs/","excerpt":"","text":"Nested LSTMs摘要：最近，一种新的 Nested LSTMs 网络被提出。在通常的LSTM网络中，我们通过将LSTM单元堆叠，从而形成深度RNN网络，提高其效果；Nested LSTM则通过嵌套而不是堆栈来增添LSTM的深度。在NLSTM中，记忆单元的值是由LSTM单元计算的，其中，LSTM单元具有自身内在的记忆单元。具体而言，NLSTM记忆单元并不是按照等式：$ct^{outer} = f_t \\odot c{t-1} + it \\odot g_t$ 对（外部）记忆单元的值进行计算，而是使用级联：$(f_t \\odot c{t-1}, i_t \\odot g_t)$ 将其作为内部LSTM（或NLSTM）记忆单元的输入，并设定 $c_t^{outer} = h_t^{inner}$。在访问内部记忆时，Nested LSTM 相比传统的堆栈 LSTM 有更高的自由度，从而能处理更长时间规模的内部记忆；实验也表明，在参数数量相似的情况下，NLSTM 在多种任务上都超越了堆栈 LSTM。作者认为Nested LSTM 有潜力直接取代堆栈 LSTM。 1. Introduction学习长期的依赖关系是当前人工智能领域中，尤其是在nlp领域，机器学习方法的关键性挑战。基于循环神经网络的体系结构已经在使得机器能够模仿这种能力方面取得了显著进展。 single-layer LSTM RNN的输入是以当前的状态为依据，适合学习时间上的抽象特征。在实践中，许多专家已经证明，更为复杂的体系结构是解决许多任务的关键。其中一个原因是梯度消失问题（Hochreiter于1991年、Bengio等人于1994年提出），它使得简单的RNN难以学习长期依赖关系。Hochreiter和Schmidhuber于1997年提出了LSTM，包含能够改善梯度消失问题的记忆机制。单层LSTM如上图，图中的三个单元实际上是同一个单元，它循环地将内部的参数传递给自己。 Stacked LSTMs 堆栈 LSTM 架构使用一系列 LSTM 一层层地堆叠在一起来处理数据，一层的输出成为下一层的输入。上图为一个两层的LSTM网络。 引入多层的结构，即将多个LSTM单元堆叠，每一层的输出成为下一层的输入。 每层处理我们希望解决的任务的一部分，并将其传递给下一层。额外的隐藏层可以添加到多层感知器神经网络，使其有更深入的“理解”。 额外的隐藏层被认为重新组合了来自先前层的学习表示，并在高度抽象层次上找到新的表示。 例如，从线条到形状到对象。 Nested LSTMs在 NLSTM 中，LSTM 的记忆单元可以访问内部记忆。相比于传统的堆栈 LSTM，这一关键特征使得该模型能实现更有效的时间层级。在 NLSTM 中，外部记忆单元可自由选择读取、编写的相关长期信息到内部单元。相比之下，在Stacked LSTM 中，高层级的激活（类似内部记忆）直接生成输出，因此必须包含所有的与当前预测相关的短期信息。换言之，Stacked LSTM 与Nested LSTM 之间的主要不同在于，NLSTM 可以选择性地访问内部记忆。这使得，即使这些事件与当前事件不相关，内部记忆也能够记住、处理更长时间规模上的事件。我们在后面一章更详细地介绍它。 2. Model of Nested LSTMsLSTM 中的输出门会编码可能与当前的时间步骤不相关，但是仍然值得记忆的信息。Nested LSTM 根据这一直观理解来创造一种记忆的时间层级。以同样的方式被gate控访问内部记忆，因此长期信息只有在情景相关的条件下才能选择性地访问。 The architecture在 LSTM 网络中，单元状态的更新公式和门控机制可以表示为以下方程式： i_t = \\sigma_i (x_t W_{xi} + h_{t-1} W_{hi} + b_i) \\\\ f_t = \\sigma_t (x_t W_{xf} + h_{t-1} W_{hf} + b_i) \\\\ c_t = f_t \\odot c_{c-1} + \\sigma_c (x_t W_{xc} + h_{t-1} W_{hc} + b_c) \\\\ o_t = \\sigma_o (x_t W_{xo} + h_{t-1} W_{ho} + b_o) \\\\ h_t = o_t \\odot \\sigma_h(c_t)Nested LSTM 使用已学习的状态函数 $ct = m_t(f_t\\odot c{t−1}, it \\odot g_t)​$ 来替代 LSTM 中计算 $c_t​$ 的加运算。我们将函数的状态表示为 m 在时间 t 的内部记忆（inner memory），调用该函数以计算 $c_t​$ 和 $m{t+1}​$。我们可以使用另一个 LSTM 单元来实现该记忆函数，就生成了 Nested LSTM。同样，该记忆函数能够由另一个 Nested LSTM 单元替换，因此就能构建任意深的嵌套网络。 因此，我们得到NLSTM 中记忆函数的输入和隐藏状态： \\tilde{h}_{t-1} = f_t \\odot c_{t-1} \\\\ \\tilde{x}_t = i_t \\odot \\sigma_c (x_t W_{xc} + h_{t-1} W_{hc} + b_c)注意如果记忆函数是加性的，那么$ct = f_t \\odot c{c-1} + \\sigmac (x_t W{xc} + h{t-1} W{hc} + bc) = \\tilde{h}{t-1} + \\tilde{x}_t $，整个系统将退化到经典的 LSTM。 LSTM、Stacked LSTM 和 Nested LSTM 的计算图形。隐藏的状态、外部记忆单元和内部记忆单元分别由h、c和d进行表示。虽然当前的隐藏状态可以直接影响下一个内部记忆单元的内容，但内部记忆只有通过外部记忆才能够影响隐藏状态。 \\widetilde{i}_t = \\widetilde{\\sigma}_i (\\widetilde{x}_t \\widetilde{W}_{xi} + \\widetilde{h}_{t-1} \\widetilde{W}_{hi} + \\widetilde{b}_i) \\\\ \\widetilde{f}_t = \\widetilde{\\sigma}_t (\\widetilde{x}_t \\widetilde{W}_{xf} + \\widetilde{h}_{t-1} \\widetilde{W}_{hf} + \\widetilde{b}_i) \\\\ \\widetilde{c}_t = \\widetilde{f}_t \\odot \\widetilde{c}_{c-1} + \\widetilde{\\sigma}_c (\\widetilde{x}_t \\widetilde{W}_{xc} + \\widetilde{h}_{t-1} \\widetilde{W}_{hc} + \\widetilde{b}_c) \\\\ \\widetilde{o_t} = \\widetilde{\\sigma}_o (\\widetilde{x}_t \\widetilde{W}_{xo} + \\widetilde{h}_{t-1} \\widetilde{W}_{ho} + \\widetilde{b}_o) \\\\ \\widetilde{h}_t = \\widetilde{o}_t \\odot \\widetilde{\\sigma}_h(\\widetilde{c}_t)现在，外部 LSTM 的单元状态更新方式为 $ ct = \\tilde{h}{t} $ 。 3. Experiments见附件论文[1] 4. ConclusionNested LSTM（NLSTM）是LSTM模型的简单扩展，通过嵌套来增加深度，而不是通过堆叠。 NLSTM的内部存储器单元形成内部存储器，其仅通过外部存储器单元被其他计算元件访问，实现了时间层级的形式。 论文[1]的实验表明，在相似的参数设置下，Nested LSTM 在多种字符级语言建模任务中的表现都超越了Stacked LSTM和single-layer LSTM，并且和Stacked LSTM 的高层级单元相比，LSTM 的内部记忆可以学习更长期的依赖关系。 NLSTM的Tensorflow实现 NLSTM的Keras实现 Bibliographies笔记参考：http://www.sohu.com/a/220745456_390227，http://posts.careerengine.us/p/5a768ab3381fe136215b3de5?from=latest-posts-panel&amp;type=title [1] Moniz, Joel Ruben Antony, and David Krueger. “Nested LSTMs.” Asian Conference on Machine Learning. 2017. [2] Hochreiter, Sepp, and Jürgen Schmidhuber. “Long short-term memory.” Neural computation 9.8 (1997): 1735-1780.","categories":[{"name":"research","slug":"research","permalink":"https://LorrinWWW.github.io/categories/research/"}],"tags":[{"name":"LSTM","slug":"LSTM","permalink":"https://LorrinWWW.github.io/tags/LSTM/"},{"name":"RNN","slug":"RNN","permalink":"https://LorrinWWW.github.io/tags/RNN/"}]},{"title":"A convolution BiLSTM neural network model for chinese event extraction 笔记","slug":"[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction","date":"2018-01-29T07:00:00.000Z","updated":"2018-03-12T20:22:36.988Z","comments":true,"path":"[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction/","link":"","permalink":"https://LorrinWWW.github.io/[2018.1.29]A-convolution BiLSTM-neural-network-model-for-chinese-event-extraction/","excerpt":"","text":"A convolution BiLSTM neural network model for chinese event extraction摘要：中文事件提取是信息抽取中的一项具有挑战性的任务，以前的方法高度依赖于复杂的特征工程和复杂的自然语言处理（NLP）工具。 在文献[1]中，提出了一种结合LSTM和CNN的卷积双向LSTM神经网络来捕获句级和词汇信息。最终的测试中达到相当不错的水平。 1. Introduction在事件提取中，我们需要提取事件类别、参与者和其他属性（时间、地点等）。根据Automatic Content Extraction（ACE）定义的事件抽取任务，我们定义： 触发词：最主要的、用于表达一个事件的词，通常是句子的谓语。 事件属性：实体、短语或数值。在一个事件中扮演特定作用。 因此，我们把事件抽取分为两步，即触发词标注和事件属性标注。例如： S1：Intel在中国成立了研究中心。 其中，“成立”表明该句子表达了一个商业事件；Intel、中国、研究中心则是事件的属性，属性将被标注为参与者、地点、时间等。 目前的 state-of-the-art [2-4] 通常很依赖于特征的选择。这些特征通常可以被划分为语义特征和结构特征。再给两个包含”成立“的例子，但它在其中并不表达一个商业事件。 S2：它成立于1994年，现在是一支深受欢迎的摇滚乐队。 S3：医院已成立救援中心。 从结构特征上来看，S2可以被缩写为“它是乐队”，因此“成立”在这个句子中不是一个触发词，这个句子不是一个事件。 从语义特征上来看，S3中的“救援中心”的语义上看，这个事件不是一个商业行为，因此“成立”不表达一个商业事件。 传统的方法[2, 3]通常依赖于大量的NLP工具，对于语义特征而言，有词性标注、命名实体识别等；对于结构特征而言，有依存关系分析。尽管最终效果很好，但是这需要大量的人工特征，并且需要忍受传递误差。 Chen et al. [5] 提出了一个用于完成事件抽取的卷积神经网络。受此激发，本文提出一个卷积双向LSTM神经网络，用来同时捕获语义特征和结构特征。我们首先使用双向LSTM将整个句子中的单词的语义编码成句子级别的特征。 然后，我们可以利用卷积神经网络来捕获突出的局部词汇特征，以便在没有任何POS标签或NER帮助的情况下进行触发词消歧。 2. Trigger Labeling2.1 Language Specific Issues由于中文的特殊性，触发词可以被分为两类： 多词触发词：任何拆开后就无法被人为是触发词的，我们把它组合起来认为是触发词。例如“犯罪嫌疑人都落入法网”，其中“落入法网”被认为是触发词。 单词触发词：往往是谓语，但也可以是组合词中的一部分。例如“警察击毙了一名歹徒”中的“击毙”，“这是一件预谋的凶杀案”中的“凶杀” 为了解决这个问题，我们将事件检测视为序列标记任务而不是分类任务。 采用BIO方案，其中标记B是事件触发词的开始，I型是在触发词内，否则标记为O。我们利用卷积双向LSTM神经网络来完成这个任务。 我们基于单词模型的主要架构。 （a）中的每个词wt的局部上下文特征ct（灰色矩形）由CNN计算（b）所示。 我们的卷积神经网络学习了关于中心词“落入”的本地上下文信息的表示。 这里的上下文大小是7（中心词的左右各3个词），我们使用一个大小为4的内核与两个特征映射。 （b）句子中的符号P表示填充词。 2.2 Word-Based MethodLSTM Network 在nlp任务中LSTM相对常用，特别的，双向LSTM能够联系历史和未来的信息，能够重复利用句子信息，有利于我们进行判断。因为之前的报告已经叙述过，故这里略写。 CNN 卷积神经网络最一开始用于图像领域，近年也在nlp领域大放光彩。这里，我们采用卷积神经网络来提取句子中每个单词的局部上下文信息。 给定一个包含n个单词{w1, w2, … , wn}的句子和当前中心词wt，卷积运算包含一个内核，将其应用于wt周围的单词以生成特征映射。 我们可以利用不同宽度的多个内核来提取不同粒度的局部特征。 然后在每个map上执行最大汇集，以便仅记录每个特征地图的最大数量。 池的一个特性是它产生一个固定大小的输出向量，这使我们能够应用不同的大小内核。 而通过执行最大操作，我们保持最显着的信息。 最后，将固定长度的输出向量cwt作为关于中心词wt的本地上下文信息的表示。 在我们的实现中，滑动窗口大小为7（中心词的左右各3个词），并且我们使用不同的内核来捕获各种粒度的上下文信息。 Output Layer 我们将BiLSTM的隐藏状态与CNN在每个时间步t提取的上下文特征cwt连接起来。 然后[ht; cwt]被送入softmax层以产生wt的每个标记的对数概率。然而，基于单词的方法仍然不能解决内部词触发引起的一致性问题，即无法识别长词内部的触发词。 2.3 Character-Based Method为了解决一致性问题，我们可以采用Character-embedding，唯一的区别就在input layer。 3. Argument Labeling上面介绍的触发词标注模型依然可以被沿用，我们将介绍用于触发词标注和事件属性标注的模型之间的主要区别。 3.1 Input Layer作为一个pipeline系统，除了word embeddings之外，还可以使用从上面触发词标记任务中提取的信息。 因此，我们提出了另外四种类型的特征embedding来形成BiLSTM和CNN的输入层。 触发位置特征：一个单词是否属于触发词的一部分 触发类型特征：单词触发类型，NONE类型对于非触发词 实体位置特征：一个单词是否属于实体的一部分 实体类型特征：单词的实体类型，NONE类型对于非实体。 ACE数据集提供了实体识别的结果，无需使用外部NLP工具。（思考：若数据集不提供实体信息，两种解决方法：1. 不embed实体特征；2. 借助外部工具）然后，我们通过查表将这些特征转换成矢量，并将它们与原始单词嵌入级联，作为BiLSTM和CNN的最终输入层。 3.2 Output Layer值得一提的是，事件属性标注不再是一个序列标注任务，而是一个分类任务。 ACE数据集提供了实体识别的结果，它保证了事件属性只能出现在这些实体。 因此，我们只需要预测标记实体的角色，而不是整个句子中的每个单词。 例如，S4中有三个触发器（粗体字）和三个实体（斜体字），它们共同组成九对要分类的触发词和事件属性候选。 S7：六起谋杀案发生在法国，包括Bob的暗杀和Joe的杀害。 我们修改CNN和BiLSTM网络的输出层以适应新的任务。 对于BiLSTM，我们仍然试图利用其记忆长序列的能力，所以我们把最后一个单词hN的隐藏状态视为句子信息。 对于CNN，我们把整个句子的所有单词作为上下文，而不是每个中心单词的浅窗口。 最后，我们将来自两个网络的输出向量的串联输入到softmax分类器中，就像处理之前的触发词标注任务一样。 4. Conclusion论文[1]主要提出了卷积双向LSTM神经网络，用以完成中文事件抽取任务，在ACE 2005数据集上获得了不错的结果。我在暑假时，将事件抽取认为为一个序列标注任务，使用BiLSTM+CRF；相比而言，论文[1]的模型考虑更全面，并充分利用已知的实体信息。不过对于现实问题而言，标注实体信息的成本也很高，故在没有实体标注的情况下保持性能也是一个难点。 Bibliography[1] Zeng, Y., Yang, H., Feng, Y., Wang, Z., &amp; Zhao, D. (2016). A convolution BiLSTM neural network model for Chinese event extraction. In Natural Language Understanding and Intelligent Applications (pp. 275-287). Springer, Cham. [2] Chen, C., Ng, V.: Joint modeling for Chinese event extraction with rich linguistic features. In: COLING, pp. 529–544. Citeseer (2012) [3] Chen, Y., Xu, L., Liu, K., Zeng, D., Zhao, J.: Event extraction via dynamic multipooling convolutional neural networks. In: Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, vol. 1, pp. 167–176 (2015) [4] Li, Q., Ji, H., Huang, L.: Joint event extraction via structured prediction with global features. In: ACL (1), pp. 73–82 (2013) [5] Chen, Y., Xu, L., Liu, K., Zeng, D., Zhao, J.: Event extraction via dynamic multipooling convolutional neural networks. In: Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, vol. 1, pp. 167–176 (2015)","categories":[{"name":"research","slug":"research","permalink":"https://LorrinWWW.github.io/categories/research/"}],"tags":[{"name":"convolution","slug":"convolution","permalink":"https://LorrinWWW.github.io/tags/convolution/"},{"name":"BiLSTM","slug":"BiLSTM","permalink":"https://LorrinWWW.github.io/tags/BiLSTM/"},{"name":"event-extraction","slug":"event-extraction","permalink":"https://LorrinWWW.github.io/tags/event-extraction/"}]},{"title":"Event detection and co-reference with minimal supervision 笔记","slug":"[2018.1.21]Event-detection-and-co-referentce","date":"2018-01-21T07:00:00.000Z","updated":"2018-03-12T20:22:21.146Z","comments":true,"path":"[2018.1.21]Event-detection-and-co-referentce/","link":"","permalink":"https://LorrinWWW.github.io/[2018.1.21]Event-detection-and-co-referentce/","excerpt":"","text":"Event detection and co-reference with minimal supervision [1]摘要：该论文使用了一种弱监督的算法解决了事件检测与共指问题。事件共指问题可以看作是一种事件之间的相似度计算问题，而在该文中，事件检测问题也被看作是一种相似度检测问题。对于ACE或rich ERE划分的所有事件类型，使用每个类型中的几个实例作为该类型事件的向量，然后计算新事件向量与每个类型事件向量之间的相似度，根据这一相似度对事件进行判断。该文的另一个特点在于事件特征的选择，在将事件表示为向量的过程中，使用了Freebase作为特征来对事件进行表示。 1. Introduction 上图是论文提出的MSEP（Minimally Supervised Event Pipeline）框架。这里 Event examples 是唯一的监督来源，用于产生 Example vectors。在MSEP框架中不需要训练。 这篇论文主要是针对两个问题： Event detection 指的是对一段文本内容，检测是否存在符合要求的事件。 Co-reference problem. 为了更好的理解和利用事件的信息，我们需要从文本中提取出时间、地点、人物、行为等信息。此外，我们还需要了解两个事件的关系，例如，判断两个事件是否表示同一个事件，这就是Co-reference problem。 在本文中，我们提出了一种更加可行且更加可测的方法来描述事件。对于一个事件e，event detection 所要做的就是判断是否存在一个事件集合，事件e在语义上是否有关联，以至于可以被划分到该集合内；而 co-reference problem 则是判断两个事件e1、e2是否在语义上表述足够接近，以至于我们认为它们所表示的实际上是同一个事件。可以看到两个任务实际上都需要判断相似性，我们可以把它们转化为语义相似性问题。 现在主要问题有：1. 如何表示一个事件；2. 如何表达相似性。前者我们采用了semantic role labeling representation（SRL），来结构化地描述一个事件；对于后者，我们将对事件做一个embedding，通过计算其余弦距离来表达相似性。 我们提出了一个通用事件检测和指代消解框架，它基本上不需要标记数据。在实践中，为了将一个事件提法（event mention）和一个事件本体（event ontology）相联系起来，我们只需要一些事件示例。这种定义类型的方式是非常合理的，因为给出例子是定义事件类型的最简单的方法。我们的方法比标准的无监督方法要求更少假设，在我们的模型中，给定事件类型的定义（以事件例子的形式），我们可以将单个事件分类到已知本体，并确定两个事件是否是 co-reference 的。 2. The MSEP System2.1 Structured Vector Representation事件结构和句子结构之间有一个平行关系。我们发现一般来说，事件的触发词往往是谓语，所以可以针对谓语对其做一些改进： Basic event vector representation。基本事件向量由它的各个组成部分组成。 Augmented event vector representation。在这里，“+” 表示我们首先将文本片段放在一起，然后将组合的文本片段转换成ESA向量。 2.2 Event Mention Detection我们定义 Event type representation 为该类别下的事件向量的平均值。 我们定义定义相似度如下 S(e_1, e_2) = \\frac{vec(e_1) · vec(e_2)}{||vec(e_1)||·||vec(e_2)||} \\\\ = \\frac{\\sum_a{vec(a_1) · vec(a_2)}}{\\sqrt{\\sum_a{||vec(a_1)||^2} · \\sum_a{||vec(a_2)||^2}}}其中 e1 是待处理事件，e2 是事件的类别。a 就是事件里的各个组件。若遇到 a 缺失的情况（如地点、时间等），我们用非缺失的部分的平均值来代替它。具体的操作方法参见原文。 2.3 Event co-reference这里如上一节的内容所说，通过余弦距离$S(e_1, e_2)$来计算两个事件的相似度。 对于每一个事件，我们分别比较$agnet{sub}, agnet{obj}$，若都不相同，我们认为它们是独立的；如果有缺失，我们认为它和任意值匹配。这样，我们可以得到一个不重复的事件集合，$Set_{conflict}$。 接下来遍历所有事件，对于事件k+1， e_p = argmax_{e\\in \\{e_1,...,e_k\\} e \\notin Set_{conflit}} {S(e_p, e_{k+1})}如果$S(ep, e{k+1})$的值大于我们设定的阈值，我们就认为它是同一个事件；否则，我们把他分为一个新的类。 3. Vector Representation我们可以看到，其实文章之前的内容都不依赖于 embedding 的具体选择，事实上，作者也测试了很多的方法，可以根据实际情况来选择。 Explicit Semantic Analysis Brown Cluster Word2Vec Dependency-Based Embedding 4. Semantic Role Labeling上面工作建立在已经完成了 Semantic Role Labeling 的情况下，这里我们在讨论一下如何进行 Semantic Role Labeling。 对于标注任务来说大同小异，现在往往使用神经网络模型来进行标注，例如[2]，缺点是需要大量标注数据。目前业内比较主流的解决方案是RNN-CRF模型，一般来说分为： Embedding layer Bi-directional RNN (usually LSTM) layer Tanh hidden layer CRF layer 在实际应用上，可能还会增加Attention机制等来进一步提高它的效果。 目前已有的系统如哈工大的语言技术平台LTP，能够用于 Semantic Role Labeling 等。 5. Conclusion这一篇文章提出了一种新颖的事件检测和指代消解方法。其最重要的部分就是提出了一种结构化的向量，能够更好地表示event，用以进行事件分类、指代消解等工作。这个方法在一些关键指标上甚至能优于最新的监督方法，并且能够更好地适应新的领域。 Bibliography[1] Peng, H., Song, Y., &amp; Roth, D. (2016). Event Detection and Co-reference with Minimal Supervision. In EMNLP (pp. 392-402). [2] Zhou, J., &amp; Xu, W. (2015). End-to-end learning of semantic role labeling using recurrent neural networks. In ACL (1) (pp. 1127-1137).","categories":[{"name":"research","slug":"research","permalink":"https://LorrinWWW.github.io/categories/research/"}],"tags":[{"name":"event-detection","slug":"event-detection","permalink":"https://LorrinWWW.github.io/tags/event-detection/"},{"name":"co-reference","slug":"co-reference","permalink":"https://LorrinWWW.github.io/tags/co-reference/"}]},{"title":"几个 relation extraction 远程监督模型","slug":"[2018.1.14]Models-for-relation-extraction","date":"2018-01-14T07:00:00.000Z","updated":"2018-03-12T20:21:58.365Z","comments":true,"path":"[2018.1.14]Models-for-relation-extraction/","link":"","permalink":"https://LorrinWWW.github.io/[2018.1.14]Models-for-relation-extraction/","excerpt":"","text":"几个 relation extraction 远程监督模型摘要：远程监督（Distant supervision）显著地减少了建立用于分类任务的训练集所需要的人工。但是这一项技术也会带来很大的噪音，并可能因此而大大地影响了模型的性能表现。这里，我们以 relation extraction 这项任务为例，深入讨论分析该噪声的分布。文献[1]提出了 dynamic-transition matrix，并证明了它能很好地代表了由 distant supervision 所带来的噪声。通过该矩阵，我们能够大大提高 relation extraction 的效果。文献[2]则是一种经典的方法，通过定义规则，定义否定模式（negative pattern）过滤掉一些噪音数据，可以很大程度提高性能。缺点是规则依赖人工定义，但是方法本身简单有效。文献[3]将 relation extraction 定义为一个 Multi-instance Multi-label 学习问题，一定程度上解决了错误标签的问题。 1. Problem of distant supervisionDistant supervision 是一种生成关系抽取训练集的常用方法。它把现有知识库中的三元组 \\ （或写成\\）作为种子，匹配同时含有 e1 和 e2 的文本，得到的文本用作关系 r 的标注数据。这样可以省去大量人工标记的工作。 但是，相比于人工标注方法，这种匹配方式会产生很多噪音：比如三元组\\，可能对齐到“Donald Trump was born in New York”，也可能对齐到“DonaldTrump worked in New York”。其中前一句是我们想要的标注数据，后一句则是噪音数据，它并不表示born-in关系。如何去除这些噪音数据，是一个重要的研究课题。 2. Approaches to this problems 拟合噪音 dynamic-transition matrix [1] 去除噪音 通过定义规则过滤掉一些噪音数据[2]，缺点是依赖人工定义，并且被关系种类所限制。 Multi-instance learning[3], 把训练语句分包学习，包内取平均值，或者用 attention 加权，可以中和掉包内的噪音数据。缺点是受限于 at-least-one-assumption：每个包内至少有一个正确的数据。 下面我们简单介绍这几个模型。 2.1 Learning with dynamic-transition matrix [1]文献[1] 提出了 dynamic-transition matrix，用于表达 Distant supervision 所产生的噪声。dynamic-transition matrix 可以通过基于 curriculum learning 的方法训练得到。通过该矩阵，我们能够大大提高 relation extraction 的效果，能够达到目前该领域的 state-of-the-art。 Transition matrix 是一个转移矩阵，记为T，大小为 n*n，n是关系种类的数目。T 的元素，$T_{ij}$的值是 p( j| i )，即该句子代表关系为 i，但被误判为 j 的概率。 这样我们就可以得到：𝑃𝑟𝑒𝑑𝑖𝑐𝑡𝑒𝑑 𝑑𝑖𝑠𝑡𝑟𝑖𝑏𝑢𝑡𝑖𝑜𝑛 × 𝑇𝑟𝑎𝑠𝑖𝑡𝑖𝑜𝑛 𝑚𝑎𝑡𝑟𝑖𝑥=𝑂𝑏𝑠𝑒𝑟𝑣𝑒𝑑 𝑑𝑖𝑠𝑡𝑟𝑖𝑏𝑢𝑡𝑖𝑜𝑛 其中，predicted 是我们想要的真实分布，observed 是我们观测到的噪音分布，这样就可以用噪音数据进行联合训练了。作者在 timeRE 和 entityRE(NYT) 上均进行了训练，取得了降噪的 state-of-art。具体分析结果可以参照论文。 2.2 Reducing Wrong Labels [2]在关系提取方面，远程监督试图通过使用知识库（如Freebase）作为监督来源，从文本中提取实体之间的关系。 当一个句子和一个知识库引用同一个实体对时，这种方法试图用知识库中的对应关系来启发式地标注句子。 然而，这种启发式可能会导致一些句子被错误地标记。 这种嘈杂的标记数据导致较差的抽取性能。 在本文中，我们提出了一种减少错误标签数量的方法。 我们提出了一个新的生成模型，直接模拟远程监督的启发式标签过程。 该模型通过其隐藏变量来预测分配的标签是正确的还是错误的。在实验中，我们也发现错误的标签减少提高了关系抽取的性能。 NegPat(r)即为事先定义的对于r的否定模式（negative pattern）。在我们的方法中，我们按如下所示去除错误标签：（i）给定一个已标注的语料库，我们首先验证其中的模式是否表达一种relation，然后（ii）使用否定模式列表（NegPat）去除错误的标签， 即该模式被定义为不表示relation的模式。 第一步，我们引入新的生成模型，直接模拟DS的标注过程并进行预测。 第二步在算法1中描述，见上图。对于关系提取，我们使用上述得到的标注数据来训练分类器（给定实体对，该分类器预测所属关系）。 2.3 Multi-instance Multi-label Learning [3]很多的共现 entities 都没有什么关系，仅仅是出现在同一个句子中；而有的 entities 之间的关系其实并不仅仅只有一种，可能有多种，比如奥巴马和美国的关系，可能是 born in，也可能是 is the president of 的关系。 因此训练集会产生大量的错误标记，比如两个实体有多种关系或者根本在这句话中没有任何关系，这样的训练数据会对关系抽取器产生影响。正因为如此，传统的监督式学习，假设每个实例明确地映射到一个标签，是不合适的。 对于这个问题，我们将关系抽取定义为一个 Multi-instance Multi-label 学习问题，它使用带有潜在变量的图模型，对文本中一对实体的所有实例以及它们的所有标签进行联合建模。 该模型在 relation extraction 领域表现出色。 3. Conclusion上面提到的几个模型都有其新颖的地方，其中[1]这种拟合噪音的思想很有创新点，实际的效果也很理想；而后两个模型主要都是在数据预处理阶段进行，因此可以和其他 relation extraction 模型很好的结合。 References*笔记部分参考论文浅尝 | Learning with Noise: Supervised Relation Extraction [1] Luo, Bingfeng, et al. “Learning with noise: enhance distantly supervised relation extraction with dynamic transition matrix.” arXiv preprint arXiv:1705.03995 (2017). [2] Takamatsu, Shingo, Issei Sato, and Hiroshi Nakagawa. “Reducing wrong labels in distant supervision for relation extraction.” Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1. Association for Computational Linguistics, 2012. [3] Surdeanu, Mihai, et al. “Multi-instance multi-label learning for relation extraction.” Proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning. Association for Computational Linguistics, 2012.","categories":[{"name":"research","slug":"research","permalink":"https://LorrinWWW.github.io/categories/research/"}],"tags":[{"name":"relation-extraction","slug":"relation-extraction","permalink":"https://LorrinWWW.github.io/tags/relation-extraction/"},{"name":"distant-supervision","slug":"distant-supervision","permalink":"https://LorrinWWW.github.io/tags/distant-supervision/"}]},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","slug":"[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction","date":"2018-01-04T07:00:00.000Z","updated":"2018-03-12T20:21:27.342Z","comments":true,"path":"[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/","link":"","permalink":"https://LorrinWWW.github.io/[2018.1.4]Overcoming-Limited-Supervision-in-Relation-Extraction/","excerpt":"","text":"这次主要阅读的论文是《Overcoming Limited Supervision in Relation Extraction: A Pattern-enhanced Distributional Representation Approach》[1]。该文主要针对了现有模型对标注数据的依赖，提出一种比较有意思的思路。基于分布的方法（distributional approach）利用两个实体共同出现的统计频率来预测他们的关系，需要大量标注数据，而基于模式的方法（pattern-based approach）一般使用神经网络建模，但这种方法需要更多的标注数据。本文同时建立两个模型，互相为对方提供监督。以分布模型作为判别模型，模式模型作为生成模型。训练过程中不断迭代，从而提升两个模型的性能。 1. Introduction1.1 Weakly Supervised Learning弱监督学习介于监督学习和无监督学习之间，它提供的标注数据带有较大的噪音，或标注的相对粗糙，标注结果可能出错。对于关系抽取而言，就是将一些关系实例作为seed，用它们从大型语料库中去除冗余信息并提取更多的实例。 弱监督学习的基本思路： 用容易获得的标注替代较难获得的标注 选择最需要做精细标注的样例 模型训练和自动标注交替进行 1.2 Co-training strategy以往的工作主要是单个模型，该文采用了co-training策略[2]，将两个模型互相协作，取得了比较好的效果。 co-training策略是一种半监督方法，核心就是利用少量已标记样本，通过两个（或多个）模型去学习，对未标记样本进行标记，挑选置信度最高的样本加入已标记样本阵营。 1.3 REPEL (Relation Extraction with pattern-enhanced Embedding)REPEL是本文提出的一个模型。基于模式的模型学习用于关系抽取文本的模式，基于分布的模型作为分类器，两者互补，互相提供监督。前者相当于一个生成器，基于模式生成候选实例；而后者作为判别器，从中选择最优实例，并将选择结果反馈给前者。训练完成相当于得到了两个关系抽取模型。 2. Problem definition实体识别：使用现成的工具标注。 关系识别：实体对 $(e_h, e_t)$，三元组$(e_h, e_t, r)$ 给定语料库D，关系集合R。给定少量seed实例$ {(eh^{r(k)}, e_t^{r(k)}, r)} {k=1}^{Nr} $，提取尽可能多的$ {(e_h^{r(i)}, e_t^{r(i)}, r)} {i=1}^M $；换言之，对于每个$ r \\in R $，我们要提取尽可能多的$ {(eh^{r(i)}, e_t^{r(i)})} {i=1}^{M_r} $。 3. REPEL Framework模式模型：找到文本中的模式集合 分布模型：学习实体表示，以及打分函数 目标函数： max_{P,D}O = max_{P,D}\\{O_p + O_d + \\lambda O_i\\}上面公式中，P表示模式模型的参数，给定关系的全部模式集合。D表示分布模型的参数，实体表示和打分函数。Op和Od分别表示两个目标函数，Oi表示两个模型交互的目标。 注意这里只考虑关系抽取，实体识别使用现有的工具或模型。 3.1 Pattern Module对于一个指定的关系r，我们的目标是找到K个最可靠的模式，然后进一步使用它们来发现更多的关系实例。 基于模式关系抽取主要分为两种：path-based pattern、meta pattern。对于一句话中的实体对，前者定义为两个实体通过依存信息跳转的最短路径；后者则是两个实体附近的文字序列。利用这两种模式从语料库中寻找匹配的实体对。这样就得到了很多候选模式，每个模式又能分别找到许多匹配的实体对。 对于一个模式$\\pi$，我们通过以下式子计算它的置信度： R(\\pi)=\\frac{|G(\\pi)\\cap S_{pair}|}{|G(\\pi)|}$G(\\pi)$表示被模式$\\pi$所匹配的所有实体对，$S_{pair}$表示seed实体对。可以看到，R实际表示的是，在满足$\\pi$模式的实体对中，seed实体对所占的比例。显然，该比值越高，该模式越符合seed的分布。由此，我们定义： O_p = \\sum_{\\pi \\in P}R(\\pi)下面说明一下整个进行的过程： 给定seed实体对，我们通过模式关系抽取的方法获得一系列候选模式。 计算每个候选模式的R值，取最高的K个 3.2 Distributional Module该模块学习语料中的实体全局分布信息。我们利用给定的关系实例作为打分函数。 对于一个实体e，和一个词w P(w|e) =\\frac{exp(x_e*c_w)}{Z}$x_e$表示需要训练的实体表示向量， $c_w$是预训练的word embedding，Z是归一化项。 O_{text} = \\sum_{w,e}n_{w,e}log(P(w|e))$n_{w,e}$是字与实体之间边的权重，也就是实体和这个字同时出现的统计频率。我们希望分布概率能够拟合经验分布概率。 定义打分函数： L_D(f|r)=1-||x_{e_h} + y_r- x_{e_t} ||^2_2实体向量$(x{e_h} - x{e_t})$和$y_r$（关系r的表示，也是要学习的参数）越接近，$L_D$就越接近1；反之则会非常小。 O_{seed} = \\sum_{f\\in S_{pair}} \\sum_{f'\\in(e'_h,e'_t)} {min\\{1, L_D(f|r) - L_D(f'|r)\\}}$(e’_h,e’_t)$是随机选取的实体对。最小值函数是为了防止两个分数差距太多，因为往往$L_D(f’|r)$会是一个很小的负数。 最后有总目标函数中的Od： O_d = O_{text} + \\eta O_{seed}$\\eta$用于调整两部分的比值。 3.3 Modeling the Module Interaction O_i = E_{f\\in G(P)}[L_D(f|r)]这里E指的是期望。 我们给模式模型生成的实体对也打分。Oi作为目标函数，为了最大化它，模式集合P应该尽可能包含那些可靠有效的模式。也就是说，模式模型生成的实体对应该得到的打分越大越好。这样一来分布模型就能为模式模型提供监督（打分）。并且，对于分布模型来说，最大化该目标函数能够给实体对分配更高的打分（也就是说，要令Oi最大化，G(P)和LD都要合适）。通过这种方式两个模型能够互相提供监督。 4. The Joint Optimization Problem 具体算法如上图原文，为了优化总目标函数，采用协梯度下降算法。 先固定模式模型，将seed实体对$S_{pair}$和模式模型生成的实体对$G(P)$训练分布模型。图中的Eqn.11就是下式： max_D \\{ O_d + \\lambda O_i \\} = max_D \\{ O_d + \\lambda E_{f \\in G(P)}[L_D(f|r)] \\}然后再固定分布模型，对实体对筛选后得到的$S_{pair}$训练模式模型。图中的Eqn.12就是下式： max_P \\{ O_p + \\lambda O_i \\} = max_P \\{ \\sum_{\\pi \\in P}(R(\\pi) + \\lambda E_{f \\in G(\\pi)}[L_D(f|r)]) \\}往复迭代。 5. Conclusion利用两个模型进行互补的思路很新颖，从论文的测试结果上来看，本文提出的模型并不逊色于神经网络，可见两个模型互补的效果是相当不错的。但是这种弱监督学习需要的人工标注数据非常少，降低了对标注数据的依赖性。 Reference*笔记部分参考https://zhuanlan.zhihu.com/p/32364723 [1] Qu, M., Ren, X., Zhang, Y., &amp; Han, J. (2017). Overcoming Limited Supervision in Relation Extraction: A Pattern-enhanced Distributional Representation Approach. arXiv preprint arXiv:1711.03226. [2] Blum, Avrim, and Tom Mitchell. “Combining labeled and unlabeled data with co-training.” Proceedings of the eleventh annual conference on Computational learning theory. ACM, 1998.","categories":[{"name":"research","slug":"research","permalink":"https://LorrinWWW.github.io/categories/research/"}],"tags":[{"name":"relation-extraction","slug":"relation-extraction","permalink":"https://LorrinWWW.github.io/tags/relation-extraction/"},{"name":"limited-supervision","slug":"limited-supervision","permalink":"https://LorrinWWW.github.io/tags/limited-supervision/"},{"name":"weak-supervision","slug":"weak-supervision","permalink":"https://LorrinWWW.github.io/tags/weak-supervision/"}]},{"title":"Relation Classification via Attention Model 笔记","slug":"[2017.12.17]Relation-Classification-via-Attention-Model","date":"2017-12-17T07:00:00.000Z","updated":"2018-03-12T20:14:01.933Z","comments":true,"path":"[2017.12.17]Relation-Classification-via-Attention-Model/","link":"","permalink":"https://LorrinWWW.github.io/[2017.12.17]Relation-Classification-via-Attention-Model/","excerpt":"","text":"Relation Classification via Attention Model这个笔记主要是阅读论文[1]，它的工作重点是在神经网络构成的端到端学习的关系抽取任务中加入Attention机制。作者主要通过自动学习关系句中注意力较高的部分，而引入attention机制，对反映实体关系更加重要的词语给予更大的attention，较好地提高了关系抽取的效果。 1. Attention1.1 概述Attention机制最早是在视觉图像领域被提出来的。在NLP任务上，Bahdanau[2]等人使用类似attention的机制在机器翻译任务上将翻译和对齐同时进行。接着类似的基于attention机制的深度学习模型开始广泛应用到各种NLP任务中。 1.2 Recurrent Models of Visual Attention人们在进行观察图像的时候，其实并不是一次就把整幅图像的每个位置像素都看过，大多是根据需求将注意力集中到图像的特定部分。由此，在传统的RNN上加入了attention机制，每次当前状态，都会根据前一个状态学习得到的要关注的位置和当前输入的图像，去处理注意力部分像素。可以看到应用Attention机制后，任务的复杂度被降低了很多。 1.3 Attention-based RNN in NLP[1]的成果是在机器翻译任务，一般机器翻译工作由一个Encoder和一个Decoder构成，一个典型的Seq2seq任务。Encoder将源句子进行编码，再利用Decoder将编码后的向量解码成目标语言。 我们在求注意力分配概率分布的时候，对于输入句子中任意一个单词都给出个概率，从而得到一个概率分布，再对输入句子所有单词的概率进行加权求和，得到Decoder的注意力分配。如下图。 另一个扩展性更好的论文是[3]，他们的工作告诉了大家attention在RNN中可以如何进行扩展。 1.4 Attention-based CNN in NLP[4]这篇论文研究的是两个CNN网络，分别处理两个句子，最后输入到分类器中处理。但是这样的模型在输入分类器前句对间是没有相互联系的，作者就想通过设计attention机制将不同cnn通道的句对联系起来。于是提出了3中在CNN中使用attention的方法。 ABCNN-1: 在卷积前进行attention，通过attention矩阵计算出相应句对的attention feature map，然后连同原来的feature map一起输入到卷积层。 ABCNN-2: 在池化时进行attention，通过attention对卷积后的表达重新加权，然后再进行池化. ABCNN-3: ABCNN-1 + ABCNN-2 2. Relation Classification 2.1 Classification Objective作者提出了一种距离函数，即正则化向量差的L2范数： \\delta_{\\theta}(S,y) = ||\\frac{w^O}{|w^O|} - W_y^L||_{L^2} \\\\ S:\\text{Sentence}, y:\\text{Output relation}, w^O: \\text{Network output}, W^L:\\text{Relation embedding}基于此，作者定义了目标函数： \\mathcal{L} = [\\delta_\\theta(S,y) + (1-\\delta_\\theta(S, \\hat{y}^-))] + \\beta||\\theta||^2 \\\\ \\hat{y}^- : \\text{A selected incorrect relation label chosen as the one with the highest score among all i.e.} \\\\ \\hat{y}^- = argmax_{y'\\in \\mathcal{Y},y'\\ne y}(\\delta(S, y'))目标中的两个距离分别为网络输出向量与正例和与某负例的距离，该负例是所有错误类别中与该输出最接近的。最后加上一个正则项，通过使该目标函数最小化来训练网络中的各参数，$\\beta$用于控制其比重。 2.2 Input Representation现有句子，以及两个已知的实体e1,e2： S = (w_1,w_2,...,w_n) \\\\ e_1 := w_p, e_2 := w_t . p,t\\in [1,n], p\\ne t为了得到它们的关系，我们把所有词转为词向量；并且根据每个词与实体的相对位置，也转为word position embeddings，每个词与两个实体有两个相对位置，所以得到第i个词的Embedding： w_i^M = [(w_i^d)^T, (w_{i,2}^p)^T,(w_{i,2}^p)^T]^T为了充分得到上下文的信息，再考虑大小为k的滑窗，得到最终的input representation z_i = [(w_{i - (k-1)/2}^M)^T,...,(w_{i + (k-1)/2}^M)^T]^T2.3 Input Attention Mechanism 输入级的attention机制是设计两个关于实体对上下文相关的对角矩阵，该矩阵中各元素反映该词语与给定实体间联系的强弱，如$A_{i,i}^j=f(e_j,w_i)$反映了wi和ej之间的联系强弱，这里作者给的 f 就是内积。我们定义： \\alpha_i^j = \\frac{exp(A_{i,i}^j)}{\\sum_{i'=1}^{n}{exp(A_{i',i}^j)}}对于j=1,2 两个相关因子，作者提出了三种处理方式: 平均 r_i = z_i \\frac{\\alpha_i^1 + \\alpha_i^2}{2} 串联 r_i = [(z_i \\alpha_i^1)^T, (z_i \\alpha_i^2)^T]^T 距离 r_i = z_i \\frac{\\alpha_i^1 - \\alpha_i^2}{2} 最终得到$R = [r_1, r_2,…,r_n]$ 2.4 Convolutional Max-Pooling with Secondary Attention将前面得到的矩阵R送入卷积核大小为dc的卷积层，卷积操作可形式化表示为: R^* = tanh(W_fR+B_f), \\text{where the siaze of Wf is } d^c \\times k(d^w+2d^p)然后构建一个相关性矩阵来捕获卷积层输出R*与实体关系WL之间的联系 G = R^{*T}UW^L, \\\\U :\\text{weighting matrix learnt by the network}再用softmax函数来处理相关性矩阵G，获得attention pooling matrix Ap: A_{i,j}^p = \\frac{exp(G_{i,j})}{\\sum_{i'=1}^n{exp(G_{i',j})}}最后用Ap与卷积层输出R*相乘，也就是加入混合中的attention，然后取出每一维度的最大值，得到网络的输出 w_i^O = max_j(R^*A^p)_{i,j}3. 总结从[1]中提到的结果上看，attention的表现确实是在重要的词上有更好的权重，在Sem-Eval-2010 Task 8数据集上取得了显著的效果提升。对于关系抽取来说无疑是非常大的一个进步。 但是还是有一些不足： 它要求实体已知，因此需要其他工作来完成实体的识别，使得一些信息的丢失以及错误累加。此时并行模型或端到端模型，同时完成实体识别可能效果会更好； 关系是事先定义的集合，因此更多的是对关系的分类，若能启发式地抽取关系可能会有更广的应用空间； 对于一些上下文没有明显帮助的隐式关系或是使用了比喻之类的修辞，较为容易出错。 这次选择读这篇文章也是想更具体地了解Attention机制，同时了解一些关系抽取的方案，它也有一个pytorch版本的实现，可以辅以参考。 Reference*笔记部分参考https://zhuanlan.zhihu.com/p/22867750 [1] Wang, L., Cao, Z., Melo, G. D., &amp; Liu, Z. (2016). Relation Classification via Multi-Level Attention CNNs. Meeting of the Association for Computational Linguistics (pp.1298-1307). [2] Bahdanau, D., Cho, K., &amp; Bengio, Y. (2014). Neural machine translation by jointly learning to align and translate. Computer Science. [3] Luong, M. T., Pham, H., &amp; Manning, C. D. (2015). Effective approaches to attention-based neural machine translation. Computer Science. [4] Yin, W., Schütze, H., Xiang, B., &amp; Zhou, B. (2015). Abcnn: attention-based convolutional neural network for modeling sentence pairs. Computer Science.","categories":[{"name":"research","slug":"research","permalink":"https://LorrinWWW.github.io/categories/research/"}],"tags":[{"name":"relation-extraction","slug":"relation-extraction","permalink":"https://LorrinWWW.github.io/tags/relation-extraction/"},{"name":"relation-classification","slug":"relation-classification","permalink":"https://LorrinWWW.github.io/tags/relation-classification/"},{"name":"attention","slug":"attention","permalink":"https://LorrinWWW.github.io/tags/attention/"}]},{"title":"实体解析 Entity resolution","slug":"[2017.12.10]Entity-resolution","date":"2017-12-10T07:00:00.000Z","updated":"2018-01-27T11:07:30.139Z","comments":true,"path":"[2017.12.10]Entity-resolution/","link":"","permalink":"https://LorrinWWW.github.io/[2017.12.10]Entity-resolution/","excerpt":"","text":"1. Entity resolution1.1 Sequence labeling我们通常在ML中把Named Entity Recognition任务认为是一个Sequence labeling任务，事实上很多nlp任务都可以被转化为sequence labeling。暑假实习的时候也在这方面看了一些文献。目前业内比较主流的解决方案是RNN-CRF模型，一般来说分为： Embedding layer Bi-directional RNN (usually LSTM) layer Tanh hidden layer CRF layer 从结果上来看，该模型对大多数sequence labeling任务有较好的效果，如named entity recognition等。但是对于一些更灵活的标注任务（如暑假实习时，我曾试图将event recognition转化为seq labeling任务），尤其是在训练集不足的情况下，往往效果还是不能令人满意。 1.1.1 应用Attention [1]在 RNN-CRF 模型结构基础上，重点改进了词向量与字符向量的拼接。使用 attention 机制将原始的字符向量和词向量拼接改进为了权重求和，使用两层传统神经网络隐层来学习 attention 的权值，这样就使得模型可以动态地利用词向量和字符向量信息。实验结果表明比原始的拼接方法效果更好。 [2]在原始 BiLSTM-CRF 模型上，加入了音韵特征，并在字符向量上使用 attention 机制来学习关注更有效的字符。 ​ — from paperweekly 1.1.2 使用少量标注数据深度学习方法一般需要大量标注数据，但是在一些领域很难有海量的标注数据。所以在基于神经网络结构方法中如何使用少量标注数据也是一个重点。 Deep Active Learning for Named Entity Recognition[7] ICLR 2018看到的paper。这片文章把active learning应用到了CNN-CNN-LSTM模型，用于处理NER问题，也就是seq labeling问题。它能够仅使用25%的数据，达到state-of-the-art的水平。 这篇paper总结了很多做seq labeling的方法，本身的思路也深入简出。decoder使用了LSTM而不是常用的CRF，发现LSTM比CRF有一些的优势。同时该文也证明了active learning能提高seq labeling的表现。 Semi-supervised sequence tagging with bidirectional language models[4] 该论文使用海量无标注语料库训练了一个双向神经网络语言模型，然后使用这个训练好的语言模型来获取当前要标注词的语言模型向量（LM embedding），然后将该向量作为特征加入到原始的双向 RNN-CRF 模型中。 实验结果表明，在少量标注数据上，加入这个语言模型向量能够大幅度提高 NER 效果，即使在大量的标注训练数据上，加入这个语言模型向量仍能提供原始 RNN-CRF 模型的效果。 1.2 Relation extraction实体的关系的抽取方法可以简单分为两类：一类是pipeline抽取方法。另一类是并行或联合抽取方法。 pipeline方法需要先识别entity，然后采用关系抽取模型得到实体对之间的关系。缺点是实体识别的结果会进一步影响关系抽取的结果，导致误差累积，也降低信息使用率，分开抽取也造成了信息冗余。 [9]提出了一种联合实体检测参数共享的关系抽取模型，模型中有两个双向的LSTM-RNN，一个是基于word sequence（bidirectional sequential LSTM-RNNs），主要用于实体检测；一个基于Tree Structures （bidirectional tree- structured LSTM-RNNs），主要用于关系抽取；后者堆在前者上，前者的输出和隐含层作为后者输入的一部分。下图为整个模型的结构图： 该paper用了参数共享，实体的识别过程和关系的判断过程并没有交互的过程，还无法称其为真正意义上的joint。 [7]提出了一种端到端的基于序列标注的的方法进行关系抽取，它将实体发现任务和关系抽取任务转化为一个标注任务。在 encoder-decoder 框架下，采用主流的 bi-lstm 为 encoder，lstm 为 decoder。对每个词标注上 BIEM+关系类型+实体的序号。目前这种思路有人测试下来发现，总的来说，联合抽取比pipeline的方法好，序列标注联合抽取要比其他联合抽取方法好，然而目前实体关系抽取任务的 F1 值仍然不到 0.5。因此虽然效果还可以，但是就实际使用还有一段距离。 此外，该模型还无法处理一个句子有多个关系三元组，和一个实体在多个关系中出现的一对多的问题。一个改进方向是把最后的softmax改成多分类器以实现多标签，这样就能实现一个实体的多关系抽取。其次，该方法是非开放域的关系抽取，关系词是从预定义的关系集里抽取的。 2. Others这里主要是有相关性不强但挺有意思，或泛用性很强的一些文章。 Ngram2vec[5] 一个词向量生成的方法，基于经典的 word2vec 的思想，在其之上加入了 ngram 的共现信息，取得了更好的结果。代码实现：https://github.com/zhezhaoa/ngram2vec/ AutoML google在五月份发布的模型，主要思想是将reinforcement learning应用在神经网络的构建、参数确定上。我们对网络进行测试，将反馈的结果返回到控制器中，以此来帮助提升下一次循环中的训练设定。生成新的架构、测试、把反馈传送给控制器以吸取经验。以此往复以得到更优的结构。 Introspection:Accelerating Neural Network Training By Learning Weight Evolution[6] 这个本质上是meta learning的问题。他们训练了一个网络，网络的输入是某个时间点之前随机选取的4个旧参数的值，输出就是新的参数。因此可以将训练其他模型时得到的这个网络，用于加速其他模型。他们训练了mnist的两层conv net，用该任务的参数更新历史训练网络。他们最后将pretrained好的这个网络用于更新大网络，结果都能更好。 Reference[1] Rei, M., Crichton, G. K., &amp; Pyysalo, S. (2016). Attending to Characters in Neural Sequence Labeling Models. arXiv preprint arXiv:1611.04361. [2] Mortensen, A. B. D., &amp; Carbonell, C. D. J. G. (2016). Phonologically aware neural model for named entity recognition in low resource transfer settings. [3] Yang, Z., Salakhutdinov, R., &amp; Cohen, W. W. (2017). Transfer learning for sequence tagging with hierarchical recurrent networks. arXiv preprint arXiv:1703.06345. [4] Peters, M. E., Ammar, W., Bhagavatula, C., &amp; Power, R. (2017). Semi-supervised sequence tagging with bidirectional language models. arXiv preprint arXiv:1705.00108. [5] Zhao, Z., Liu, T., Li, S., Li, B., &amp; Du, X. (2017). Ngram2vec: Learning Improved Word Representations from Ngram Co-occurrence Statistics. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (pp. 244-253). [6] Sinha, A., Sarkar, M., Mukherjee, A., &amp; Krishnamurthy, B. (2017). Introspection: Accelerating Neural Network Training By Learning Weight Evolution. arXiv preprint arXiv:1704.04959. [7] Shen, Yanyao, Yun, Hyokun, Lipton, Zachary C, Kronrod, Yakov, &amp; Anandkumar, Animashree. (2017). Deep active learning for named entity recognition. [8] Zheng, S., Wang, F., Bao, H., Hao, Y., Zhou, P., &amp; Xu, B. (2017). Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme. arXiv preprint arXiv:1706.05075. [9] Miwa, M., &amp; Bansal, M. (2016). End-to-end relation extraction using lstms on sequences and tree structures. arXiv preprint arXiv:1601.00770.","categories":[{"name":"research","slug":"research","permalink":"https://LorrinWWW.github.io/categories/research/"}],"tags":[{"name":"entity-resolution","slug":"entity-resolution","permalink":"https://LorrinWWW.github.io/tags/entity-resolution/"},{"name":"sequence-labeling","slug":"sequence-labeling","permalink":"https://LorrinWWW.github.io/tags/sequence-labeling/"},{"name":"relation-extraction","slug":"relation-extraction","permalink":"https://LorrinWWW.github.io/tags/relation-extraction/"},{"name":"LSTM","slug":"LSTM","permalink":"https://LorrinWWW.github.io/tags/LSTM/"},{"name":"RNN","slug":"RNN","permalink":"https://LorrinWWW.github.io/tags/RNN/"}]},{"title":"Note of NLP","slug":"Note-of-NLP","date":"2017-06-26T18:52:45.000Z","updated":"2017-06-28T09:13:58.000Z","comments":true,"path":"Note-of-NLP/","link":"","permalink":"https://LorrinWWW.github.io/Note-of-NLP/","excerpt":"","text":"NLP “One-hot” representation 每一个词都作为一个特征，用一个很大的向量来描述文章。 [0,0,0,0,0,0,0,0,0,1,0,0,0] Main idea of word2vec Two algorithms Skip-grams Continuous bag of words (CBOW) Two training methods Hierarchical softmax Negative sampling 基于深度学习的关系提取 [Zeng et al. 2014] 提出采用卷积神经网络进行关系抽取。他们采用词汇向量和词的位置向量作为卷积神经网络的输入，通过卷积层、池化层和非线性层得到句子表示。通过考虑实体的位置向量和其他相关的词汇特征，句子中的实体信息能够被较好地考虑到关系抽取中。后来，[Santos et al. 2015]还提出了一种新的卷积神经网络进行关系抽取，其中采用了新的损失函数，能够有效地提高不同关系类别之间的区分性。 [Miwa et al. 2016] 提出了一种基于端到端神经网络的关系抽取模型。该模型使用双向 LSTM（Long-Short Term Memory，长短时记忆模型）和树形 LSTM 同时对实体和句子进行建模。目前，基于卷积神经网络的方法在关系抽取的标准数据集 SemEval-2010 Task 8 上取得了最好的效果。 —基于深度学习的关系抽取技术进展刘知远熊德意 RNN在NLP中应用较多，有关它的文章：The Unreasonable Effectiveness of Recurrent Neural Networks 译文递归神经网络不可思议的有效性 其中目前比较流行的是LSTM，有关它的文章：Understanding LSTM Networks 译文 理解LSTM网络 LSTM的tensorflow简单实现 GAN ?ACGAN可用于分类问题，Discriminator输出正伪的同时还会输出类别。适合类别数量已给定的情况。 InfoGAN。 半监督GAN。","categories":[{"name":"programming","slug":"programming","permalink":"https://LorrinWWW.github.io/categories/programming/"}],"tags":[{"name":"deep-learning","slug":"deep-learning","permalink":"https://LorrinWWW.github.io/tags/deep-learning/"},{"name":"machine-learning","slug":"machine-learning","permalink":"https://LorrinWWW.github.io/tags/machine-learning/"},{"name":"nlp","slug":"nlp","permalink":"https://LorrinWWW.github.io/tags/nlp/"}]},{"title":"Generative Adversarial Network","slug":"Generative-Adversarial-Network","date":"2017-06-25T12:41:15.000Z","updated":"2018-03-12T20:26:12.755Z","comments":true,"path":"Generative-Adversarial-Network/","link":"","permalink":"https://LorrinWWW.github.io/Generative-Adversarial-Network/","excerpt":"","text":"Generative Adversarial NetworkGenerater Auto encoder input =&gt; nn encoder =&gt; code =&gt; nn decoder =&gt; output Output compared with input as close as possible [code =&gt; nn decoder =&gt; output] := a generater VAE Auto-encoder Variational Bayes: input =&gt; nn encoder =&gt; { ​ code : [$m_i$], ​ variation : [$\\sigma_i$], ​ error : [$e_i$], } =&gt; {$c_i = exp(\\sigma_i) \\times e_i + m_i$} =&gt; nn decoder =&gt; output The goal is to monimize the expression as followed: \\sum(exp(\\sigma_i) - (1+\\sigma_i) + (m_i)^2) GAN 相当于是由一个生成器和分类器(true or false) 极大似然$P_{data}(x; \\theta) , P_G(x;\\theta)$ Generator G G is a function, input z, output x Given a prior distribution $P_{prior}(z)$, a probability distribution $P_G(x)$ is defined by function G Discriminator D D is a function, input x, output scalar Evaluate the “difference” between $PG(x)$ and $P{data}(x)$ Function V(G, D) G^* = {arg} {min}_G {max}_D V(G,D) 从G*中可以看出，D是在给定G的情况下，尽其所能地提高V，即发现$P_{data}$和$P_G$的最多的差异。而G则是使该值尽量减小。在G和D的博弈中模型逐渐完善。 给定V V = E_{x~P_{data}}[logD(x)]+ E_{x~P_G}[log(1-D(x))] \\\\ = \\int_x P_{data}(x)logD(x)dx +\\int_xP_G(x)log(1-D(X))dx \\\\ = \\int_x[P_{data}(x)logD(x) + P_G(x)log(1-D(x))]dx要让V的大小可以由积分内的式子决定，即 P_{data}(x)logD(x) + P_G(x)log(1-D(x))i.e. find D* maximizing: $f(D) = alog(D)+blog(1-D)$ => D^* = \\frac{a}{a+b} = \\frac{P_{data}(x)}{P_{data}(x)+P_{G}(x)}所以 max_D V(G,D) = V(G, D^*) \\\\ = -2log2 + \\int_x P_{data}(x) log\\frac{P_{data}(x)}{(P_{data}(x)+P_{G}(x))/2}dx \\\\ + \\int_x P_{data}(x) log\\frac{P_{G}(x)}{(P_{data}(x)+P_{G}(x))/2}dx \\\\ = -2log2 + KL(P_{data}(x) ||\\frac{P_{data}(x)+P_{G}(x)}{2}) \\\\ + KL(P_{G}(x) ||\\frac{P_{data}(x)+P_{G}(x)}{2}) \\\\ = -2log2 + 2JSD(P_{data}(X||P_G(x))其中 KL := KL divergence \\\\ JSD(P||Q) = \\frac{1}{2}(KL(P||M) + KL(Q||M)), M= \\frac{P+Q}{2}所以$maxD(G,D)$，当且仅当$P_G = P{data}$ 总结一下算法 Given $G_0$ Find $D_0^*$ maximizing $V(G_0,D)$ $\\theta_G \\leftarrow \\theta_G - \\eta \\partial V(G, D_0^*)/ \\partial \\theta_G $ =&gt; Obtain G1 Find $D_1^*$ maximizing $V(G_1,D)$ … 实际操作我们知道实际上是不能求期望，即作不能作积分的。因此需要一定的近似。 我们需要将其离散化，取m个样本，V可以写为 V = \\frac{1}{m}\\sum logD(x_i) + \\frac{1}{m} \\sum log(1-D(x_i^G)) \\\\ where \\{x_1, ..., x_m\\} from P_{data}(x), \\{x_1^G,...,x_m^G\\} from P_G(x)","categories":[{"name":"programming","slug":"programming","permalink":"https://LorrinWWW.github.io/categories/programming/"}],"tags":[{"name":"GAN","slug":"GAN","permalink":"https://LorrinWWW.github.io/tags/GAN/"},{"name":"deep-learning","slug":"deep-learning","permalink":"https://LorrinWWW.github.io/tags/deep-learning/"}]},{"title":"Note of knowledge graph","slug":"Note-of-knowledge-graph","date":"2017-06-25T09:21:18.000Z","updated":"2017-06-25T05:40:19.000Z","comments":true,"path":"Note-of-knowledge-graph/","link":"","permalink":"https://LorrinWWW.github.io/Note-of-knowledge-graph/","excerpt":"","text":"知识图谱 Knowledge graph知识图谱的概念由Google提出，目前成为一大热点。总体而言，英文知识图谱的文献相对齐全。中英文知识图谱的差别主要体现于对自然语言的处理，对于中文而言，首先准确的分词，其次要应对中文想对自由的语法和语言形式。 下面是一些笔记。 知识图谱的构建： 1. 信息抽取 information extraction我们需要能够自动化地从结构化、半结构化和无结构数据中抽取实体(entity)、关系(relationship)以及实体属性等。 实体抽取 entity extraction 实体提取： liu等人，Knn算法和条件随机场模型 lin等人，字典和最大熵算法 实体分类： 2012，ling等人，借鉴freebase归纳实体分类方法，条件随机场模型进行实体边界识别，自适应感知机算法实现对实体的自动分类，结果优于Stanford NER等主流命名实体识别系统 预定义实体分类并不好，新思路：对于给定实体，采用统计机器学习等方法，从目标数据集中抽取与之俱有相似的上下文特征等实体，从而实现实体的分类和聚类。参考whitelaw的解决方案 ​ 关系抽取 relationship extraction","categories":[{"name":"programming","slug":"programming","permalink":"https://LorrinWWW.github.io/categories/programming/"}],"tags":[{"name":"machine-learning","slug":"machine-learning","permalink":"https://LorrinWWW.github.io/tags/machine-learning/"},{"name":"datamining","slug":"datamining","permalink":"https://LorrinWWW.github.io/tags/datamining/"},{"name":"knowledge-graph","slug":"knowledge-graph","permalink":"https://LorrinWWW.github.io/tags/knowledge-graph/"}]},{"title":"MongoDB, Docker and Python","slug":"MongoDB-Docker-and-Python","date":"2017-06-22T12:53:44.000Z","updated":"2017-06-22T07:07:59.000Z","comments":true,"path":"MongoDB-Docker-and-Python/","link":"","permalink":"https://LorrinWWW.github.io/MongoDB-Docker-and-Python/","excerpt":"","text":"安装需要预先安装 Docker Python pymongo Docker创建容器。若只使用一次，可以加上—rm；若要后台运行，加上-d 27017是mongodb的默认端口 1docker run --name my-mongo -it -p 27017:27017 mongo 创建完成后需要使用时： 1docker start my-mongo Python1234567891011121314from pymongo import MongoClient# connectionclient = MongoClient()client.server_info()db = client.test# loop cursorcursor = db.cars.find()for doc in cursor: print(doc)# or just find onedb.cars.find_one()","categories":[{"name":"programming","slug":"programming","permalink":"https://LorrinWWW.github.io/categories/programming/"}],"tags":[{"name":"python","slug":"python","permalink":"https://LorrinWWW.github.io/tags/python/"},{"name":"mongo","slug":"mongo","permalink":"https://LorrinWWW.github.io/tags/mongo/"},{"name":"mongodb","slug":"mongodb","permalink":"https://LorrinWWW.github.io/tags/mongodb/"},{"name":"docker","slug":"docker","permalink":"https://LorrinWWW.github.io/tags/docker/"}]},{"title":"Hands on Scrapy","slug":"Hands-on-Scrapy","date":"2017-06-21T18:00:27.000Z","updated":"2017-06-22T14:15:11.000Z","comments":true,"path":"Hands-on-Scrapy/","link":"","permalink":"https://LorrinWWW.github.io/Hands-on-Scrapy/","excerpt":"","text":"各个结构新建项目1scrapy startproject &lt;project_name&gt; [project_dir] 文件结构： 12345678910111213├── author.json├── runSpider.py├── scrapy.cfg└── projectname ├── __init__.py ├── items.py ├── middlewares.py ├── pipelines.py ├── settings.py // 设置 └── spiders // 具体的爬虫 ├── __init__.py ├── spider0.py └── spider1.py 爬虫主体示例如下，一个带登录的爬虫。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647from scrapy.spiders import CrawlSpiderfrom scrapy.selector import Selectorfrom scrapy.http import Request, FormRequestfrom tutorial.settings import *class SampleSpider(CrawlSpider): name = 'sampleSpider' allowed_domains = ['sample.com'] start_url = 'https://sample.xxx.xx.x.x.x.xxx' def __init__(self): self.headers = HEADER def start_requests(self): return [Request( \"https://sample.com/signin\", meta = &#123;'cookiejar' : 1&#125;, callback = self.post_login )] def post_login(self, response): return [FormRequest( 'https://www.sample.com/login/', method='POST', meta=&#123;'cookiejar': response.meta['cookiejar']&#125;, formdata = &#123; 'email':'xxxx', 'password':'yyyy', &#125;, callback = self.after_login )] def after_login(self, response): return [Request( self.start_url, meta=&#123;'cookiejar': response.meta['cookiejar']&#125;, callback=self.parse, errback=self.parse_err, )] def parse(self, response): # do something pass def parse_err(self, response): print('eeeerrrrrrooooooorrrrrr!!!!!!') 选择器自带选择器： CSS Selector XPath 一般情况下CSS选择期即可满足要求，语法忘了google即可。 1234query = 'h1.title::text'response.css(query).extract()[0]response.css(query).extract_first()# response.css(query).extract_first().strip() # 一般用strip清理多余的空格等 BeautifulSoup方便，但是慢。不能解析js。 lxml用于解析XML，当然也可用于HTML。 Item内建的一种抽象数据结构类，自行定义所需要的。 使用时可以研究下ItemLoader。 Item PipelineItem被返回是被送往Item Pipeline进行进一步处理，即每次return Item等操作时，都会调用pipeline中的相应函数。一般有以下用途： 验证Item 12345678910111213from scrapy.exceptions import DropItemclass PricePipeline(object): vat_factor = 1.15 def process_item(self, item, spider): if item['price']: if item['price_excludes_vat']: item['price'] = item['price'] * self.vat_factor return item else: raise DropItem(\"Missing price in %s\" % item) 写到JSON文件 1234567891011121314import jsonclass JsonWriterPipeline(object): def open_spider(self, spider): self.file = open('items.jl', 'w') def close_spider(self, spider): self.file.close() def process_item(self, item, spider): line = json.dumps(dict(item)) + \"\\n\" self.file.write(line) return item 写到MongoDB 123456789101112131415161718192021222324252627import pymongoclass MongoPipeline(object): collection_name = 'scrapy_items' def __init__(self, mongo_uri, mongo_db): self.mongo_uri = mongo_uri self.mongo_db = mongo_db @classmethod def from_crawler(cls, crawler): return cls( mongo_uri=crawler.settings.get('MONGO_URI'), mongo_db=crawler.settings.get('MONGO_DATABASE', 'items') ) def open_spider(self, spider): self.client = pymongo.MongoClient(self.mongo_uri) self.db = self.client[self.mongo_db] def close_spider(self, spider): self.client.close() def process_item(self, item, spider): self.db[self.collection_name].insert_one(dict(item)) return item 等。 例子登陆 POST表单模拟登录 首先要找到表单，在打开chrome的控制台，切换到”network”。 手动登录，并停止network录制(不然可能随着页面的跳转，记录都被刷掉了)。 依次查找，主要看headers中的form data，是否有和登录相关的信息 模拟发送该表单(注意Request url) 具体写法见上面爬虫的例子 如果有验证码的情况，简单的可以用pil，复杂的可能就比较困难了，可以考虑使用cookie模拟登录。 利用Cookie模拟登录 同样思路，打开network，并手动登录 在headers中找cookies 将cookies整理成字典形式，作为Request的参数即可 CrawlSpider区别于普通的Spider，CrawSpider更适合于用于爬全网，注意不能覆盖parse函数。 CrawlSpider通过Rule来限定搜索区域，parse会根据follow、callback的情况作出不同反应。 如果需要登录，则使用表单登录后，注意最后一次callback需设置为parse，否则程序将会停止，CrawlSpider失效。","categories":[{"name":"programming","slug":"programming","permalink":"https://LorrinWWW.github.io/categories/programming/"}],"tags":[{"name":"scrapy","slug":"scrapy","permalink":"https://LorrinWWW.github.io/tags/scrapy/"},{"name":"python","slug":"python","permalink":"https://LorrinWWW.github.io/tags/python/"},{"name":"spider","slug":"spider","permalink":"https://LorrinWWW.github.io/tags/spider/"},{"name":"crawl","slug":"crawl","permalink":"https://LorrinWWW.github.io/tags/crawl/"}]},{"title":"OS notes","slug":"OS-notes","date":"2017-04-15T16:59:08.000Z","updated":"2017-04-16T18:40:56.000Z","comments":true,"path":"OS-notes/","link":"","permalink":"https://LorrinWWW.github.io/OS-notes/","excerpt":"","text":"Operating SystemThis is the note or keywords of a course in udacity. Overview Processes and process management Threads and concurrency resource management OS services OS support for distributed services Data cendter and cloud IntroductionOS ElementsAbstractions Process, thread, file, socket, memory pape Mechanisms Create, schedule, open, write, allocate Policies Least - recently used (LRU) etc. System callTwo ways for a user process to execute a priviledged call. User process calls hardware directly, and that will cause a trap, the kernel check if it is legal User process executes system call. user/kernel transitions are not cheap! OS services Scheduler =&gt; CPU Mem manager Block device driver file system … Linux ArchitectureUser interface : Users &lt;= user mode Library interface : Standards utility programs (shell, editor, compilors) &lt;= user mode System call interface : Standard licrary (open, close, read, write, fork) &lt;= user mode Linux operating system &lt;= kernel mode Hardware Processes and Process ManagementA process: state of execution Program counter stack parts and temporary holding area Data, register May require special hardware IO devices Process == state of a program when executing. Process Control Block (PCB)a data structure storing status of a process PCB created when process is created Certain filds are updated when process state changes other fields change too frequently Context switchHot cache, cold cache CPU schedulerOS must preempt schedule dispatch Multi ProcessesP1(web server), P2(Database) Inter - process communication (IPC) : Message - passing IPC Shared memory IPC","categories":[{"name":"programming","slug":"programming","permalink":"https://LorrinWWW.github.io/categories/programming/"},{"name":"unfinished","slug":"programming/unfinished","permalink":"https://LorrinWWW.github.io/categories/programming/unfinished/"}],"tags":[{"name":"OS","slug":"OS","permalink":"https://LorrinWWW.github.io/tags/OS/"}]},{"title":"EDP finite element method","slug":"EDP-finite-element-method","date":"2017-03-31T20:42:08.000Z","updated":"2017-03-31T20:48:32.000Z","comments":true,"path":"EDP-finite-element-method/","link":"","permalink":"https://LorrinWWW.github.io/EDP-finite-element-method/","excerpt":"","text":"","categories":[{"name":"math","slug":"math","permalink":"https://LorrinWWW.github.io/categories/math/"},{"name":"unfinished","slug":"math/unfinished","permalink":"https://LorrinWWW.github.io/categories/math/unfinished/"}],"tags":[{"name":"math","slug":"math","permalink":"https://LorrinWWW.github.io/tags/math/"},{"name":"FEM","slug":"FEM","permalink":"https://LorrinWWW.github.io/tags/FEM/"}]},{"title":"EDP basic models","slug":"EDP-basic-models","date":"2017-03-06T09:04:58.000Z","updated":"2017-03-06T11:11:26.000Z","comments":true,"path":"EDP-basic-models/","link":"","permalink":"https://LorrinWWW.github.io/EDP-basic-models/","excerpt":"","text":"Examples of moedels Problem of Cauchy Nomenclature : problem of Cauchy Equation : system of 2 differential equations of order 2 Conditions : initials (all the data from the same point) i.e. fix time \\begin{equation} \\begin{cases} \\frac{dS}{dt}(t) = F(S,R) , \\\\ \\frac{dR}{dt}(t) = G(S,R) , \\\\ S(0) = S^0 \\\\ R(0) = R^0 \\end{cases} \\end{equation} Problem of Dirichlet Nomenclature : problem of Dirichlet Equation : scalar differential equation of order 2. Conditions : prescribed value at the edge \\begin{equation} \\begin{cases} - \\frac{d}{dx} ( k \\frac{d\\theta}{dx})(x) + \\theta(x) = f(x) 0","categories":[{"name":"math","slug":"math","permalink":"https://LorrinWWW.github.io/categories/math/"}],"tags":[{"name":"EDP","slug":"EDP","permalink":"https://LorrinWWW.github.io/tags/EDP/"},{"name":"math","slug":"math","permalink":"https://LorrinWWW.github.io/tags/math/"}]},{"title":"Sobolev space","slug":"Sobolev-space","date":"2017-02-11T13:37:53.000Z","updated":"2017-02-11T16:48:47.000Z","comments":true,"path":"Sobolev-space/","link":"","permalink":"https://LorrinWWW.github.io/Sobolev-space/","excerpt":"","text":"索伯列夫空间 Sobolev space感性认识摘自wiki： 数学上，一个索博列夫空间是一个由函数组成的赋范向量空间，对于某个给定的p ≥ 1，它对一个函数f和它的直到某个k阶导数加上有限Lp范数的这个条件。 我们通常在C1(函数1阶可导并且1阶导数连续)上研究微分方程的解，但是在偏微分方程上并不是特别方便，人们便开始研究索伯列夫空间。 范数 ||f||_{k,p} = (\\sum_{i=0}^k{||f^{(i)}||_p^p})^{1/p} \\\\ =||f^{(i)}||_p +||f||_p赋予了范数的$W^{k,p}$是一个完备空间。 $H^k$一些记号： H^k = W^{k,2} \\\\ H^1(\\Omega) := \\{ v\\in L^2(\\Omega) : \\nabla v \\in (L^2(\\Omega))^d \\} \\\\ \\\\ (.,.)_{H^1} : (u,v) \\in H^1 \\times H^1 \\mapsto (u,v)_{L^2} +(\\nabla u,\\nabla v)_{L^2} \\\\ \\qquad \\qquad \\qquad = \\int_\\Omega {uv} + \\sum_{i=1}^d{\\int_ \\Omega {\\partial_{x_i}{u} \\partial_{x_i}{v}}}范数和半范数： ||.||_{H^1}: v \\mapsto \\sqrt{||v||_{L^2}^2+||\\nabla v||_{L^2}^2} \\\\ |.|_{H^1(\\Omega)} := ||\\nabla .||_{L^2(\\Omega)} \\\\ ||.||_{H_0^1} := |.|_{H^1}","categories":[{"name":"math","slug":"math","permalink":"https://LorrinWWW.github.io/categories/math/"},{"name":"unfinished","slug":"math/unfinished","permalink":"https://LorrinWWW.github.io/categories/math/unfinished/"}],"tags":[{"name":"EDP","slug":"EDP","permalink":"https://LorrinWWW.github.io/tags/EDP/"},{"name":"math","slug":"math","permalink":"https://LorrinWWW.github.io/tags/math/"},{"name":"analyse","slug":"analyse","permalink":"https://LorrinWWW.github.io/tags/analyse/"},{"name":"Sobolev","slug":"Sobolev","permalink":"https://LorrinWWW.github.io/tags/Sobolev/"}]},{"title":"Hilbert space","slug":"Hilbert-space","date":"2017-02-11T12:10:06.000Z","updated":"2017-02-11T12:57:32.000Z","comments":true,"path":"Hilbert-space/","link":"","permalink":"https://LorrinWWW.github.io/Hilbert-space/","excerpt":"","text":"希尔伯特空间 Hilbert space感性认识摘自wiki: 在数学里，希尔伯特空间即完备的内积空间，也就是说一个带有内积的完备向量空间。 要弄清楚希尔伯特空间，我们需要从几个基本的地方出发。 几个形容词 线性 有线性结构的。 赋范 空间中用于度量“长度”，引入范数。 完备(complet) 其上所有的柯西序列会收敛到此空间里的一点，序列的极限依然在此空间内。通俗的说，我们认为这个空间是不缺点、有皮的。 内积 补充“角度”概念，引入内积。 几类空间 线性空间 度量空间 最基本的空间。前者有线性结构，后者有度量空间，二者没有交集。 赋范线性空间 引入范数 内积空间 内积空间是赋范线性空间。 希尔伯特空间 希尔伯特空间是完备的内积空间 ((线性空间 + 范数 = 赋范空间 + 线性结构) + 内积 = 内积空间) + 完备性 = 希尔伯特空间 可以由以下空间获得 欧几里得空间 定义内积。 序列空间 勒贝格空间 特指L^2空间，定义其内积: ​ (f|g) = \\int{\\overline{f}g}还需证明其完备性. 索伯列夫空间 一般表示为H^s或W^(s,2)。在偏微分方程中常用。 希尔伯特空间的基 所有基的范数为1 彼此正交 其线性扩张稠密：即其中的所有元素的有限的线性组合是H的一个稠密子集。 相关 内积(produit scalaire) 满足以下四个条件： \\forall (x,y) \\in H^2 \\qquad \\varphi(y,x) = \\overline{\\varphi(x,y)} \\\\ \\forall (x,y,z) \\in H^3, \\forall(\\lambda,\\mu) \\in \\mathbb{C}^2 \\qquad \\varphi(z,\\lambda x+\\mu y) = \\lambda\\varphi(z,x) + \\mu \\varphi(z,y) \\\\ \\forall x \\in H^2 \\qquad \\varphi(x,x) \\ge 0 \\\\ \\varphi (x,x) = 0 \\Longrightarrow x = 0","categories":[{"name":"math","slug":"math","permalink":"https://LorrinWWW.github.io/categories/math/"}],"tags":[{"name":"math","slug":"math","permalink":"https://LorrinWWW.github.io/tags/math/"},{"name":"Hilbert","slug":"Hilbert","permalink":"https://LorrinWWW.github.io/tags/Hilbert/"},{"name":"analyse","slug":"analyse","permalink":"https://LorrinWWW.github.io/tags/analyse/"}]},{"title":"EDP基础：矩阵的复习","slug":"EDP-basic-matrix-review","date":"2017-02-07T09:22:27.000Z","updated":"2017-02-07T10:40:09.000Z","comments":true,"path":"EDP-basic-matrix-review/","link":"","permalink":"https://LorrinWWW.github.io/EDP-basic-matrix-review/","excerpt":"","text":"注意点： A est une matrice hermitienne &lt;=&gt; A=A* et donc A est normale A est une matrice unitaire (酉矩阵) &lt;=&gt; A^-1 = A* et donc A est normale A est une matrice orthogonale &lt;=&gt; A=A^T Schur定理（矩陣三角化） A = [a_{ij}]_{n\\times n} , 特征值\\lambda_i , 对应特征向量x_i \\\\ S = [x_1,...,x_n], D = diag[\\lambda_1,...,\\lambda_n] \\\\ 则,A是可对角化矩阵 \\\\ A = SDS^{-1}如果A是n阶的复方阵，则存在n阶酉矩阵U，n阶上三角矩阵T，使得： A = UTU^{-1}酉矩阵 UU^* = I \\\\ i.e. \\to U^*= U^{-1}U是酉矩阵。 Cholesky分解条件： une matrice hermitienne：矩阵中的元素共轭对称（复数域的定义，类比于实数对称矩阵）。Hermitiank意味着对于任意向量x和y，(x*)Ay共轭相等 Positive-definite：正定矩阵A意味着，对于任何向量x，(x^T)Ax总是大于零(复数域是(x*)Ax&gt;0) 则存在L为下三角矩阵，使得 A = LL*。 QR分解目标：A = QR，Q是正交矩阵（意味着QTQ = I）而R是上三角矩阵。","categories":[{"name":"math","slug":"math","permalink":"https://LorrinWWW.github.io/categories/math/"}],"tags":[{"name":"EDP","slug":"EDP","permalink":"https://LorrinWWW.github.io/tags/EDP/"},{"name":"matrix","slug":"matrix","permalink":"https://LorrinWWW.github.io/tags/matrix/"}]},{"title":"Note of statistic","slug":"Note-of-statistic","date":"2017-01-28T17:13:13.000Z","updated":"2017-01-28T18:02:06.000Z","comments":true,"path":"Note-of-statistic/","link":"","permalink":"https://LorrinWWW.github.io/Note-of-statistic/","excerpt":"","text":"各种定理大数定理，中心极限定理略 Slutskyif \\Upsilon_n \\to(loi) \\Upsilon \\text{ et } Z_n \\to (P)cthen \\Upsilon_n + Z_n \\to(L) \\Upsilon+c \\text{ et } \\Upsilon_n Z_n \\to(L) \\Upsilon c假设检验un test de $\\chi^2$列表： Liste a b c pi xx xx xx npi xx xx xx ni xx xx xx 计算： T= \\sum_{j=1}^n{\\frac{(n_i-np_i)^2}{np_i}}其服从卡方分布，自由度为n-1，从而进行检验。","categories":[{"name":"math","slug":"math","permalink":"https://LorrinWWW.github.io/categories/math/"},{"name":"unfinished","slug":"math/unfinished","permalink":"https://LorrinWWW.github.io/categories/math/unfinished/"}],"tags":[{"name":"math","slug":"math","permalink":"https://LorrinWWW.github.io/tags/math/"},{"name":"statistic","slug":"statistic","permalink":"https://LorrinWWW.github.io/tags/statistic/"}]},{"title":"贝叶斯估计 Bayes estimation","slug":"Bayes-estimation","date":"2017-01-07T15:46:31.000Z","updated":"2017-01-07T16:48:18.000Z","comments":true,"path":"Bayes-estimation/","link":"","permalink":"https://LorrinWWW.github.io/Bayes-estimation/","excerpt":"","text":"在机器学习中，贝叶斯这个名字被不止一次的提到。正好又看到了贝叶斯参数估计，简单记录方法，可能没有严谨的数学推导，见谅。 首先我们必须提及的就是历史上的两大学派——经典统计学派和贝叶斯统计学派。 这里我们假设要估计的参数为$\\theta$，简单来说，经典统计学派认为$\\theta$是一个未知但固定的常数，而贝叶斯学派认为$\\theta$是一个变量。 贝叶斯公式我们为什么需要贝叶斯估计呢？ 我们不妨看一个例子，经典统计学派我们使用极大似然估计法。 假如学生在做一道题，当一个学生会做这道题时，他的正确率是98%。当他不会做这道题时，答题的正确率为5%。现在，有一个学生的对这道题测验结果为错误，问这个人会做这道题吗？既然会做并做对的概率为98%，不会做但做对的概率为5%，如果用最大似然估计的方法，我们认为这个人已经感染了病毒。 但是如果如果这道题十分容易呢？比如，题目是1+1=？，事实上，就算是幼儿园的小孩也会做。那么这个估计结果其实是有时偏颇的。 此时我们用贝叶斯方法进行估计，如果我们得知有一个先验概率，比如整体学生中只有1%的人会感染此种病毒，那么由贝叶斯公式： p(y_i|x) = \\frac{p(x|y_i)p(y_i)}{p(x)} p(不会做|做错)=\\frac{p(做错|不会做)p(不会做)}{p(做错)}=\\frac{0.95\\times 0.01}{0.95 \\times 0.01 + 0.02 \\times 0.99} = 0.324这么看来，还是会做的概率比较高，只不过这个学生比较粗心罢了。 利用贝叶斯进行参数估计贝叶斯估计中，需要对参数有一个先验估计，并记录其分布为： \\theta: \\pi(\\theta)已知样本为 \\widetilde{X} = \\{ X_1,...,X_n \\}参数的联合分布为 p(\\widetilde{x},\\theta) = p(\\widetilde{x} |\\theta)\\pi(\\theta)所以 \\pi(\\theta|\\widetilde{x} ) = \\frac{p(\\widetilde{x} ,\\theta)}{p(\\widetilde{x})} \\\\ = \\frac{p(\\widetilde{x} |\\theta)\\pi(\\theta)}{\\int{p(\\widetilde{x} |\\theta)\\pi(\\theta) d\\theta}}特别的，若T是一个充分统计量 p(\\widetilde{x}|\\theta) = p(\\widetilde{x} |T = t)p_T(t|\\theta) \\varpropto p_T(t|\\theta)从而 \\pi(\\theta|\\widetilde{x} ) = \\pi(\\theta|t )这样，参数的分布就已经得到了，接下来一般通过三种方法进行估计： 后验分布的众数进行估计 后验分布的中位数进行估计 后验分布的期望进行估计 总结一下，贝叶斯估计就是，认为被估计参数是一个随机变量，通过已有的数据得到一个先验分布，结合这个先验分布再通过现有的条件，最后得到一个较为合理的后验分布。 注： 如果x的方差趋于无穷，意味着样本没有任何意义，估计结果等于先验估计。 如果样本数量趋于无穷，则估计结果与先验估计无关。这说明先验估计实际上是为了弥补样本的不足。","categories":[{"name":"math","slug":"math","permalink":"https://LorrinWWW.github.io/categories/math/"}],"tags":[{"name":"Bayes","slug":"Bayes","permalink":"https://LorrinWWW.github.io/tags/Bayes/"},{"name":"statistic","slug":"statistic","permalink":"https://LorrinWWW.github.io/tags/statistic/"}]},{"title":"learning OS and building LorriOS","slug":"learning-OS-and-building-LorriOS","date":"2016-12-28T14:00:08.000Z","updated":"2017-04-17T15:12:41.000Z","comments":true,"path":"learning-OS-and-building-LorriOS/","link":"","permalink":"https://LorrinWWW.github.io/learning-OS-and-building-LorriOS/","excerpt":"","text":"暂时用以记录操作系统的学习，以及开发自己的一个toy OS，作为一个项目日志?(如果能成功的话。。)。写这个东西的目的还是给我自己看的（估计其他人也会看的一头雾水吧，笑），毕竟人不太可能一次就找到正确的道路，所以日志写起来可能非常乱糟糟。所以说这只是一个记录，没有什么真正的参考价值。等这个toy os完成的差不多了，我应该会再来总结和记录一下。 另外，下面的DayN并不是真正的天数，毕竟平时也要上课，不可能连续进行。所以大概是按每一次尝试，来进行划分的。 刚开始浏览了一下操作系统的基本知识，按考研复习的来吧，然后看的忽然有点跃跃欲试就想能不能写一个简单的OS。 源代码 Day1:一开始从《30天自制操作系统》开始看，以前有一点汇编基础所以看起来感觉还是可以接受的。 看到第三天就下不去了，倒不是因为内容看不懂，而是环境的问题吧。作者是基于win，用了很多小程序，我用的是os x，很多时候要找一些替代方法。参考网上linux的解决方案，毕竟os x还是不太一样，到下面这一步就是不行，把haribote.bin写到myos.img。 1234567# makefilemkdir -p /tmp/floppymount -o loop myos.img /tmp/floppy -o fat=12 # 这一条在mac不适用sleep 1cp haribote.bin /tmp/floppysleep 1umount /tmp/floppy Os x 上貌似不能直接mount，当然我也尝试了一些其他的办法，最终失败，感觉不太想再花时间在细枝末节上，打算回法国后在ubuntu下开发，或者和书上保持一样，在win下开发。 *后面通过Docker解决 Day2:第二天开始阅读《ORANGE’S：一个操作系统的实现》，并打算先阅读，等了解大致情况再动手，不然也只是抄代码，并且在一知半解的情况下容易卡在一些细枝末节的地方。 同时也可以参考xv6，这个是MIT用于教学写的一个类unix的OS，有配套的课程。 两天下来感觉mac用于web开发十分方便，但是内核开发感觉网上的文档、案例偏少，当然主要也是我比较弱，不能够举一反三。一般来说，要不是面向初学者如《30天》而采用win，要不就是使用linux（果然linux才是王道= =） Day3:目前跟着ORANGE，暂时比较顺利，开发环境仍然是mac。基于freedos下成功进入了保护模式。 下面就保护模式做一点笔记。 关于保护模式网上的资料很多，在为什么需要保护模式这个问题上，这里有一个非常直接的要求——我们的系统要运行在32位上，实模式仅支持16位寻址。当然保护模式远远不止如此，但是对于初学者（比如现在的我，囧），它应该是最大的用途之一。 GDT 首先要先说一下GDT(Global Descriptor Table)，它是全局描述符表。GDT只有一张，位置任意，通过LGDT指令被存储在GDTR寄存器。 Selector 由GDTR访问全局描述符表是通过“段选择子”（实模式下的段寄存器）来完成的。 LDT 局部描述符表LDT(Local Descriptor Table)。和GDT类似，直观上与GDT结构相同（其段选择子TI置位），功能上隶属于GDT。GDT只有一张，LDT有多张，每个任务可以有一张。 如果第一次听到上面几个名词，可能还是比较懵逼。我们看看它是怎么运作的。 进行一系列初始化，包括定义GDT，LDT，选择子等 系统默认进入实模式 加载GDT，进入保护模式 加载不同的LDT，以进入不同的子任务 退出保护模式，反回实模式。 上面只列出了关键步骤，实际上还有很多细节问题，这里只是简单记录其大致思想，所以没有提。 不管怎么说，姑且是跨入了保护模式的大门。 Day4:特权级在IA32分段机制中，特权级有4个特权级别， LEVEL 0: 内核 LEVEL 1, 2: 服务 LEVEL 3: 应用程序 处理器通过识别CPL(current privilege level)、DPL(descriptor privilege level)、RPL(requested privilege level)这三个特权级进行特权级检验。 CPL一般被存储在cs和ss的第0位和第1位。CPL等于所在代码段的特权级。当遇到一致代码段时，CPL不改变。（一致代码段能被小于等于它的特权的代码访问） DPL是段或门的特权级，写在描述符的属性中。 RPL是通过选择子的第0位和第1位表现的。 简单而言，只要CPL和RPL都小于被访问的数据段的DPL就可以了。 门特权级转移可以分为两大类。jmp和call的直接转移；通过描述符间接转移。由于直接转移有诸多的限制，我们常常使用间接转移。 其中，间接转移又分为： 指向一个包含目标代码段选择子的门描述符 指向一个包含目标代码段选择子的TSS 指向一个任务门，它又指向一个包含目标代码段选择子的TSS 门描述符分为四种： 调用门 Call gate 中断门 Interrupt gate 陷阱门 Trap gate 任务门 Task gate Day5:页表实模式下，int 15h 获得内存信息 页表这一块看的有点心急，感觉理解不是很到位。以后等弄的更清楚了再来总结。 中途调试的时候总是出问题，没想到最后发现一开始32位代码的权限就定义错了。从现在出错的经历来看，os如果崩了的话，一般是权限问题（可能是因为我现在写的内容不多吧）。所以务必小心检查权限。 书上的代码有一点不是很理解，在PagingDemo它使用了大量的push却没有pop。据我的理解的话，push是把数据存到栈中，而无法改变栈外的内存，它也没有使用ret之类的与栈相关的指令，目前不是很理解。 不理解细节也没办法，先试着看看后面的内容吧。 中断和异常IDT，中断描述符表。IDT中的描述符可以是： 中断门描述符 陷阱门描述符 任务门描述符 Day6:写着是第六天，实际应该是好久之后了= = 期间已经实现了中断和异常的初始化，并能捕获异常，占个位置以后再详细写。 现在着手实现进程。 进场表、进程体、GDT和TSS的关系： 进程表和GDT。进程表内的LDT选择子对应一个GDT中的描述符，也就是说每一个进程表对应一个GDT中的描述符。 进程表和进程。进程表用于描述进程。 GDT和TSS。GDT中有一个描述符对应TSS。 初始化进场表、进程体、GDT和TSS： 准备任意进程体。 任意函数即可。 初始化进程表。 进程表结构定义在process.h。在global.c中声明一个全局进程表，它是一个进程的数组，包含所有进程。 在新建进程前，需要初始化进程表。 填充GDT中进程LDT的描述符，在protect.c。 准备GDT和TSS。 TSS结构定义在protect.h。 填充GDT中TSS描述符，在protect.c。 TSS准备好了，然后加载tr，在kernel.asm。 从启动时间顺序上，第一个进程的启动过程如下： 准备进程体 =&gt; 初始化GDT中的TSS和LDT两个描述符，初始化TSS =&gt; 准备进程表 =&gt; 完成跳转 ring0 -&gt; ring1 添加一个任务的总结按orange书所说，或参考minix，建立了task_table包含所有进程信息，通过遍历它初始化进程。 添加一个进程体，即一个函数，并声明它 (一般在proto.h中声明) 在task_table中添加一个进程 (global.c) 添加宏：修改进程数量(++)和定义堆栈空间 (process.h) 再次编译即可","categories":[{"name":"programming","slug":"programming","permalink":"https://LorrinWWW.github.io/categories/programming/"},{"name":"unfinished","slug":"programming/unfinished","permalink":"https://LorrinWWW.github.io/categories/programming/unfinished/"}],"tags":[{"name":"OS","slug":"OS","permalink":"https://LorrinWWW.github.io/tags/OS/"},{"name":"kernel","slug":"kernel","permalink":"https://LorrinWWW.github.io/tags/kernel/"}]},{"title":"数据挖掘-分类与预测","slug":"datamining-class-pred","date":"2016-12-26T13:56:57.000Z","updated":"2016-12-26T15:52:42.000Z","comments":true,"path":"datamining-class-pred/","link":"","permalink":"https://LorrinWWW.github.io/datamining-class-pred/","excerpt":"","text":"基本知识主要分为两个步骤： 建立一个描述已知数据集类别或概念的模型。 分类规则形式、决策树形式，或数学公式形式。 利用所获得的模型进行分类操作。 首先要对模型分类准确率进行估计。 若其准确率可以接受，则可以使用该模型进行分类。 可以根据以下几条标准对各种分类方法进行比较: 预测准确率，它描述(学习所获)模型能够正确预测未知对象类别或(类别)数值的能力。 速度，它描述在构造和使用模型时的计算效率。 鲁棒性，它描述在数据带有噪声和有数据遗失情况下，(学习所获)模型仍能进行正确预测的能力。 可扩展性，它描述对处理大量数据并构造相应学习模型所需要的能力。 易理解性，它描述学习所获模型表示的可理解程度。 在本章的后面各节，将要陆续介绍上述有关问题的实现方法。 基于决策树的分类 决策树的生成算法 基本决策树算法 本质是一个贪心算法，自上而下分而治之。 属性选择方法 信息量 I(s_1,…,s_m) = - \\sum_{i=1}^{m}{p_i \\log (p_i)}利用属性A划分当前样本集合所需要的信息可以计算： E(A) = \\sum_{j=1}^{v}{\\frac{s_{1j}+s_{2j}+...+s_{mj}}{s}I(s_1,…,s_m) }E(A)的值越小，说明其子集划分结果越纯，即越好。而对于一个子集Sj，它的信息为 I(s_{1j},…,s_{mj}) = - \\sum_{i=1}^{m}{p_{ij} \\log (p_{ij})}这样利用属性A对当前分支结点进行相应样本集合划分所获得的信息增益是: Gain(A) =I(s_1,…,s_m)- E(A)也就是说Gain(A)被认为是利用属性A进行划分后，所获的的信息熵的减少量。 决策树归纳算法计算每个属性的Gain，选择信息增益最大的属性，作为测试属性并由此产生相应的分支节点。 树枝的修剪 事前修剪 基本原理是设置一个阀值，当某一节点的的样本数少于阀值，则停止细分。难点在于设置一个合理的阀值。 事后修剪 从一个充分生长的树中修建。 可以使用基于代价成本的方法，也可以使用最短描述长度(MDL)。 前者计算其分类错误率，若修剪枝导致分类错误率变高，则保留，否则剪枝；后者选择最简单的，无需独立的数据测试 规则获取 在已经获得了一个决策树的基础上，可以使用”if…else…”语句描述该决策树。 基本决策树方法改进及其扩展性 贝叶斯分类方法贝叶斯分类器是一个统计分类器，基于贝叶斯定理。 简单贝叶斯分类器的分类性能可以与决策树和神经网络相比。 简单贝叶斯分类器假设一个指定类别中各属性的取值是互相独立的，它可以帮助减少计算量。 贝叶斯定理 设X为一个类别未知的数据样本，H为某个假设，则： P(H|X) = \\frac{P(X|H)P(H)}{P(X)} 简单贝叶斯分类方法 步骤说明如下： 每一个数据都是有一个n维特征向量X={x1,…,xn}构成，分别描述其n个属性(A1,…,An)。 若有m个不同的类别(C1,…,Cm)，分类器将未知X归属到类别Ci，当仅当 P(C_i|X) = \\max(P(C_j|X)|1\\le j\\le m)所以我们要找到最大的 P(C_i|X) = \\frac{P(X|C_i)P(C_i)}{P(X)} 要找到它，只需要P(X|Ci)P(Ci)取最大即可。由于各类别的事前概率是未知的，我们假设P(Ci)是互相相等的，这样，第二步中的式子取最大就转化为了寻找最大的P(X|Ci) 由于所含的属性比较多，直接计算P(X|Ci)的计算量还是很大的。由于简单贝叶斯分类器假设各属性独立，则： P(X|C_i) = \\prod{P(x_k|C_i)}可以根据训练数据样本估算P(xk|Ci)的值。具体如下： 若Ak是符号量 P(x_k|Ci)=\\frac{s_{ik}}{s_i}这里sik为训练样本中类别为Ci且属性Ak取vk的样本数。si为训练样本中类别为Ci的样本数。 若Ak是连续量 P(x_k|Ci)=g(x_k,\\mu_{C_i},\\sigma_{C_i}) = \\frac{1}{\\sqrt{2\\pi}\\sigma_{C_i}}e^{-\\frac{(x-\\mu_{C_i})^2}{2\\sigma^2_{C_i}}}其中$g(xk,\\mu{Ci},\\sigma{C_i})$ï为属性为Ak的高斯规范密度函数。 为了预测X的分类，我们通过上述方法估计各个类别的正确率，将X归属到可能性最高的类别。 贝叶斯信念网络 简单贝叶斯假设属性相互独立，从而简化计算，而实际上属性相互依赖的情况比较常见，所以又出现了贝叶斯信念网络，用于描述这种相互关联的概率分布。 贝叶斯信念网络提供了一个图形模型来描述其中的因果关系。信念网络包括两个部分。 第一部分是有向无环图 每一个节点代表随机变量。 每一条弧代表一个概率依赖。 给定父节点，每个变量有条件的独立于图中非子节点。 第二部分是包含所有变量的条件概率表(CPT) 对于一个变量Z，CPT定义了P(Z|parent(Z))，由此可以得到一个表。 例如，LunCancer的父节点是FamilyHistory和Smoker， FH, S FH, ~S ~FH, S ~FH, ~S LungCancer 0.8 0.5 0.7 0.1 ~LungCancer 0.2 0.5 0.3 0.9 数据对象的联合概率通过如下公式计算。 P(z_1,...,z_n) = \\prod{P(z_i|parent(z_i))} 贝叶斯信念网络的学习 学习算法步骤如下： 计算下降梯度 \\frac{\\partial\\ln{P_w(S)}}{\\partial w_{ijk}} = \\sum_{d=1}^{s}{\\frac{P(Y_i=y_{ij}, U_i=u_{ik} |X_d)}{w_{ijk}}}​左边是计算训练集合S中每个样本Xd的概率。设这一概率为p。 由Yi和Ui表示隐含变量，p可通过样本中可观察到的变量以及标准贝叶斯网络推理计算得到。 沿梯度方向前进一小步，权重更新计算如下 w_{ijk} \\leftarrow w_{ijk} + (l)\\frac{\\partial\\ln{P_w(S)}}{\\partial w_{ijk}}l为学习速率代表学习步长。通常学习速率为较小的常数。 重新规格化权重 保证wijk的取值在0～1，其和等于1. 神经网络分类方法 多层反馈神经网络 输入同时赋给第一层(输入层)单元，这些单元输出赋予权重，输出给第二层(隐藏层)。 隐藏层的带权输出又作为输入再馈给另一隐层。 最后的隐层结点带权输出馈给输出层单元，最终给出相应样本的预测输出。 该网络是前馈的，即每一个反馈只能发送到前面的输出层或隐含层。 它也是全连接的，即每一个层中单元均与前面一层的各单元相连接。 只要中间隐层足够多的话，多层前馈网络中的线性阈值函数，可以充分逼近任何函数。 神经网络结构 确定结构，即： 输入层的单元数 隐含层的个数(和层数) 每个隐含层的单元数目 输出层单元数目 对输入值规格化，一般取值在0～1. 离散数据可以为每一个取值增加一个节点进行编码。例如A={a0,a1,a2}则我们设立三个输入单元I0, I1, I2，每个单元默认为0，若A=a0，则I1置为1. 此外没有什么特定的规律或规则来指导隐含层的最佳单元数量，它的确定是一个不断试错的过程。 后传方法 后传方法不断地处理一个训练样本集。将处理结果与训练样本已知类别比较所获误差，修改权重，使网络输出与实际类别之间的均方差最小。权重的修改是后传的，即从前向后的。 其收敛性不能保证。 流程(伪代码)： 12345678910111213141516171819202122# 定义sum函数，以变量x对f(x)求和def sum(f(x), x): pass# 初始化init();while !conditions: # 条件不满足时 for X in samples: for each layer: # 每个隐含层和输出层 O[j] = 1 / (1 + exp( - I[j])) I[j] = sum(w[i][j]O[i], i) + theta[j] for each unit of output layer as j: # 每个输出层单元j，计算向后传播误差 Err[j] = O[j] * (1 - O[j]) * (T[j] - O[j]) for each unit of hidden layer as j: # 每个隐含层单元j，从最后一层到第一层隐含层 Err[j] = O[j] * (1 - O[j]) * sum(Err[k] * w[i][j][k], k) for each w[i][j] in the network: # network中的权重wij delta_w[i][j] = (l) * Err[j] * O[i] # (l)是学习速率，取值在0～1 w[i][j] += delta_w[i][j] for each theta[j] in the network: # network中的偏差thetaij delta_theta[j] = (l) * Err[j] v[i] = theta[j] + delta_theta[j] 看伪代码基本就能明白它的过程了。虽然伪代码可能写的比较迷。。。 基于关联的分类方法略，后面会详细说。 其他分类其他分类我也没有细看，纪录一下名字，以后有时间再看。 k-最邻近方法 这个很简单，就是比较两个点的欧式距离。比较基本的分类方法，略。 基于示例推理 遗传算法 粗糙集方法 模糊集合方法 预测方法 线形与多变量回归 线性代数、概率统计、数值计算方法里都学过。核心是利用最小二乘法。 非线性回归 例如多项式回归 Y = \\alpha + \\beta_1 X+\\beta_2 X^2+\\beta_3 X^3为了将其转化为线性形式，令： X_1 = X;X_2 = X^2 ; X_3 = X^3接下来就用最小二乘法即可。 其他回归模型 对数回归、泊松回归等 分类器准确性分层k次交叉验证方法普遍应用于对分类器预测准确性的评估方面。而bagging方法和boosting方法则通过学习和组合多个(单)分类器来帮助提高整个(数据训练样本所获)分类器的预测准确性。","categories":[{"name":"programming","slug":"programming","permalink":"https://LorrinWWW.github.io/categories/programming/"}],"tags":[{"name":"programming","slug":"programming","permalink":"https://LorrinWWW.github.io/tags/programming/"},{"name":"datamining","slug":"datamining","permalink":"https://LorrinWWW.github.io/tags/datamining/"},{"name":"classification","slug":"classification","permalink":"https://LorrinWWW.github.io/tags/classification/"},{"name":"prediction","slug":"prediction","permalink":"https://LorrinWWW.github.io/tags/prediction/"}]},{"title":"ML CNN","slug":"ML-CNN","date":"2016-12-14T09:34:43.000Z","updated":"2016-12-15T15:36:49.000Z","comments":true,"path":"ML-CNN/","link":"","permalink":"https://LorrinWWW.github.io/ML-CNN/","excerpt":"","text":"笔记向 目前来说，深度学习中比较火的两大类是卷积神经网络(CNN)和递归神经网络(RNN)。 全连接层全连接层一般由两个部分— 线性部分和非线性部分组成。 线性部分输入： x = [x_0,x_1,...,x_n]^T输出： z = [z_0,z_1,...,z_m]^T参数为一个矩阵 W_{m \\times n}偏执项： b = [b_0,b_1,...,b_m]^T于是： W * x + b = z非线性部分根据我们线性代数的知识，如果一系列变换都是线形变换，那么我们可以使用一次线形变换来代替之前的所有变换。也就是说，若没有非线性部分，那么多元神经元在这里也就没有意义了(因为只需要一层神经元就可以代替其他所有的)。 当然，非线性部分也不是为了存在而存在的，用途我们会在后面有更深的理解。 非线性部分的模型也不是唯一的，但是我们常常使用以下几种。 sigmoid 记得就在前几天，在数理统计的书上也看到了这个函数— sigmoid。 先写出它的形式： f(x) = \\frac{1}{1+e^{-x}}这个函数把一个取值在R上的变量，变成了一个取值在(0, 1)上的变量。 双曲正切 f(x) = \\frac{e^x - e^{-x}}{e^x+e^{-x}}它的取值范围是(-1,1)。可以看出，它是有正有负的，而sigmoid是全为正的。","categories":[{"name":"programming","slug":"programming","permalink":"https://LorrinWWW.github.io/categories/programming/"},{"name":"unfinished","slug":"programming/unfinished","permalink":"https://LorrinWWW.github.io/categories/programming/unfinished/"}],"tags":[{"name":"algo","slug":"algo","permalink":"https://LorrinWWW.github.io/tags/algo/"},{"name":"programming","slug":"programming","permalink":"https://LorrinWWW.github.io/tags/programming/"},{"name":"machine-learning","slug":"machine-learning","permalink":"https://LorrinWWW.github.io/tags/machine-learning/"},{"name":"CNN","slug":"CNN","permalink":"https://LorrinWWW.github.io/tags/CNN/"}]},{"title":"四叉树 QuadTree","slug":"QuadTree","date":"2016-12-13T17:45:37.000Z","updated":"2016-12-13T21:20:12.000Z","comments":true,"path":"QuadTree/","link":"","permalink":"https://LorrinWWW.github.io/QuadTree/","excerpt":"","text":"在ECP的实验课用到了四叉树，故来记录一下。 四叉树是一种数据结构，每一个节点有四个孩子。一般需要用到四叉树的情况往往是二位平面，通过把区域分成四个区块来定义。 在确定并显示一条曲线的具体位置时，需要使用一种算法。一种比较直观的想法就是，将平面细分成小块，通过计算验证曲线是否在这个小块内部。但是这种算法也有一个十分致命的缺点— 复杂度极高，计算量极大。而事实上，平面上大多数的区域是空白的，我们不需要对其每一个小块进行检测。仿照二分法的思路我们可以按如下方法进行。 123456789101112131415161718# 确保使用的是等宽字体，用嵌入代码=_=\"\"\"3-------2 | || || |0-------1检查曲线是否在内，若是，此节点为枝，分为四个小区域，作为四个儿子；若否，此节点为叶，没有儿子。=&gt;3---7---2| | |8---4---6| | |0---5---1以此分别对四个儿子执行以上操作，直到达到所需精度。=&gt;...\"\"\" 按照这个数据结构，若曲线不在内部，则将节点设为叶，无需继续细分。寻找一个点的话复杂度是O(logn)。 在实验课的内容里，用python进行数据处理，用paraview进行数据可视化。代码其实挺简单（算法就不难= =），实验课还没结束暂时不附代码了，附一个效果图。","categories":[{"name":"programming","slug":"programming","permalink":"https://LorrinWWW.github.io/categories/programming/"}],"tags":[{"name":"algo","slug":"algo","permalink":"https://LorrinWWW.github.io/tags/algo/"},{"name":"data-structure","slug":"data-structure","permalink":"https://LorrinWWW.github.io/tags/data-structure/"}]},{"title":"machine learning","slug":"machine-learning","date":"2016-12-12T21:04:47.000Z","updated":"2016-12-12T21:07:40.000Z","comments":true,"path":"machine-learning/","link":"","permalink":"https://LorrinWWW.github.io/machine-learning/","excerpt":"","text":"笔记这里先占个位，等有空来填。 参考轻松看懂机器学习十大常用算法","categories":[{"name":"programming","slug":"programming","permalink":"https://LorrinWWW.github.io/categories/programming/"},{"name":"unfinished","slug":"programming/unfinished","permalink":"https://LorrinWWW.github.io/categories/programming/unfinished/"}],"tags":[{"name":"machine-learning","slug":"machine-learning","permalink":"https://LorrinWWW.github.io/tags/machine-learning/"}]},{"title":"企业管理复习笔记","slug":"management-of-the-firm","date":"2016-12-10T16:48:37.000Z","updated":"2016-12-13T21:26:46.000Z","comments":true,"path":"management-of-the-firm/","link":"","permalink":"https://LorrinWWW.github.io/management-of-the-firm/","excerpt":"","text":"This is the note of learning “Management of the firm”, with the book of GESTION (ecole centrale paris). The firm and the managementthe firm the definition of the firm The company is an economic entity or a place of creation of value It designs and / or produces and / or distributes goods and services to meet the demand of CUSTOMERS on MARKETS. It uses (destroys) resources (mobilized from partners) it generates positive or negative externalities on its environment the different types of resources used by the company The work, provided by employees who sell their working time Financial capital, contributed on a perpetual basis by shareholders or temporary by banks, capital is used to acquire or develop: Tangible resources (实体资源) Intangible resources such as cognitive resources (knowledge or knowledge, patents or technologies) or brands the natural resources (materials and energy) transformed by the company in its process or incorporated in its equipment the main functional areas (or functions) of a company and their objective (point not covered in the course but seen in case study!) Design or R&amp;D Manufacturing or production Marketing / sales Finance the importance of the company’s relations with its partners in the framework of contracts and with its stakeholders more generally. The customers to whom its offers are addressed, the people who carry out its activities and the shareholders who bring the capital and hold the company’s share capital With the sectoral communities (suppliers, distributors …), economic (financiers, prescribers, …) and social (legislation, populations); The management the definitions of management Manages a planning / organization function and a control function (animation and evaluation) of the activity. POCCC: prevoir, organiser, commander, coordonner, controle the difference between the operational mode and the strategic management mode manage operationally : to ensure that one does well what one has to do (doing the things right), or it means making good use of resources to reach the objective, or it is to seek efficiency. manage strategically: to make sure that you do the right things (doing the right thing) That is to say, to choose the assets and the areas where to invest is to build the potential of the company and ensure that it has the relevant resources, this is reflected in the company’s balance sheet (stock). MarketingThings important: PESTEL: ​Policy, especially government stability and regulations. Economic, in particular state of the economic situation and economic situation Sociocultural, particularly demography and changing lifestyles. Technological, in particular public and private R&amp;D expenditure. Ecological, in particular legislation on the protection of the environment Legal, in particular the law of competition and the law of labor. SWOT model: Strengths - Internal Weaknesses - Internal Opportunities - External Threats - External Marketing mix (营销组合) 4P =&gt; 4C: Product =&gt; Consumer wants and needs Price =&gt; Cost Promotion =&gt; communication Place (distribution) =&gt; Convenience the definition of marketing Aggregate of methods to adapt its offer to changing demand Assess and anticipate relevant changes Understand customers’ needs and desires Act on supply and its perception But also to orient the behavior of various publics (consumers, distributors, public authorities) in a way favorable to the company. The difference between marketing and sales The marketing aims to facilitate and accompany the act of sale, The marketing Collects / Synthesizes Customer and Market Information The marketing helps to define offers (products / services) adapted to the customers The sales representative is in charge of the act of sale and the relationship with customers Marketing provides elements for sales support. ​The distinction between strategic and operational marketing Strategic marketing is devoted to the conception of the offer: It covers the choice of targets, the analysis of needs, the evaluation of competing offers, the generation and the collection of ideas for solutions, the drafting of specifications (Marketing briefs), estimation of forecast volumes and margins, and the launch plan. Operational marketing refers to the activity of preparation and support to the sales effort, once the offer is constituted. Efforts then was given to the choice of distribution channels, on communication, on the construction of sales pitches and support documents, on the definition of price levels, on the accompaniment and monitoring of sales forces. Market segmentation The process of dividing markets comprising the heterogeneous needs of many consumers into segments comprising the homogeneous needs of smaller groups Strategythings important: Tools and Analysis Methods Level Internal Analysis External Analysis Corporate strategy Management system analysisBCG MatrixResources and competence analysis PESTEL Business strategy Value chainGeneral business strategyResources and competence of each domain of activity Porter’s 5 forces modelStrategic group mappingProduct life cycleKey success factors Porter’s 5 forces mode Suppliers -&gt; induxtry competitors Potential entrants -&gt; Buyers -&gt; Substitutes -&gt; Strategic group map Something like that below: ^(high) | O | O |(low)———————————————————————&gt;(high) Product life circle (PLC) market development growth maturity decline Key success factors The factors we must have to compete in a market. The rule of the game common to all players The necessary conditions to compete in a market. Segnentation DAS (战略分析方法) Stracgical activities areas Value chain analysis Support activities Firm infrastructure HR Technology Development Procurement Primary activities: Inbound logistics Outbound logistics Operations Marketing Service Design …… …… …… …… …… …… BCG matrix market growth ^(high) | Star ? | | Cow Dog(狗带) | |(low)(high)———————————————————————&gt;(low) market share Definition of strategy A firm’s theory about how to excel in the game it is playing A firm’s theory about how to create a unique position in the markets and industries within which it is operating Competitive advantage: doing different things Resource-based view (internal analysis) Human Physical Financial Organizational Development of the firmGrowth orientations: Integration — 一体化 Diversification — 多样化 International strategies — 国际战略 Modes of growth: Internal = organic growth Based on own funds Slower External Rapid market share, or competency gain Accelerator to grow internationally Organizational structure Simple structure Owner/Director -&gt; Employees Taylorism Fayol Complex structure Functional Ex: Finance, R&amp;D, Communication, IT Advantages: – Specialization – Accumulation of experience Disadvantages: – Coordination and collaboration Divisional 每个产品都有自己独立的研发销售部门。 Advantages – Coordination between functions – Responsibility of results better defined Disadvantages – Problem of reinventing the wheel – Internal competition Staff and line 专门分出一个“staff”来协调各部门。 Advantages: – Specialized expertise Disadvantages: – Conflict between staff and line Matrix 产品部门和各专业职能垂直构建。 Advantages – Specialization and coordination are facilitated Disadvantages – Each employee has two bosses – Decision making","categories":[{"name":"other","slug":"other","permalink":"https://LorrinWWW.github.io/categories/other/"}],"tags":[{"name":"management","slug":"management","permalink":"https://LorrinWWW.github.io/tags/management/"},{"name":"firm","slug":"firm","permalink":"https://LorrinWWW.github.io/tags/firm/"}]},{"title":"数据挖掘-定性归纳","slug":"datamining-qualitative-induction","date":"2016-12-08T20:31:50.000Z","updated":"2016-12-23T06:55:11.000Z","comments":true,"path":"datamining-qualitative-induction/","link":"","permalink":"https://LorrinWWW.github.io/datamining-qualitative-induction/","excerpt":"","text":"数据泛化和概念对比 数据立方方法 利用数据立方方法进行数据泛化，被分析的数据存放在一个多维数据库(数据立方)中。 数据立方的维是通过一系列能够形成层次的属性或网格，例如:日期可以包含属性天、周、月、季和年，这些属性构成了维的网格。一个数据立方中存放着预先对部分或所有维(属性)的合计计算结果。 roll up: 数据泛化 drill down: 数据细化 基于属性的归纳方法 它是一种在线数据分析技术方法。 基于属性归纳的基本思想就是，首先利用关系数据库查询来收集与任务相关的数据，并通过对任务相关数据集中各属性不同值个数的检查，完成数据泛化操作。数据泛化操作是通过属性消减或属性泛化(又称为概念层次提升)操作来完成的。通过合并(泛化后)相同行并累计它们相应的个数。这就自然减少了泛化后的数据集大小。所获(泛化后)结果以图表和规则等多种不同形式提供给用户。 涉及的操作有两种： 属性消减 例如，有四个属性：街道、城市、省份，国家，那么街道的更高层次是由后面三个属性来表示的，此时取消街道属性相当于泛化操作。 属性泛化 它是基于以下规则进行:若一个属性(在初始数据集中) 有许多不同数值，且该属性存在一组泛化操作，则可以选择一个泛化操作对该属性进行处理。 这里我们注意到泛化的过程不足，则依然不方便分析；若泛化太过，则会使其失去本身所含有的意义。通常我们通过以下两种方法控制泛化程度。 属性泛化阀值控制 将属性泛化至设定的阀值。 泛化关系阀值控制 用户都应能调整阀值。 属性关联分析 从概念上讲，属性关联分析的过程如下 数据收集 利用保守AOI方法进行属性相关分析 一般来说，这里主要是消除数据集中取不同值个数过多的属性或对可泛化属性进行泛化。一般而言，保险起见，这里的阀值一般比较大。 利用所确定评估标准评估每个初选后的属性。 如信息增益方法。 消除无关或弱相关的属性 利用AOI方法生成概念描述 采用更严格的属性泛化控制阈值来进行 基于属性的归纳操作。 概念对比 概念对比方法和实现 数据收集 属性相关分析 应用分析概念对比方法，以便保留相关程度最高的若干属性(维)供稍后分析处理。 同步泛化 卷上卷下 挖掘结果表示 t_weight - 同一行所占比例 d_weight - 同一列所占比例 挖掘数据库 计算中心趋势 算数平均 加权算术平均 中间值 计算数据分布 最常用的数据分布度量参数就是五值摘要(四分值)、值间范围和标准偏差。 四分值的三个分度为Q1,Q2,Q3。 中间数为M。 最小数据Minimum，最大数据Maximum。 分值间范围：IQR = Q3 - Q1 五值摘要：Minimum, Q3, M, Q1, Maximum 数据变化程度用方差来表示，自由度为n-1.","categories":[{"name":"programming","slug":"programming","permalink":"https://LorrinWWW.github.io/categories/programming/"}],"tags":[{"name":"programming","slug":"programming","permalink":"https://LorrinWWW.github.io/tags/programming/"},{"name":"datamining","slug":"datamining","permalink":"https://LorrinWWW.github.io/tags/datamining/"},{"name":"qualitative-induction","slug":"qualitative-induction","permalink":"https://LorrinWWW.github.io/tags/qualitative-induction/"}]},{"title":"数据挖掘-预处理","slug":"datamining-pretreatment","date":"2016-12-06T21:05:57.000Z","updated":"2016-12-08T20:29:26.000Z","comments":true,"path":"datamining-pretreatment/","link":"","permalink":"https://LorrinWWW.github.io/datamining-pretreatment/","excerpt":"","text":"由于数据的快速膨胀，我们获得的数据往往带有大量的噪声，所以我们需要对其进行一定的预处理。 主要包括数据清洗、数据集成、 数据转换和数据消减。 数据清洗 填补遗漏的数据值、平滑有噪声数据、识别或除去异常值，以及解决不一致问题。 数据集成 将来自多个数据源的数据合并到一起。 数据转换 主要是对数据进行规格化操作。 数据消减 缩小所挖掘数据的规模，但却不会影响(或基本不影响)最终的挖掘结果。 数据聚合 消减维数 数据压缩 数据块消减 数据清洗 处理空数据 忽略该数据 手工填补 利用缺省填补 利用均值填补 利用同类别均值 利用最可能的值 回归分析 贝叶斯计算公式 ​最后一种是最常用的。 噪声处理 Bin方法 — 排序，分组，平滑处理（分组取均值、按边界等） 聚类方法 人机结合检查方法 回归方法 — 借助回归曲线 数据的集成与转换 数据的集成处理 几个问题： 模式集成，例如：”custom_id”, “cum_num” 是不是同一个模式？ 冗余问题，若一个属性能从其他属性推算出来，那么它是冗余的。 我们可以通过相关系数的推算来确定： r_{A,B} = \\frac{\\sum{A-\\bar{A}}}{(n-1)\\sigma_A \\sigma_B} 数据值冲突检测与消除，比如单位不同，语意偏差。 数据的转化处理 平滑处理，bin、聚类、回归 合计处理，对数据进行总结合计操作。 数据泛化处理，例如：年龄映射到老年、中老年、青年等。 规格化，例如：将成绩(可能总分是10分、20分或100分)折算成4分制绩点。 最大最小规格化方法 — 一种线性规格化，绩点的处理属于这种 零均值规格化方法 v' = \\frac{v - \\bar{v}}{\\sigma} 十基数变换规格化方法 v' = \\frac{v}{10^j} 属性构造，根据已有的属性构造新的属性，例如根据高宽生成面积。 数据消减数据消减技术正是用于帮助从原有庞大数据集中获得一个精简的数据集合，并使这一精简数据集保持原有数据集的完整性，这样在精简数据集上进行数据挖掘显然效率 更高，并且挖掘出来的结果与使用原有数据集所获得结果基本相同。 数据立方合计 以三个轴，生成一个数据立方。 每一层次的数据立方都是对其低一层数据的进一步抽象，因此它是一种有效的数据消减。 维数消减 数据压缩 小波分析 主要素分析 数据块消减 回归与线性对数模型 直方图 聚类 采样 离散化与概念层次生成 bin方法 直方图 聚类 基于熵的离散化 自然划分分段法 自动生成概念层次树对于数值属性，可以利用划分规则、直方图分析和聚类分析方法对数据进行分段并构造相应的概念层次树；而对于类别属性，则可以利用概念层次树所涉及属性的不同值个数，构造相应的概念层次树。","categories":[{"name":"programming","slug":"programming","permalink":"https://LorrinWWW.github.io/categories/programming/"}],"tags":[{"name":"programming","slug":"programming","permalink":"https://LorrinWWW.github.io/tags/programming/"},{"name":"datamining","slug":"datamining","permalink":"https://LorrinWWW.github.io/tags/datamining/"}]},{"title":"数据挖掘笔记 Note of datamining","slug":"Note-of-datamining","date":"2016-12-06T14:35:24.000Z","updated":"2016-12-06T21:04:53.000Z","comments":true,"path":"Note-of-datamining/","link":"","permalink":"https://LorrinWWW.github.io/Note-of-datamining/","excerpt":"","text":"构成一般说的数据挖掘指的是知识挖掘，构成如下： 数据清洗 - data clearing 数据集成 - data integration 数据转换 - data transformation 数据挖掘 - data mining 模式评估 - pattern evaluation 知识表示 - knowledge presentation 分析 关联分析 age(X, “20-29”) ^ income(X, “20K-30K”) =&gt; buys(X, “MP3”) [support = 2%, confidence = 60%] 分类预测 分类挖掘的主要表示手段有： 分类规则 决策树 数学公式 神经网络 聚类分析 与分类预测的主要区别是，后者是分类已知，属于监督学习方法；前者无事先确定类别归属，属于无监督学习方法。 聚类分析中，首先需要根据“各聚集内部数据对象间的相似度最大化；而各聚集对象间相似度最小化”的基本聚类分析原则，以及度量数据对象之间相似度的计算公式，将聚类分析的数据对象划分为若干组。 异类分析 不符合大多数数据对象所构成的规律(模型)的数据对象就被称为异类。 我们往往将异类作为噪声或者意外而将其排除，但是某些情况下这些异类反而更有价值，对异类数据的分析处理通常就称为异类挖掘。 例如，通过对银行账户的异类分析，发现信用卡诈骗等。 演化分析 数据演化分析就是对随时间变化的数据对象的变化规律和趋势进行建模描述。 例如股票市场。 数据挖掘结果评估一个数据挖掘系统在完成一次分析后会获得大量的模式或规则，其中只有很小的一部分是有实际使用价值的。 通常，有以下几个标准 易于用户理解; 对新数据或测试数据能够确定有效程度; 具有潜在价值; 新奇的 数据挖掘系统分类数据挖掘系统可以按照三种标准进行划分，它们是数据库类型、所挖掘的知识和所使用的技术。","categories":[{"name":"programming","slug":"programming","permalink":"https://LorrinWWW.github.io/categories/programming/"},{"name":"unfinished","slug":"programming/unfinished","permalink":"https://LorrinWWW.github.io/categories/programming/unfinished/"}],"tags":[{"name":"datamining","slug":"datamining","permalink":"https://LorrinWWW.github.io/tags/datamining/"}]},{"title":"participe présent et gérondif","slug":"participe-present-et-gerondif","date":"2016-12-06T11:31:56.000Z","updated":"2016-12-13T21:28:32.000Z","comments":true,"path":"participe-present-et-gerondif/","link":"","permalink":"https://LorrinWWW.github.io/participe-present-et-gerondif/","excerpt":"","text":"现在分词(le participe présent)1. 构成现在分词没有人称和数的变化。反身动词的现在分词仍然保留原反身动词的反身代词。 去掉直陈式第一人称复数的词尾-ons，另加-ant faire : nous faisons 特殊情况： etre-etant savoir-sachant 2. 简述用法 用作定语，紧接在被修饰词之后，相当于qui引导的从句 L’étranger cherche à trouver quelqu’un connaissant (=qui conaisse) à la fois français et l’anglais. 表原因、时间等，作为状语 Voyant(=Comme elle voit) que tout le monde est dejà assis,elle va vite à sa place. Ayant(=Comme il a) mal à la tete,il décide de rester au lit. 3. 细分I. 现在分词形容词特性的表现许多现在分词已经转化成形容词，有性数变化，可以作为定于或表语。 动词形容词表示一种状态或性质，后不跟宾语或状语 Ses yeux brillants disent la convoitises. 他那双闪光的眼睛流露出他贪婪的本性。 Je l’ai trouvé toute tremblante. 我发现她浑身颤抖。 在一些习惯性搭配中表示一些潜在的主语 Une seance payante (=où l’on paie) 一场要收费的演出。 une collation soupante (=si copieuse qu’elle tient lieu de souper) 一顿丰盛的小吃 这里分词的主语与它的被修饰语不同。 少数已变成了纯粹的形容词，书写有所区别 现在分词 形容词 provoquant provocant fatiguant fatigant vaquant vacant naviguant navigant II. 现在分词动词特性的表现现在分词具有动词的性质，可以有宾语或状语。 代替qui引导的关系从句 C’est un film captivant les spectateurs. 这是一部非常吸引观众的影片。 Je le vois lisant. 我看见他在念书。 起状语作用 Prenant l’escabeau, il s’offoree d’atteindre le dernier rayon. 他拿了一把小梯子乡尽力够上最后一格。 Croyant le bureau vide, il entra. 他以为办公室没人所以就进去了。 表示并列动作 Le train repartit, courant vers le Midi. 火车又开动了，向南方驶去。 依附于另一主语，构成独立分词从句 Midi sonnant, on se met à table. 钟敲十二时，我们都上桌用餐。 副动词(le gérondif)1. 构成副动词没有人称、性数变化 在现在分词前加en就构成副动词 faire : en faisant 注：ayant、étant前不能用en，不能构成副动词。 2. 用法 时间状语，表示动作的同时性 N’oubliez pas de fermer la port en sortant.出去时别忘了关门。 Ne lis pas en mangeant. 不要以便吃饭以一边看书 方式状语 Elle arriva en courant. 她跑来了 条件状语 En se levant plus tot le matin, il n’arrivera pas en retard. 如果早上起的早点，他就不会迟到了。 让步状语 En voyant critiquant, il n’avait nulle intention de nous décourager. 他尽管批评了我们，但毫无使我们气馁之意。 原因状语 En voyant son embarras, l’agent se fit plus aimable. 警察见他不知所措，就变得更加和蔼了。 *副动词表示的动作的延续 副动词可以和aller合用，表示延续发展的动作，介词en可以省略。 Sa vue va en s’affaiblissant. 他的视力日益衰退。 3. 副动词的强化 Tous + 副动词表示强化，这种结构常见于书面。 Rien que + 副动词表示“只要…” 现在分词和副动词的异同共同点它们在作状语时应和主体谓语共一个主语，并和主体谓语发生的时间同步。 不同点 现在分词动作的施动者不一定是该句的主语，而副动词一定是所在句子的主语。 *在一些从古法语流传迄今的成语、谚语等之中，副动词也可能拥有不同的主语，而这种主语是不明显的。 La fortune vient en dormant. 飞来横财。 见具体用法","categories":[{"name":"francais","slug":"francais","permalink":"https://LorrinWWW.github.io/categories/francais/"}],"tags":[{"name":"francais","slug":"francais","permalink":"https://LorrinWWW.github.io/tags/francais/"},{"name":"language","slug":"language","permalink":"https://LorrinWWW.github.io/tags/language/"}]},{"title":"proba-ch6 条件期望","slug":"proba-ch6","date":"2016-12-02T20:32:30.000Z","updated":"2016-12-02T21:09:54.000Z","comments":true,"path":"proba-ch6/","link":"","permalink":"https://LorrinWWW.github.io/proba-ch6/","excerpt":"","text":"P(A|B) = \\frac{P(A\\cap B)}{P(B)} E[X|Y=y] = E_Q[X] = \\sum_{x \\in \\tilde E}{xP(X=x|Y=y)} \\psi : y \\mapsto E[X|Y=y] \\text{ if } P(Y=y) > 0 \\\\ otherwise\\ 0 E[X|Y] = \\psi(Y) 定义 \\mathcal{G} 是一个sous-tribu。\\\\ E[X|\\mathcal{G}] 是一个随机变量Y,满足 \\\\ Y \\in L^1, \\forall A \\in \\mathcal{G}, \\int_A X dP = \\int_AYdP 定义 L2 X \\in L^2, \\mathcal{G} 是一个sous-tribu \\\\ E[X|\\mathcal{G}] 是一个随机变量Y,满足 \\\\ \\forall Z \\in L^2, E[XZ] = E[YZ] 性质 E[E[X|\\mathcal{G}]] = E[X] 性质 若X \\in L^1, E[X|\\mathcal{G}] = E[X] \\Leftrightarrow X是\\mathcal{G}可测的。 性质 X是\\mathcal{G}可测的, X,Y,XY \\in L^1 \\\\ \\Rightarrow E[XY|\\mathcal{G}] = XE[Y|\\mathcal{G}] 定义 E[X|Y] = E[X|\\sigma(Y)] 性质 若XY是实随机变量，则存在一个函数h E[X|Y] = h(Y)","categories":[{"name":"math","slug":"math","permalink":"https://LorrinWWW.github.io/categories/math/"}],"tags":[{"name":"math","slug":"math","permalink":"https://LorrinWWW.github.io/tags/math/"},{"name":"probability","slug":"probability","permalink":"https://LorrinWWW.github.io/tags/probability/"}]},{"title":"proba-ch5 数列和随机变量系列","slug":"proba-ch5","date":"2016-12-01T22:14:28.000Z","updated":"2016-12-11T11:05:17.000Z","comments":true,"path":"proba-ch5/","link":"","permalink":"https://LorrinWWW.github.io/proba-ch5/","excerpt":"","text":"数列和随机变量系列 零一律 零一律是概率论中的一个定律，它是安德雷·柯尔莫哥洛夫发现的，因此有时也叫柯尔莫哥洛夫零一律。其内容是：有些事件发生的概率不是几乎一（几乎肯定发生），就是几乎零（几乎肯定不发生）。这样的事件被称为“尾事件”。 — wiki (X_n)_{n \\in \\mathbb{N}} v.a. \\\\ \\mathcal{T}_n = \\sigma(X_k; k \\ge n) \\\\ \\mathcal{T}_{\\infty} = \\cap_{n \\in \\mathbb{N}}{\\mathcal{T}_n} \\text{ est appelee tribu de queue de la suite } (X_n)_{n \\in \\mathbb{N}} 定理(loi de zero-un) 如果存在一个事件A属于$\\mathcal{T}_{\\infty}$，则P(A)等于0或1。 不同定义 convergence presque sure 定义，当存在一个事件满足以下条件 \\exists \\Omega^*, \\forall \\omega \\in \\Omega^*, \\lim_{n \\rightarrow \\infty}{X_n(\\omega)} = X(\\omega)我们认为一个随机变量序列趋向(p.s.)一个随机变量。即 P\\{\\omega \\in \\Omega^*: \\lim_{n \\rightarrow \\infty}{X_n(\\omega)} = X(\\omega)\\} = 1 Convergence dans Lp 定义。首先Xn和X都在Lp空间下，并且其差值的绝对值的p次方的期望的极限等于零。- -||| 也就是 \\lim_{n \\rightarrow \\infty}{E[|X_n-X|^p]} = 0 convergence en probabilite 定义，满足以下条件，称依概率收敛。 \\forall \\varepsilon, \\lim_{n \\to \\infty}{P\\{\\omega:X_n(\\omega)-X(\\omega)>\\varepsilon}\\}=0 \\\\ ou\\ \\lim_{n \\to \\infty}{P\\{X_n-X>\\varepsilon}\\}=0定理， 如果Xn趋向X(p.s.)，则fXn趋向fX(p.s.) 如果Xn趋向X(P)，则fXn趋向fX(P) 定理， Xn是实随机变量，以下关系等价 X_n \\xrightarrow{P} X \\Leftrightarrow \\lim_{n \\to \\infty} E(\\frac{|X_n-X|}{1+|X_n-X|}) = 0定理， Xn是实随机变量，则 若X_n \\xrightarrow{L^P}X, X_n \\xrightarrow{P}{X} \\\\ 若X_n \\to X p.s., X_n \\xrightarrow{P}{X} \\\\定理， Xn是实随机变量，若Xn依概率收敛于X，我们能找到一个序列 (X_{n_k})_{k \\in \\mathbb{N}}使得 X_n \\to X, p.s.定理， Xn是实随机变量，若Xn依概率收敛于X，并存在一个$Y \\in L^p, |X_n| \\le Y$，则 X \\in L^p并且 X_n \\xrightarrow{L^p}X 波莱尔－坎泰利引理 大致上，波莱尔－坎泰利引理说明了，如果有无穷个概率事件，它们发生的概率之和是有限的，那么其中的无限多个事件一同发生的概率是零。这个定理实际上是测度论的结论在概率论中的应用。 —wiki 如果有无穷个概率事件，它们发生的概率之和是有限的，那么其中的无限多个事件一同发生的概率是零。 如果有无穷个概率事件，无限多个事件一同发生的概率是零，那么它们发生的概率之和是有限的。 大数定律 在数学与统计学中，大数定律又称大数法则、大数律，是描述相当多次数重复实验的结果的定律。根据这个定律知道，样本数量越多，则其平均就越趋近期望值。 — wiki 强大数定律 X_n \\in L^2 \\text{若它们独立同分布且在同一个概率空间，则} \\\\ lim_{n \\to \\infty}{\\frac{\\sum_{i=1}^{n}{X_i}}{n}} = \\mu \\ p.s. 切比雪夫大数定律 X_n \\text{若它们独立同分布且在同一个概率空间，则} \\\\ lim_{n \\to \\infty}{\\frac{\\sum_{i=1}^{n}{X_i}}{n}} = \\mu \\ p.s. \\text{ 当且仅当} E[X_i] \\text{对于所有i存在} Convergence en loi 弱趋向： \\int_{\\mathbb{R}^N}{f(x)\\nu_n(dx)} \\xrightarrow{n \\to \\infty} \\int_{\\mathbb{R}^N}{f(x)\\nu(dx)}定义，若PXn弱趋向于PX，则Xn是 convergence en loi vers X。记为： X_n \\xrightarrow{\\mathcal{D}} X定理， X_n \\xrightarrow{\\mathcal{D}} X \\Leftrightarrow \\lim_{n \\to \\infty}E[f(X_n)] = E[f(X)]定理， X_n \\xrightarrow{P} X \\Rightarrow X_n \\xrightarrow{\\mathcal{D}} X定理，si Xn converge en loi vers une v.a. constante presque surement, alors elle converge en probabilite. 离散变量的convergence en loi 分布函数的convergence en loi 特征函数的convergence en loi 均略=_= 中心极限定理 若Var(X_n) < \\infty \\\\ 记S_n = \\sum{Xi} \\\\ \\frac{S_n-n\\mu}{\\sigma\\sqrt{n}} \\xrightarrow{\\mathcal{D}} \\mathcal{N}(0,1)​","categories":[{"name":"math","slug":"math","permalink":"https://LorrinWWW.github.io/categories/math/"}],"tags":[{"name":"math","slug":"math","permalink":"https://LorrinWWW.github.io/tags/math/"},{"name":"probability","slug":"probability","permalink":"https://LorrinWWW.github.io/tags/probability/"}]},{"title":"proba-ch4 高斯向量","slug":"proba-ch4","date":"2016-12-01T15:28:52.000Z","updated":"2016-12-19T13:11:37.000Z","comments":true,"path":"proba-ch4/","link":"","permalink":"https://LorrinWWW.github.io/proba-ch4/","excerpt":"","text":"定义 Un vecteur aléatoire est dit gaussien si toute combinaison linéaire de ses composantes suit une loi gaussienne. 也就是： \\forall \\lambda_1,...,\\lambda_N \\in \\mathbb{R} , \\sum_{j=1}^{N}{\\lambda_jX_j \\text{ suit une loi normale.}} 特征函数 \\varphi_X: t \\rightarrow e^{i < \\mu, t> -\\frac{1}{2}} \\\\ => \\varphi_X = e^{i\\sum_{j=1}^{N}{\\mu_j t_j}- \\frac{1}{2}\\sum_{1\\le j,k\\le N}{t_j D_{j,k}t_k}}$\\mu$, le vecteur moyenne, 平均向量 D, la matrice de covariances de X, 协方差矩阵 理论，X=(X1,…,XN) 是高斯向量，则Xj相互独立当仅当其协方差矩阵D为对角线矩阵。（也就是只有自己和自己的协方差不为零） Densite de la loi d’un vecteur gaussien D \\ne 0 \\\\ \\mathcal{N}(\\mu,D) \\text{在Lebesgue测度下，在}\\mathbb{R}^N\\text{上绝对连续} \\\\ x \\longmapsto \\frac{1}{(2\\pi)^{N/2}\\sqrt{\\det D}}e^{-\\frac{1}{2}}","categories":[{"name":"math","slug":"math","permalink":"https://LorrinWWW.github.io/categories/math/"}],"tags":[{"name":"math","slug":"math","permalink":"https://LorrinWWW.github.io/tags/math/"},{"name":"probability","slug":"probability","permalink":"https://LorrinWWW.github.io/tags/probability/"},{"name":"vector","slug":"vector","permalink":"https://LorrinWWW.github.io/tags/vector/"}]},{"title":"proba-ch3 实域概率和特征方程","slug":"proba-ch3","date":"2016-12-01T13:54:10.000Z","updated":"2016-12-01T15:28:37.000Z","comments":true,"path":"proba-ch3/","link":"","permalink":"https://LorrinWWW.github.io/proba-ch3/","excerpt":"","text":"几种常见分布 若不在区间内，f=0 loi uniforme f(x)=\\frac{1}{b-a} si\\ x \\in [a,b] loi exponentielle f(x) = \\lambda e^{-\\lambda x} si\\ x \\ge 0 \\\\ E[X] = \\frac{1}{\\lambda}, Var(X) = \\lambda^2 Loi de weibull f(x) = \\alpha \\lambda^\\alpha x^{\\alpha-1}e^{-(\\lambda x)^\\alpha} si\\ x \\ge 0\\\\ E[X] = \\frac{\\Gamma(1+1/\\alpha)}{\\lambda}, Var(X) = \\frac{\\Gamma(1+2/\\alpha)}{\\lambda^2} loi gamma f(x) = \\frac{\\lambda}{\\Gamma(p)}(\\lambda x)^{p-1}e^{-\\lambda x}\\\\ E[X] = \\frac{p}{\\lambda}, Var(X) = \\frac{p}{\\lambda^2} loi normale f(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma}exp[-\\frac{(x-m)^2}{2\\sigma^2}], x \\in R\\\\ E[X] = m, Var(X) = \\sigma^2 Loi lognormale f(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma x}exp[-\\frac{(\\ln x-m)^2}{2\\sigma^2}], x \\ge 0 \\\\ E[X] = e^{m+\\sigma^2/2}, Var(X) = e^{2m+\\sigma^2}(e^{\\sigma^@}-1) Loi du $ \\chi^2 $ X = U_1^2+...+U_n^2 \\\\ U_i\\ est\\ de\\ loi\\ \\mathcal{N}(0,1) \\\\ E[X] = n, Var(X) = 2n Loi de Student X = \\frac{U}{\\sqrt{\\frac{Z}{n}}} \\\\ U\\ suit\\ la\\ loi\\ normale\\ \\mathcal{N}(0,1) \\\\ Z\\text{ est independante de U et suit la loi du }\\chi^2\\text{ a n degres de liberte} 特征函数 \\varphi_X: \\mathbb{R^N} \\rightarrow \\mathbb{C} \\\\ t \\rightarrow \\varphi_X(t) = E[e^{i}] = \\int_{\\mathbb{R}^N}{e^{i}P_X(dx)}我们说特征函数是对X进行基于PX的傅立叶变换 = =好绕 若PX存在概率密度 $f \\in L^1$ \\varphi_X(t) = \\int_{\\mathbb{R}^N}{e^{i}f(x)dx} 性质 \\forall t \\in \\mathbb{R}^N, |\\varphi_X(t)| \\le 1=> \\varphi_X(0) = 1 \\varphi_{\\lambda X+a}(t) = e^{iat}\\varphi_X(\\lambda t) \\varphi_X 是一个半正函数，即 \\\\ \\forall z_1,...,z_n\\in \\mathbb{C}, \\sum_{1 \\le j, k\\le n}{z_j \\varphi_X(t_j-t_k)\\bar{z_k}} \\ge 0 其他性质 特征函数连续 PX以lebesgue测度绝对连续，则 \\lim_{|t|\\rightarrow \\infty}{\\varphi_X(t)} = 0 XY，它们拥有相同的P，当且仅当 \\varphi_X=\\varphi_Y 逆变换 \\forall x \\in \\mathbb{R}^N, f(x)=\\frac{1}{(2\\pi)^N}\\int_{\\mathbb{R}^N}{e^{-i}\\varphi_X(t)}dt 特征方程和独立性 定理 X_1,...,X_N \\text{ 是独立的，当且仅当它们的特征方程满足：} \\\\ \\forall t = (t_1,...,t_N) \\in \\mathbb{R}^N; \\varphi_X(t) = \\prod_{k=1}^{N}{\\varphi_{X_k}(t_k) } \\\\ where\\ X = (X_1,...,X_N) 性质 X_1,...,X_n independants, P_{X_1},...,P_{X_N}. \\\\ \\text{La loi de } \\sum{X_i} \\text{est le produit de concolution } \\prod{P_{X_i}} \\\\ \\text{Pour fonction caracteristique } \\sum{\\varphi_{X_i}} \\text{definie par} \\\\ \\forall t \\in \\mathbb{R}^N; \\varphi_{X+...+X_n}(t) = \\prod_{i=1}^{n}{\\varphi_{X_i}(t) }","categories":[{"name":"math","slug":"math","permalink":"https://LorrinWWW.github.io/categories/math/"}],"tags":[{"name":"math","slug":"math","permalink":"https://LorrinWWW.github.io/tags/math/"},{"name":"probability","slug":"probability","permalink":"https://LorrinWWW.github.io/tags/probability/"}]},{"title":"proba-ch2 随机变量","slug":"proba-ch2","date":"2016-12-01T11:17:33.000Z","updated":"2016-12-01T13:44:00.000Z","comments":true,"path":"proba-ch2/","link":"","permalink":"https://LorrinWWW.github.io/proba-ch2/","excerpt":"","text":"​ \\pi-system R上的概率测度 分布函数 la fonction de repartition F:x\\xrightarrow{}F(x) = P(] - \\infty,x] ) \\\\ F:x\\xrightarrow{}F(x) = \\int_{ ]-\\infty,x] }{f(t).\\lambda(dt)} 定理 如果F是分布函数，则当且仅当满足以下三个条件。 (i) F 递增\\\\(ii)F 右连续\\\\(iii) \\lim_{n \\rightarrow - \\infty}{F(x)=0}, \\lim_{n \\rightarrow+\\infty}{F(x)=1} 性质 P(\\{x\\}) = F(x) - F(x-) R^N上的概率测度 定义 F:(x_1,...,x_N)\\xrightarrow{}F(x_1,...,x_N) = P(\\prod_{i=1}^{N}] - \\infty,x_i] ) 定义 P(dx_1,...,dx_N) = f(x_1,...,x_N)dx_1...dx_N \\\\ where||f||_{L^2} 随机变量 定义 Soit(\\Omega,{\\cal F})\\rightarrow (E, {\\cal E}); X:\\Omega \\rightarrow E \\\\ \\forall A\\in {\\cal E}; X^{-1}(A) \\in {\\cal F}X就是一个随机变量 定义 tribu engendree par X X^{-1}({\\cal E}) = \\{X^{-1}(A); A \\in {\\cal E}\\} \\\\ \\sigma(X) 随机变量的loi 定义 \\forall A \\in {\\cal E}; P_X(A) = P(\\{\\omega: X(\\omega) \\in A\\}) = P(X^{-1}(A)) 积分 数学期望定义 E[X] = \\int_{\\Omega}{X(\\omega).P(d\\omega)}= \\int_{\\Omega}{X.dP} 定理 (X_n)_{n\\in N} 是随机变量 单调趋向性：若Xn非负且趋于X，则 \\lim_{n \\rightarrow \\infty}{E[X_n]} = E[X] Fatou引理：若Xn非负 E[\\lim_{n \\rightarrow \\infty}{\\inf{X_n}}] \\leq \\lim_{n \\rightarrow \\infty}{\\inf{E[X_n]}} convergence dominee: 若limXn = X p.s 若存在Z in L1，且|Xn|&lt;=Z，则 \\lim_{n \\rightarrow \\infty}{E[X_n]} = E[X] 性质(Inegalite de Markov) 若X是随机变量并admettant un moment d’ordre 1, 对于实数a&gt;0 P(|X| \\geq a) \\leq \\frac{E[|X|]}{a} 转化定理 E[h(X)] = \\int_E{h(x)P_X(dx)} 定义 un\\ moment\\ d'ordre\\ n: \\int_\\Omega{|X|^ndP} < \\infty 性质，若0&lt;p&lt;q，可以得到Lq被Lp包含。 定义方差，前面说过了 Var(aX+b) = a^2 Var(X) \\\\ P(|X-E[X]| \\geq a) \\leq \\frac{Var(X)}{a^2} 若X几乎处处等于同一个常数，当仅当Var(X) = 0 定理，与离散相似的 E[h(X)] = \\int_E{h(x)f_X(dx)}特别的 P(X \\in A) = E[1_{\\{X\\in A\\}}] = \\int_A{f_X(dx)} Vecteurs aleatoires X = (X1,…,XN) 与多元离散随机变量类似。 协方差 Cov(X,Y) = E[(X-E[X])(Y-E[Y])] = E[XY] - E[X]E[Y] 性质 Cov(X,X) =Var(X) \\\\ Cov(X,Y) = Cov(Y,X) \\\\ Var(X+Y) = Var(X) +Var(Y) +2Cov(X,Y) 相关系数 \\rho(X,Y) = \\frac{Cov(X,Y)}{\\sigma(X)\\sigma(Y)}, \\sigma(X) = \\sqrt{Var(X)}, -1 \\leq \\rho \\leq 1 改变变量 \\forall y \\in R^N; f_Y(y) = \\frac{f_X(h^{-1}(y))}{|det(Jh(h^{-1}))|} 1_D(y) 边际概率，顾名思义，略 独立变量 定义略 定理，满足下列条件之一，则XY独立 对于所有A，B P(X \\in A, Y\\in B) = P(X\\in A) 对于所有f，g E[f(X)g(Y)] = E[f(X)] E[g(Y)] 对于所有f，g，f(X)和g(Y)是独立的 性质 \\text{Soient (X, Y) un couple de variables aleatoires a valeurs dans } E\\otimes F \\text{ muni de la tribu produit } \\mathcal{E} \\otimes \\tilde{\\mathcal{E}}. \\\\ \\text{ X et Y sont independantes si et seulement si la lor jointe du couple (X, Y) est egale a la mesure produit } P_X \\otimes P_Y. \\text{注：对于XY相互独立，} P((X,Y)\\in A \\times B) = P_X(A)P_Y(B) \\\\ P_X \\otimes P_Y(A \\times B) = P_X(A)P_Y(B) 定理，对于随机变量XY，他们相互独立，当且仅当 \\forall (x,y) \\in R^2; F_{(X,Y)}(x,y) = F_X(x)F_Y(y) 定理，对于(X,Y)在lebesgue测度上绝对连续，XY相互独立当且仅当 \\forall (x,y) \\in R^2; f_{(X,Y)}(x,y) = f_X(x)f_Y(y) 性质，XY admettant un moment d’ordre 1, 相互独立，则 E[XY]=E[X]E[Y] 性质，XY admettant un moment d’ordre 2, Cov(X,Y)=0, 则 Var(X+Y) =Var(X) + Var(Y)​","categories":[{"name":"math","slug":"math","permalink":"https://LorrinWWW.github.io/categories/math/"}],"tags":[{"name":"math","slug":"math","permalink":"https://LorrinWWW.github.io/tags/math/"},{"name":"probability","slug":"probability","permalink":"https://LorrinWWW.github.io/tags/probability/"},{"name":"aleatoire","slug":"aleatoire","permalink":"https://LorrinWWW.github.io/tags/aleatoire/"}]},{"title":"proba-ch1 概率空间","slug":"proba-ch1","date":"2016-12-01T11:17:29.000Z","updated":"2016-12-01T11:19:38.000Z","comments":true,"path":"proba-ch1/","link":"","permalink":"https://LorrinWWW.github.io/proba-ch1/","excerpt":"","text":"一个基本的不等式，当 P(\\cup(A_n)) \\leq \\sum_{n \\in N}^{}{P(A_n)} 如果An递增 P(\\cup(A_n)) = \\lim_{n -> + \\inf}{P(A_n)} 如果An递减 P(\\cap(A_n)) = \\lim_{n -> + \\inf}{P(A_n)} 条件概率定义，略 性质 P({A_1}\\cap{...}\\cap{A_{n-1}}) = P(A_1)P(A_2|A_1)...P(A_n|A_1\\cap{...}\\cap{A_{n-1}}) 定理(Equation de partition) P(A) = \\sum_{n}{P(A|E_n)P(E_n)} 定理(de Bay) P(E_n|A) = \\frac{P(A|E_n)P(E_n)}{\\sum_{m}{P(A|E_m)PE_m}} 两个独立事件概率，略 定理 对一可数或有限事件集，P({wn}) = pn，若对pn求和，则P存在且唯一。 均值定义， E[X] = \\sum_{\\omega \\in{\\Omega}}{X(\\omega) P(\\omega)} 方差定义 Var(X) = E[(X - E(X))^2] = E[X^2] - (E(X)^2) 几种分布 Loi discrete uniforme， \\forall k \\in \\{1,...,n\\}, P(X=k) = \\frac{1}{n} E[X] = \\frac{n+1}{2} Loi de Bernoulli， P(X=1) = p, t P(X=0) = 1-p E[N] = p, Var(N) = p(1-p) Loi binomiale 二项分布，略 E[N] = np, Var(N) = np(1-p) Loi geometrique P(N=n) = P^n(1-p),$$E[N] = np, Var(N) = np(1-p)$$ Distribution de Poisson P(X=n)=\\frac{\\lambda^n}{n!}e^{-\\lambda}, E[X] = \\lambda, Var(X) = \\lambda","categories":[{"name":"math","slug":"math","permalink":"https://LorrinWWW.github.io/categories/math/"}],"tags":[{"name":"math","slug":"math","permalink":"https://LorrinWWW.github.io/tags/math/"},{"name":"probability","slug":"probability","permalink":"https://LorrinWWW.github.io/tags/probability/"}]},{"title":"解决hexo使用公式冲突问题 hexo with latex","slug":"hexo-with-latex","date":"2016-11-30T21:19:02.000Z","updated":"2016-11-30T21:45:34.000Z","comments":true,"path":"hexo-with-latex/","link":"","permalink":"https://LorrinWWW.github.io/hexo-with-latex/","excerpt":"","text":"在hexo中使用大量公式的同学一定会发现，在hexo很多公式的渲染不正常。这是由于markdown渲染器和latex渲染器冲突的问题（具体说，就是公式中的特殊字符首先被markdown渲染器转义了）。我们可以通过更换Markdown渲染插件来解决这个问题。 Google了一些博文，在如何处理Hexo和MathJax的兼容问题这篇文章中发现了一个符合要求的插件：hexo-renderer-kramed。作者fork了 hexo-renderer-marked 项目，并且只对MathJax支持进行了改进，其他特性完全一致。 简单粗暴的讲，卸载原渲染器，安装我们需要的渲染器。 依次运行下列语句即可。 12$ npm uninstall hexo-renderer-marked --save$ npm install hexo-renderer-kramed --save 现在公式的显示已经正常。 注：传说行内代码渲染仍然有问题，在代码块中工作正常。","categories":[{"name":"other","slug":"other","permalink":"https://LorrinWWW.github.io/categories/other/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://LorrinWWW.github.io/tags/hexo/"},{"name":"latex","slug":"latex","permalink":"https://LorrinWWW.github.io/tags/latex/"},{"name":"mathjax","slug":"mathjax","permalink":"https://LorrinWWW.github.io/tags/mathjax/"},{"name":"marked","slug":"marked","permalink":"https://LorrinWWW.github.io/tags/marked/"}]},{"title":"概率论笔记 Note of probability","slug":"Note-of-probability","date":"2016-11-30T18:35:53.000Z","updated":"2016-12-05T14:16:49.000Z","comments":true,"path":"Note-of-probability/","link":"","permalink":"https://LorrinWWW.github.io/Note-of-probability/","excerpt":"","text":"该笔记只是记录在法国学的一些东西，因为学的是法语的翻译可能有些摸不着头脑，并且记的凌乱，望见谅。 另外，这里的概率论是基于测度论的概率论，这里假设已经学过测度学了。 Part 1 公理、概率空间Savoir plus Part 2 概率与随机变量Savoir plus Part 3 实域概率和特征方程Savoir plus Part 4 高斯向量Savoir plus Part 5 数列和随机变量系列Savoir plus Part 6 条件数学期望Savoir plus","categories":[{"name":"math","slug":"math","permalink":"https://LorrinWWW.github.io/categories/math/"}],"tags":[{"name":"math","slug":"math","permalink":"https://LorrinWWW.github.io/tags/math/"},{"name":"probability","slug":"probability","permalink":"https://LorrinWWW.github.io/tags/probability/"}]},{"title":"Chrome插件开发 - Hello world","slug":"projet-enjeu-plugin-chrome-101","date":"2016-11-30T11:01:24.000Z","updated":"2016-11-30T11:19:02.000Z","comments":true,"path":"projet-enjeu-plugin-chrome-101/","link":"","permalink":"https://LorrinWWW.github.io/projet-enjeu-plugin-chrome-101/","excerpt":"","text":"在学校选的projet是关于chrome插件开发的，这里记录一下。 Hello world!凡事先从hello world开始。 阅读chrome的开发手册，新建一个项目文件夹 我们需要manifest.json文件，告诉chrome我们的配置，去哪里找我们文件。 下面的写的是我们目前的设置，只写hello world的话只需要配置基本的设置以及default_popup。 1234567891011121314&#123; \"manifest_version\": 2, \"name\": \"TrelloGement\", \"description\": \"Organiser ses recherches d'appartement sur Paris grâce à Trello!\", \"version\": \"0.2.1\", \"browser_action\": &#123; \"default_icon\": \"icon.png\", \"default_popup\": \"popup.html\" &#125;, \"background\": &#123; \"scripts\": [\"background.js\", \"jquery-3.1.1.min.js\", \"client.js\"] &#125;, \"permissions\": [\"activeTab\", \"storage\", \"tabs\", \"https://api.trello.com/*\", \"https://trello.com/*\"]&#125; 新建popup.html 12345678910&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;Trellogement&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;h1&gt; Hello, world! &lt;/h1&gt;&lt;/body&gt;&lt;/html&gt; 一个简单的Hello world就实现了，在chrome加载这个文件夹作为未打包的插件，在popup的位置点击可以看到“Hello, world!”","categories":[{"name":"programming","slug":"programming","permalink":"https://LorrinWWW.github.io/categories/programming/"}],"tags":[{"name":"web","slug":"web","permalink":"https://LorrinWWW.github.io/tags/web/"},{"name":"chrome","slug":"chrome","permalink":"https://LorrinWWW.github.io/tags/chrome/"}]},{"title":"图论 graph","slug":"graph","date":"2016-11-27T13:42:24.000Z","updated":"2017-02-11T12:08:30.000Z","comments":true,"path":"graph/","link":"","permalink":"https://LorrinWWW.github.io/graph/","excerpt":"","text":"The symbolsIn graph thery, a graph G = (V, E) is a collection of points. V, called vertices and lines connecting some subset of them E, called edges, is contained by V ✖ V Union-Find Others以后涉及相关问题再回过头来补充，暂时引用wiki凑数： 图论 重要算法： 戴克斯特拉算法(D.A) 克鲁斯卡尔算法(K.A) 普里姆算法(P.A) 拓扑排序算法(TSA) 关键路径算法(CPA) 广度优先搜索算法(BFS‘s A) 深度优先搜索算法(DFS‘s A)","categories":[{"name":"programming","slug":"programming","permalink":"https://LorrinWWW.github.io/categories/programming/"}],"tags":[{"name":"algo","slug":"algo","permalink":"https://LorrinWWW.github.io/tags/algo/"},{"name":"programming","slug":"programming","permalink":"https://LorrinWWW.github.io/tags/programming/"},{"name":"graph","slug":"graph","permalink":"https://LorrinWWW.github.io/tags/graph/"}]},{"title":"编码解码 compression","slug":"compression","date":"2016-11-27T13:22:11.000Z","updated":"2016-11-30T10:46:12.000Z","comments":true,"path":"compression/","link":"","permalink":"https://LorrinWWW.github.io/compression/","excerpt":"","text":"Run length encoding ex: 0101 — 0,101 — “3 pixels are color ‘0’” 1101 — 1,101 — “6 pixels are color ‘1’” You can also define other signification like: 1 |1111|1111|1111|1111|0111|0111|0111|0111| with the first 1 meaning encoding in rank, while 0 meaning encoding in row. Huffman Defined in wiki Normally we supppose higher number with “1”. ex: ​ (1) ​ 0/ \\1 ​ a(0.45) (0.55) ​ 0/ \\1 ​ b(0.25) c(0.30) Lempel-Ziv Encode: Origin: ababcbab… Init: a:0, b:1, c:2 Extensions du dico: ab: 3, ba: 4, abc: 5, cb: 6… Result: 01324… Decode: pass ​","categories":[{"name":"programming","slug":"programming","permalink":"https://LorrinWWW.github.io/categories/programming/"}],"tags":[{"name":"algo","slug":"algo","permalink":"https://LorrinWWW.github.io/tags/algo/"},{"name":"programming","slug":"programming","permalink":"https://LorrinWWW.github.io/tags/programming/"},{"name":"compression","slug":"compression","permalink":"https://LorrinWWW.github.io/tags/compression/"}]},{"title":"面向考试常用编程思想 Method of programming facing to exams","slug":"Method-of-programming-facing-to-exams","date":"2016-11-27T12:49:31.000Z","updated":"2016-11-30T10:45:24.000Z","comments":true,"path":"Method-of-programming-facing-to-exams/","link":"","permalink":"https://LorrinWWW.github.io/Method-of-programming-facing-to-exams/","excerpt":"","text":"穷举法 略 贪心法 略 分治法 1234567891011def merge(A, B): # merge two small solved problems into one. return mergeddef divideConquer(S, divide, combine): if len(S) == 1: return S # divide a grand problems L, R = divide(S) A = divideConquer(L, divide, combine) B = divideConquer(R, divide, combine) return merge(A, B) 上面的情形只供参考，参数等视具体情况而定。 动态规划 比较经典的就是背包问题。代码来源 1234567891011121314151617181920212223242526272829303132def bag(n,c,w,v): res=[[-1 for j in range(c+1)] for i in range(n+1)] for j in range(c+1): res[0][j]=0 for i in range(1,n+1): for j in range(1,c+1): res[i][j]=res[i-1][j] if j&gt;=w[i-1] and res[i][j]&lt;res[i-1][j-w[i-1]]+v[i-1]: res[i][j]=res[i-1][j-w[i-1]]+v[i-1] return res def show(n,c,w,res): print('最大价值为:',res[n][c]) x=[False for i in range(n)] j=c for i in range(1,n+1): if res[i][j]&gt;res[i-1][j]: x[i-1]=True j-=w[i-1] print('选择的物品为:') for i in range(n): if x[i]: print('第',i,'个,',end='') print('') if __name__=='__main__': n=5 c=10 w=[2,2,6,5,4] v=[6,3,5,4,6] res=bag(n,c,w,v) show(n,c,w,res) ​","categories":[{"name":"programming","slug":"programming","permalink":"https://LorrinWWW.github.io/categories/programming/"}],"tags":[{"name":"algo","slug":"algo","permalink":"https://LorrinWWW.github.io/tags/algo/"},{"name":"programming","slug":"programming","permalink":"https://LorrinWWW.github.io/tags/programming/"}]},{"title":"算法笔记 Note of learning Algo","slug":"Note-of-learning-Algo","date":"2016-11-27T11:38:32.000Z","updated":"2016-11-30T10:56:38.000Z","comments":true,"path":"Note-of-learning-Algo/","link":"","permalink":"https://LorrinWWW.github.io/Note-of-learning-Algo/","excerpt":"","text":"Data StructureHow to calcul the complexity? How to make a compression? The data : int, double, etc. The basic data structure Container Table Stack Queue List Tree Arbres binaires - 二叉树 ABR - 二叉树的查询 Arbres n-aires Dictionary Hash table - Table de hachage - 哈希表 Heap - Tas - 堆 二叉堆 堆排序 Find-Union Method of programmingSavoir plus Echaustive / force brute - 穷举法 Try-error - Essai-erreur 试错法 Glouton - 贪心法 Recursif - recursive - 递归 Divede merge - Diviser pour regner - 分治 Dynamic - Dynamique - 动态规划 GraphSavoir plus Traversal - parcours - 遍历 BFS DFS Critical path method - plus court chemin Dijkstra Floyd Bellman-Ford Tree - arbre Prim Kruskal *backtracking *Branch and bound Others 排序 选择，冒泡 分治（fusion） 快速 tas","categories":[{"name":"programming","slug":"programming","permalink":"https://LorrinWWW.github.io/categories/programming/"}],"tags":[{"name":"algo","slug":"algo","permalink":"https://LorrinWWW.github.io/tags/algo/"},{"name":"programming","slug":"programming","permalink":"https://LorrinWWW.github.io/tags/programming/"}]},{"title":"复杂度 complexity","slug":"complexity","date":"2016-11-27T09:52:05.000Z","updated":"2016-11-30T10:43:08.000Z","comments":true,"path":"complexity/","link":"","permalink":"https://LorrinWWW.github.io/complexity/","excerpt":"","text":"计算复杂度 - Calcul the complexity这里我们只考虑时间复杂度。 There, we only discuss the time complexity. 通常 - Normal Single operation - O(1) Loop 123def fun(n): for i in range(n): pass 123def fun(n): while i &lt; n : i += 1 — O(n) 123def fun(n): while i &lt; n : i *= 2 — O(logn) Etc. 递归 - Recursion 代入法 预估其复杂度，代入原方程，若相符，则可能为解。 ex: T(n) = 2*T(n/2) + O(n) 假设 T(n) = kn^2 代入原式，成立。 接下来用数学归纳法验证。 迭代法 迭代地展开递归方程的右端，使其成为一个非递归的和式，随后进行复杂度计算 ex: T(n) = T(n-1) + O(1) T(1) = O(1) 所以，T(n) = T(n-1) + O(1) = T(n-1) + 2 O(1) = … = nO(1) = O(n) 套用公式 对于形如 T(n) = aT(n/b) +f(n) 的方程已有固定判断法，套用公式即可。 差分方程法 利用查分方程解，这个比较复杂，这里先占个位，以后在补。","categories":[{"name":"programming","slug":"programming","permalink":"https://LorrinWWW.github.io/categories/programming/"}],"tags":[{"name":"algo","slug":"algo","permalink":"https://LorrinWWW.github.io/tags/algo/"},{"name":"programming","slug":"programming","permalink":"https://LorrinWWW.github.io/tags/programming/"},{"name":"complexity","slug":"complexity","permalink":"https://LorrinWWW.github.io/tags/complexity/"}]}]}