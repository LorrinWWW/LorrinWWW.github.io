{"pages":[{"title":"Jue Wang","text":"Hello, I am a PhD student in Data Intelligence Lab of Zhejiang University, advised by Prof. Lidan Shou. I am visiting ETH Zurich now, where I am advised by Prof. Ce Zhang. I work on Natural Language Processing. More specifically, my research interests lie in Efficient Algorithms for NLP (both training and inference), Information Extraction, and NLP in low-resource scenarios. If you want to get in touch, please send me an email. My resume. Updates Apr 2022: We got a paper accepted to IJCAI 2022. Mar 2022: I started visiting ETH Zurich. Feb 2022: As the first author, I had one long paper accepted to ACL 2022. Jun 2021: I graduated from CentraleSupélec with diplôme d’Ingénieur (master degree), cheers! Dec 2020: As the first author, I had one long paper accepted to AAAI 2021. Sep 2020: As the first author, I had one long paper accepted to EMNLP 2020. Apr 2020: As the first author, I had one long paper accepted to ACL 2020. Education Zhejiang University, PhD student in Computer Science (Current), Sep 2018 - Jun 2023 (Expected) Université Paris Saclay (CentraleSupélec), Master (Engineer) in General Engineering, Sep 2016 - Jun 2018 Zhejiang University, Bachelor in Electrical Engineering, Sep 2014 - Jun 2018 Preprints Fine-tuning Language Models over Slow Networks using Activation Compression with GuaranteesJue Wang$^{*}$, Binhang Yuan$^{*}$, Luka Rimanic$^{*}$, Yongjun He, Tri Dao, Beidi Chen, Christopher Re, Ce Zhang.[Paper] [Code] Selected Publications SkipBERT: Efficient Inference with Shallow Layer SkippingJue Wang, Ke Chen, Gang Chen, Lidan Shou, and Julian McAuley.In Proc. of ACL 2022.[Paper] [Code] Effective Slot Filling via Weakly-Supervised Dual-Model LearningJue Wang, Ke Chen, Lidan Shou, Sai Wu, and Gang Chen.In Proc. of AAAI 2021.[Paper] [Code] [Video] Two are Better than One: Joint Entity and Relation Extraction with Table-Sequence EncodersJue Wang and Lu Wei.In Proc. of EMNLP 2020.[Paper] [Code] [Video] Pyramid: A Layered Model for Nested Named Entity RecognitionJue Wang, Lidan Shou, Ke Chen, and Gang Chen.In Proc. of ACL 2020.[Paper] [Code] [Video] ContactCollege of Computer Science and Technology, Zhejiang University 38 Zheda Rd, Xihu Qu, Hangzhou, Zhejiang, 310027 Email: zjuwangjue@gmail.com","link":"/about/index.html"},{"title":"categories","text":"","link":"/categories/index.html"},{"title":"tags","text":"","link":"/tags/index.html"}],"posts":[{"title":"Jue Wang","text":"Hello, I am a PhD student in Data Intelligence Lab of Zhejiang University, advised by Prof. Lidan Shou. I am visiting ETH Zurich now, where I am advised by Prof. Ce Zhang. I work on Natural Language Processing. More specifically, my research interests lie in Efficient Algorithms for NLP (both training and inference), Information Extraction, and NLP in low-resource scenarios. If you want to get in touch, please send me an email. My resume. Updates Apr 2022: We got a paper accepted to IJCAI 2022. Mar 2022: I started visiting ETH Zurich. Feb 2022: As the first author, I had one long paper accepted to ACL 2022. Jun 2021: I graduated from CentraleSupélec with diplôme d’Ingénieur (master degree), cheers! Dec 2020: As the first author, I had one long paper accepted to AAAI 2021. Sep 2020: As the first author, I had one long paper accepted to EMNLP 2020. Apr 2020: As the first author, I had one long paper accepted to ACL 2020. Education Zhejiang University, PhD student in Computer Science (Current), Sep 2018 - Jun 2023 (Expected) Université Paris Saclay (CentraleSupélec), Master (Engineer) in General Engineering, Sep 2016 - Jun 2018 Zhejiang University, Bachelor in Electrical Engineering, Sep 2014 - Jun 2018 Preprints Fine-tuning Language Models over Slow Networks using Activation Compression with GuaranteesJue Wang$^{*}$, Binhang Yuan$^{*}$, Luka Rimanic$^{*}$, Yongjun He, Tri Dao, Beidi Chen, Christopher Re, Ce Zhang.[Paper] [Code] Selected Publications SkipBERT: Efficient Inference with Shallow Layer SkippingJue Wang, Ke Chen, Gang Chen, Lidan Shou, and Julian McAuley.In Proc. of ACL 2022.[Paper] [Code] Effective Slot Filling via Weakly-Supervised Dual-Model LearningJue Wang, Ke Chen, Lidan Shou, Sai Wu, and Gang Chen.In Proc. of AAAI 2021.[Paper] [Code] [Video] Two are Better than One: Joint Entity and Relation Extraction with Table-Sequence EncodersJue Wang and Lu Wei.In Proc. of EMNLP 2020.[Paper] [Code] [Video] Pyramid: A Layered Model for Nested Named Entity RecognitionJue Wang, Lidan Shou, Ke Chen, and Gang Chen.In Proc. of ACL 2020.[Paper] [Code] [Video] ContactCollege of Computer Science and Technology, Zhejiang University 38 Zheda Rd, Xihu Qu, Hangzhou, Zhejiang, 310027 Email: zjuwangjue@gmail.com","link":"/posts/about/"},{"title":"RegEx with NN","text":"这两天在实习没有太多的时间写笔记orz，正好趁着公司内部分享的时候稍微写几笔。（然而一直没发出来） Marrying Up Regexs with Neural Networks概述 正则表达式 简明、扼要、可调，不依赖大规模标注数据 泛化性能差，所以变体、同义词都需要人为编写 神经网络 拟合能力强、泛化性能强 需要大量标注数据，解释性差 因此工程上常常结合两个，正则解决部分cases，剩下交给统计模型，一般来说就是神经网络了。 那么有没有可能正则和神经网络结合起来？Bingfeng et al. 2018[^1]给出了一些思路。 Problem def. and the baselines文章主要解决两个问题，intent detection和slot filling，也可以认为是classification和seq2seq的任务。这里的baselines主要有两个，正则表达式的尝试和Liu, Bing, and Ian Lane. 2016[^2]提出的attention-based rnn。思路可以见下图。 Approaches文章主要在三个方面进行尝试：input level、network level、output level。 Input level For intent detection, two possible approach: Append the embedding to all words (deprecated &lt;= 从结果上看会导致网络过于依赖正则) Append the embedding to the input of softmax layer(① in Fig(a) ) For slot filling, Embed and average the REtags into a vector fi for each word and append it to the corresponding word embedding wi (① in Fig(b) ) Network level For intent detection, For each intent label k, use different attention ak , which is used to generate the sentence embedding sk (② in Fig(a) ) Note that a RE can also indicate that a sentence does not express intent k (negative REs), it is also necessary to set another group of attention. $$ s_{k} = \\sum_i {\\alpha_{ki} h_i}, \\quad \\alpha_{ki} = \\frac{\\exp(h_i^T W_{a} c_k)}{\\sum_i {\\exp(h_i^T W_{a} c_k)}}$$ For slot filling, The mechanism introduced for intent detection is unsuitable for slot filling. A simple version of the two-side attention, where all the slot labels share the same set of positive and negative attention. (② in Fig(b) ) $$s_{pi} = \\sum_j {\\alpha_{pij} h_j}, \\quad \\alpha_{pij} = \\frac{\\exp(h_j^T W_{sp} h_i)}{\\sum_j {\\exp(h_j^T W_{sp} h_i)}}$$ Output levelLet $z_k$ be a 0-1 indicator of whether there is at least one matched RE that leads to target label $k$ (intent or slot label), the final logits of label k for a sentence (or a spefic word for slot filling) is:$$logit_k = logit_k’ + w_k z_k$$where $logit′_k$ is the logit produced by the original NN, and $w_k$ is a trainable weight indicating the overall confidence for REs that lead to target label $k$. Experimental Results Bibliography[^1]: Luo, Bingfeng, et al. Marrying up Regular Expressions with Neural Networks: A Case Study for Spoken Language Understanding.&nbsp;arXiv preprint arXiv:1805.05588&nbsp;(2018).[^2]: Liu, Bing, and Ian Lane. Attention-based recurrent neural network models for joint intent detection and slot filling. arXiv preprint arXiv:1609.01454 (2016).","link":"/posts/%5B2018.8.22%5DRegEx-with-NN/"},{"title":"intro-about-KG","text":"知识图谱之前其实陆陆续续地看了一些知识图谱相关的论文，对其的理解始终停留在比较浅显的层面，或者说一个非常不全面的状态，以至于，在实习中真的要尝试着手建立一个通用知识图谱的时候，却不知道如何下手。在实习期间打算写这篇综述来整理一下之前看到的一些琐碎的知识。 介绍和定义知识图谱（knowledge graph）最初是又Google提出的概念，目的是确定一种面相知识的存储结构。通常我们将数据存在一个关系型数据库或是key-value型数据库，数据和数据之间的关系是通过表来定义的；当数据的关系比较复杂而且比较灵活的时候，传统数据库的表达能力和响应速度都有较大的局限性，事实上，知识就是一种关系很强的数据，而且知识千变万化，难以用一些简单的规则来预先确定表。由此，知识图谱的想法也就很自然了，知识是一种关系性很强的数据，知识的存储结构必然应该是类似于图的。 通常来说，在实际应用中，我们可以简单的认为知识图谱就是一个多关系图，其中我们用实体（entity）来表示节点，用关系（relation）来表示边，因此知识的表达是通过一个三元组—（实体h-关系r-实体t）来实现。 TODO 知识图谱构建某种程度上来说，知识图谱最困难，最需要人力的部分就是知识图谱的构建。数据来源通常有以下几种 结构化数据这个比较理想，但是信息一定是不完善或是滞后的，可以作为初期的构建，后期还是要自己来维护。这里略过。 非结构化数据大数据时代更多的是这样的数据，需要一定的信息抽取、ner等nlp技术的支持，必要时需要人工进行审核。在构建类似的图谱过程当中，主要涉及以下几个方面的自然语言处理技术： 实体命名识别（Name Entity Recognition） 关系抽取（Relation Extraction） 实体统一（Entity Resolution） 指代消解（Coreference Resolution） 知识图谱的存储TODO： RDF和图数据库等 知识图谱的应用","link":"/posts/intro-about-KG/"},{"title":"Learning beyond datasets - Knowledge Graph Augmented Neural Networks for Natural language Processing 阅读笔记","text":"Knowledge Graph Augmented Neural Networks for Natural language Processing 阅读笔记摘要：机器学习的效果一般依赖于具体的训练数据。一些学习模型可以结合贝叶斯中的先验知识，但是这些模型不具备根据需要访问任何有组织的知识的能力。在这项工作[^1]中，我们以知识图谱（KG）的形式为NLP模型提供先验知识，使得模型取得更好的效果。我们的目标是开发一种深度学习模型，可以根据任务使用attention机制从知识图谱中提取相关的先验支持事实。为了减少attention空间，我们引入了基于卷积的模型来学习知识图谱实体和关系集的表示。提出的方法是高度可扩展的，并可应用于常用的NLP任务。使用这种方法，我们在实验中显示了文本分类性能的显着提高。我们还证明了，当深度学习模型使用知识图谱以辅助时，可以用较为少量的标记训练数据进行训练。 1. Introduction现在机器学习主要是针对特定任务、特定训练数据进行训练的模型。虽然transfer learning试图将学习从一个任务迁移到另一个任务，但在可扩展性方面有局限性，通常是具体地针对某个的任务。另一方面，我们知道人类具有一种内在的能力，可以根据需求从脑中获取所需的知识，并结合我们新学习的概念来解决问题。 这就引出了我们要在本文中讨论的问题：是否有可能设计一个学习模型，除了从培训数据集中学习外，还可以在预测时利用大量外在的知识？ 我们有一个基本的想法如上图，$\\mathcal{X}$是原本的输入，$\\mathcal{Y}$是输出。通过知识库补充、增强$\\mathcal{X}$，以得到$\\mathcal{X_w}$，将两这串联，获得$\\mathcal{X’}$作为新的输入。 这里我们知识库以知识图谱的形势呈现，主要将一个事实（fact）表现为三元组：(subject entity, relation, object entity)简记为(h,r,t)。其他关于知识图谱的介绍可以参考以前的笔记和相关的文献，这里不再赘述。 通常我们通过训练集来得到我们所需的模型，但是它往往缺乏world knowledge或者常识，结果往往会有偏差。例如 ：“特朗普慰问了得克萨斯州的飓风幸存者和他们的家人”，我们需要知道$\\langle \\text{特朗普},\\text{总统},\\text{美国} \\rangle$和$\\langle \\text{得克萨斯州},\\text{州},\\text{美国} \\rangle$才能判断这是一个政治事件。因此我们认为对于机器学习模型，除了代表ground-truth的用于训练的数据集以外，我们还可以从结构化的知识库获取相关知识，以提高整体性能。 因此我们提出了一个深度学习模型，可以根据需求从知识图谱中提取相关的事实，并将其也作为输入特征加以补充。特别的，当知识图谱非常大的时候，即其中的三元组数量非常大，以至于我们不可能逐一比较来提出相关信息时，我们提出了一种基于深度学习的搜寻机制，来大大提高搜寻速度，我们将在后文具体描述。 2. Knowledge graph representations实体和关系需要进行embedding以进行后续处理，目前有很多种知识图谱的表示方法，主要可以被分为：structure-based embedding，semantically-enriched embedding。 structure-based embedding其中包括经典的TransE以及其各种变体，它的基本假设就是$h + r = t$. 之前有一篇笔记主要介绍的就是这类知识图谱表示这里。 semantically-enriched embedding这些embedding技术学习表示KG的实体/关系及其语义信息。 神经张量网络（NTN）Socher et al [2011]是该领域的先驱工作，它使用平均词嵌入和基于张量的操作初始化实体向量。 最近涉及这个想法的作品是“联合对齐”Zhong et al。 [2015]和SSP Xiao et al。[2017]。 DKRL Xie et al [2016]是一种KG表示技术，它也保留了TransE模型简单结构的文本描述性。 预训过的word2vec被用来形成实体表示，通过一个卷积神经网络（CNN）来约束要保持的关系。 作者采用了DKRL，因为它强调了文本的语义描述，同时，它也继承了TransE的方法。因此我们能够通过$t = h + r$的关系来提取相关实体或关系。这大大减少了提取fact（三元组）的复杂度，因为实体关系的组合数远小于三元组的数量，因此能够让这个过程的速度更快。 3. The proposed model我们以一个文本分类模型为例，模型的参数为$\\Theta$，训练集为$x$，标签为$y$，我们需要最大化下列方程：$$\\max_{\\Theta}{P(y|x, \\Theta)}$$ 因此：$$\\Theta = \\arg\\max_{\\Theta} {\\log{P(y|x, \\Theta)}}$$这里，我们通过结合world knowledge特征$x_w$来增强监督学习过程。使用数据$x$检索world knowledge特征，使用单独的模型，其中$x_w = F(x, \\Theta^{(2)})$。 因此，我们修改的目标函数为：$$\\max_{\\Theta}{P(y|x, x_w, \\Theta^{(1)})}$$其中$\\Theta = {\\Theta^{(1)}, \\Theta^{(2)}}$。可以获得优化的参数：$$\\Theta = \\arg\\max_{\\Theta} {\\log{P(y|x,F(x, \\Theta^{(2)}), \\Theta^{(1)})}}$$后面的部分着重于函数F的表达，该函数使用数据x进行事实三重检索。在实验中，我们使用经过softmax的输入的LSTM Greff et al. [2015]编码作为P的形式。对于F，我们使用soft attention。 基于此，我们提出两种模型：A. 朴素模型；B. 基于卷积的实体和关系集群表示 A. 朴素模型前面解释过，KG的实体和关系使用DKRL进行编码。 令$e_i \\in \\mathbb{R}^m$代表实体i的编码，$r_j\\in \\mathbb R^m$代表KG中第j个关系。 输入文本以串联的单词向量$x =(x_1,x_2,…,x_T)$的形式首先使用LSTM Greff et al. [2015]模块如下，$$h_t = f(x_t, h_{t-1})$$以及$$o = \\frac{1}{T}\\sum_{t=1}^{T}{h_t}$$$h_t \\in \\mathbb{R}^n$是LSTM的隐藏状态，f是非线性函数，T是序列长度。 然后如下形成一个上下文向量，$$C = ReLU(o^T W)$$其中，$W\\in \\mathbb R^n \\times m$表示权重参数。设置两个同样的过程以形成两个独立的上下文向量，一个用于实体检索($C_E$)和一个用于关系检索($C_R$)。 由于在朴素模型中KG的事实三元组的数量是数以百万计的，所以我们分别对实体和关系空间产生注意力。 然后使用检索到的实体和关系形成事实。使用实体上下文向量的实体的注意力由下式给出：$$\\alpha_{e_i} = \\frac{\\exp{C_E^T{e_i}}}{\\sum_{j=0}^{|E|} \\exp{C_E^T{e_j}}}$$同理，关系的注意力：$$\\alpha_{r_i} = \\frac{\\exp{C_R^T{r_i}}}{\\sum_{j=0}^{|R|} \\exp{C_R^T{r_j}}}$$ 图2显示了实体/关系检索的示意图。在计算出最终的实体和关系向量之后，我们希望补充事实三元组。用于实验的KG技术是DKRL，其使用TransE模型假设($h + r = t$)。因此，使用主题实体(subject entity)和关系，我们将对象实体形成为$t = e + r$。 因此，检索的事实三元组是$\\mathcal F = [e，r，e + r]$，其中$F \\in \\mathbb R^{3m}$。 该检索到的事实信息与使用LSTM模块获得的输入x的上下文向量(C)一起连接。 最终分类标签$\\mathbb y$的计算如下，$$\\mathcal F’ = ReLU(\\mathcal F^T V) \\\\mathbb y = softmax([\\mathcal F’ : C]^T U)$$其中，$V∈\\mathbb R^{3m \\times u}$和$U\\in \\mathbb R^{2u\\times u}$是要学习的模型参数。 $\\mathbb y$是预测结果，用于计算交叉熵损失。我们尽量减少训练样本的平均损失，以便使用随机梯度下降来学习各种模型参数。最后的预测$\\mathbb y$现在包含了来自数据集特定信息和世界知识的信息，以帮助提高性能。在联合培训注意力机制的同时调整自己，以检索进行最终分类所需的相关事实。 A+. 预训练KG检索模型 朴素模型需要考虑整个实体/关系空间，这不是一个好的方法，因为我们观察到每个attention的容易饱和。在一起训练分类和检索模块时，模型往往会忽略KG部分，而梯度只通过分类模块进行传播。这在一定程度上是可以预料的，因为当前任务的大多数相关信息来自训练样本，只有背景辅助信息来自KG。经过几个训练阶段后，KG检索到的事实总是收敛到一个固定的向量。为了克服这个问题，我们试图单独预先训练KG检索部分。预训练的KG模型用于检索事实，然后与分类模块连接，同时，在联合训练时通过预先训练的模型可能会导致传递误差。我们推断，KG不会返回噪音，并且对于任务具有基本信息，因为单独的KG部件单独显示出显着的性能（News20为59％，SNLI为66％）。图3描述了整个训练过程。该程序解决了联合训练时KG检索部分中的梯度饱和问题。但是，attention机制必须覆盖大量实体/关系的关键问题依然存在。 B. 基于卷积的实体和关系集群表示在本节中，我们提出了一种机制来减少知识图谱中需要attention的大量实体/关系。 我们通过学习类似实体/关系向量的表示来减少attention空间。 为了聚类相似的实体/关系向量，我们使用k-means聚类，并在每个聚类中形成具有相同数量的实体/关系向量的$l$个聚类。然后使用CNN对每个cluster进行编码。k-means聚类的输出是一个实体/关系向量序列${e^T_1,e^T_2,…,e^T_q}$，其中$e_i \\in\\mathbb R^m$，每个聚类中的元素个数为$ q =⌈\\frac{| E |}{l}⌉$。对于每个cluster，这些矢量被堆叠形成$\\mathcal E$作为到CNN编码器的2D输入，其中$\\mathcal E\\in\\mathbb R^{m\\times q}$。 在寻找合适滤波器形状的实验中，我们发现使用2-D滤波器，该模型无法收敛。因此，我们推断，向量$e_i$中两个不同索引的潜在表示不应该被卷积修改。然后，我们采用一维卷积滤波器，只沿$\\mathcal E$列滑动，如图4所示。沿着y轴的步长是窗口长度k，卷积层的输出表示为：$$\\mathcal E’(i,j) = W^T[e_{i,j}, e_{i+1,j},…,e_{i+k-1, j}]^T$$其中，$\\mathcal E’(i,j)$是输出矩阵$\\mathcal E’$的第(i, j)个元素，$W\\in \\mathbb R^k$是卷积权重滤波器。为了减少参数空间，在卷积层之后放置一个pooling层，我们只沿y轴使用一维窗口，类似于上面提到的卷积核。我们使用了一个双层卷积网络，其步长k和最大池窗口n被调整以获得输出$\\mathcal E_i\\in \\mathbb R^m$，其中i是聚类索引。对于关系也进行类似的聚类过程，接着对聚类实体进行编码。这样，实体和关系空间都被缩减为包含更少的元素，每个cluster都有一个元素。在形成紧凑的实体空间$E$和关系空间$R$之后，我们采用了与之前相同的步骤来形成attention，但是现在，由于梯度有效地传递并且没有被过大的空间所阻塞，所以训练更有效。此外，由于卷积架构也同时得到训练，所以attention机制并没有像以前那样通过实体和关系的巨大空间来学习。 4. Conclusion实验表明，引入KG不仅降低了深度学习模型对训练集的依赖，还显著地提高了预测结果的准确度，在数据集不够的情况下效果更佳拔群。此外，本文的方法对world knowledge的处理、embedding的方法是高度可扩展，可以应用于各种NLP任务。 Bibliography笔记参考 https://blog.csdn.net/TgqDT3gGaMdkHasLZv/article/details/80118742 [^1]: Annervaz, K. M., Somnath Basu Roy Chowdhury, and Ambedkar Dukkipati. ‘Learning beyond datasets: Knowledge Graph Augmented Neural Networks for Natural language Processing.’ arXiv preprint arXiv:1802.05930 (2018).","link":"/posts/%5B2018.5.10%5DKnowledge-Graph-Augmented-Neural-Networks-for-NLP/"},{"title":"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记","text":"Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记摘要：现有关系分类方法依赖远程监督(distant supervision)，它假定提到实体对的句子都描述了这个实体对的关系。这样的方法一般在句子集合进行分类，不能识别关系和句子之间的映射，并且很大程度上受到标签噪音问题的影响。在这篇论文[^1]中，作者提出了一个从有噪声多数据的句子层次的关系分类模型。该模型有两个模块：一个实例选择器和一个关系分类器。实例选择器通过增强学习选择高质量的句子，并将选定的句子输入到关系分类器中，关系分类器进行句子级预测，并向实例选择器提供奖励。这两个模块共同训练以优化实例选择和关系分类过程。实验结果表明，我们的模型可以有效地处理数据中的噪音，并在句子级别获得更好的关系分类性能。 1. Introduction关系抽取是nlp领域中一个非常重要的任务，尤其在知识图谱构建等任务上。相关的工作可以参考我之前写的笔记，主要还是分为两种：传统的手工特征方法，和深度神经网络。 为了获得更大量的训练数据集，半监督、远程监督，甚至无监督模型被提出。半监督模型对一开始的少量数据要求较高，容易产生较大的偏差；无监督学习目前还没有比较成熟的解决方案。 这里主要提一下远程监督模型。远程监督模型有一个很强的假设：如果两个实体在给定的知识库中有一种关系，则包含这两个实体的所有句子都会提及该关系，实际上当然会有很大问题，会带来很多标注错误的信息。有一些解决方法就是转化为bag-level的关系标注。一个bag包含提及相同实体对的句子，但有可能描述不同的关系，如下图。 不过实际上还是会有问题：1. 不能处理句子级别的关系分类；2. 如果一个bag里的句子都不含知识库中的关系，即都是噪声，这样对性能会有很大影响。 为解决上述的两个缺陷，作者提出了实例选取器，并将其定义为一个强化学习任务。它有两个特征：1. 句子选择是一个反复试错的过程，需要从分类器中得到选取句子质量的反馈；2. 反馈在挑选结束后得到，因此是滞后的。这两点非常满足强化学习的特点。 2. Methodology作者提出了一个新的关系分类框架，它可以从噪音数据中选择正确的句子用于更好的关系分类。 所提出的框架可以从清理的数据中预测句子级别的关系，而不是在bag级别。句子级别的预测对需要理解句子的任务（如QA和语义分析）更加友好。我们的框架包含两个关键模块：从噪声数据中选择正确句子的实例选择器，以及预测关系并使用清理数据更新其参数的关系分类器。 这两个模块相互作用共同训练。 Problem definition我们定义两个子任务：实例选择和一个关系分类。 我们定义实例选择问题：给定一组(sentence, relation label)，如$X = {(x_1,r_1),(x_2,r_2),…,(x_n,r_n)}$，其中$x_i$是与两个实体$(e_{1i},e_{2i})$相关的句子，$r_i$是由远程监督产生的关系标签。我们的目标是确定哪个句子真正描述了这种关系，并且应该被选作训练实例。 关系分类问题表述如下：给定一个句子$x_i$和所提到的实体对$(h_i,t_i)$，目标是预测$x_i$中的语义关系$r_i$。模型估计概率：$p_{\\Phi}(r_i | x_i,h_i,t_i)$。 Overview Instance Selector这里的Instance Selector被当作强化学习问题来处理，因此policy的更新有滞后性，作者为了加快更新速度，把所有句子实例$X={x_1, …,x_n}$分为N个bag $B = {B^1, B^2, …, B^N}$，每一个$B^k$都包含同一个实体对，且标注关系都为$r^k$。 State：编码以下信息：1) 当前句子的向量表示，从CNN的非线性层获得用于关系分类的当前句子的向量表示; 2) 被选的句子集合的表示，它是所有选择句子的向量表示的平均值; 3) 一个句子中实体对的向量表示，从预先训练的知识图谱embedding中获得。 Action：定义action $a_i \\in {0,1}$ 表示是否选取第i个句子，$a_i$的取值由policy function得到，定义如下：$$\\pi_{\\Theta}(s_i,a_i) = a_i \\sigma(W * F(s_i) + b) + (1 - a_i)(1 - \\sigma(W * F(s_i)+b))$$ Reward：reward function是所选句子效用的指标。对于某些bag $B = {x_1,… ,x_{| B |}}$，我们为每个句子选择一个action，以确定是否应该选择当前句子。我们假设该模型在完成所有选择时具有最终奖励。 因此，我们只在终端状态$s_{| B | +1}$收到延迟奖励，其他的奖励为零。 reward是基于CNN的分类反馈得到。 Relation Classifier这里用的分类模型比较简单，一个经典的CNN。 输入层可以被分为两部分： word embedding position embedding：两个固定长度的向量，表示该词分别到两个实体店距离。 3. Analysis作者用的数据集是New York Times，作者随机挑选300句子人工标注，再对其做评测。对比的baseline包括CNN、CNN+Max(每个bag选一个正确的句子)、CNN+ATT。最后结果上看出，本文的CNN+RL模型取得了最好的结果，这表明强化学习对于该任务是行之有效的；并且句子层次的模型在评测中普遍好于bag模型。 强化学习是目前比较火热的技术，它在nlp相关任务的应用仍在探索中，但是最近的论文确实有很多都在讨论它，并且也做到了不错的效果。希望从这篇文章为入口，开始了解强化学习及其在nlp上的应用。 这篇文章中，强化学习主要是用于选择噪声数据，用以减少数据集的偏差等。但是我们相信强化学习能做的不仅如此，事实上最近的顶会还是有一些相关的工作的，可以放到以后再看。 Bibliography本文的c++实现：https://github.com/JuneFeng/RelationClassification-RL [^1]: Feng, J., Huang, M., Zhao, L., Yang, Y., &amp; Zhu, X. (2018). Reinforcement Learning for Relation Classification from Noisy Data.","link":"/posts/%5B2018.4.14%5DReinforcement-Learning-for-Relation-Classification-from-Noisy-Data/"},{"title":"A Neural Model for Joint Event Detection and Summarization 阅读笔记","text":"A Neural Model for Joint Event Detection and Summarization 阅读笔记摘要：Twitter事件检测旨在识别推文流中的first stories。一般认为由两个子任务组成。首先，过滤掉普通的或不相关的推文。其次，推文会被自动分类到event cluster中。传统上，尽管这两个任务之间存在相互依赖关系，它们仍被单独处理，并通过pipeline整合。另外，还有一个相关的任务是摘要，即提取能够代表event cluster的简洁摘要。这里和上个暑假看的Wang, Z.[^2]的工作比较相似。 在本文[^1]中，我们构建了一个joint model来筛选、聚类和摘要推文中的event。特别的，我们利用深度表示学习来对推文进行矢量化处理。Neural stacking model用于整合不同子任务的pipeline，并更好地共享前后参数。实验表明，我们提出的neural joint model比pipeline更有效。 1. Introduction有文献证明了推特、微博这类体裁比起传统的媒体，对新闻事件有更快的反应速度，因此今年对于推特的事件监测也是今年的热点之一，引起了广泛关注。我们在本文主要检测一些典型的事件类别，比如地震、DDos攻击等。我们提出了一个神经网络模型，该模型监视特定事件类别的推特流，共同检测并摘要该类别下的新闻事件。 整体的架构如上图所示，给定一个推特流，我们的模型考虑三个子任务：推文过滤，事件聚类和事件摘要。 典型的推特事件检测模型的核心部分是聚类，其中包括增量聚类和locality sensitive hashing。主要的想法是将同一主题的推文进行分组，以便在新推文不属于现有主题类的情况下检测到新主题。这样的聚类算法通常依赖于推特内容的特征，如TFIDF，用于测量推文之间的相似度。 第二个子任务是摘要，它并不直接涉及event detection，但仍然与之高度相关，因为检测到的事件群可能比较大并且包含不同程度信息的推文。 从系统功能的角度来说，对于推特事件检测系统，摘要是十分必要的，因为我们没办法直接读取事件群，只有将其抽取摘要，并将事件摘要作为输出，才能够为我们所用。 此外，由于大部分推文流包含普通或不相关的信息，推文过滤是我们考虑的第三个子任务。 我们的目标是根据其与潜在新事件的相关性对传入推文进行分类，以便只保留信息性推文。 过滤可以在聚类之前或之后进行。 在本文中，我们在聚类之前执行这项任务。 这三个子任务形成了一个三阶段pipeline（过滤→聚类→摘要），其中各个阶段是密切相关的。 例如，完整描述事件的推文应该在相关性过滤和抽象摘要步骤中得到高分。 另外，更好地理解推文对于相关性过滤和事件聚类都有帮助。 受此启发，我们考虑使用一个joint model进行筛选、聚类和摘要。 神经网络在近年也在类似任务上展示了良好的表现。我们使用两种联合建模策略。首先，我们以推文的语义表示作为关键连接因素，通过参数共享整合三项任务。其次，我们将neural stacking应用于pipeline，将前一个子模型的隐藏神经层作为其后继子模型的附加输入特征，并将后继的误差传播给前者，使得在培训期间，让前后子模型之间的信息得到更好的共享。 2. Model我们系统的输入是一个推特流，实时输出事件报告。三个主要的子任务定义如下： 事件筛选：我们将流中的每条推文分类为与相关事件相关或不相关的事件。由于我们的目标是仅检测某些类型的事件（即地震和DDOS攻击事件），因此我们使用相应的一组关键字来过滤推文流作为预处理步骤。相关分类在预处理步骤之后执行，因为并非所有包含关键字的推文都是相关的。 事件聚类：我们在事件检测后对推文进行增量聚类。给定一个提到事件的推文，其任务是确定它是否存在于现有的事件集群中、是否是一个新的事件。这个任务的关键是推文之间的相似度计算。 事件摘要：当一个事件集群足够大时，我们通过提取前n个包含最多信息的推文来创建相应事件的报告。这个子任务可以被看作是一个多文档摘要任务。 模型的整体构架如上图，H是推文的embedding，Hd是事件筛选步骤的隐藏层，Hc是聚类的隐藏层，Ps是摘要的输出。我们可以看到，每一个子任务都充分利用了前面的信息，使得各个子模型项目补充辅助，让结果更理想。 结果表明，与独立的pipeline相比，这种neural stacking方法可以产生更好的子模型。请注意，虽然计算两个tweets之间的相似性的过程是用LSTM模型监督的，但聚类算法是一种无监督的在线聚类算法。 2.1 Shared Tweet Representation我们使用标准的LSTM模型来学习不同任务之间的推文表示。 假设$X =(w_1,w_2,…,w_n)$是推文，其中$n$是推文长度，$w_i$是第i个标记。 我们使用$w_i$的word embedding将每个$w_i$转换为实值向量$x_i$，通过查找预先训练的word embedding表D获得。我们使用skip-gram算法来训练embedding。LSTM用以生成隐藏序列$(h_1,h_2,…,h_n)$。 在每个步骤t，基于当前向量$x_t$和前一个向量$h_{t-1}$和$h_t = LSTM(x_t，h_{t-1})$计算LSTM模型的隐藏向量$h_t$。 初始状态和所有LSTM参数随机初始化并在训练过程中调整。 我们使用$H = h_n$作为X的共享表示。 2.2 Joint ModelEvent mention detection事件提及检测是一个二元分类任务，使用多层感知器进行描述。给出输入向量H，隐含层用于激发一组高级特征：$$H_d = \\sigma(W_d^h H + b_d^h)$$$H_d$被用于softmax输出层的输入：$$P_d = softmax(W_dH_d + B_d)$$这里$W_d^h, b_d^b, W_d$都是模型参数。$P_d$ 长度为2，$P_d(0)$表示推文X相关的概率，$P_d(1)$表示不相关的概率。 Event clustering 我们使用基于流的聚类算法将传入的推文分为不同的事件组，每个事件组表示一个特定的事件。该算法随着流中的每个传入推文增长式地工作，计算新推文与现有事件群中的每条推文之间的相似度分数。新传入的推文与每个事件群中最相似的对应文件之间的相似度用于衡量新推文与事件群集之间的相似度。使用阈值$\\mu - 3\\cdot \\sigma$来检测新推文是否属于现有聚类，其中μ是所有先前相似度分数的均值，σ是标准偏差。如果推特和所有现有群集之间的相似度低于阈值，则建立新的事件群集。否则，将推文添加到最相似的现有事件群集中。为了计算两条推文$X_i$和$X_j$之间的相似度，我们使用了一个连体网络，它采用共享表示向量$H_i$和$H_j$，并通过参数化计算相似概率得分$P_c$:$$H_c = \\sigma(W_c^h(H_i \\oplus H_j)+ b_c^h) \\P_c = softmax(W_cH_c + B_c)$$⊕表示向量级联。 $W_c^h，b_b^c,W_c,b_c$是模型参数。 为了更好地整合事件提及检测和事件聚类，我们还将$X_i$和$X_j$的隐藏特征矢量$H_d$馈送到Siamese网络，从而$$H_c = \\sigma(W_c^h(H_i \\oplus H_j \\oplus H_{d_i} \\oplus H_{d_j})+ b_c^h)$$ Event summarization 为了生成事件集群的摘要，我们使用概率分数$P_s$对集群中的所有推文进行排序。 对于集群中的每个推特X，使用多层感知器来估计$P_s$，其中输入是$H \\oplus \\bar{H_c^h}$。 这里的矢量$\\bar{H_c^h}$是推文X与同一集群中所有其他推特之间的$H_c^h$之和。 它有两个用途。首先，$H_c^h$提供有关整个群集的信息，这对于更好地确定群集中给定推文的相关性很有用。 其次，$H_c^h$还把聚类和摘要联系了起来，从而强化了信息共享。$$P_s = softmax(W_sH_s + B_s)$$where$$H_s = \\sigma(W_s^h(H\\oplus \\bar{H_c^h})+ b_s^h)$$$W_s^h, b_s^b, Ws$是模型的参数 2.3 Training我们的培训目标是尽量减少这三项任务中标注的标签和预测标签之间的cross-entropy loss。 我们应用在线培训，使用Adagrad调整模型参数。为了避免过拟合，对word embedding使用0.2点dropout。隐藏层$H_d, H_c,H_s$的大小都设置为32。我们使用Skip-gram算法训练word embedding，并在训练期间对它们进行微调。 Word embedding的大小是128。 3. Experiment见paper原文 4. Conclusion文献[^1]提出了一个joint model，通过使用全局共享表示和不同子任务之间的堆叠来共同检测、聚类和摘要事件。 实验表明，我们提出的joint model比pipeline模型更有效。该联合神经系统优于采用离散或神经网络进行新闻事件检测和摘要的最新baseline。 Bibliography代码开源：https://github.com/wangzq870305/joint_event_detection [^1]: Wang, Z., &amp; Zhang, Y. (2017, August). A neural model for joint event detection and summarization. In Proceedings of the 26th International Joint Conference on Artificial Intelligence (pp. 4158-4164). AAAI Press.[^2]: Wang, Z., Shou, L., Chen, K., Chen, G., &amp; Mehrotra, S. (2015). On summarization and timeline generation for evolutionary tweet streams. IEEE Transactions on Knowledge and Data Engineering, 27(5), 1301-1315.","link":"/posts/%5B2018.4.4%5DA-Neural-Model-for-Joint-Event-Detection-and-Summarization/"},{"title":"Event detection 的几个神经网络模型","text":"Event detection 的几个神经网络模型摘要： 根据ace的定义，事件被分为 trigger word 和 attributes，因此 event detection 也可以被认为是 trigger word detection。目前基于神经网络的方法的思路基本大同小异，本文挑选并阐述3篇paper的主要内容，并比较其特点。 1. Dual CNN这篇[^1]主要是对通常的CNN的改进，增加了一层语义层用以感知上下文信息。 整个pipline可以总结为如下图： Text Processing: 数据清洗、分词等，便于后续处理； Word Vector Initialisation: 初始化词向量，包括加载 pre-trained word embedding 等； Concept Extraction: 与2.并行运行，这里利用外部工具实现实体的语意概念； Concept Vector Initialisation: 将实体和实体相关的概念向量化； Dual-CNN Training: 这一步利用我们提出的 Dual-CNN 训练； Dual-CNN我们知道CNN可以用来作为分类器，因此也可以被构造为一个事件检测模型，并能够分出类别。在这个模型中，我们增加一层语意层。一般从正常逻辑出发，我们可以增加一个channel来存放entity related embedding，就像我们图像的多个channel一样；但是这要求实体和原来的句子完全对齐，因此作者用两个CNN并行训练。 事实上，这篇文章提出的embedding方法并不能跟原来的句子对齐。我们有一个句子 $D = $ ‘Obama attends vigil for Boston Marathon bombing victims.’；分词为 $T_w = $ [‘obama’, ‘attends’, ‘vigil’, ‘for’, ‘boston’, ‘marathon’, ‘bombing’, ‘victims’]；而语意分词将分为$T_s = $ [‘obama’, ‘politician’, ‘none’, ‘none’, ‘none’, ‘boston’, ‘location’, ‘none’, ‘none’, ‘none’]。我们可以看到语意分词采用了 entity-type 的方法，这导致了$T_w$和$T_s$长度并不相同 ，因此无法把他们并为两个channels。（其实这里我感觉可以把entity和type分别embedding之后级联起来，这样就能够对齐了，不知道这样可不可行？） 最后的结果显示，它能够较好地检测和识别事件类别，对比CNN有一些提升；但是对于颗粒度较细的事件反而有所下降。但是这些结果都好于传统的机器学习方法。 2. convolution BiLSTM文献[^2]其实之前就看过，详细写在2018.1.29的笔记里，这里再简单提一下，模型如下。 简单来说，就是在通常的biLSTM模型下，并行地训练一个CNN模型，其输出和biLSTM的输出的向量进行连接，Output layer接Softmax输出标签的概率分布。创新点在于引入了CNN捕获局部语意信息，也获得不错的效果。本模型也适用于其他sequence labeling任务。 3. BiLSTM + CNN这篇文章[^3]和文章[^2]的思路也很相似，主要的想法是BiLSTM对文本的语意进行编码，后面串联CNN来捕获局部结构信息。 4. Conclusion如果认为是一个文本分类任务，CNN能够很好的完成任务，而且由于它本身的特性训练速度比较快；另一方面可以用RNN来做数据标注任务，仅标注触发词；此外可以利用好biLSTM能够处理长距离前后文信息、CNN着重局部信息的关系等特性，构造不同的变体，对于实际任务可能也有不错的效果。 从结果上来看实际上各种变体效果差距并不大，对特定种类特定体裁可能会有较大的差别；可能更重要的可能是如何构造特征（除了word embedding 之外，还可以考虑entity embedding？entity type？还有词性的embedding？） Bibliographies[^1]: Burel, G., Saif, H., Fernandez, M., &amp; Alani, H. (2017). On semantics and deep learning for event detection in crisis situations. [^2]: Zeng, Y., Yang, H., Feng, Y., Wang, Z., &amp; Zhao, D. (2016). A convolution BiLSTM neural network model for Chinese event extraction. In Natural Language Understanding and Intelligent Applications (pp. 275-287). Springer, Cham.[^3]: Feng, X., Huang, L., Tang, D., Ji, H., Qin, B., &amp; Liu, T. (2016). A language-independent neural network for event detection. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) (Vol. 2, pp. 66-71).","link":"/posts/%5B2018.3.20%5DEvent-detection/"},{"title":"Several models for knowledge graph representing and completing 几个知识图谱模型","text":"2018.3.10Several models for knowledge graph representing and completing摘要：上次看到的ConMask在开放领域knowledge graph completion有着不错的表现，这次我们不考虑开放领域，介绍几个经典的模型。 1. Series of Trans1.1 TransETransE [^1] 可能是最为常用也最为基础的方法是一种基于强化学习(RL)的模型. 它有一个简单实用的假设：$$h+r = t$$其中h是head entity的向量，t是tail entity的向量，r是关系向量。 TransE定义了loss function：$$\\mathcal{L(T)} = \\sum_{&lt;h,r,t&gt;\\in T} [\\gamma + E(&lt;h,r,t&gt;) - E(&lt;h’,r’,t’&gt;)]+$$其中 $T$ 代表一个三元组的集合；$E(&lt;h,r,t&gt;) = ||h+r-t||{L_n}$是energy function；$&lt;h,r,t&gt;$是G中的一个三元组；$&lt;h’,r’,t’&gt;$代表一个不存在于 $T$ 的三元组，通过随机替换一部分$&lt;h,r,t&gt;$来得到；$\\gamma$ 表示边际距离 算法的核心是令正例的 h+r-t 趋近于 0，而负例的 h+r-t 趋近于无穷大。整个 TransE 模型的训练过程比较简单，首先对头尾节点以及关系进行初始化，然后每对一个正例取一个负例样本，然后利用 hinge loss function 尽可能使正例和负例分开，最后采用 SGD(Stochastic Gradient Descent) 方法更新参数。 由TransE又衍生出了许多模型。 1.2 TransH虽然 TransE 模型训练速度快、易于实现，但是它不能够解决多对一和一对多关系的问题。为了解决这个问题并仍然维持较低的复杂度，TransH [^2] 不再严格要求 h+r-l=0，而是只需要保证头结点和尾节点在关系平面上的投影在一条直线上，因此能够得到图中头结点向量正确的表示。 论文另一个亮点是设计了一种负类抽样的方法，即一对多的时候，给head更多的抽样概率， 同样的多对一的时候，给tail更多抽样概率。 1.3 TransR/CTransTransE和TransH都假设了实体和关系都在同一个空间内，但由于实体和关系本身的不同，简单地认为它们embedding在同一个空间内是不充分的。因此，TransR[^3]对于每一个r，我们设置mapping矩阵$M_r$，使得$$h_{\\perp} + r \\simeq t_{\\perp} \\quad \\text{where } h_{\\perp} = M_{r}\\cdot h, t_{\\perp} = M_{r} \\cdot t$$后面的方法就和TransE较为类似了。 1.4 TransD实际上TransD[^4]是对TransR/CTrans的改进。TransR有以下缺点： 对于每个r，所有的实体都使用同一个mapping矩阵$M_{r}$，但是实际上对应于一个r的实体有不同的种类、特征，这么做会有一些问题； mapping是entity和relation之间的交互过程，mapping矩阵仅由关系决定是不合理的； 矩阵与向量的运算的计算比较复杂，如果在一个Knowledge graph里有较多的relation，那么就会有大量的参数，以及较高的复杂度，因此导致计算量过大，无法应用到大规模knowledge graph。 对于任意一个entity或relation，TransD定义两个向量，第一个表示entity或relation的含义，另一个用于把entity投影到relation空间（或者说用于构造mapping矩阵）。因此对于每个entity-relation pair，都有一个动态生成的唯一的mapping矩阵。此外，上述过程没有用到矩阵向量运算，而用向量的运算代替了。 TransD大大降低计算复杂度，但仍然保持不错的效果。 1.5 TransA前面的模型都是基于欧式距离计算，也就是认为每一维的重要性是相同的。但实际上，有一些维度能较好的区分不同的entity和relation，但也有许多不包含什么有效信息，因此甚至可以被认为是噪音，因此不同维度的重要性显然是不同的。 TransA[^5] 模型通过引入加权矩阵，赋给每一维度权重。 结果如下图，欧式距离每一维都是同等重要的，体现为圆形；而TransA体现为椭圆形，显然更符合数据的分布。 2. ProjEProjE[^6]的作者就是Open-World Knowledge Graph Completion的作者，两篇文章分别被AAAI2017和AAAI2018收录，可以认为是KGC任务的 state-of-the-art。这里提一下ProjE的思路。 2.1 Model Architecture给出&lt;h, r, ?&gt;，已知h、r，要求预测 ? 。通常的做法就是把所有的候选entity都拿来打分，得分最高的就是预测的结果。为了得到这一系列打分，首先我们通过一个运算符来合并h和r，得到一个向量，然后我们把所有候选entity投影到这个向量上，随后运算得到分数。 现有的模型（包括上面提到的一系列trans）往往通过mapping矩阵来合并entity和relation，这里作者也是这么做，但是他认为目前还不需要考虑各个维度之间的相互作用，因此这个mapping矩阵应该是一个对角矩阵。所以这个合并操作可以被定义为：$$e \\oplus r = D_e e + D_r r + b_c$$其中$D_e$和$D_r$就是两个对角矩阵。 由此，我们可以定义embedding映射函数：$$h(e, r) = g(W^c f(e \\oplus r) + b_r )$$其中f和g是激活函数(activation function)，我们在后面定义；$W^c$是候选entity构成的矩阵。因此h就是对所有候选entity的打分矩阵。 上图是ProjE的结构，包括两部分神经网络层，其中上半部分是合并操作，即$e \\oplus r$；下半部分是映射函数，或者说打分函数，即$h(e,r)$。 2.2 Loss function我们这里分析方便只看pointwise loss function：$$\\mathcal{L}(e, r, y) = - \\sum_{i\\in{i|y_i=1} } {log(h(e,r)i)} - \\sum{m} {\\mathbb{E}_{j \\sim P_y} log(h(e,r)_j)}$$其中$y$是一个布尔向量，1为正例，0为反例；m个反例。pointwise ProjE 可以被看作是多类分类问题，所以我们选取 f 和 g 分别为 sigmoid 和 tanh 函数。 代码实现 https://github.com/bxshi/ProjE Bibliographies笔记参考： http://www.infosec-wiki.com/?p=175755 https://www.jiqizhixin.com/articles/2017-11-03-5 [^1]: Bordes, A., Usunier, N., Garcia-Duran, A., Weston, J., &amp; Yakhnenko, O. (2013). Translating embeddings for modeling multi-relational data. In Advances in neural information processing systems (pp. 2787-2795).[^2]: Wang, Z., Zhang, J., Feng, J., &amp; Chen, Z. (2014, July). Knowledge Graph Embedding by Translating on Hyperplanes. In AAAI (Vol. 14, pp. 1112-1119). [^3]: Lin, Y., Liu, Z., Sun, M., Liu, Y., &amp; Zhu, X. (2015, January). Learning entity and relation embeddings for knowledge graph completion. In AAAI (Vol. 15, pp. 2181-2187).[^4]: Ji, G., He, S., Xu, L., Liu, K., &amp; Zhao, J. (2015). Knowledge graph embedding via dynamic mapping matrix. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) (Vol. 1, pp. 687-696).[^5]: Xiao, H., Huang, M., Hao, Y., &amp; Zhu, X. (2015). TransA: An adaptive approach for knowledge graph embedding. arXiv preprint arXiv:1509.05490.[^6]: Shi, B., &amp; Weninger, T. (2017, February). ProjE: Embedding Projection for Knowledge Graph Completion. In AAAI (Vol. 17, pp. 1236-1242).","link":"/posts/%5B2018.3.10%5DSeveral-models-for-kenowledge-graoh-representing-and-completing/"},{"title":"Open-World Knowledge Graph Completion 笔记","text":"Open-World Knowledge Graph Completion摘要：[1]文首先讨论了Closed-World KGC，它无法处理从 KG 外部加入的新实体，并严重依赖已有KG连接的，不能对弱连接有好的预测。为此定义了 Open-World KGC，可以接收 新的实体并链接到 KG；并依此提出了ConMask模型，在给定关系和实体名、实体描述的前提下，利用attention机制通过关系定位实体描述中最相关的词，再以这些词和实体得到要链接的实体。 1. Introduction知识图谱（KG）是一种信息网络，它用三元组 $(h,r,t)$ 来表示知识（h: head entity, t: tail entity, r: relation），目前比较出名的KG有 DBPedia，ConceptNet 等，目前的大多数KG都有噪音且不完整，比如基于Wikipedia的DBPedia有460万个实体，但其中一半实体拥有少于5个的关系。 这说明了大部分的知识图谱仍然是非常不完善的，我们必须从一开始就要考虑系统的修改、补充完善的可能性。这项任务被定义为Knowledge Graph Completion (KGC)。 Closed-World KGC给定一个不完整的KG $G=(E,R,T)$ 其中 $E,R,T$ 分别表示实体集，关系集以及三元组集，Closed-World KGC的任务就是通过找到一系列丢失的三元组 $ T’ = { \\langle h,r,t \\rangle|h \\in E, r \\in R, t \\in E, \\langle h,r,t \\rangle \\notin T }$ 来补充现有的 $G$. 一个很重要的地方在于，Closed-World KGC 假定了新的实体、关系都被原有的 $G$ 包含，对于不在 $G$ 中的实体则一筹莫展。 目前的Closed-World KGC方法很多往往使用TranE或者低维特征表示模型，前者的核心思想就是 $h+r=t$ ，后者则指 Embedding 等。 该方法仅对固定的或者缓慢更新的KG有效，对于快速变更的KG则效果一般。 Open-World KGC给定一个不完整的KG $G=(E,R,T)$ 其中 $E,R,T$ 分别表示实体集，关系集以及三元组集，Open-World KGC 的任务就是找到 $G$ 中没有的三元组集，$T’ ={&lt;h,r,t&gt;|h\\in E^i,r\\in R, t\\in E^i,&lt;h,r,t&gt;\\notin T}$ 其中 $E^i$ 是G的实体超集。 Closed-World方法就是根据知识图谱的拓扑结构更新一个随机的向量作为实体和关系的embedding，但对于不在网络中的实体，这个方法就失效了，这个时候就需要用别的特征来代替这个用网络拓扑结构得到的特征。 一般直觉就是用实体的描述（entity description），根据实体的描述来得到特征，但从非结构化文本中学习向量表示比在网络的拓扑结构中要难得多，原因如下： 在Closed-world KGC模型中，每个实体都有一个embedding (从与它相连的实体上学得的)，但Open-World KGC模型则需要从实体描述的word embedding中得到entity embedding。而无论实体之间的联系情况是什么，word embedding的更新都会导致有相同词的entities的更新。 因为使用了非结构化文本，所以Open-World KGC模型可能会引入噪音或者冗余信息。 2. Closed-World KGC在 Closed-World KGC 中，最为常用也最为基础的方法是一种给予强化学习(RL)的模型，被称为TransE [2]. 它有一个简单实用的假设：$$h+r = t$$其中h是head entity的向量，t是tail entity的向量，r是关系向量。 TransE定义了loss function：$$\\mathcal{L(T)} = \\sum_{&lt;h,r,t&gt;\\in T} [\\gamma + E(\\langle h,r,t \\rangle) - E(\\langle h’,r’,t’ \\rangle)]+$$其中 $T$ 代表一个三元组的集合；$E(\\langle h,r,t \\rangle) = ||h+r-t||{L_n}$是energy function；$\\langle h,r,t \\rangle$是G中的一个三元组；$h’,\\langle r’,t’ \\rangle$代表一个不存在于 $T$ 的三元组，通过随机替换一部分$\\langle h,r,t \\rangle$来得到。 这里还略去了很多TransE的变体等其他模型，但它们都是基于Closed-World KGC来做的。 3. ConMask for Open-World KGC首先通过一个例子来说明： 任务：填补三元组 $\\langle \\text{Ameen Sayani, residence, ?}\\rangle$，其中KG中并没有Ameen Sayani这个实体。 描述：“… Ameen Sayani was introduced to All India Radio, Bombay, by his brother Hamid Sayani. Ameen participated in English programmes there for ten years …” ， 目标预测实体：Bombay (or Mumbai) 为了找到Ameen Sayani的住址，在处理这个任务的过程中，我们不会从头看到尾，而是找到相关的关键词比如家庭或工作相关的词。这里，我们发现Ameen Sayani的工作地点All India Radio在Bombay，因此我们推测Ameen Sayani也住在Bombay（Bombay就是现在的Mumbai）。 这个过程也可以被归纳为： 定位与该任务相关的信息。 根据上下文和相关文本推断。 根据相关文本推出正确目标实体。 仿照这个过程，ConMask的工作方式被设计为： Relationship-dependent content masking – 标记那些与任务相关的词语。 Target fusion – 从相关文本中抽取目标实体的embedding。 Target entity resolution – 通过计算KG中的候选目标实体，2中抽取出的实体embedding以及其它文本特征之间的相似度来选定目标实体。 ConMask模型总体结构如上，ConMask通过选择与给定关系相关的词来避免引入不相关的和有噪音的词。对于相关的文本，ConMask通过全连接卷积神经网络（FCN）来提取word-embedding。最后它将提取的embedding于KG中存在的实体进行比较，从而获得一系列目标实体。 3.1 Relationship-dependent content maskingConMask根据给定的关系预处理输入文本，来选择一些相关的小片段，从而屏蔽掉无关文本。content-masking这一灵感来源于基于attention机制的RNN网络[3]，关于attention之前的笔记也有学习过。 基于相似度得到选择最相关的词，具体公式如下：$$\\tau(\\phi(e), \\psi(r)) = W_{\\phi(e)} \\circ f_w(W_{\\phi(e)}, W_{\\psi(r)})$$其中 $e$ 是一个实体，$r$ 是某个关系, $\\phi$ 是description function并返回一个向量用于表示对一个实体或关系的描述，$\\psi$ 是name mapping function并返回一个向量用于表示一个实体或关系的名字， $ W_{\\phi{(e)}} \\in \\mathbb{R}^{|\\phi(r)|\\times k} $ 是一个描述矩阵每一行表示一个k维的描述中的word-embedding， $W_{\\phi{(e)}} \\in \\mathbb{R}^{|\\phi(r)|\\times k} $ 是一个名字矩阵每一行表示一个k维的实体名字word-embedding，$\\circ$ 是row-wise product，$f_w$ 用于计算的每一行的屏蔽比重。 作者给了一个简单的$f_w$ ，Maximal Word-Relationship Weights(MWRW)，就是计算实体描述中每个词向量与关系名称的每个词向量的最大cos相似度:$$f_w^{MWRW}(W_{\\phi(e)}, W_{\\psi(r)}){[i]} = max_j(\\frac{\\sum_m^k{W{\\phi(e)[i,m]} W_{\\psi(r)[j,m]}}}{\\sqrt{\\sum_m^k{W^2_{\\phi(e)[i,m]}}}\\sqrt{\\sum_m^k{W^2_{\\psi(e)[j,m]}}}})$$这个公式会给与给定关系无关的词更小的权重，与关系语义接近的词更大的权重，但权重最高的词一般不是目标实体，如下图所示，给定关系spouse，得到最大权重的是married，虽然married与spouse在语义上接近，但它并不是目标实体，因此作者称这种有着最大MWRW权重的词为指示词（indicator word），因为正确的词一般就在该词附近，在下图例子中可以发现目标实体barack obama就在married后面。 为了给目标实体word正确的权重，作者改进了这个公式，具体公式如下，这个公式就是每个词的权重不会小于之前 $k_m$ 称为 Maximal Context-Relationship Weights (MCRW)：$$f_w^{MCRW}(W_{\\phi(e)}, W_{\\psi(r)}){[i]} = max(f_w^{MWRW}(W{\\phi(e)}, W_{\\psi(r)})_{[i-k_m:i]})$$ 3.2 Target Fusion这一步骤用于输出基于词的实体embedding，这个过程记为$\\xi$，使用Conetent Masking $\\tau$ 的输出。它使用全连接卷积网络，其结构如下： Semantic Averaging 我们可以对所有实体进行embedding，但是这会产生大量的参数，使计算变得非常复杂。事实上，因为Target fusion函数用于抽取，所以对不需要抽取的实体名字使用target fusion就会显得很奇怪也很没有必要。 这里作者提出了一个简单的语义平均法来计算这些实体的embedding：$\\eta(W) = \\frac{1}{k_l}\\sum_i^{k_i}W_i$ 3.3 Loss function为了加速训练，我们参考 list-wise ranking loss function (Shi and Weninger 2017)，并设计 partial list-wise ranking loss function，拥有正负目标采样。正样本就是训练集的标注内容，记为$E^+$；负样本就是替换正样本的head entity或tail entity所得到的，记为$E^-$ 。$$\\mathcal{L}(h, r, t) = \\begin{cases}\\sum_{h_+\\in E^+}{-\\frac{log(S(h_+,r,t,E^+\\cup E^-))}{|E^+|}}, &amp; \\text{if }p_c &gt; 0.5; \\\\sum_{h_+\\in E^+}{-\\frac{log(S(h,r,t_+,E^+\\cup E^-))}{|E^+|}}, &amp; \\text{if }p_c \\le 0.5; .\\end{cases}$$$p_c$ 服从$[0,1]$的均匀分布，大于0.5时，把输入实体作为tail entity，小于0.5的时候就是作为head entity，表示替换head entity和tail entity的概率各为50%。另有$S$, 即 softmax normalized output of ConMask：$$S(h,r,t,E^+) = \\begin{cases}\\sum_{e \\in E^\\pm}^{exp(ConMask(h,r,t))}{exp(ConMask(e,r,t))} &amp; \\text{if } p_c &gt; 0.5 \\\\sum_{e \\in E^\\pm}^{exp(ConMask(e,r,t))}{exp(ConMask(h,r,t))} &amp; \\text{if } p_c \\le 0.5 \\\\end{cases}$$ 4. Results从结果上看，对比其他模型，在开放领域，ConMask获得了最佳的效果；在Closed-World中，尽管ConMask不是为此设计的，但是对比TransE和TransR依然不逊色，结果相仿。 目前而言，ConMask模型只能预测在实体描述中表达的关系，将来还应考虑扩展它，使其能够发现新的或隐含的关系。 Bibliographies笔记参考：https://zhuanlan.zhihu.com/p/33026043，http://blog.csdn.net/TgqDT3gGaMdkHasLZv/article/details/79224178 代码实现：https://github.com/bxshi/ConMask [1] Shi, Baoxu, and Tim Weninger. “Open-World Knowledge Graph Completion.” arXiv preprint arXiv:1711.03438 (2017). [2] Bordes, A., Usunier, N., Garcia-Duran, A., Weston, J., &amp; Yakhnenko, O. (2013). Translating embeddings for modeling multi-relational data. In Advances in neural information processing systems (pp. 2787-2795). [3] Chorowski, J. K., Bahdanau, D., Serdyuk, D., Cho, K., &amp; Bengio, Y. (2015). Attention-based models for speech recognition. In Advances in neural information processing systems (pp. 577-585).","link":"/posts/%5B2018.2.26%5DOpen-World-Knowledge-Graph-Completion/"},{"title":"Nested LSTMs 笔记","text":"Nested LSTMs摘要：最近，一种新的 Nested LSTMs 网络被提出。在通常的LSTM网络中，我们通过将LSTM单元堆叠，从而形成深度RNN网络，提高其效果；Nested LSTM则通过嵌套而不是堆栈来增添LSTM的深度。在NLSTM中，记忆单元的值是由LSTM单元计算的，其中，LSTM单元具有自身内在的记忆单元。具体而言，NLSTM记忆单元并不是按照等式：$c_t^{outer} = f_t \\odot c_{t-1} + i_t \\odot g_t$ 对（外部）记忆单元的值进行计算，而是使用级联：$(f_t \\odot c_{t-1}, i_t \\odot g_t)$ 将其作为内部LSTM（或NLSTM）记忆单元的输入，并设定 $c_t^{outer} = h_t^{inner}$。在访问内部记忆时，Nested LSTM 相比传统的堆栈 LSTM 有更高的自由度，从而能处理更长时间规模的内部记忆；实验也表明，在参数数量相似的情况下，NLSTM 在多种任务上都超越了堆栈 LSTM。作者认为Nested LSTM 有潜力直接取代堆栈 LSTM。 1. Introduction学习长期的依赖关系是当前人工智能领域中，尤其是在nlp领域，机器学习方法的关键性挑战。基于循环神经网络的体系结构已经在使得机器能够模仿这种能力方面取得了显著进展。 single-layer LSTM RNN的输入是以当前的状态为依据，适合学习时间上的抽象特征。在实践中，许多专家已经证明，更为复杂的体系结构是解决许多任务的关键。其中一个原因是梯度消失问题（Hochreiter于1991年、Bengio等人于1994年提出），它使得简单的RNN难以学习长期依赖关系。Hochreiter和Schmidhuber于1997年提出了LSTM，包含能够改善梯度消失问题的记忆机制。单层LSTM如上图，图中的三个单元实际上是同一个单元，它循环地将内部的参数传递给自己。 Stacked LSTMs 堆栈 LSTM 架构使用一系列 LSTM 一层层地堆叠在一起来处理数据，一层的输出成为下一层的输入。上图为一个两层的LSTM网络。 引入多层的结构，即将多个LSTM单元堆叠，每一层的输出成为下一层的输入。 每层处理我们希望解决的任务的一部分，并将其传递给下一层。额外的隐藏层可以添加到多层感知器神经网络，使其有更深入的“理解”。 额外的隐藏层被认为重新组合了来自先前层的学习表示，并在高度抽象层次上找到新的表示。 例如，从线条到形状到对象。 Nested LSTMs在 NLSTM 中，LSTM 的记忆单元可以访问内部记忆。相比于传统的堆栈 LSTM，这一关键特征使得该模型能实现更有效的时间层级。在 NLSTM 中，外部记忆单元可自由选择读取、编写的相关长期信息到内部单元。相比之下，在Stacked LSTM 中，高层级的激活（类似内部记忆）直接生成输出，因此必须包含所有的与当前预测相关的短期信息。换言之，Stacked LSTM 与Nested LSTM 之间的主要不同在于，NLSTM 可以选择性地访问内部记忆。这使得，即使这些事件与当前事件不相关，内部记忆也能够记住、处理更长时间规模上的事件。我们在后面一章更详细地介绍它。 2. Model of Nested LSTMsLSTM 中的输出门会编码可能与当前的时间步骤不相关，但是仍然值得记忆的信息。Nested LSTM 根据这一直观理解来创造一种记忆的时间层级。以同样的方式被gate控访问内部记忆，因此长期信息只有在情景相关的条件下才能选择性地访问。 The architecture在 LSTM 网络中，单元状态的更新公式和门控机制可以表示为以下方程式：$$i_t = \\sigma_i (x_t W_{xi} + h_{t-1} W_{hi} + b_i) \\f_t = \\sigma_t (x_t W_{xf} + h_{t-1} W_{hf} + b_i) \\c_t = f_t \\odot c_{c-1} + \\sigma_c (x_t W_{xc} + h_{t-1} W_{hc} + b_c) \\o_t = \\sigma_o (x_t W_{xo} + h_{t-1} W_{ho} + b_o) \\h_t = o_t \\odot \\sigma_h(c_t)$$Nested LSTM 使用已学习的状态函数 $c_t = m_t(f_t\\odot c_{t−1}, i_t \\odot g_t)​$ 来替代 LSTM 中计算 $c_t​$ 的加运算。我们将函数的状态表示为 m 在时间 t 的内部记忆（inner memory），调用该函数以计算 $c_t​$ 和 $m_{t+1}​$。我们可以使用另一个 LSTM 单元来实现该记忆函数，就生成了 Nested LSTM。同样，该记忆函数能够由另一个 Nested LSTM 单元替换，因此就能构建任意深的嵌套网络。 因此，我们得到NLSTM 中记忆函数的输入和隐藏状态：$$\\tilde{h}{t-1} = f_t \\odot c{t-1} \\\\tilde{x}t = i_t \\odot \\sigma_c (x_t W{xc} + h_{t-1} W_{hc} + b_c)$$注意如果记忆函数是加性的，那么$c_t = f_t \\odot c_{c-1} + \\sigma_c (x_t W_{xc} + h_{t-1} W_{hc} + b_c) = \\tilde{h}_{t-1} + \\tilde{x}_t $，整个系统将退化到经典的 LSTM。 LSTM、Stacked LSTM 和 Nested LSTM 的计算图形。隐藏的状态、外部记忆单元和内部记忆单元分别由h、c和d进行表示。虽然当前的隐藏状态可以直接影响下一个内部记忆单元的内容，但内部记忆只有通过外部记忆才能够影响隐藏状态。$$\\widetilde{i}t = \\widetilde{\\sigma}i (\\widetilde{x}t \\widetilde{W}{xi} + \\widetilde{h}{t-1} \\widetilde{W}{hi} + \\widetilde{b}_i) \\\\widetilde{f}t = \\widetilde{\\sigma}t (\\widetilde{x}t \\widetilde{W}{xf} + \\widetilde{h}{t-1} \\widetilde{W}{hf} + \\widetilde{b}_i) \\\\widetilde{c}t = \\widetilde{f}t \\odot \\widetilde{c}{c-1} + \\widetilde{\\sigma}c (\\widetilde{x}t \\widetilde{W}{xc} + \\widetilde{h}{t-1} \\widetilde{W}{hc} + \\widetilde{b}c) \\\\widetilde{o_t} = \\widetilde{\\sigma}o (\\widetilde{x}t \\widetilde{W}{xo} + \\widetilde{h}{t-1} \\widetilde{W}{ho} + \\widetilde{b}_o) \\\\widetilde{h}_t = \\widetilde{o}_t \\odot \\widetilde{\\sigma}_h(\\widetilde{c}t)$$现在，外部 LSTM 的单元状态更新方式为 $ c_t = \\tilde{h}{t} $ 。 3. Experiments见附件论文[1] 4. ConclusionNested LSTM（NLSTM）是LSTM模型的简单扩展，通过嵌套来增加深度，而不是通过堆叠。 NLSTM的内部存储器单元形成内部存储器，其仅通过外部存储器单元被其他计算元件访问，实现了时间层级的形式。 论文[1]的实验表明，在相似的参数设置下，Nested LSTM 在多种字符级语言建模任务中的表现都超越了Stacked LSTM和single-layer LSTM，并且和Stacked LSTM 的高层级单元相比，LSTM 的内部记忆可以学习更长期的依赖关系。 NLSTM的Tensorflow实现 NLSTM的Keras实现 Bibliographies笔记参考：http://www.sohu.com/a/220745456_390227，http://posts.careerengine.us/p/5a768ab3381fe136215b3de5?from=latest-posts-panel&amp;type=title [1] Moniz, Joel Ruben Antony, and David Krueger. “Nested LSTMs.” Asian Conference on Machine Learning. 2017. [2] Hochreiter, Sepp, and Jürgen Schmidhuber. “Long short-term memory.” Neural computation 9.8 (1997): 1735-1780.","link":"/posts/%5B2018.2.5%5DNested-LSTMs/"},{"title":"A convolution BiLSTM neural network model for chinese event extraction 笔记","text":"A convolution BiLSTM neural network model for chinese event extraction摘要：中文事件提取是信息抽取中的一项具有挑战性的任务，以前的方法高度依赖于复杂的特征工程和复杂的自然语言处理（NLP）工具。 在文献[1]中，提出了一种结合LSTM和CNN的卷积双向LSTM神经网络来捕获句级和词汇信息。最终的测试中达到相当不错的水平。 1. Introduction在事件提取中，我们需要提取事件类别、参与者和其他属性（时间、地点等）。根据Automatic Content Extraction（ACE）定义的事件抽取任务，我们定义： 触发词：最主要的、用于表达一个事件的词，通常是句子的谓语。 事件属性：实体、短语或数值。在一个事件中扮演特定作用。 因此，我们把事件抽取分为两步，即触发词标注和事件属性标注。例如： S1：Intel在中国成立了研究中心。 其中，“成立”表明该句子表达了一个商业事件；Intel、中国、研究中心则是事件的属性，属性将被标注为参与者、地点、时间等。 目前的 state-of-the-art [2-4] 通常很依赖于特征的选择。这些特征通常可以被划分为语义特征和结构特征。再给两个包含”成立“的例子，但它在其中并不表达一个商业事件。 S2：它成立于1994年，现在是一支深受欢迎的摇滚乐队。 S3：医院已成立救援中心。 从结构特征上来看，S2可以被缩写为“它是乐队”，因此“成立”在这个句子中不是一个触发词，这个句子不是一个事件。 从语义特征上来看，S3中的“救援中心”的语义上看，这个事件不是一个商业行为，因此“成立”不表达一个商业事件。 传统的方法[2, 3]通常依赖于大量的NLP工具，对于语义特征而言，有词性标注、命名实体识别等；对于结构特征而言，有依存关系分析。尽管最终效果很好，但是这需要大量的人工特征，并且需要忍受传递误差。 Chen et al. [5] 提出了一个用于完成事件抽取的卷积神经网络。受此激发，本文提出一个卷积双向LSTM神经网络，用来同时捕获语义特征和结构特征。我们首先使用双向LSTM将整个句子中的单词的语义编码成句子级别的特征。 然后，我们可以利用卷积神经网络来捕获突出的局部词汇特征，以便在没有任何POS标签或NER帮助的情况下进行触发词消歧。 2. Trigger Labeling2.1 Language Specific Issues由于中文的特殊性，触发词可以被分为两类： 多词触发词：任何拆开后就无法被人为是触发词的，我们把它组合起来认为是触发词。例如“犯罪嫌疑人都落入法网”，其中“落入法网”被认为是触发词。 单词触发词：往往是谓语，但也可以是组合词中的一部分。例如“警察击毙了一名歹徒”中的“击毙”，“这是一件预谋的凶杀案”中的“凶杀” 为了解决这个问题，我们将事件检测视为序列标记任务而不是分类任务。 采用BIO方案，其中标记B是事件触发词的开始，I型是在触发词内，否则标记为O。我们利用卷积双向LSTM神经网络来完成这个任务。 我们基于单词模型的主要架构。 （a）中的每个词wt的局部上下文特征ct（灰色矩形）由CNN计算（b）所示。 我们的卷积神经网络学习了关于中心词“落入”的本地上下文信息的表示。 这里的上下文大小是7（中心词的左右各3个词），我们使用一个大小为4的内核与两个特征映射。 （b）句子中的符号P表示填充词。 2.2 Word-Based MethodLSTM Network 在nlp任务中LSTM相对常用，特别的，双向LSTM能够联系历史和未来的信息，能够重复利用句子信息，有利于我们进行判断。因为之前的报告已经叙述过，故这里略写。 CNN 卷积神经网络最一开始用于图像领域，近年也在nlp领域大放光彩。这里，我们采用卷积神经网络来提取句子中每个单词的局部上下文信息。 给定一个包含n个单词{w1, w2, … , wn}的句子和当前中心词wt，卷积运算包含一个内核，将其应用于wt周围的单词以生成特征映射。 我们可以利用不同宽度的多个内核来提取不同粒度的局部特征。 然后在每个map上执行最大汇集，以便仅记录每个特征地图的最大数量。 池的一个特性是它产生一个固定大小的输出向量，这使我们能够应用不同的大小内核。 而通过执行最大操作，我们保持最显着的信息。 最后，将固定长度的输出向量cwt作为关于中心词wt的本地上下文信息的表示。 在我们的实现中，滑动窗口大小为7（中心词的左右各3个词），并且我们使用不同的内核来捕获各种粒度的上下文信息。 Output Layer 我们将BiLSTM的隐藏状态与CNN在每个时间步t提取的上下文特征cwt连接起来。 然后[ht; cwt]被送入softmax层以产生wt的每个标记的对数概率。然而，基于单词的方法仍然不能解决内部词触发引起的一致性问题，即无法识别长词内部的触发词。 2.3 Character-Based Method为了解决一致性问题，我们可以采用Character-embedding，唯一的区别就在input layer。 3. Argument Labeling上面介绍的触发词标注模型依然可以被沿用，我们将介绍用于触发词标注和事件属性标注的模型之间的主要区别。 3.1 Input Layer作为一个pipeline系统，除了word embeddings之外，还可以使用从上面触发词标记任务中提取的信息。 因此，我们提出了另外四种类型的特征embedding来形成BiLSTM和CNN的输入层。 触发位置特征：一个单词是否属于触发词的一部分 触发类型特征：单词触发类型，NONE类型对于非触发词 实体位置特征：一个单词是否属于实体的一部分 实体类型特征：单词的实体类型，NONE类型对于非实体。 ACE数据集提供了实体识别的结果，无需使用外部NLP工具。（思考：若数据集不提供实体信息，两种解决方法：1. 不embed实体特征；2. 借助外部工具）然后，我们通过查表将这些特征转换成矢量，并将它们与原始单词嵌入级联，作为BiLSTM和CNN的最终输入层。 3.2 Output Layer值得一提的是，事件属性标注不再是一个序列标注任务，而是一个分类任务。 ACE数据集提供了实体识别的结果，它保证了事件属性只能出现在这些实体。 因此，我们只需要预测标记实体的角色，而不是整个句子中的每个单词。 例如，S4中有三个触发器（粗体字）和三个实体（斜体字），它们共同组成九对要分类的触发词和事件属性候选。 S7：六起谋杀案发生在法国，包括Bob的暗杀和Joe的杀害。 我们修改CNN和BiLSTM网络的输出层以适应新的任务。 对于BiLSTM，我们仍然试图利用其记忆长序列的能力，所以我们把最后一个单词hN的隐藏状态视为句子信息。 对于CNN，我们把整个句子的所有单词作为上下文，而不是每个中心单词的浅窗口。 最后，我们将来自两个网络的输出向量的串联输入到softmax分类器中，就像处理之前的触发词标注任务一样。 4. Conclusion论文[1]主要提出了卷积双向LSTM神经网络，用以完成中文事件抽取任务，在ACE 2005数据集上获得了不错的结果。我在暑假时，将事件抽取认为为一个序列标注任务，使用BiLSTM+CRF；相比而言，论文[1]的模型考虑更全面，并充分利用已知的实体信息。不过对于现实问题而言，标注实体信息的成本也很高，故在没有实体标注的情况下保持性能也是一个难点。 Bibliography[1] Zeng, Y., Yang, H., Feng, Y., Wang, Z., &amp; Zhao, D. (2016). A convolution BiLSTM neural network model for Chinese event extraction. In Natural Language Understanding and Intelligent Applications (pp. 275-287). Springer, Cham. [2] Chen, C., Ng, V.: Joint modeling for Chinese event extraction with rich linguistic features. In: COLING, pp. 529–544. Citeseer (2012) [3] Chen, Y., Xu, L., Liu, K., Zeng, D., Zhao, J.: Event extraction via dynamic multipooling convolutional neural networks. In: Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, vol. 1, pp. 167–176 (2015) [4] Li, Q., Ji, H., Huang, L.: Joint event extraction via structured prediction with global features. In: ACL (1), pp. 73–82 (2013) [5] Chen, Y., Xu, L., Liu, K., Zeng, D., Zhao, J.: Event extraction via dynamic multipooling convolutional neural networks. In: Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, vol. 1, pp. 167–176 (2015)","link":"/posts/%5B2018.1.29%5DA-convolution%20BiLSTM-neural-network-model-for-chinese-event-extraction/"},{"title":"Event detection and co-reference with minimal supervision 笔记","text":"Event detection and co-reference with minimal supervision [1]摘要：该论文使用了一种弱监督的算法解决了事件检测与共指问题。事件共指问题可以看作是一种事件之间的相似度计算问题，而在该文中，事件检测问题也被看作是一种相似度检测问题。对于ACE或rich ERE划分的所有事件类型，使用每个类型中的几个实例作为该类型事件的向量，然后计算新事件向量与每个类型事件向量之间的相似度，根据这一相似度对事件进行判断。该文的另一个特点在于事件特征的选择，在将事件表示为向量的过程中，使用了Freebase作为特征来对事件进行表示。 1. Introduction 上图是论文提出的MSEP（Minimally Supervised Event Pipeline）框架。这里 Event examples 是唯一的监督来源，用于产生 Example vectors。在MSEP框架中不需要训练。 这篇论文主要是针对两个问题： Event detection 指的是对一段文本内容，检测是否存在符合要求的事件。 Co-reference problem. 为了更好的理解和利用事件的信息，我们需要从文本中提取出时间、地点、人物、行为等信息。此外，我们还需要了解两个事件的关系，例如，判断两个事件是否表示同一个事件，这就是Co-reference problem。 在本文中，我们提出了一种更加可行且更加可测的方法来描述事件。对于一个事件e，event detection 所要做的就是判断是否存在一个事件集合，事件e在语义上是否有关联，以至于可以被划分到该集合内；而 co-reference problem 则是判断两个事件e1、e2是否在语义上表述足够接近，以至于我们认为它们所表示的实际上是同一个事件。可以看到两个任务实际上都需要判断相似性，我们可以把它们转化为语义相似性问题。 现在主要问题有：1. 如何表示一个事件；2. 如何表达相似性。前者我们采用了semantic role labeling representation（SRL），来结构化地描述一个事件；对于后者，我们将对事件做一个embedding，通过计算其余弦距离来表达相似性。 我们提出了一个通用事件检测和指代消解框架，它基本上不需要标记数据。在实践中，为了将一个事件提法（event mention）和一个事件本体（event ontology）相联系起来，我们只需要一些事件示例。这种定义类型的方式是非常合理的，因为给出例子是定义事件类型的最简单的方法。我们的方法比标准的无监督方法要求更少假设，在我们的模型中，给定事件类型的定义（以事件例子的形式），我们可以将单个事件分类到已知本体，并确定两个事件是否是 co-reference 的。 2. The MSEP System2.1 Structured Vector Representation事件结构和句子结构之间有一个平行关系。我们发现一般来说，事件的触发词往往是谓语，所以可以针对谓语对其做一些改进： Basic event vector representation。基本事件向量由它的各个组成部分组成。 Augmented event vector representation。在这里，“+” 表示我们首先将文本片段放在一起，然后将组合的文本片段转换成ESA向量。 2.2 Event Mention Detection我们定义 Event type representation 为该类别下的事件向量的平均值。 我们定义定义相似度如下$$S(e_1, e_2) = \\frac{vec(e_1) · vec(e_2)}{||vec(e_1)||·||vec(e_2)||} \\= \\frac{\\sum_a{vec(a_1) · vec(a_2)}}{\\sqrt{\\sum_a{||vec(a_1)||^2} · \\sum_a{||vec(a_2)||^2}}}$$其中 e1 是待处理事件，e2 是事件的类别。a 就是事件里的各个组件。若遇到 a 缺失的情况（如地点、时间等），我们用非缺失的部分的平均值来代替它。具体的操作方法参见原文。 2.3 Event co-reference这里如上一节的内容所说，通过余弦距离$S(e_1, e_2)$来计算两个事件的相似度。 对于每一个事件，我们分别比较$agnet_{sub}, agnet_{obj}$，若都不相同，我们认为它们是独立的；如果有缺失，我们认为它和任意值匹配。这样，我们可以得到一个不重复的事件集合，$Set_{conflict}$。 接下来遍历所有事件，对于事件k+1，$$e_p = argmax_{e\\in {e_1,…,e_k} e \\notin Set_{conflit}} {S(e_p, e_{k+1})}$$如果$S(e_p, e_{k+1})$的值大于我们设定的阈值，我们就认为它是同一个事件；否则，我们把他分为一个新的类。 3. Vector Representation我们可以看到，其实文章之前的内容都不依赖于 embedding 的具体选择，事实上，作者也测试了很多的方法，可以根据实际情况来选择。 Explicit Semantic Analysis Brown Cluster Word2Vec Dependency-Based Embedding 4. Semantic Role Labeling上面工作建立在已经完成了 Semantic Role Labeling 的情况下，这里我们在讨论一下如何进行 Semantic Role Labeling。 对于标注任务来说大同小异，现在往往使用神经网络模型来进行标注，例如[2]，缺点是需要大量标注数据。目前业内比较主流的解决方案是RNN-CRF模型，一般来说分为： Embedding layer Bi-directional RNN (usually LSTM) layer Tanh hidden layer CRF layer 在实际应用上，可能还会增加Attention机制等来进一步提高它的效果。 目前已有的系统如哈工大的语言技术平台LTP，能够用于 Semantic Role Labeling 等。 5. Conclusion这一篇文章提出了一种新颖的事件检测和指代消解方法。其最重要的部分就是提出了一种结构化的向量，能够更好地表示event，用以进行事件分类、指代消解等工作。这个方法在一些关键指标上甚至能优于最新的监督方法，并且能够更好地适应新的领域。 Bibliography[1] Peng, H., Song, Y., &amp; Roth, D. (2016). Event Detection and Co-reference with Minimal Supervision. In EMNLP (pp. 392-402). [2] Zhou, J., &amp; Xu, W. (2015). End-to-end learning of semantic role labeling using recurrent neural networks. In ACL (1) (pp. 1127-1137).","link":"/posts/%5B2018.1.21%5DEvent-detection-and-co-referentce/"},{"title":"几个 relation extraction 远程监督模型","text":"几个 relation extraction 远程监督模型摘要：远程监督（Distant supervision）显著地减少了建立用于分类任务的训练集所需要的人工。但是这一项技术也会带来很大的噪音，并可能因此而大大地影响了模型的性能表现。这里，我们以 relation extraction 这项任务为例，深入讨论分析该噪声的分布。文献[1]提出了 dynamic-transition matrix，并证明了它能很好地代表了由 distant supervision 所带来的噪声。通过该矩阵，我们能够大大提高 relation extraction 的效果。文献[2]则是一种经典的方法，通过定义规则，定义否定模式（negative pattern）过滤掉一些噪音数据，可以很大程度提高性能。缺点是规则依赖人工定义，但是方法本身简单有效。文献[3]将 relation extraction 定义为一个 Multi-instance Multi-label 学习问题，一定程度上解决了错误标签的问题。 1. Problem of distant supervisionDistant supervision 是一种生成关系抽取训练集的常用方法。它把现有知识库中的三元组 &lt;e1, r, e2&gt; （或写成&lt;subj, r, obj&gt;）作为种子，匹配同时含有 e1 和 e2 的文本，得到的文本用作关系 r 的标注数据。这样可以省去大量人工标记的工作。 但是，相比于人工标注方法，这种匹配方式会产生很多噪音：比如三元组&lt;DonaldTrump, born-in, New York&gt;，可能对齐到“Donald Trump was born in New York”，也可能对齐到“DonaldTrump worked in New York”。其中前一句是我们想要的标注数据，后一句则是噪音数据，它并不表示born-in关系。如何去除这些噪音数据，是一个重要的研究课题。 2. Approaches to this problems 拟合噪音 dynamic-transition matrix [1] 去除噪音 通过定义规则过滤掉一些噪音数据[2]，缺点是依赖人工定义，并且被关系种类所限制。 Multi-instance learning[3], 把训练语句分包学习，包内取平均值，或者用 attention 加权，可以中和掉包内的噪音数据。缺点是受限于 at-least-one-assumption：每个包内至少有一个正确的数据。 下面我们简单介绍这几个模型。 2.1 Learning with dynamic-transition matrix [1]文献[1] 提出了 dynamic-transition matrix，用于表达 Distant supervision 所产生的噪声。dynamic-transition matrix 可以通过基于 curriculum learning 的方法训练得到。通过该矩阵，我们能够大大提高 relation extraction 的效果，能够达到目前该领域的 state-of-the-art。 Transition matrix 是一个转移矩阵，记为T，大小为 n*n，n是关系种类的数目。T 的元素，$T_{ij}$的值是 p( j| i )，即该句子代表关系为 i，但被误判为 j 的概率。 这样我们就可以得到：𝑃𝑟𝑒𝑑𝑖𝑐𝑡𝑒𝑑 𝑑𝑖𝑠𝑡𝑟𝑖𝑏𝑢𝑡𝑖𝑜𝑛 × 𝑇𝑟𝑎𝑠𝑖𝑡𝑖𝑜𝑛 𝑚𝑎𝑡𝑟𝑖𝑥=𝑂𝑏𝑠𝑒𝑟𝑣𝑒𝑑 𝑑𝑖𝑠𝑡𝑟𝑖𝑏𝑢𝑡𝑖𝑜𝑛 其中，predicted 是我们想要的真实分布，observed 是我们观测到的噪音分布，这样就可以用噪音数据进行联合训练了。作者在 timeRE 和 entityRE(NYT) 上均进行了训练，取得了降噪的 state-of-art。具体分析结果可以参照论文。 2.2 Reducing Wrong Labels [2]在关系提取方面，远程监督试图通过使用知识库（如Freebase）作为监督来源，从文本中提取实体之间的关系。 当一个句子和一个知识库引用同一个实体对时，这种方法试图用知识库中的对应关系来启发式地标注句子。 然而，这种启发式可能会导致一些句子被错误地标记。 这种嘈杂的标记数据导致较差的抽取性能。 在本文中，我们提出了一种减少错误标签数量的方法。 我们提出了一个新的生成模型，直接模拟远程监督的启发式标签过程。 该模型通过其隐藏变量来预测分配的标签是正确的还是错误的。在实验中，我们也发现错误的标签减少提高了关系抽取的性能。 NegPat(r)即为事先定义的对于r的否定模式（negative pattern）。在我们的方法中，我们按如下所示去除错误标签：（i）给定一个已标注的语料库，我们首先验证其中的模式是否表达一种relation，然后（ii）使用否定模式列表（NegPat）去除错误的标签， 即该模式被定义为不表示relation的模式。 第一步，我们引入新的生成模型，直接模拟DS的标注过程并进行预测。 第二步在算法1中描述，见上图。对于关系提取，我们使用上述得到的标注数据来训练分类器（给定实体对，该分类器预测所属关系）。 ####2.3 Multi-instance Multi-label Learning [3] 很多的共现 entities 都没有什么关系，仅仅是出现在同一个句子中；而有的 entities 之间的关系其实并不仅仅只有一种，可能有多种，比如奥巴马和美国的关系，可能是 born in，也可能是 is the president of 的关系。 因此训练集会产生大量的错误标记，比如两个实体有多种关系或者根本在这句话中没有任何关系，这样的训练数据会对关系抽取器产生影响。正因为如此，传统的监督式学习，假设每个实例明确地映射到一个标签，是不合适的。 对于这个问题，我们将关系抽取定义为一个 Multi-instance Multi-label 学习问题，它使用带有潜在变量的图模型，对文本中一对实体的所有实例以及它们的所有标签进行联合建模。 该模型在 relation extraction 领域表现出色。 3. Conclusion上面提到的几个模型都有其新颖的地方，其中[1]这种拟合噪音的思想很有创新点，实际的效果也很理想；而后两个模型主要都是在数据预处理阶段进行，因此可以和其他 relation extraction 模型很好的结合。 References*笔记部分参考论文浅尝 | Learning with Noise: Supervised Relation Extraction [1] Luo, Bingfeng, et al. “Learning with noise: enhance distantly supervised relation extraction with dynamic transition matrix.” arXiv preprint arXiv:1705.03995 (2017). [2] Takamatsu, Shingo, Issei Sato, and Hiroshi Nakagawa. “Reducing wrong labels in distant supervision for relation extraction.” Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1. Association for Computational Linguistics, 2012. [3] Surdeanu, Mihai, et al. “Multi-instance multi-label learning for relation extraction.” Proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning. Association for Computational Linguistics, 2012.","link":"/posts/%5B2018.1.14%5DModels-for-relation-extraction/"},{"title":"Overcoming Limited Supervision in Relation Extraction 笔记","text":"这次主要阅读的论文是《Overcoming Limited Supervision in Relation Extraction: A Pattern-enhanced Distributional Representation Approach》[1]。该文主要针对了现有模型对标注数据的依赖，提出一种比较有意思的思路。基于分布的方法（distributional approach）利用两个实体共同出现的统计频率来预测他们的关系，需要大量标注数据，而基于模式的方法（pattern-based approach）一般使用神经网络建模，但这种方法需要更多的标注数据。本文同时建立两个模型，互相为对方提供监督。以分布模型作为判别模型，模式模型作为生成模型。训练过程中不断迭代，从而提升两个模型的性能。 1. Introduction1.1 Weakly Supervised Learning弱监督学习介于监督学习和无监督学习之间，它提供的标注数据带有较大的噪音，或标注的相对粗糙，标注结果可能出错。对于关系抽取而言，就是将一些关系实例作为seed，用它们从大型语料库中去除冗余信息并提取更多的实例。 弱监督学习的基本思路： 用容易获得的标注替代较难获得的标注 选择最需要做精细标注的样例 模型训练和自动标注交替进行 1.2 Co-training strategy以往的工作主要是单个模型，该文采用了co-training策略[2]，将两个模型互相协作，取得了比较好的效果。 co-training策略是一种半监督方法，核心就是利用少量已标记样本，通过两个（或多个）模型去学习，对未标记样本进行标记，挑选置信度最高的样本加入已标记样本阵营。 1.3 REPEL (Relation Extraction with pattern-enhanced Embedding)REPEL是本文提出的一个模型。基于模式的模型学习用于关系抽取文本的模式，基于分布的模型作为分类器，两者互补，互相提供监督。前者相当于一个生成器，基于模式生成候选实例；而后者作为判别器，从中选择最优实例，并将选择结果反馈给前者。训练完成相当于得到了两个关系抽取模型。 2. Problem definition实体识别：使用现成的工具标注。 关系识别：实体对 $(e_h, e_t)$，三元组$(e_h, e_t, r)$ 给定语料库D，关系集合R。给定少量seed实例$ {(e_h^{r(k)}, e_t^{r(k)}, r)} _{k=1}^{N_r} $，提取尽可能多的$ {(e_h^{r(i)}, e_t^{r(i)}, r)} _{i=1}^M $；换言之，对于每个$ r \\in R $，我们要提取尽可能多的$ {(e_h^{r(i)}, e_t^{r(i)})} _{i=1}^{M_r} $。 3. REPEL Framework模式模型：找到文本中的模式集合 分布模型：学习实体表示，以及打分函数 目标函数：$$max_{P,D}O = max_{P,D}{O_p + O_d + \\lambda O_i}$$上面公式中，P表示模式模型的参数，给定关系的全部模式集合。D表示分布模型的参数，实体表示和打分函数。Op和Od分别表示两个目标函数，Oi表示两个模型交互的目标。 注意这里只考虑关系抽取，实体识别使用现有的工具或模型。 3.1 Pattern Module对于一个指定的关系r，我们的目标是找到K个最可靠的模式，然后进一步使用它们来发现更多的关系实例。 基于模式关系抽取主要分为两种：path-based pattern、meta pattern。对于一句话中的实体对，前者定义为两个实体通过依存信息跳转的最短路径；后者则是两个实体附近的文字序列。利用这两种模式从语料库中寻找匹配的实体对。这样就得到了很多候选模式，每个模式又能分别找到许多匹配的实体对。 对于一个模式$\\pi$，我们通过以下式子计算它的置信度：$$R(\\pi)=\\frac{|G(\\pi)\\cap S_{pair}|}{|G(\\pi)|}$$$G(\\pi)$表示被模式$\\pi$所匹配的所有实体对，$S_{pair}$表示seed实体对。可以看到，R实际表示的是，在满足$\\pi$模式的实体对中，seed实体对所占的比例。显然，该比值越高，该模式越符合seed的分布。由此，我们定义：$$O_p = \\sum_{\\pi \\in P}R(\\pi)$$下面说明一下整个进行的过程： 给定seed实体对，我们通过模式关系抽取的方法获得一系列候选模式。 计算每个候选模式的R值，取最高的K个 3.2 Distributional Module该模块学习语料中的实体全局分布信息。我们利用给定的关系实例作为打分函数。 对于一个实体e，和一个词w$$P(w|e) =\\frac{exp(x_e*c_w)}{Z}$$$x_e$表示需要训练的实体表示向量， $c_w$是预训练的word embedding，Z是归一化项。$$O_{text} = \\sum_{w,e}n_{w,e}log(P(w|e))$$$n_{w,e}$是字与实体之间边的权重，也就是实体和这个字同时出现的统计频率。我们希望分布概率能够拟合经验分布概率。 定义打分函数：$$L_D(f|r)=1-||x_{e_h} + y_r- x_{e_t} ||^2_2$$实体向量$(x_{e_h} - x_{e_t})$和$y_r$（关系r的表示，也是要学习的参数）越接近，$L_D$就越接近1；反之则会非常小。$$O_{seed} = \\sum_{f\\in S_{pair}} \\sum_{f’\\in(e’_h,e’_t)} {min{1, L_D(f|r) - L_D(f’|r)}}$$$(e’_h,e’_t)$是随机选取的实体对。最小值函数是为了防止两个分数差距太多，因为往往$L_D(f’|r)$会是一个很小的负数。 最后有总目标函数中的Od：$$O_d = O_{text} + \\eta O_{seed}$$$\\eta$用于调整两部分的比值。 3.3 Modeling the Module Interaction$$O_i = E_{f\\in G(P)}[L_D(f|r)]$$ 这里E指的是期望。 我们给模式模型生成的实体对也打分。Oi作为目标函数，为了最大化它，模式集合P应该尽可能包含那些可靠有效的模式。也就是说，模式模型生成的实体对应该得到的打分越大越好。这样一来分布模型就能为模式模型提供监督（打分）。并且，对于分布模型来说，最大化该目标函数能够给实体对分配更高的打分（也就是说，要令Oi最大化，G(P)和LD都要合适）。通过这种方式两个模型能够互相提供监督。 4. The Joint Optimization Problem 具体算法如上图原文，为了优化总目标函数，采用协梯度下降算法。 先固定模式模型，将seed实体对$S_{pair}$和模式模型生成的实体对$G(P)$训练分布模型。图中的Eqn.11就是下式：$$max_D { O_d + \\lambda O_i } = max_D { O_d + \\lambda E_{f \\in G(P)}[L_D(f|r)] }$$然后再固定分布模型，对实体对筛选后得到的$S_{pair}$训练模式模型。图中的Eqn.12就是下式：$$max_P { O_p + \\lambda O_i } = max_P { \\sum_{\\pi \\in P}(R(\\pi) + \\lambda E_{f \\in G(\\pi)}[L_D(f|r)]) }$$往复迭代。 5. Conclusion利用两个模型进行互补的思路很新颖，从论文的测试结果上来看，本文提出的模型并不逊色于神经网络，可见两个模型互补的效果是相当不错的。但是这种弱监督学习需要的人工标注数据非常少，降低了对标注数据的依赖性。 Reference*笔记部分参考https://zhuanlan.zhihu.com/p/32364723 [1] Qu, M., Ren, X., Zhang, Y., &amp; Han, J. (2017). Overcoming Limited Supervision in Relation Extraction: A Pattern-enhanced Distributional Representation Approach. arXiv preprint arXiv:1711.03226. [2] Blum, Avrim, and Tom Mitchell. “Combining labeled and unlabeled data with co-training.” Proceedings of the eleventh annual conference on Computational learning theory. ACM, 1998.","link":"/posts/%5B2018.1.4%5DOvercoming-Limited-Supervision-in-Relation-Extraction/"},{"title":"Relation Classification via Attention Model 笔记","text":"Relation Classification via Attention Model这个笔记主要是阅读论文[1]，它的工作重点是在神经网络构成的端到端学习的关系抽取任务中加入Attention机制。作者主要通过自动学习关系句中注意力较高的部分，而引入attention机制，对反映实体关系更加重要的词语给予更大的attention，较好地提高了关系抽取的效果。 1. Attention1.1 概述Attention机制最早是在视觉图像领域被提出来的。在NLP任务上，Bahdanau[2]等人使用类似attention的机制在机器翻译任务上将翻译和对齐同时进行。接着类似的基于attention机制的深度学习模型开始广泛应用到各种NLP任务中。 1.2 Recurrent Models of Visual Attention人们在进行观察图像的时候，其实并不是一次就把整幅图像的每个位置像素都看过，大多是根据需求将注意力集中到图像的特定部分。由此，在传统的RNN上加入了attention机制，每次当前状态，都会根据前一个状态学习得到的要关注的位置和当前输入的图像，去处理注意力部分像素。可以看到应用Attention机制后，任务的复杂度被降低了很多。 1.3 Attention-based RNN in NLP[1]的成果是在机器翻译任务，一般机器翻译工作由一个Encoder和一个Decoder构成，一个典型的Seq2seq任务。Encoder将源句子进行编码，再利用Decoder将编码后的向量解码成目标语言。 我们在求注意力分配概率分布的时候，对于输入句子中任意一个单词都给出个概率，从而得到一个概率分布，再对输入句子所有单词的概率进行加权求和，得到Decoder的注意力分配。如下图。 另一个扩展性更好的论文是[3]，他们的工作告诉了大家attention在RNN中可以如何进行扩展。 1.4 Attention-based CNN in NLP[4]这篇论文研究的是两个CNN网络，分别处理两个句子，最后输入到分类器中处理。但是这样的模型在输入分类器前句对间是没有相互联系的，作者就想通过设计attention机制将不同cnn通道的句对联系起来。于是提出了3中在CNN中使用attention的方法。 ABCNN-1: 在卷积前进行attention，通过attention矩阵计算出相应句对的attention feature map，然后连同原来的feature map一起输入到卷积层。 ABCNN-2: 在池化时进行attention，通过attention对卷积后的表达重新加权，然后再进行池化. ABCNN-3: ABCNN-1 + ABCNN-2 2. Relation Classification 2.1 Classification Objective作者提出了一种距离函数，即正则化向量差的L2范数：$$\\delta_{\\theta}(S,y) = ||\\frac{w^O}{|w^O|} - W_y^L||{L^2} \\S:\\text{Sentence}, y:\\text{Output relation}, w^O: \\text{Network output}, W^L:\\text{Relation embedding}$$基于此，作者定义了目标函数：$$\\mathcal{L} = [\\delta_\\theta(S,y) + (1-\\delta_\\theta(S, \\hat{y}^-))] + \\beta||\\theta||^2 \\\\hat{y}^- : \\text{A selected incorrect relation label chosen as the one with the highest score among all i.e.} \\\\hat{y}^- = argmax{y’\\in \\mathcal{Y},y’\\ne y}(\\delta(S, y’))$$目标中的两个距离分别为网络输出向量与正例和与某负例的距离，该负例是所有错误类别中与该输出最接近的。最后加上一个正则项，通过使该目标函数最小化来训练网络中的各参数，$\\beta$用于控制其比重。 2.2 Input Representation现有句子，以及两个已知的实体e1,e2：$$S = (w_1,w_2,…,w_n) \\e_1 := w_p, e_2 := w_t . p,t\\in [1,n], p\\ne t$$为了得到它们的关系，我们把所有词转为词向量；并且根据每个词与实体的相对位置，也转为word position embeddings，每个词与两个实体有两个相对位置，所以得到第i个词的Embedding：$$w_i^M = [(w_i^d)^T, (w_{i,2}^p)^T,(w_{i,2}^p)^T]^T$$为了充分得到上下文的信息，再考虑大小为k的滑窗，得到最终的input representation$$z_i = [(w_{i - (k-1)/2}^M)^T,…,(w_{i + (k-1)/2}^M)^T]^T$$ 2.3 Input Attention Mechanism 输入级的attention机制是设计两个关于实体对上下文相关的对角矩阵，该矩阵中各元素反映该词语与给定实体间联系的强弱，如$A_{i,i}^j=f(e_j,w_i)$反映了wi和ej之间的联系强弱，这里作者给的 f 就是内积。我们定义：$$\\alpha_i^j = \\frac{exp(A_{i,i}^j)}{\\sum_{i’=1}^{n}{exp(A_{i’,i}^j)}}$$对于j=1,2 两个相关因子，作者提出了三种处理方式: 平均$$r_i = z_i \\frac{\\alpha_i^1 + \\alpha_i^2}{2}$$ 串联$$r_i = [(z_i \\alpha_i^1)^T, (z_i \\alpha_i^2)^T]^T$$ 距离$$r_i = z_i \\frac{\\alpha_i^1 - \\alpha_i^2}{2}$$ 最终得到$R = [r_1, r_2,…,r_n]$ 2.4 Convolutional Max-Pooling with Secondary Attention将前面得到的矩阵R送入卷积核大小为dc的卷积层，卷积操作可形式化表示为:$$R^\\star = tanh(W_fR+B_f), \\text{where the siaze of Wf is } d^c \\times k(d^w+2d^p)$$然后构建一个相关性矩阵来捕获卷积层输出R*与实体关系WL之间的联系$$G = R^{\\star T}UW^L, \\U :\\text{weighting matrix learnt by the network}$$ 再用softmax函数来处理相关性矩阵G，获得attention pooling matrix Ap:$$A_{i,j}^p = \\frac{exp(G_{i,j})}{\\sum_{i’=1}^n{exp(G_{i’,j})}}$$最后用Ap与卷积层输出R*相乘，也就是加入混合中的attention，然后取出每一维度的最大值，得到网络的输出$$w_i^O = max_j(R^\\star A^p)_{i,j}$$ 3. 总结从[1]中提到的结果上看，attention的表现确实是在重要的词上有更好的权重，在Sem-Eval-2010 Task 8数据集上取得了显著的效果提升。对于关系抽取来说无疑是非常大的一个进步。 但是还是有一些不足： 它要求实体已知，因此需要其他工作来完成实体的识别，使得一些信息的丢失以及错误累加。此时并行模型或端到端模型，同时完成实体识别可能效果会更好； 关系是事先定义的集合，因此更多的是对关系的分类，若能启发式地抽取关系可能会有更广的应用空间； 对于一些上下文没有明显帮助的隐式关系或是使用了比喻之类的修辞，较为容易出错。 这次选择读这篇文章也是想更具体地了解Attention机制，同时了解一些关系抽取的方案，它也有一个pytorch版本的实现，可以辅以参考。 Reference笔记部分参考https://zhuanlan.zhihu.com/p/22867750 [1] Wang, L., Cao, Z., Melo, G. D., &amp; Liu, Z. (2016). Relation Classification via Multi-Level Attention CNNs. Meeting of the Association for Computational Linguistics (pp.1298-1307). [2] Bahdanau, D., Cho, K., &amp; Bengio, Y. (2014). Neural machine translation by jointly learning to align and translate. Computer Science. [3] Luong, M. T., Pham, H., &amp; Manning, C. D. (2015). Effective approaches to attention-based neural machine translation. Computer Science. [4] Yin, W., Schütze, H., Xiang, B., &amp; Zhou, B. (2015). Abcnn: attention-based convolutional neural network for modeling sentence pairs. Computer Science.","link":"/posts/%5B2017.12.17%5DRelation-Classification-via-Attention-Model/"},{"title":"实体解析 Entity resolution","text":"1. Entity resolution###1.1 Sequence labeling 我们通常在ML中把Named Entity Recognition任务认为是一个Sequence labeling任务，事实上很多nlp任务都可以被转化为sequence labeling。暑假实习的时候也在这方面看了一些文献。目前业内比较主流的解决方案是RNN-CRF模型，一般来说分为： Embedding layer Bi-directional RNN (usually LSTM) layer Tanh hidden layer CRF layer 从结果上来看，该模型对大多数sequence labeling任务有较好的效果，如named entity recognition等。但是对于一些更灵活的标注任务（如暑假实习时，我曾试图将event recognition转化为seq labeling任务），尤其是在训练集不足的情况下，往往效果还是不能令人满意。 1.1.1 应用Attention [1]在 RNN-CRF 模型结构基础上，重点改进了词向量与字符向量的拼接。使用 attention 机制将原始的字符向量和词向量拼接改进为了权重求和，使用两层传统神经网络隐层来学习 attention 的权值，这样就使得模型可以动态地利用词向量和字符向量信息。实验结果表明比原始的拼接方法效果更好。 [2]在原始 BiLSTM-CRF 模型上，加入了音韵特征，并在字符向量上使用 attention 机制来学习关注更有效的字符。 ​ — from paperweekly 1.1.2 使用少量标注数据深度学习方法一般需要大量标注数据，但是在一些领域很难有海量的标注数据。所以在基于神经网络结构方法中如何使用少量标注数据也是一个重点。 Deep Active Learning for Named Entity Recognition[7] ICLR 2018看到的paper。这片文章把active learning应用到了CNN-CNN-LSTM模型，用于处理NER问题，也就是seq labeling问题。它能够仅使用25%的数据，达到state-of-the-art的水平。 这篇paper总结了很多做seq labeling的方法，本身的思路也深入简出。decoder使用了LSTM而不是常用的CRF，发现LSTM比CRF有一些的优势。同时该文也证明了active learning能提高seq labeling的表现。 Semi-supervised sequence tagging with bidirectional language models[4] 该论文使用海量无标注语料库训练了一个双向神经网络语言模型，然后使用这个训练好的语言模型来获取当前要标注词的语言模型向量（LM embedding），然后将该向量作为特征加入到原始的双向 RNN-CRF 模型中。 实验结果表明，在少量标注数据上，加入这个语言模型向量能够大幅度提高 NER 效果，即使在大量的标注训练数据上，加入这个语言模型向量仍能提供原始 RNN-CRF 模型的效果。 1.2 Relation extraction实体的关系的抽取方法可以简单分为两类：一类是pipeline抽取方法。另一类是并行或联合抽取方法。 pipeline方法需要先识别entity，然后采用关系抽取模型得到实体对之间的关系。缺点是实体识别的结果会进一步影响关系抽取的结果，导致误差累积，也降低信息使用率，分开抽取也造成了信息冗余。 [9]提出了一种联合实体检测参数共享的关系抽取模型，模型中有两个双向的LSTM-RNN，一个是基于word sequence（bidirectional sequential LSTM-RNNs），主要用于实体检测；一个基于Tree Structures （bidirectional tree- structured LSTM-RNNs），主要用于关系抽取；后者堆在前者上，前者的输出和隐含层作为后者输入的一部分。下图为整个模型的结构图： 该paper用了参数共享，实体的识别过程和关系的判断过程并没有交互的过程，还无法称其为真正意义上的joint。 [7]提出了一种端到端的基于序列标注的的方法进行关系抽取，它将实体发现任务和关系抽取任务转化为一个标注任务。在 encoder-decoder 框架下，采用主流的 bi-lstm 为 encoder，lstm 为 decoder。对每个词标注上 BIEM+关系类型+实体的序号。目前这种思路有人测试下来发现，总的来说，联合抽取比pipeline的方法好，序列标注联合抽取要比其他联合抽取方法好，然而目前实体关系抽取任务的 F1 值仍然不到 0.5。因此虽然效果还可以，但是就实际使用还有一段距离。 此外，该模型还无法处理一个句子有多个关系三元组，和一个实体在多个关系中出现的一对多的问题。一个改进方向是把最后的softmax改成多分类器以实现多标签，这样就能实现一个实体的多关系抽取。其次，该方法是非开放域的关系抽取，关系词是从预定义的关系集里抽取的。 2. Others这里主要是有相关性不强但挺有意思，或泛用性很强的一些文章。 Ngram2vec[5] 一个词向量生成的方法，基于经典的 word2vec 的思想，在其之上加入了 ngram 的共现信息，取得了更好的结果。代码实现：https://github.com/zhezhaoa/ngram2vec/ AutoML google在五月份发布的模型，主要思想是将reinforcement learning应用在神经网络的构建、参数确定上。我们对网络进行测试，将反馈的结果返回到控制器中，以此来帮助提升下一次循环中的训练设定。生成新的架构、测试、把反馈传送给控制器以吸取经验。以此往复以得到更优的结构。 Introspection:Accelerating Neural Network Training By Learning Weight Evolution[6] 这个本质上是meta learning的问题。他们训练了一个网络，网络的输入是某个时间点之前随机选取的4个旧参数的值，输出就是新的参数。因此可以将训练其他模型时得到的这个网络，用于加速其他模型。他们训练了mnist的两层conv net，用该任务的参数更新历史训练网络。他们最后将pretrained好的这个网络用于更新大网络，结果都能更好。 Reference[1] Rei, M., Crichton, G. K., &amp; Pyysalo, S. (2016). Attending to Characters in Neural Sequence Labeling Models. arXiv preprint arXiv:1611.04361. [2] Mortensen, A. B. D., &amp; Carbonell, C. D. J. G. (2016). Phonologically aware neural model for named entity recognition in low resource transfer settings. [3] Yang, Z., Salakhutdinov, R., &amp; Cohen, W. W. (2017). Transfer learning for sequence tagging with hierarchical recurrent networks. arXiv preprint arXiv:1703.06345. [4] Peters, M. E., Ammar, W., Bhagavatula, C., &amp; Power, R. (2017). Semi-supervised sequence tagging with bidirectional language models. arXiv preprint arXiv:1705.00108. [5] Zhao, Z., Liu, T., Li, S., Li, B., &amp; Du, X. (2017). Ngram2vec: Learning Improved Word Representations from Ngram Co-occurrence Statistics. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (pp. 244-253). [6] Sinha, A., Sarkar, M., Mukherjee, A., &amp; Krishnamurthy, B. (2017). Introspection: Accelerating Neural Network Training By Learning Weight Evolution. arXiv preprint arXiv:1704.04959. [7] Shen, Yanyao, Yun, Hyokun, Lipton, Zachary C, Kronrod, Yakov, &amp; Anandkumar, Animashree. (2017). Deep active learning for named entity recognition. [8] Zheng, S., Wang, F., Bao, H., Hao, Y., Zhou, P., &amp; Xu, B. (2017). Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme. arXiv preprint arXiv:1706.05075. [9] Miwa, M., &amp; Bansal, M. (2016). End-to-end relation extraction using lstms on sequences and tree structures. arXiv preprint arXiv:1601.00770.","link":"/posts/%5B2017.12.10%5DEntity-resolution/"}],"tags":[{"name":"deep-learning","slug":"deep-learning","link":"/tags/deep-learning/"},{"name":"machine-learning","slug":"machine-learning","link":"/tags/machine-learning/"},{"name":"CNN","slug":"CNN","link":"/tags/CNN/"},{"name":"knowledge-graph","slug":"knowledge-graph","link":"/tags/knowledge-graph/"},{"name":"entity-resolution","slug":"entity-resolution","link":"/tags/entity-resolution/"},{"name":"sequence-labeling","slug":"sequence-labeling","link":"/tags/sequence-labeling/"},{"name":"relation-extraction","slug":"relation-extraction","link":"/tags/relation-extraction/"},{"name":"LSTM","slug":"LSTM","link":"/tags/LSTM/"},{"name":"RNN","slug":"RNN","link":"/tags/RNN/"},{"name":"relation-classification","slug":"relation-classification","link":"/tags/relation-classification/"},{"name":"attention","slug":"attention","link":"/tags/attention/"},{"name":"distant-supervision","slug":"distant-supervision","link":"/tags/distant-supervision/"},{"name":"event-detection","slug":"event-detection","link":"/tags/event-detection/"},{"name":"co-reference","slug":"co-reference","link":"/tags/co-reference/"},{"name":"convolution","slug":"convolution","link":"/tags/convolution/"},{"name":"BiLSTM","slug":"BiLSTM","link":"/tags/BiLSTM/"},{"name":"event-extraction","slug":"event-extraction","link":"/tags/event-extraction/"},{"name":"limited-supervision","slug":"limited-supervision","link":"/tags/limited-supervision/"},{"name":"weak-supervision","slug":"weak-supervision","link":"/tags/weak-supervision/"},{"name":"KGC","slug":"KGC","link":"/tags/KGC/"},{"name":"reinforcement-learning","slug":"reinforcement-learning","link":"/tags/reinforcement-learning/"},{"name":"neural-network","slug":"neural-network","link":"/tags/neural-network/"},{"name":"summarization","slug":"summarization","link":"/tags/summarization/"},{"name":"NLP","slug":"NLP","link":"/tags/NLP/"},{"name":"regular-expression","slug":"regular-expression","link":"/tags/regular-expression/"},{"name":"knowledge-reasoning","slug":"knowledge-reasoning","link":"/tags/knowledge-reasoning/"}],"categories":[{"name":"research","slug":"research","link":"/categories/research/"}]}