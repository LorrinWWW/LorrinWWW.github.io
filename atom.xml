<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Jue&#39;s Blog</title>
  
  <subtitle>Ph.D Student @ ZJU</subtitle>
  <link href="https://juewang.me/atom.xml" rel="self"/>
  
  <link href="https://juewang.me/"/>
  <updated>2022-09-15T03:58:40.241Z</updated>
  <id>https://juewang.me/</id>
  
  <author>
    <name>Jue Wang</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Jue Wang</title>
    <link href="https://juewang.me/posts/about/"/>
    <id>https://juewang.me/posts/about/</id>
    <published>2022-09-15T08:00:00.000Z</published>
    <updated>2022-09-15T03:58:40.241Z</updated>
    
    <content type="html"><![CDATA[<p>Hello, I am a PhD student in <a href="http://59.111.103.237:8081/">Data Intelligence Lab</a> of Zhejiang University, advised by <a href="https://person.zju.edu.cn/en/should">Prof. Lidan Shou</a>. I am visiting ETH Zurich now, where I am advised by <a href="https://ds3lab.inf.ethz.ch/members/ce-zhang.html">Prof. Ce Zhang</a>.</p><p>My research interests lie in Efficient Algorithms for NLP (both training and inference), Information Extraction, and NLP in low-resource scenarios. If you want to get in touch, please <a href="mailto:zjuwangjue@gmail.com">send me an email</a>. </p><p>My <a href="/about/resume-Jue.Wang.pdf">resume</a>. </p><h2 id="Updates"><a href="#Updates" class="headerlink" title="Updates"></a>Updates</h2><ul><li>Sep 2022: We had one paper accepted to NeurIPS 2022. Congratulation and thanks to all the collaborators!</li><li>Apr 2022: We got a paper accepted to IJCAI 2022.</li><li>Mar 2022: I had a visit to ETH Zurich.</li><li>Feb 2022: As the first author, I had one long paper accepted to ACL 2022.</li><li>Jun 2021: I graduated from <a href="https://www.centralesupelec.fr/">CentraleSupélec</a> with diplôme d’Ingénieur (master degree), cheers!</li><li>Dec 2020: As the first author, I had one long paper accepted to AAAI 2021.</li><li>Sep 2020: As the first author, I had one long paper accepted to EMNLP 2020.</li><li>Apr 2020: As the first author, I had one long paper accepted to ACL 2020.</li></ul><!---- Feb 2020: I had a remote internship at [StatNLP](https://statnlp-research.github.io/) under the guidance of [Prof. Wei Lu](https://istd.sutd.edu.sg/people/faculty/lu-wei).- Aug 2019: I was enrolled in ByteCamp hosted by [ByteDance](https://bytedance.com/en), where I mainly deal with Multimodal Classification.- July 2019: We got one demo paper accepted to SIGIR 2019. I attended the conference as the assistant presenter.- Jun 2018 to Dec 2018: I did an internship in [Rokid](https://www.rokid.com/), where I mainly deal with Spoken Language Understanding.- Jun 2017 to Aug 2018: I did an research internship in [Data Intelligence Lab](http://59.111.103.237:8081/).---><h2 id="Education"><a href="#Education" class="headerlink" title="Education"></a>Education</h2><ul><li><strong>Zhejiang University</strong>, PhD student in Computer Science (Current), Sep 2018 - Jun 2023 (Expected)</li><li><strong>Université Paris Saclay (CentraleSupélec)</strong>, Master (Engineer) in General Engineering, Sep 2016 - Jun 2018</li><li><strong>Zhejiang University</strong>, Bachelor in Electrical Engineering, Sep 2014 - Jun 2018</li></ul><h2 id="Selected-Publications"><a href="#Selected-Publications" class="headerlink" title="Selected Publications"></a>Selected Publications</h2><ul><li><strong><a href="https://arxiv.org/abs/2206.01299">Fine-tuning Language Models over Slow Networks using Activation Compression with Guarantees</a></strong><br><strong>Jue Wang</strong>$^{*}$, Binhang Yuan$^{*}$, Luka Rimanic$^{*}$, Yongjun He, Tri Dao, Beidi Chen, Christopher Re, Ce Zhang.<br>In Proc. of NeurIPS 2022.<br><a href="https://arxiv.org/abs/2206.01299">[Paper]</a> <a href="https://github.com/DS3Lab/AC-SGD">[Code]</a></li><li><strong><a href="https://aclanthology.org/2022.acl-long.503/">SkipBERT: Efficient Inference with Shallow Layer Skipping</a></strong><br><strong>Jue Wang</strong>, Ke Chen, Gang Chen, Lidan Shou, and Julian McAuley.<br>In Proc. of ACL 2022.<br><a href="https://aclanthology.org/2022.acl-long.503/">[Paper]</a> <a href="https://github.com/LorrinWWW/SkipBERT">[Code]</a></li><li><strong><a href="https://ojs.aaai.org/index.php/AAAI/article/view/17643">Effective Slot Filling via Weakly-Supervised Dual-Model Learning</a></strong><br><strong>Jue Wang</strong>, Ke Chen, Lidan Shou, Sai Wu, and Gang Chen.<br>In Proc. of AAAI 2021.<br><a href="https://ojs.aaai.org/index.php/AAAI/article/view/17643">[Paper]</a> <a href="https://github.com/LorrinWWW/weakly-supervised-slot-filling">[Code]</a> <a href="https://slideslive.com/38948796/effective-slot-filling-via-weaklysupervised-dualmodel-learning">[Video]</a></li><li><strong><a href="https://aclanthology.org/2020.emnlp-main.133/">Two are Better than One: Joint Entity and Relation Extraction with Table-Sequence Encoders</a></strong><br><strong>Jue Wang</strong> and Lu Wei.<br>In Proc. of EMNLP 2020.<br><a href="https://aclanthology.org/2020.emnlp-main.133/">[Paper]</a> <a href="https://github.com/LorrinWWW/two-are-better-than-one">[Code]</a> <a href="https://slideslive.com/38939302/two-are-better-than-one-joint-entity-and-relation-extraction-with-tablesequence-encoders">[Video]</a></li><li><strong><a href="https://aclanthology.org/2020.acl-main.525/">Pyramid: A Layered Model for Nested Named Entity Recognition</a></strong><br><strong>Jue Wang</strong>, Lidan Shou, Ke Chen, and Gang Chen.<br>In Proc. of ACL 2020.<br><a href="https://aclanthology.org/2020.acl-main.525/">[Paper]</a> <a href="https://github.com/LorrinWWW/Pyramid">[Code]</a> <a href="https://slideslive.com/38929230/pyramid-a-layered-model-for-nested-named-entity-recognition">[Video]</a></li></ul><h2 id="Contact"><a href="#Contact" class="headerlink" title="Contact"></a>Contact</h2><p>College of Computer Science and Technology, Zhejiang University</p><p>38 Zheda Rd, Xihu Qu, Hangzhou, Zhejiang, 310027</p><p>Email: <a href="mailto:zjuwangjue@gmail.com">zjuwangjue@gmail.com</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Hello, I am a PhD student in &lt;a href=&quot;http://59.111.103.237:8081/&quot;&gt;Data Intelligence Lab&lt;/a&gt; of Zhejiang University, advised by &lt;a href=&quot;</summary>
      
    
    
    
    
  </entry>
  
</feed>
