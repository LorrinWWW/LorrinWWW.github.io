<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Jue Wang</title>
  
  <subtitle>Ph.D Student @ ZJU</subtitle>
  <link href="https://juewang.me/atom.xml" rel="self"/>
  
  <link href="https://juewang.me/"/>
  <updated>2022-11-28T14:29:03.056Z</updated>
  <id>https://juewang.me/</id>
  
  <author>
    <name>Jue Wang</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Jue Wang</title>
    <link href="https://juewang.me/posts/about/"/>
    <id>https://juewang.me/posts/about/</id>
    <published>2022-11-19T08:00:00.000Z</published>
    <updated>2022-11-28T14:29:03.056Z</updated>
    
    <content type="html"><![CDATA[<p>Hello, I am a PhD student in <a href="http://59.111.103.237:8081/">Data Intelligence Lab</a> of Zhejiang University, advised by <a href="https://person.zju.edu.cn/en/should">Prof. Lidan Shou</a>. I am visiting ETH Zurich, where I am advised by <a href="https://ds3lab.inf.ethz.ch/members/ce-zhang.html">Prof. Ce Zhang</a>.</p><p>My current research interests lie in Distributed Systems and Efficient Algorithms for NLP (both training and inference). I am also interested in NLP in a broad sense, e.g. Information Extraction and NLP in low-resource scenarios. If you want to get in touch, please <a href="mailto:zjuwangjue@gmail.com">send me an email</a>. </p><p>My <a href="/about/resume-Jue.Wang.pdf">resume</a>. </p><h2 id="Updates"><a href="#Updates" class="headerlink" title="Updates"></a>Updates</h2><ul><li>Nov 2022: Check out our <a href="https://huggingface.co/spaces/togethercomputer/GPT-JT">demo of GPT-JT</a>!</li><li>Nov 2022: We had a paper accepted to AAAI 2023. Congratulation to the collaborators!</li><li>Nov 2022: Check out our <a href="https://nlp.stanford.edu/helm/current/">benchmark</a> on LLMs!</li><li>Sep 2022: We had one paper accepted to NeurIPS 2022. Congratulation and thanks to all the collaborators!</li><li>Apr 2022: We got a paper accepted to IJCAI 2022.</li><li>Mar 2022: I had a visit to ETH Zurich.</li><li>Feb 2022: As the first author, I had one long paper accepted to ACL 2022.</li><li>Jun 2021: I graduated from <a href="https://www.centralesupelec.fr/">CentraleSupélec</a> with diplôme d’Ingénieur (master degree), cheers!</li><li>Dec 2020: As the first author, I had one long paper accepted to AAAI 2021.</li><li>Sep 2020: As the first author, I had one long paper accepted to EMNLP 2020.</li><li>Apr 2020: As the first author, I had one long paper accepted to ACL 2020.</li></ul><!---- Feb 2020: I had a remote internship at [StatNLP](https://statnlp-research.github.io/) under the guidance of [Prof. Wei Lu](https://istd.sutd.edu.sg/people/faculty/lu-wei).- Aug 2019: I was enrolled in ByteCamp hosted by [ByteDance](https://bytedance.com/en), where I mainly deal with Multimodal Classification.- July 2019: We got one demo paper accepted to SIGIR 2019. I attended the conference as the assistant presenter.- Jun 2018 to Dec 2018: I did an internship in [Rokid](https://www.rokid.com/), where I mainly deal with Spoken Language Understanding.- Jun 2017 to Aug 2018: I did an research internship in [Data Intelligence Lab](http://59.111.103.237:8081/).---><h2 id="Education"><a href="#Education" class="headerlink" title="Education"></a>Education</h2><ul><li><strong>Zhejiang University</strong>, PhD student in Computer Science (Current), Sep 2018 - Jun 2023 (Expected)</li><li><strong>Université Paris Saclay (CentraleSupélec)</strong>, Master (Engineer) in General Engineering, Sep 2016 - Jun 2018</li><li><strong>Zhejiang University</strong>, Bachelor in Electrical Engineering, Sep 2014 - Jun 2018</li></ul><h2 id="Manuscripts"><a href="#Manuscripts" class="headerlink" title="Manuscripts"></a>Manuscripts</h2><ul><li><p><a href="https://arxiv.org/abs/2211.09110"><strong>Holistic Evaluation of Language Models</strong></a></p><p>Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher D. Manning, Christopher Ré, Diana Acosta-Navas, Drew A. Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, <strong>Jue Wang</strong>, Keshav Santhanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, Yuta Koreeda.</p><p><a href="https://arxiv.org/abs/2211.09110">[Paper]</a> <a href="https://github.com/stanford-crfm/helm/">[Code]</a> </p></li></ul><h2 id="Selected-Publications"><a href="#Selected-Publications" class="headerlink" title="Selected Publications"></a>Selected Publications</h2><ul><li><p><strong>Effective Continual Learning for Text Classification with Lightweight Snapshots</strong></p><p><strong>Jue WANG</strong>$^{*}$, Dajie Dong$^{*}$, Lidan Shou, Ke Chen, Gang Chen<br>To appear at AAAI 2023</p></li><li><p><strong><a href="https://arxiv.org/abs/2206.01299">Fine-tuning Language Models over Slow Networks using Activation Compression with Guarantees</a></strong><br><strong>Jue Wang</strong>$^{*}$, Binhang Yuan$^{*}$, Luka Rimanic$^{*}$, Yongjun He, Tri Dao, Beidi Chen, Christopher Re, Ce Zhang.<br>In Proc. of NeurIPS 2022.<br><a href="https://arxiv.org/abs/2206.01299">[Paper]</a> <a href="https://github.com/DS3Lab/AC-SGD">[Code]</a></p></li><li><p><strong><a href="https://aclanthology.org/2022.acl-long.503/">SkipBERT: Efficient Inference with Shallow Layer Skipping</a></strong><br><strong>Jue Wang</strong>, Ke Chen, Gang Chen, Lidan Shou, and Julian McAuley.<br>In Proc. of ACL 2022.<br><a href="https://aclanthology.org/2022.acl-long.503/">[Paper]</a> <a href="https://github.com/LorrinWWW/SkipBERT">[Code]</a></p></li><li><p><strong><a href="https://ojs.aaai.org/index.php/AAAI/article/view/17643">Effective Slot Filling via Weakly-Supervised Dual-Model Learning</a></strong><br><strong>Jue Wang</strong>, Ke Chen, Lidan Shou, Sai Wu, and Gang Chen.<br>In Proc. of AAAI 2021.<br><a href="https://ojs.aaai.org/index.php/AAAI/article/view/17643">[Paper]</a> <a href="https://github.com/LorrinWWW/weakly-supervised-slot-filling">[Code]</a> <a href="https://slideslive.com/38948796/effective-slot-filling-via-weaklysupervised-dualmodel-learning">[Video]</a></p></li><li><p><strong><a href="https://aclanthology.org/2020.emnlp-main.133/">Two are Better than One: Joint Entity and Relation Extraction with Table-Sequence Encoders</a></strong><br><strong>Jue Wang</strong> and Lu Wei.<br>In Proc. of EMNLP 2020.<br><a href="https://aclanthology.org/2020.emnlp-main.133/">[Paper]</a> <a href="https://github.com/LorrinWWW/two-are-better-than-one">[Code]</a> <a href="https://slideslive.com/38939302/two-are-better-than-one-joint-entity-and-relation-extraction-with-tablesequence-encoders">[Video]</a></p></li><li><p><strong><a href="https://aclanthology.org/2020.acl-main.525/">Pyramid: A Layered Model for Nested Named Entity Recognition</a></strong><br><strong>Jue Wang</strong>, Lidan Shou, Ke Chen, and Gang Chen.<br>In Proc. of ACL 2020.<br><a href="https://aclanthology.org/2020.acl-main.525/">[Paper]</a> <a href="https://github.com/LorrinWWW/Pyramid">[Code]</a> <a href="https://slideslive.com/38929230/pyramid-a-layered-model-for-nested-named-entity-recognition">[Video]</a></p></li></ul><h2 id="Contact"><a href="#Contact" class="headerlink" title="Contact"></a>Contact</h2><p>College of Computer Science and Technology, Zhejiang University</p><p>38 Zheda Rd, Xihu Qu, Hangzhou, Zhejiang, 310027</p><p>Email: <a href="mailto:zjuwangjue@gmail.com">zjuwangjue@gmail.com</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Hello, I am a PhD student in &lt;a href=&quot;http://59.111.103.237:8081/&quot;&gt;Data Intelligence Lab&lt;/a&gt; of Zhejiang University, advised by &lt;a href=&quot;</summary>
      
    
    
    
    
  </entry>
  
</feed>
