<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Jue&#039;s Blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Jue Wang (王珏)"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Jue Wang (王珏)"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="Effective and Efficient NLP"><meta property="og:type" content="blog"><meta property="og:title" content="Jue&#039;s Blog"><meta property="og:url" content="https://juewang.me/"><meta property="og:site_name" content="Jue&#039;s Blog"><meta property="og:description" content="Effective and Efficient NLP"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://juewang.me/img/og_image.png"><meta property="article:author" content="Jue Wang"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://juewang.me"},"headline":"Jue's Blog","image":["https://juewang.me/img/og_image.png"],"author":{"@type":"Person","name":"Jue Wang"},"publisher":{"@type":"Organization","name":"Jue's Blog","logo":{"@type":"ImageObject","url":"https://juewang.me/img/favicon.svg"}},"description":"Effective and Efficient NLP"}</script><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons/css/academicons.min.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=UA-115582186-1" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'UA-115582186-1');</script><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="Jue's Blog" type="application/atom+xml">
</head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/favicon.svg" alt="Jue&#039;s Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/posts/about/">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2018-05-10T00:00:00.000Z" title="5/10/2018, 8:00:00 AM">2018-05-10</time></span><span class="level-item">Updated&nbsp;<time dateTime="2020-11-03T03:26:05.090Z" title="11/3/2020, 11:26:05 AM">2020-11-03</time></span><span class="level-item"><a class="link-muted" href="/categories/research/">research</a></span><span class="level-item">24 minutes read (About 3557 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/posts/%5B2018.5.10%5DKnowledge-Graph-Augmented-Neural-Networks-for-NLP/">Learning beyond datasets - Knowledge Graph Augmented Neural Networks for Natural language Processing 阅读笔记</a></h1><div class="content"><h2 id="Knowledge-Graph-Augmented-Neural-Networks-for-Natural-language-Processing-阅读笔记"><a href="#Knowledge-Graph-Augmented-Neural-Networks-for-Natural-language-Processing-阅读笔记" class="headerlink" title="Knowledge Graph Augmented Neural Networks for Natural language Processing 阅读笔记"></a>Knowledge Graph Augmented Neural Networks for Natural language Processing 阅读笔记</h2><p><strong>摘要</strong>：机器学习的效果一般依赖于具体的训练数据。一些学习模型可以结合贝叶斯中的先验知识，但是这些模型不具备根据需要访问任何有组织的知识的能力。在这项工作[^1]中，我们以知识图谱（KG）的形式为NLP模型提供先验知识，使得模型取得更好的效果。我们的目标是开发一种深度学习模型，可以根据任务使用attention机制从知识图谱中提取相关的先验支持事实。为了减少attention空间，我们引入了基于卷积的模型来学习知识图谱实体和关系集的表示。提出的方法是高度可扩展的，并可应用于常用的NLP任务。使用这种方法，我们在实验中显示了文本分类性能的显着提高。我们还证明了，当深度学习模型使用知识图谱以辅助时，可以用较为少量的标记训练数据进行训练。</p>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>现在机器学习主要是针对特定任务、特定训练数据进行训练的模型。虽然transfer learning试图将学习从一个任务迁移到另一个任务，但在可扩展性方面有局限性，通常是具体地针对某个的任务。另一方面，我们知道人类具有一种内在的能力，可以根据需求从脑中获取所需的知识，并结合我们新学习的概念来解决问题。</p>
<p>这就引出了我们要在本文中讨论的问题：是否有可能设计一个学习模型，除了从培训数据集中学习外，还可以在预测时利用大量外在的知识？<img src="https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-05-03-WX20180503-190446%402x.png" alt="X20180503-190446@2"></p>
<p>我们有一个基本的想法如上图，$\mathcal{X}$是原本的输入，$\mathcal{Y}$是输出。通过知识库补充、增强$\mathcal{X}$，以得到$\mathcal{X_w}$，将两这串联，获得$\mathcal{X’}$作为新的输入。</p>
<p>这里我们知识库以知识图谱的形势呈现，主要将一个事实（fact）表现为三元组：(subject entity, relation, object entity)简记为(h,r,t)。其他关于知识图谱的介绍可以参考以前的笔记和相关的文献，这里不再赘述。</p>
<p>通常我们通过训练集来得到我们所需的模型，但是它往往缺乏world knowledge或者常识，结果往往会有偏差。例如 ：“特朗普慰问了得克萨斯州的飓风幸存者和他们的家人”，我们需要知道$\langle \text{特朗普},\text{总统},\text{美国} \rangle$和$\langle \text{得克萨斯州},\text{州},\text{美国} \rangle$才能判断这是一个政治事件。因此我们认为对于机器学习模型，除了代表ground-truth的用于训练的数据集以外，我们还可以从结构化的知识库获取相关知识，以提高整体性能。</p>
<p>因此我们提出了一个深度学习模型，可以根据需求从知识图谱中提取相关的事实，并将其也作为输入特征加以补充。特别的，当知识图谱非常大的时候，即其中的三元组数量非常大，以至于我们不可能逐一比较来提出相关信息时，我们提出了一种基于深度学习的搜寻机制，来大大提高搜寻速度，我们将在后文具体描述。</p>
<h2 id="2-Knowledge-graph-representations"><a href="#2-Knowledge-graph-representations" class="headerlink" title="2. Knowledge graph representations"></a>2. Knowledge graph representations</h2><p>实体和关系需要进行embedding以进行后续处理，目前有很多种知识图谱的表示方法，主要可以被分为：structure-based embedding，semantically-enriched embedding。</p>
<h4 id="structure-based-embedding"><a href="#structure-based-embedding" class="headerlink" title="structure-based embedding"></a>structure-based embedding</h4><p>其中包括经典的TransE以及其各种变体，它的基本假设就是$h + r = t$. 之前有一篇笔记主要介绍的就是这类知识图谱表示<a target="_blank" rel="noopener" href="https://blog.lorrin.info/posts/[2018.3.10]Several-models-for-kenowledge-graoh-representing-and-completing/">这里</a>。</p>
<h4 id="semantically-enriched-embedding"><a href="#semantically-enriched-embedding" class="headerlink" title="semantically-enriched embedding"></a>semantically-enriched embedding</h4><p>这些embedding技术学习表示KG的实体/关系及其语义信息。 神经张量网络（NTN）Socher et al [2011]是该领域的先驱工作，它使用平均词嵌入和基于张量的操作初始化实体向量。 最近涉及这个想法的作品是“联合对齐”Zhong et al。 [2015]和SSP Xiao et al。[2017]。 DKRL Xie et al [2016]是一种KG表示技术，它也保留了TransE模型简单结构的文本描述性。 预训过的word2vec被用来形成实体表示，通过一个卷积神经网络（CNN）来约束要保持的关系。</p>
<p>作者采用了DKRL，因为它强调了文本的语义描述，同时，它也继承了TransE的方法。因此我们能够通过$t = h + r$的关系来提取相关实体或关系。这大大减少了提取fact（三元组）的复杂度，因为实体关系的组合数远小于三元组的数量，因此能够让这个过程的速度更快。</p>
<h2 id="3-The-proposed-model"><a href="#3-The-proposed-model" class="headerlink" title="3. The proposed model"></a>3. The proposed model</h2><p>我们以一个文本分类模型为例，模型的参数为$\Theta$，训练集为$x$，标签为$y$，我们需要最大化下列方程：<br>$$<br>\max_{\Theta}{P(y|x, \Theta)}<br>$$</p>
<p>因此：<br>$$<br>\Theta = \arg\max_{\Theta} {\log{P(y|x, \Theta)}}<br>$$<br>这里，我们通过结合world knowledge特征$x_w$来增强监督学习过程。使用数据$x$检索world knowledge特征，使用单独的模型，其中$x_w = F(x, \Theta^{(2)})$。 因此，我们修改的目标函数为：<br>$$<br>\max_{\Theta}{P(y|x, x_w, \Theta^{(1)})}<br>$$<br>其中$\Theta = {\Theta^{(1)}, \Theta^{(2)}}$。可以获得优化的参数：<br>$$<br>\Theta = \arg\max_{\Theta} {\log{P(y|x,F(x, \Theta^{(2)}), \Theta^{(1)})}}<br>$$<br>后面的部分着重于函数F的表达，该函数使用数据x进行事实三重检索。在实验中，我们使用经过softmax的输入的LSTM Greff et al. [2015]编码作为P的形式。对于F，我们使用soft attention。</p>
<p>基于此，我们提出两种模型：A. 朴素模型；B. 基于卷积的实体和关系集群表示</p>
<h4 id="A-朴素模型"><a href="#A-朴素模型" class="headerlink" title="A. 朴素模型"></a>A. 朴素模型</h4><p>前面解释过，KG的实体和关系使用DKRL进行编码。 令$e_i \in \mathbb{R}^m$代表实体i的编码，$r_j\in \mathbb R^m$代表KG中第j个关系。 输入文本以串联的单词向量$x =(x_1,x_2,…,x_T)$的形式首先使用LSTM Greff et al. [2015]模块如下，<br>$$<br>h_t = f(x_t, h_{t-1})<br>$$<br>以及<br>$$<br>o = \frac{1}{T}\sum_{t=1}^{T}{h_t}<br>$$<br>$h_t \in \mathbb{R}^n$是LSTM的隐藏状态，f是非线性函数，T是序列长度。 然后如下形成一个上下文向量，<br>$$<br>C = ReLU(o^T W)<br>$$<br>其中，$W\in \mathbb R^n \times m$表示权重参数。设置两个同样的过程以形成两个独立的上下文向量，一个用于实体检索($C_E$)和一个用于关系检索($C_R$)。</p>
<p>由于在朴素模型中KG的事实三元组的数量是数以百万计的，所以我们分别对实体和关系空间产生注意力。 然后使用检索到的实体和关系形成事实。使用实体上下文向量的实体的注意力由下式给出：<br>$$<br>\alpha_{e_i} = \frac{\exp{C_E^T{e_i}}}{\sum_{j=0}^{|E|} \exp{C_E^T{e_j}}}<br>$$<br>同理，关系的注意力：<br>$$<br>\alpha_{r_i} = \frac{\exp{C_R^T{r_i}}}{\sum_{j=0}^{|R|} \exp{C_R^T{r_j}}}<br>$$<br><img src="https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-05-10-image-201805110007120.png" alt="mage-20180511000712"></p>
<p>图2显示了实体/关系检索的示意图。在计算出最终的实体和关系向量之后，我们希望补充事实三元组。用于实验的KG技术是DKRL，其使用TransE模型假设($h + r = t$)。因此，使用主题实体(subject entity)和关系，我们将对象实体形成为$t = e + r$。 因此，检索的事实三元组是$\mathcal F = [e，r，e + r]$，其中$F \in \mathbb R^{3m}$。 该检索到的事实信息与使用LSTM模块获得的输入x的上下文向量(C)一起连接。 最终分类标签$\mathbb y$的计算如下，<br>$$<br>\mathcal F’ = ReLU(\mathcal F^T V) \<br>\mathbb y = softmax([\mathcal F’ : C]^T U)<br>$$<br>其中，$V∈\mathbb R^{3m \times u}$和$U\in \mathbb R^{2u\times u}$是要学习的模型参数。 $\mathbb y$是预测结果，用于计算交叉熵损失。我们尽量减少训练样本的平均损失，以便使用随机梯度下降来学习各种模型参数。最后的预测$\mathbb y$现在包含了来自数据集特定信息和世界知识的信息，以帮助提高性能。在联合培训注意力机制的同时调整自己，以检索进行最终分类所需的相关事实。</p>
<h4 id="A-预训练KG检索模型"><a href="#A-预训练KG检索模型" class="headerlink" title="A+. 预训练KG检索模型"></a>A+. 预训练KG检索模型</h4><p><img src="https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-05-10-image-201805110034147.png" alt="mage-20180511003414"></p>
<p>朴素模型需要考虑整个实体/关系空间，这不是一个好的方法，因为我们观察到每个attention的容易饱和。在一起训练分类和检索模块时，模型往往会忽略KG部分，而梯度只通过分类模块进行传播。这在一定程度上是可以预料的，因为当前任务的大多数相关信息来自训练样本，只有背景辅助信息来自KG。经过几个训练阶段后，KG检索到的事实总是收敛到一个固定的向量。为了克服这个问题，我们试图单独预先训练KG检索部分。预训练的KG模型用于检索事实，然后与分类模块连接，同时，在联合训练时通过预先训练的模型可能会导致传递误差。我们推断，KG不会返回噪音，并且对于任务具有基本信息，因为单独的KG部件单独显示出显着的性能（News20为59％，SNLI为66％）。图3描述了整个训练过程。该程序解决了联合训练时KG检索部分中的梯度饱和问题。但是，attention机制必须覆盖大量实体/关系的关键问题依然存在。</p>
<h4 id="B-基于卷积的实体和关系集群表示"><a href="#B-基于卷积的实体和关系集群表示" class="headerlink" title="B. 基于卷积的实体和关系集群表示"></a>B. 基于卷积的实体和关系集群表示</h4><p>在本节中，我们提出了一种机制来减少知识图谱中需要attention的大量实体/关系。 我们通过学习类似实体/关系向量的表示来减少attention空间。</p>
<img src="https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-05-10-image-201805110044225.png" width="60%">

<p>为了聚类相似的实体/关系向量，我们使用k-means聚类，并在每个聚类中形成具有相同数量的实体/关系向量的$l$个聚类。然后使用CNN对每个cluster进行编码。k-means聚类的输出是一个实体/关系向量序列${e^T_1,e^T_2,…,e^T_q}$，其中$e_i \in\mathbb R^m$，每个聚类中的元素个数为$ q =⌈\frac{| E |}{l}⌉$。对于每个cluster，这些矢量被堆叠形成$\mathcal E$作为到CNN编码器的2D输入，其中$\mathcal E\in\mathbb R^{m\times q}$。 在寻找合适滤波器形状的实验中，我们发现使用2-D滤波器，该模型无法收敛。因此，我们推断，向量$e_i$中两个不同索引的潜在表示不应该被卷积修改。然后，我们采用一维卷积滤波器，只沿$\mathcal E$列滑动，如图4所示。沿着y轴的步长是窗口长度k，卷积层的输出表示为：<br>$$<br>\mathcal E’(i,j) = W^T[e_{i,j}, e_{i+1,j},…,e_{i+k-1, j}]^T<br>$$<br>其中，$\mathcal E’(i,j)$是输出矩阵$\mathcal E’$的第(i, j)个元素，$W\in \mathbb R^k$是卷积权重滤波器。为了减少参数空间，在卷积层之后放置一个pooling层，我们只沿y轴使用一维窗口，类似于上面提到的卷积核。我们使用了一个双层卷积网络，其步长k和最大池窗口n被调整以获得输出$\mathcal E_i\in \mathbb R^m$，其中i是聚类索引。对于关系也进行类似的聚类过程，接着对聚类实体进行编码。这样，实体和关系空间都被缩减为包含更少的元素，每个cluster都有一个元素。在形成紧凑的实体空间$E$和关系空间$R$之后，我们采用了与之前相同的步骤来形成attention，但是现在，由于梯度有效地传递并且没有被过大的空间所阻塞，所以训练更有效。此外，由于卷积架构也同时得到训练，所以attention机制并没有像以前那样通过实体和关系的巨大空间来学习。</p>
<h2 id="4-Conclusion"><a href="#4-Conclusion" class="headerlink" title="4. Conclusion"></a>4. Conclusion</h2><p>实验表明，引入KG不仅降低了深度学习模型对训练集的依赖，还显著地提高了预测结果的准确度，在数据集不够的情况下效果更佳拔群。此外，本文的方法对world knowledge的处理、embedding的方法是高度可扩展，可以应用于各种NLP任务。</p>
<h2 id="Bibliography"><a href="#Bibliography" class="headerlink" title="Bibliography"></a>Bibliography</h2><p>笔记参考 <a target="_blank" rel="noopener" href="https://blog.csdn.net/TgqDT3gGaMdkHasLZv/article/details/80118742">https://blog.csdn.net/TgqDT3gGaMdkHasLZv/article/details/80118742</a></p>
<p>[^1]: Annervaz, K. M., Somnath Basu Roy Chowdhury, and Ambedkar Dukkipati. ‘Learning beyond datasets: Knowledge Graph Augmented Neural Networks for Natural language Processing.’ arXiv preprint arXiv:1802.05930 (2018).</p>
</div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous"><a href="/page/3/">Previous</a></div><div class="pagination-next"><a href="/page/5/">Next</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link" href="/">1</a></li><li><a class="pagination-link" href="/page/2/">2</a></li><li><a class="pagination-link" href="/page/3/">3</a></li><li><a class="pagination-link is-current" href="/page/4/">4</a></li><li><a class="pagination-link" href="/page/5/">5</a></li><li><span class="pagination-ellipsis">&hellip;</span></li><li><a class="pagination-link" href="/page/16/">16</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/avatar.png" alt="Jue Wang"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Jue Wang</p><p class="is-size-6 is-block">Ph.D Student</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Hangzhou, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">16</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Category</p><a href="/categories"><p class="title">1</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">26</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/lorrinWWW" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Scholar" href="https://scholar.google.com/citations?user=PykI8xcAAAAJ&amp;hl=en"><i class="ai ai-google-scholar"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/lorrinWWW/"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com/JueWANG26088228"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/atom.xml"><i class="fas fa-rss"></i></a></div></div></div><!--!--></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/favicon.svg" alt="Jue&#039;s Blog" height="28"></a><p class="is-size-7"><span>&copy; 2022 Jue Wang</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div><div id="divmap" style="visibility:hidden; position: absolute; top: 0px"><script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&amp;w=2&amp;t=n&amp;d=I_238wU69nrKH0DIm55b1z-y84aVsLuSzQSglFoJ1ww&amp;co=ffffff&amp;ct=ffffff&amp;cmo=ffffff&amp;cmn=ffffff"></script></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>