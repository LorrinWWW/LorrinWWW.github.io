<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Category: research - Jue&#039;s Blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Jue Wang (王珏)"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Jue Wang (王珏)"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="Effective and Efficient NLP"><meta property="og:type" content="blog"><meta property="og:title" content="Jue&#039;s Blog"><meta property="og:url" content="https://juewang.me/"><meta property="og:site_name" content="Jue&#039;s Blog"><meta property="og:description" content="Effective and Efficient NLP"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://juewang.me/img/og_image.png"><meta property="article:author" content="Jue Wang"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://juewang.me"},"headline":"Jue's Blog","image":["https://juewang.me/img/og_image.png"],"author":{"@type":"Person","name":"Jue Wang"},"publisher":{"@type":"Organization","name":"Jue's Blog","logo":{"@type":"ImageObject","url":"https://juewang.me/img/favicon.svg"}},"description":"Effective and Efficient NLP"}</script><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=UA-115582186-1" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'UA-115582186-1');</script><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="Jue's Blog" type="application/atom+xml">
</head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/favicon.svg" alt="Jue&#039;s Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/categories">Categories</a></li><li class="is-active"><a href="#" aria-current="page">research</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2018-09-18T03:24:29.000Z" title="9/18/2018, 11:24:29 AM">2018-09-18</time></span><span class="level-item">Updated&nbsp;<time dateTime="2018-10-06T02:10:34.775Z" title="10/6/2018, 10:10:34 AM">2018-10-06</time></span><span class="level-item"><a class="link-muted" href="/categories/research/">research</a></span><span class="level-item">12 minutes read (About 1776 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/posts/%5B2018.9%5Dnlp-short-reviews-week-1/">nlp short reviews - week 1</a></h1><div class="content"><h2 id="9-21"><a href="#9-21" class="headerlink" title="9.21"></a>9.21</h2><h3 id="Robust-Spoken-Language-Understanding-via-Paraphrasing-arXiv-1809-06444v1-cs-CL"><a href="#Robust-Spoken-Language-Understanding-via-Paraphrasing-arXiv-1809-06444v1-cs-CL" class="headerlink" title="Robust Spoken Language Understanding via Paraphrasing. (arXiv:1809.06444v1 [cs.CL])"></a>Robust Spoken Language Understanding via Paraphrasing. (arXiv:1809.06444v1 [cs.CL])</h3><p><em><strong>Abstract:</strong></em> Learning intents and slot labels from user utterances is a fundamental step in all spoken language understanding (SLU) and dialog systems. State-of-the-art neural network based methods, after deployment, often suffer from performance degradation on encountering paraphrased utterances, and out-of-vocabulary words, rarely observed in their training set. We address this challenging problem by introducing a novel paraphrasing based SLU model which can be integrated with any existing SLU model in order to improve their overall performance. We propose two new paraphrase generators using RNN and sequence-to-sequence based neural networks, which are suitable for our application. Our experiments on existing benchmark and in house datasets demonstrate the robustness of our models to rare and complex paraphrased utterances, even under adversarial test distributions.</p>
<p><em><strong>Comment:</strong></em>  一篇利用句子改写优化SLU的文章。目前基于神经网络的state-of-the-art需要大量的语料，并且对语料库中少见的说法支持较差，因此我们寄希望于句子改写。在state-of-the-art的基础上，如果产生的结果置信度低于阈值，则尝试使用句子改写，从而让SLU模型更好地工作。具体来讲，这里我们把第一次置信度较低的“O”label替换成《?》，再将《?》填充成常见的语句。但是最后的试验未能表现出较大的进步，推测句子改写仅仅只是改变几个word，对整句的判断影响并不大；另一方面它不能对句子结构进行改写，因此提升有限。</p>
<h2 id="9-20"><a href="#9-20" class="headerlink" title="9.20"></a>9.20</h2><h3 id="User-Information-Augmented-Semantic-Frame-Parsing-using-Coarse-to-Fine-Neural-Networks-arXiv-1809-06559v1-cs-CL"><a href="#User-Information-Augmented-Semantic-Frame-Parsing-using-Coarse-to-Fine-Neural-Networks-arXiv-1809-06559v1-cs-CL" class="headerlink" title="User Information Augmented Semantic Frame Parsing using Coarse-to-Fine Neural Networks. (arXiv:1809.06559v1 [cs.CL])"></a>User Information Augmented Semantic Frame Parsing using Coarse-to-Fine Neural Networks. (arXiv:1809.06559v1 [cs.CL])</h3><p><em><strong>Abstract:</strong></em> Semantic frame parsing is a crucial component in spoken language understanding (SLU) to build spoken dialog systems. It has two main tasks: intent detection and slot filling. Although state-of-the-art approaches showed good results, they require large annotated training data and long training time. In this paper, we aim to alleviate these drawbacks for semantic frame parsing by utilizing the ubiquitous user information. We design a novel coarse-to-fine deep neural network model to incorporate prior knowledge of user information intermediately to better and quickly train a semantic frame parser. Due to the lack of benchmark dataset with real user information, we synthesize the simplest type of user information (location and time) on ATIS benchmark data. The results show that our approach leverages such simple user information to outperform state-of-the-art approaches by 0.25% for intent detection and 0.31% for slot filling using standard training data. When using smaller training data, the performance improvement on intent detection and slot filling reaches up to 1.35% and 1.20% respectively. We also show that our approach can achieve similar performance as state-of-the-art approaches by using less than 80% annotated training data. Moreover, the training time to achieve the similar performance is also reduced by over 60%.</p>
<p><em><strong>Comment:</strong></em>  工作相关。目前intent classification和slot filling的state-of-the-art是attention-based-biLSTM，这篇文章的创新点在于利用了用户的信息，并将其融合进上下文，使模型对数据的依赖减少并让结果更加准确。但是似乎提升比较有限，不过思考的方向可以参考。</p>
<h3 id="Learning-Universal-Sentence-Representations-with-Mean-Max-Attention-Autoencoder-arXiv-1809-06590v1-cs-CL"><a href="#Learning-Universal-Sentence-Representations-with-Mean-Max-Attention-Autoencoder-arXiv-1809-06590v1-cs-CL" class="headerlink" title="Learning Universal Sentence Representations with Mean-Max Attention Autoencoder. (arXiv:1809.06590v1 [cs.CL])"></a>Learning Universal Sentence Representations with Mean-Max Attention Autoencoder. (arXiv:1809.06590v1 [cs.CL])</h3><p><em><strong>Abstract:</strong></em> In order to learn universal sentence representations, previous methods focus on complex recurrent neural networks or supervised learning. In this paper, we propose a mean-max attention autoencoder (mean-max AAE) within the encoder-decoder framework. Our autoencoder rely entirely on the MultiHead self-attention mechanism to reconstruct the input sequence. In the encoding we propose a mean-max strategy that applies both mean and max pooling operations over the hidden vectors to capture diverse information of the input. To enable the information to steer the reconstruction process dynamically, the decoder performs attention over the mean-max representation. By training our model on a large collection of unlabelled data, we obtain high-quality representations of sentences. Experimental results on a broad range of 10 transfer tasks demonstrate that our model outperforms the state-of-the-art unsupervised single methods, including the classical skip-thoughts and the advanced skip-thoughts+LN model. Furthermore, compared with the traditional recurrent neural network, our mean-max AAE greatly reduce the training time.</p></div><a class="article-more button is-small is-size-7" href="/posts/%5B2018.9%5Dnlp-short-reviews-week-1/#more">Read more</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2018-08-22T05:58:00.000Z" title="8/22/2018, 1:58:00 PM">2018-08-22</time></span><span class="level-item">Updated&nbsp;<time dateTime="2020-11-03T03:26:04.399Z" title="11/3/2020, 11:26:04 AM">2020-11-03</time></span><span class="level-item"><a class="link-muted" href="/categories/research/">research</a></span><span class="level-item">5 minutes read (About 702 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/posts/%5B2018.8.22%5DRegEx-with-NN/">RegEx with NN</a></h1><div class="content"><p>这两天在实习没有太多的时间写笔记orz，正好趁着公司内部分享的时候稍微写几笔。（然而一直没发出来）</p>
<h1 id="Marrying-Up-Regexs-with-Neural-Networks"><a href="#Marrying-Up-Regexs-with-Neural-Networks" class="headerlink" title="Marrying Up Regexs with Neural Networks"></a>Marrying Up Regexs with Neural Networks</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><ul>
<li><p>正则表达式 </p>
<ul>
<li><p>简明、扼要、可调，不依赖大规模标注数据</p>
</li>
<li><p>泛化性能差，所以变体、同义词都需要人为编写</p>
</li>
</ul>
</li>
<li><p>神经网络 </p>
<ul>
<li>拟合能力强、泛化性能强  </li>
<li>需要大量标注数据，解释性差</li>
</ul>
</li>
</ul>
<p>因此工程上常常结合两个，正则解决部分cases，剩下交给统计模型，一般来说就是神经网络了。</p>
<p>那么有没有可能正则和神经网络结合起来？Bingfeng et al. 2018[^1]给出了一些思路。</p>
<h2 id="Problem-def-and-the-baselines"><a href="#Problem-def-and-the-baselines" class="headerlink" title="Problem def. and the baselines"></a>Problem def. and the baselines</h2><p>文章主要解决两个问题，intent detection和slot filling，也可以认为是classification和seq2seq的任务。这里的baselines主要有两个，正则表达式的尝试和Liu, Bing, and Ian Lane. 2016[^2]提出的attention-based rnn。思路可以见下图。</p>
<p><img src="https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-09-16-063410.png" alt="image-20180916143407395"></p>
<h2 id="Approaches"><a href="#Approaches" class="headerlink" title="Approaches"></a>Approaches</h2></div><a class="article-more button is-small is-size-7" href="/posts/%5B2018.8.22%5DRegEx-with-NN/#more">Read more</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2018-07-01T12:59:58.000Z" title="7/1/2018, 8:59:58 PM">2018-07-01</time></span><span class="level-item">Updated&nbsp;<time dateTime="2018-07-01T13:27:45.722Z" title="7/1/2018, 9:27:45 PM">2018-07-01</time></span><span class="level-item"><a class="link-muted" href="/categories/research/">research</a></span><span class="level-item">5 minutes read (About 691 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/posts/intro-about-KG/">intro-about-KG</a></h1><div class="content"><h1 id="知识图谱"><a href="#知识图谱" class="headerlink" title="知识图谱"></a>知识图谱</h1><p>之前其实陆陆续续地看了一些知识图谱相关的论文，对其的理解始终停留在比较浅显的层面，或者说一个非常不全面的状态，以至于，在实习中真的要尝试着手建立一个通用知识图谱的时候，却不知道如何下手。在实习期间打算写这篇综述来整理一下之前看到的一些琐碎的知识。</p>
<h2 id="介绍和定义"><a href="#介绍和定义" class="headerlink" title="介绍和定义"></a>介绍和定义</h2><p>知识图谱（knowledge graph）最初是又Google提出的概念，目的是确定一种面相知识的存储结构。通常我们将数据存在一个关系型数据库或是key-value型数据库，数据和数据之间的关系是通过表来定义的；当数据的关系比较复杂而且比较灵活的时候，传统数据库的表达能力和响应速度都有较大的局限性，事实上，知识就是一种关系很强的数据，而且知识千变万化，难以用一些简单的规则来预先确定表。由此，知识图谱的想法也就很自然了，知识是一种关系性很强的数据，知识的存储结构必然应该是类似于图的。</p>
<p>通常来说，在实际应用中，我们可以简单的认为知识图谱就是一个多关系图，其中我们用实体（entity）来表示节点，用关系（relation）来表示边，因此知识的表达是通过一个三元组—（实体h-关系r-实体t）来实现。</p>
<p>TODO</p>
<h2 id="知识图谱构建"><a href="#知识图谱构建" class="headerlink" title="知识图谱构建"></a>知识图谱构建</h2><p>某种程度上来说，知识图谱最困难，最需要人力的部分就是知识图谱的构建。数据来源通常有以下几种</p>
<h3 id="结构化数据"><a href="#结构化数据" class="headerlink" title="结构化数据"></a>结构化数据</h3><p>这个比较理想，但是信息一定是不完善或是滞后的，可以作为初期的构建，后期还是要自己来维护。这里略过。</p></div><a class="article-more button is-small is-size-7" href="/posts/intro-about-KG/#more">Read more</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2018-05-10T00:00:00.000Z" title="5/10/2018, 8:00:00 AM">2018-05-10</time></span><span class="level-item">Updated&nbsp;<time dateTime="2020-11-03T03:26:05.090Z" title="11/3/2020, 11:26:05 AM">2020-11-03</time></span><span class="level-item"><a class="link-muted" href="/categories/research/">research</a></span><span class="level-item">24 minutes read (About 3557 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/posts/%5B2018.5.10%5DKnowledge-Graph-Augmented-Neural-Networks-for-NLP/">Learning beyond datasets - Knowledge Graph Augmented Neural Networks for Natural language Processing 阅读笔记</a></h1><div class="content"><h2 id="Knowledge-Graph-Augmented-Neural-Networks-for-Natural-language-Processing-阅读笔记"><a href="#Knowledge-Graph-Augmented-Neural-Networks-for-Natural-language-Processing-阅读笔记" class="headerlink" title="Knowledge Graph Augmented Neural Networks for Natural language Processing 阅读笔记"></a>Knowledge Graph Augmented Neural Networks for Natural language Processing 阅读笔记</h2><p><strong>摘要</strong>：机器学习的效果一般依赖于具体的训练数据。一些学习模型可以结合贝叶斯中的先验知识，但是这些模型不具备根据需要访问任何有组织的知识的能力。在这项工作[^1]中，我们以知识图谱（KG）的形式为NLP模型提供先验知识，使得模型取得更好的效果。我们的目标是开发一种深度学习模型，可以根据任务使用attention机制从知识图谱中提取相关的先验支持事实。为了减少attention空间，我们引入了基于卷积的模型来学习知识图谱实体和关系集的表示。提出的方法是高度可扩展的，并可应用于常用的NLP任务。使用这种方法，我们在实验中显示了文本分类性能的显着提高。我们还证明了，当深度学习模型使用知识图谱以辅助时，可以用较为少量的标记训练数据进行训练。</p>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>现在机器学习主要是针对特定任务、特定训练数据进行训练的模型。虽然transfer learning试图将学习从一个任务迁移到另一个任务，但在可扩展性方面有局限性，通常是具体地针对某个的任务。另一方面，我们知道人类具有一种内在的能力，可以根据需求从脑中获取所需的知识，并结合我们新学习的概念来解决问题。</p>
<p>这就引出了我们要在本文中讨论的问题：是否有可能设计一个学习模型，除了从培训数据集中学习外，还可以在预测时利用大量外在的知识？<img src="https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-05-03-WX20180503-190446%402x.png" alt="X20180503-190446@2"></p>
<p>我们有一个基本的想法如上图，$\mathcal{X}$是原本的输入，$\mathcal{Y}$是输出。通过知识库补充、增强$\mathcal{X}$，以得到$\mathcal{X_w}$，将两这串联，获得$\mathcal{X’}$作为新的输入。</p>
<p>这里我们知识库以知识图谱的形势呈现，主要将一个事实（fact）表现为三元组：(subject entity, relation, object entity)简记为(h,r,t)。其他关于知识图谱的介绍可以参考以前的笔记和相关的文献，这里不再赘述。</p>
<p>通常我们通过训练集来得到我们所需的模型，但是它往往缺乏world knowledge或者常识，结果往往会有偏差。例如 ：“特朗普慰问了得克萨斯州的飓风幸存者和他们的家人”，我们需要知道$\langle \text{特朗普},\text{总统},\text{美国} \rangle$和$\langle \text{得克萨斯州},\text{州},\text{美国} \rangle$才能判断这是一个政治事件。因此我们认为对于机器学习模型，除了代表ground-truth的用于训练的数据集以外，我们还可以从结构化的知识库获取相关知识，以提高整体性能。</p>
<p>因此我们提出了一个深度学习模型，可以根据需求从知识图谱中提取相关的事实，并将其也作为输入特征加以补充。特别的，当知识图谱非常大的时候，即其中的三元组数量非常大，以至于我们不可能逐一比较来提出相关信息时，我们提出了一种基于深度学习的搜寻机制，来大大提高搜寻速度，我们将在后文具体描述。</p>
<h2 id="2-Knowledge-graph-representations"><a href="#2-Knowledge-graph-representations" class="headerlink" title="2. Knowledge graph representations"></a>2. Knowledge graph representations</h2></div><a class="article-more button is-small is-size-7" href="/posts/%5B2018.5.10%5DKnowledge-Graph-Augmented-Neural-Networks-for-NLP/#more">Read more</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2018-04-14T00:00:00.000Z" title="4/14/2018, 8:00:00 AM">2018-04-14</time></span><span class="level-item">Updated&nbsp;<time dateTime="2020-11-03T03:26:07.006Z" title="11/3/2020, 11:26:07 AM">2020-11-03</time></span><span class="level-item"><a class="link-muted" href="/categories/research/">research</a></span><span class="level-item">13 minutes read (About 1927 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/posts/%5B2018.4.14%5DReinforcement-Learning-for-Relation-Classification-from-Noisy-Data/">Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记</a></h1><div class="content"><h2 id="Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data-阅读笔记"><a href="#Reinforcement-Learning-for-Relation-Classification-from-Noisy-Data-阅读笔记" class="headerlink" title="Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记"></a>Reinforcement Learning for Relation Classification from Noisy Data 阅读笔记</h2><p><strong>摘要</strong>：现有关系分类方法依赖远程监督(distant supervision)，它假定提到实体对的句子都描述了这个实体对的关系。这样的方法一般在句子集合进行分类，不能识别关系和句子之间的映射，并且很大程度上受到标签噪音问题的影响。在这篇论文[^1]中，作者提出了一个从有噪声多数据的句子层次的关系分类模型。该模型有两个模块：一个实例选择器和一个关系分类器。实例选择器通过增强学习选择高质量的句子，并将选定的句子输入到关系分类器中，关系分类器进行句子级预测，并向实例选择器提供奖励。这两个模块共同训练以优化实例选择和关系分类过程。实验结果表明，我们的模型可以有效地处理数据中的噪音，并在句子级别获得更好的关系分类性能。</p>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>关系抽取是nlp领域中一个非常重要的任务，尤其在知识图谱构建等任务上。相关的工作可以参考我之前写的笔记，主要还是分为两种：传统的手工特征方法，和深度神经网络。</p>
<p>为了获得更大量的训练数据集，半监督、远程监督，甚至无监督模型被提出。半监督模型对一开始的少量数据要求较高，容易产生较大的偏差；无监督学习目前还没有比较成熟的解决方案。</p>
<p>这里主要提一下远程监督模型。远程监督模型有一个很强的假设：如果两个实体在给定的知识库中有一种关系，则包含这两个实体的所有句子都会提及该关系，实际上当然会有很大问题，会带来很多标注错误的信息。有一些解决方法就是转化为bag-level的关系标注。一个bag包含提及相同实体对的句子，但有可能描述不同的关系，如下图。</p>
<p><img src="https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-04-14-WX20180414-152317%402x.png" alt="X20180414-152317@2"></p>
<p>不过实际上还是会有问题：1. 不能处理句子级别的关系分类；2. 如果一个bag里的句子都不含知识库中的关系，即都是噪声，这样对性能会有很大影响。</p>
<p>为解决上述的两个缺陷，作者提出了实例选取器，并将其定义为一个强化学习任务。它有两个特征：1. 句子选择是一个反复试错的过程，需要从分类器中得到选取句子质量的反馈；2. 反馈在挑选结束后得到，因此是滞后的。这两点非常满足强化学习的特点。</p>
<h2 id="2-Methodology"><a href="#2-Methodology" class="headerlink" title="2. Methodology"></a>2. Methodology</h2></div><a class="article-more button is-small is-size-7" href="/posts/%5B2018.4.14%5DReinforcement-Learning-for-Relation-Classification-from-Noisy-Data/#more">Read more</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2018-04-04T00:00:00.000Z" title="4/4/2018, 8:00:00 AM">2018-04-04</time></span><span class="level-item">Updated&nbsp;<time dateTime="2020-11-03T03:26:06.064Z" title="11/3/2020, 11:26:06 AM">2020-11-03</time></span><span class="level-item"><a class="link-muted" href="/categories/research/">research</a></span><span class="level-item">19 minutes read (About 2860 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/posts/%5B2018.4.4%5DA-Neural-Model-for-Joint-Event-Detection-and-Summarization/">A Neural Model for Joint Event Detection and Summarization 阅读笔记</a></h1><div class="content"><h2 id="A-Neural-Model-for-Joint-Event-Detection-and-Summarization-阅读笔记"><a href="#A-Neural-Model-for-Joint-Event-Detection-and-Summarization-阅读笔记" class="headerlink" title="A Neural Model for Joint Event Detection and Summarization 阅读笔记"></a>A Neural Model for Joint Event Detection and Summarization 阅读笔记</h2><p><strong>摘要</strong>：Twitter事件检测旨在识别推文流中的first stories。一般认为由两个子任务组成。首先，过滤掉普通的或不相关的推文。其次，推文会被自动分类到event cluster中。传统上，尽管这两个任务之间存在相互依赖关系，它们仍被单独处理，并通过pipeline整合。另外，还有一个相关的任务是摘要，即提取能够代表event cluster的简洁摘要。这里和上个暑假看的Wang, Z.[^2]的工作比较相似。</p>
<p>在本文[^1]中，我们构建了一个joint model来筛选、聚类和摘要推文中的event。特别的，我们利用深度表示学习来对推文进行矢量化处理。Neural stacking model用于整合不同子任务的pipeline，并更好地共享前后参数。实验表明，我们提出的neural joint model比pipeline更有效。</p>
<h3 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h3><p>有文献证明了推特、微博这类体裁比起传统的媒体，对新闻事件有更快的反应速度，因此今年对于推特的事件监测也是今年的热点之一，引起了广泛关注。我们在本文主要检测一些典型的事件类别，比如地震、DDos攻击等。我们提出了一个神经网络模型，该模型监视特定事件类别的推特流，共同检测并摘要该类别下的新闻事件。</p>
<img src="https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-04-04-WX20180404-225501%402x.png" width="60%">

<p>整体的架构如上图所示，给定一个推特流，我们的模型考虑三个子任务：推文过滤，事件聚类和事件摘要。</p>
<p>典型的推特事件检测模型的核心部分是<strong>聚类</strong>，其中包括增量聚类和locality sensitive hashing。主要的想法是将同一主题的推文进行分组，以便在新推文不属于现有主题类的情况下检测到新主题。这样的聚类算法通常依赖于推特内容的特征，如TFIDF，用于测量推文之间的相似度。</p>
<p>第二个子任务是<strong>摘要</strong>，它并不直接涉及event detection，但仍然与之高度相关，因为检测到的事件群可能比较大并且包含不同程度信息的推文。 从系统功能的角度来说，对于推特事件检测系统，摘要是十分必要的，因为我们没办法直接读取事件群，只有将其抽取摘要，并将事件摘要作为输出，才能够为我们所用。</p>
<p>此外，由于大部分推文流包含普通或不相关的信息，推文<strong>过滤</strong>是我们考虑的第三个子任务。 我们的目标是根据其与潜在新事件的相关性对传入推文进行分类，以便只保留信息性推文。 过滤可以在聚类之前或之后进行。 在本文中，我们在聚类之前执行这项任务。</p></div><a class="article-more button is-small is-size-7" href="/posts/%5B2018.4.4%5DA-Neural-Model-for-Joint-Event-Detection-and-Summarization/#more">Read more</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2018-03-20T00:00:00.000Z" title="3/20/2018, 8:00:00 AM">2018-03-20</time></span><span class="level-item">Updated&nbsp;<time dateTime="2020-11-03T03:26:07.533Z" title="11/3/2020, 11:26:07 AM">2020-11-03</time></span><span class="level-item"><a class="link-muted" href="/categories/research/">research</a></span><span class="level-item">8 minutes read (About 1138 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/posts/%5B2018.3.20%5DEvent-detection/">Event detection 的几个神经网络模型</a></h1><div class="content"><h2 id="Event-detection-的几个神经网络模型"><a href="#Event-detection-的几个神经网络模型" class="headerlink" title="Event detection 的几个神经网络模型"></a>Event detection 的几个神经网络模型</h2><p><strong>摘要：</strong> 根据ace的定义，事件被分为 trigger word 和 attributes，因此 event detection 也可以被认为是 trigger word detection。目前基于神经网络的方法的思路基本大同小异，本文挑选并阐述3篇paper的主要内容，并比较其特点。</p>
<h3 id="1-Dual-CNN"><a href="#1-Dual-CNN" class="headerlink" title="1. Dual CNN"></a>1. Dual CNN</h3><p>这篇[^1]主要是对通常的CNN的改进，增加了一层语义层用以感知上下文信息。</p>
<p>整个pipline可以总结为如下图：</p>
<p><img src="https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-20-WX20180320-221054%402x.png" alt="WX20180320-221054@2x"></p>
<ol>
<li>Text Processing: 数据清洗、分词等，便于后续处理；</li>
<li>Word Vector Initialisation: 初始化词向量，包括加载 pre-trained word embedding 等；</li>
<li>Concept Extraction: 与2.并行运行，这里利用外部工具实现实体的语意概念；</li>
<li>Concept Vector Initialisation: 将实体和实体相关的概念向量化；</li>
<li>Dual-CNN Training: 这一步利用我们提出的 Dual-CNN 训练；</li>
</ol>
<h4 id="Dual-CNN"><a href="#Dual-CNN" class="headerlink" title="Dual-CNN"></a>Dual-CNN</h4><p>我们知道CNN可以用来作为分类器，因此也可以被构造为一个事件检测模型，并能够分出类别。在这个模型中，我们增加一层语意层。一般从正常逻辑出发，我们可以增加一个channel来存放entity related embedding，就像我们图像的多个channel一样；但是这要求实体和原来的句子完全对齐，因此作者用两个CNN并行训练。</p>
<p><img src="https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-20-WX20180320-224531%402x.png" alt="WX20180320-224531@2x"></p></div><a class="article-more button is-small is-size-7" href="/posts/%5B2018.3.20%5DEvent-detection/#more">Read more</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2018-03-10T00:00:00.000Z" title="3/10/2018, 8:00:00 AM">2018-03-10</time></span><span class="level-item">Updated&nbsp;<time dateTime="2020-11-03T03:26:11.105Z" title="11/3/2020, 11:26:11 AM">2020-11-03</time></span><span class="level-item"><a class="link-muted" href="/categories/research/">research</a></span><span class="level-item">12 minutes read (About 1814 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/posts/%5B2018.3.10%5DSeveral-models-for-kenowledge-graoh-representing-and-completing/">Several models for knowledge graph representing and completing 几个知识图谱模型</a></h1><div class="content"><h1 id="2018-3-10"><a href="#2018-3-10" class="headerlink" title="2018.3.10"></a>2018.3.10</h1><h2 id="Several-models-for-knowledge-graph-representing-and-completing"><a href="#Several-models-for-knowledge-graph-representing-and-completing" class="headerlink" title="Several models for knowledge graph representing and completing"></a>Several models for knowledge graph representing and completing</h2><p><strong>摘要</strong>：上次看到的ConMask在开放领域knowledge graph completion有着不错的表现，这次我们不考虑开放领域，介绍几个经典的模型。</p>
<h3 id="1-Series-of-Trans"><a href="#1-Series-of-Trans" class="headerlink" title="1. Series of Trans"></a>1. Series of Trans</h3><h4 id="1-1-TransE"><a href="#1-1-TransE" class="headerlink" title="1.1 TransE"></a>1.1 TransE</h4><p>TransE [^1] 可能是最为常用也最为基础的方法是一种基于强化学习(RL)的模型. 它有一个简单实用的假设：<br>$$<br>h+r = t<br>$$<br>其中h是head entity的向量，t是tail entity的向量，r是关系向量。</p>
<p>TransE定义了loss function：<br>$$<br>\mathcal{L(T)} = \sum_{&lt;h,r,t&gt;\in T} [\gamma + E(&lt;h,r,t&gt;) - E(&lt;h’,r’,t’&gt;)]<em>+<br>$$<br>其中 $T$ 代表一个三元组的集合；$E(&lt;h,r,t&gt;) = ||h+r-t||</em>{L_n}$是energy function；$&lt;h,r,t&gt;$是G中的一个三元组；$&lt;h’,r’,t’&gt;$代表一个不存在于 $T$ 的三元组，通过随机替换一部分$&lt;h,r,t&gt;$来得到；$\gamma$ 表示边际距离</p>
<p>算法的核心是令正例的 h+r-t 趋近于 0，而负例的 h+r-t 趋近于无穷大。整个 TransE 模型的训练过程比较简单，首先对头尾节点以及关系进行初始化，然后每对一个正例取一个负例样本，然后利用 hinge loss function 尽可能使正例和负例分开，最后采用 SGD(Stochastic Gradient Descent) 方法更新参数。</p>
<p>由TransE又衍生出了许多模型。</p>
<h4 id="1-2-TransH"><a href="#1-2-TransH" class="headerlink" title="1.2 TransH"></a>1.2 TransH</h4></div><a class="article-more button is-small is-size-7" href="/posts/%5B2018.3.10%5DSeveral-models-for-kenowledge-graoh-representing-and-completing/#more">Read more</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2018-02-26T00:00:00.000Z" title="2/26/2018, 8:00:00 AM">2018-02-26</time></span><span class="level-item">Updated&nbsp;<time dateTime="2020-11-03T03:26:12.121Z" title="11/3/2020, 11:26:12 AM">2020-11-03</time></span><span class="level-item"><a class="link-muted" href="/categories/research/">research</a></span><span class="level-item">18 minutes read (About 2761 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/posts/%5B2018.2.26%5DOpen-World-Knowledge-Graph-Completion/">Open-World Knowledge Graph Completion 笔记</a></h1><div class="content"><h2 id="Open-World-Knowledge-Graph-Completion"><a href="#Open-World-Knowledge-Graph-Completion" class="headerlink" title="Open-World Knowledge Graph Completion"></a>Open-World Knowledge Graph Completion</h2><p><strong>摘要</strong>：[1]文首先讨论了Closed-World KGC，它无法处理从 KG 外部加入的新实体，并严重依赖已有KG连接的，不能对弱连接有好的预测。为此定义了 Open-World KGC，可以接收 新的实体并链接到 KG；并依此提出了ConMask模型，在给定关系和实体名、实体描述的前提下，利用attention机制通过关系定位实体描述中最相关的词，再以这些词和实体得到要链接的实体。</p>
<h3 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h3><p>知识图谱（KG）是一种信息网络，它用三元组 $(h,r,t)$ 来表示知识（h: head entity, t: tail entity, r: relation），目前比较出名的KG有 DBPedia，ConceptNet 等，目前的大多数KG都有噪音且不完整，比如基于Wikipedia的DBPedia有460万个实体，但其中一半实体拥有少于5个的关系。</p>
<p>这说明了大部分的知识图谱仍然是非常不完善的，我们必须从一开始就要考虑系统的修改、补充完善的可能性。这项任务被定义为Knowledge Graph Completion (KGC)。</p>
<h4 id="Closed-World-KGC"><a href="#Closed-World-KGC" class="headerlink" title="Closed-World KGC"></a>Closed-World KGC</h4><p>给定一个不完整的KG $G=(E,R,T)$ 其中 $E,R,T$ 分别表示实体集，关系集以及三元组集，Closed-World KGC的任务就是通过找到一系列丢失的三元组 $ T’ = { \langle h,r,t \rangle|h \in E, r \in R, t \in E, \langle h,r,t \rangle \notin T }$ 来补充现有的 $G$.</p>
<p>一个很重要的地方在于，Closed-World KGC 假定了新的实体、关系都被原有的 $G$ 包含，对于不在 $G$ 中的实体则一筹莫展。</p>
<p>目前的Closed-World KGC方法很多往往使用TranE或者低维特征表示模型，前者的核心思想就是 $h+r=t$ ，后者则指 Embedding 等。</p>
<p>该方法仅对固定的或者缓慢更新的KG有效，对于快速变更的KG则效果一般。</p></div><a class="article-more button is-small is-size-7" href="/posts/%5B2018.2.26%5DOpen-World-Knowledge-Graph-Completion/#more">Read more</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2018-02-05T00:00:00.000Z" title="2/5/2018, 8:00:00 AM">2018-02-05</time></span><span class="level-item">Updated&nbsp;<time dateTime="2020-11-03T03:26:11.654Z" title="11/3/2020, 11:26:11 AM">2020-11-03</time></span><span class="level-item"><a class="link-muted" href="/categories/research/">research</a></span><span class="level-item">13 minutes read (About 1914 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/posts/%5B2018.2.5%5DNested-LSTMs/">Nested LSTMs 笔记</a></h1><div class="content"><h2 id="Nested-LSTMs"><a href="#Nested-LSTMs" class="headerlink" title="Nested LSTMs"></a>Nested LSTMs</h2><p><strong>摘要</strong>：最近，一种新的 Nested LSTMs 网络被提出。在通常的LSTM网络中，我们通过将LSTM单元堆叠，从而形成深度RNN网络，提高其效果；Nested LSTM则通过嵌套而不是堆栈来增添LSTM的深度。在NLSTM中，记忆单元的值是由LSTM单元计算的，其中，LSTM单元具有自身内在的记忆单元。具体而言，NLSTM记忆单元并不是按照等式：$c_t^{outer} = f_t \odot c_{t-1} + i_t \odot g_t$ 对（外部）记忆单元的值进行计算，而是使用级联：$(f_t \odot c_{t-1}, i_t \odot g_t)$ 将其作为内部LSTM（或NLSTM）记忆单元的输入，并设定 $c_t^{outer} = h_t^{inner}$。在访问内部记忆时，Nested LSTM 相比传统的堆栈 LSTM 有更高的自由度，从而能处理更长时间规模的内部记忆；实验也表明，在参数数量相似的情况下，NLSTM 在多种任务上都超越了堆栈 LSTM。作者认为Nested LSTM 有潜力直接取代堆栈 LSTM。</p>
<h3 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h3><p>学习长期的依赖关系是当前人工智能领域中，尤其是在nlp领域，机器学习方法的关键性挑战。基于循环神经网络的体系结构已经在使得机器能够模仿这种能力方面取得了显著进展。</p>
<h4 id="single-layer-LSTM"><a href="#single-layer-LSTM" class="headerlink" title="single-layer LSTM"></a>single-layer LSTM</h4><img src="https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202251.jpg" width="90%">

<p>RNN的输入是以当前的状态为依据，适合学习时间上的抽象特征。在实践中，许多专家已经证明，更为复杂的体系结构是解决许多任务的关键。其中一个原因是梯度消失问题（Hochreiter于1991年、Bengio等人于1994年提出），它使得简单的RNN难以学习长期依赖关系。Hochreiter和Schmidhuber于1997年提出了LSTM，包含能够改善梯度消失问题的记忆机制。单层LSTM如上图，图中的三个单元实际上是同一个单元，它循环地将内部的参数传递给自己。</p>
<h4 id="Stacked-LSTMs"><a href="#Stacked-LSTMs" class="headerlink" title="Stacked LSTMs"></a>Stacked LSTMs</h4><img src="https://lorrin-1251763245.cos.ap-shanghai.myqcloud.com/photo/2018-03-12-202254.jpg" width="60%">

<p>堆栈 LSTM 架构使用一系列 LSTM 一层层地堆叠在一起来处理数据，一层的输出成为下一层的输入。上图为一个两层的LSTM网络。</p></div><a class="article-more button is-small is-size-7" href="/posts/%5B2018.2.5%5DNested-LSTMs/#more">Read more</a></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous is-invisible is-hidden-mobile"><a href="/categories/research/page/0/">Previous</a></div><div class="pagination-next"><a href="/categories/research/page/2/">Next</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link is-current" href="/categories/research/">1</a></li><li><a class="pagination-link" href="/categories/research/page/2/">2</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/avatar.png" alt="Jue Wang"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Jue Wang</p><p class="is-size-6 is-block">Ph.D Student</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Hangzhou, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">56</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">7</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">72</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/lorrinWWW" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/lorrinWWW/"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com/JueWANG26088228"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/atom.xml"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-02-24T08:00:00.000Z">2022-02-24</time></p><p class="title"><a href="/posts/about/">Jue Wang</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2020-02-09T08:18:11.000Z">2020-02-09</time></p><p class="title"><a href="/posts/vps-cheatsheet/">CheatSheet for Setting up New VPS</a></p><p class="categories"><a href="/categories/other/">other</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2018-09-18T03:24:29.000Z">2018-09-18</time></p><p class="title"><a href="/posts/%5B2018.9%5Dnlp-short-reviews-week-1/">nlp short reviews - week 1</a></p><p class="categories"><a href="/categories/research/">research</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2018-08-22T05:58:00.000Z">2018-08-22</time></p><p class="title"><a href="/posts/%5B2018.8.22%5DRegEx-with-NN/">RegEx with NN</a></p><p class="categories"><a href="/categories/research/">research</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2018-07-01T12:59:58.000Z">2018-07-01</time></p><p class="title"><a href="/posts/intro-about-KG/">intro-about-KG</a></p><p class="categories"><a href="/categories/research/">research</a></p></div></article></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/francais/"><span class="level-start"><span class="level-item">francais</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/math/"><span class="level-start"><span class="level-item">math</span></span><span class="level-end"><span class="level-item tag">14</span></span></a><ul><li><a class="level is-mobile" href="/categories/math/unfinished/"><span class="level-start"><span class="level-item">unfinished</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/other/"><span class="level-start"><span class="level-item">other</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/programming/"><span class="level-start"><span class="level-item">programming</span></span><span class="level-end"><span class="level-item tag">20</span></span></a><ul><li><a class="level is-mobile" href="/categories/programming/unfinished/"><span class="level-start"><span class="level-item">unfinished</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/research/"><span class="level-start"><span class="level-item">research</span></span><span class="level-end"><span class="level-item tag">16</span></span></a></li></ul></div></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2022/02/"><span class="level-start"><span class="level-item">February 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/02/"><span class="level-start"><span class="level-item">February 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/09/"><span class="level-start"><span class="level-item">September 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/08/"><span class="level-start"><span class="level-item">August 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/07/"><span class="level-start"><span class="level-item">July 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/05/"><span class="level-start"><span class="level-item">May 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/04/"><span class="level-start"><span class="level-item">April 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/03/"><span class="level-start"><span class="level-item">March 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/02/"><span class="level-start"><span class="level-item">February 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/01/"><span class="level-start"><span class="level-item">January 2018</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2017/12/"><span class="level-start"><span class="level-item">December 2017</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2017/06/"><span class="level-start"><span class="level-item">June 2017</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2017/04/"><span class="level-start"><span class="level-item">April 2017</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2017/03/"><span class="level-start"><span class="level-item">March 2017</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2017/02/"><span class="level-start"><span class="level-item">February 2017</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2017/01/"><span class="level-start"><span class="level-item">January 2017</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2016/12/"><span class="level-start"><span class="level-item">December 2016</span></span><span class="level-end"><span class="level-item tag">16</span></span></a></li><li><a class="level is-mobile" href="/archives/2016/11/"><span class="level-start"><span class="level-item">November 2016</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Bayes/"><span class="tag">Bayes</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BiLSTM/"><span class="tag">BiLSTM</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CNN/"><span class="tag">CNN</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/EDP/"><span class="tag">EDP</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/FEM/"><span class="tag">FEM</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GAN/"><span class="tag">GAN</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Hilbert/"><span class="tag">Hilbert</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/KGC/"><span class="tag">KGC</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/LSTM/"><span class="tag">LSTM</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NLP/"><span class="tag">NLP</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/OS/"><span class="tag">OS</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/RNN/"><span class="tag">RNN</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Sobolev/"><span class="tag">Sobolev</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/aleatoire/"><span class="tag">aleatoire</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/algo/"><span class="tag">algo</span><span class="tag">7</span></a></div><div class="control"><a class="tags has-addons" href="/tags/analyse/"><span class="tag">analyse</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/attention/"><span class="tag">attention</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/chrome/"><span class="tag">chrome</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/classification/"><span class="tag">classification</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/co-reference/"><span class="tag">co-reference</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/complexity/"><span class="tag">complexity</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/compression/"><span class="tag">compression</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/convolution/"><span class="tag">convolution</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/crawl/"><span class="tag">crawl</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/data-structure/"><span class="tag">data-structure</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/datamining/"><span class="tag">datamining</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/deep-learning/"><span class="tag">deep-learning</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/distant-supervision/"><span class="tag">distant-supervision</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/docker/"><span class="tag">docker</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/entity-resolution/"><span class="tag">entity-resolution</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/event-detection/"><span class="tag">event-detection</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/event-extraction/"><span class="tag">event-extraction</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/firm/"><span class="tag">firm</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/francais/"><span class="tag">francais</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/graph/"><span class="tag">graph</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/hexo/"><span class="tag">hexo</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/kernel/"><span class="tag">kernel</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/knowledge-graph/"><span class="tag">knowledge-graph</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/knowledge-reasoning/"><span class="tag">knowledge-reasoning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/language/"><span class="tag">language</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/latex/"><span class="tag">latex</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/limited-supervision/"><span class="tag">limited-supervision</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/machine-learning/"><span class="tag">machine-learning</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/management/"><span class="tag">management</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/marked/"><span class="tag">marked</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/math/"><span class="tag">math</span><span class="tag">12</span></a></div><div class="control"><a class="tags has-addons" href="/tags/mathjax/"><span class="tag">mathjax</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/matrix/"><span class="tag">matrix</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/mongo/"><span class="tag">mongo</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/mongodb/"><span class="tag">mongodb</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/neural-network/"><span class="tag">neural-network</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/nlp/"><span class="tag">nlp</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/prediction/"><span class="tag">prediction</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/probability/"><span class="tag">probability</span><span class="tag">7</span></a></div><div class="control"><a class="tags has-addons" href="/tags/programming/"><span class="tag">programming</span><span class="tag">9</span></a></div><div class="control"><a class="tags has-addons" href="/tags/python/"><span class="tag">python</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/qualitative-induction/"><span class="tag">qualitative-induction</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/regular-expression/"><span class="tag">regular-expression</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/reinforcement-learning/"><span class="tag">reinforcement-learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/relation-classification/"><span class="tag">relation-classification</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/relation-extraction/"><span class="tag">relation-extraction</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/review/"><span class="tag">review</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/rss/"><span class="tag">rss</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/scrapy/"><span class="tag">scrapy</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/sequence-labeling/"><span class="tag">sequence-labeling</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/spider/"><span class="tag">spider</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/statistic/"><span class="tag">statistic</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/summarization/"><span class="tag">summarization</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/vector/"><span class="tag">vector</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/vps/"><span class="tag">vps</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/weak-supervision/"><span class="tag">weak-supervision</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/web/"><span class="tag">web</span><span class="tag">1</span></a></div></div></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/favicon.svg" alt="Jue&#039;s Blog" height="28"></a><p class="is-size-7"><span>&copy; 2022 Jue Wang</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div><div id="divmap" style="visibility:hidden; position: absolute; top: 0px"><script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&amp;w=2&amp;t=n&amp;d=I_238wU69nrKH0DIm55b1z-y84aVsLuSzQSglFoJ1ww&amp;co=ffffff&amp;ct=ffffff&amp;cmo=ffffff&amp;cmn=ffffff"></script></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>