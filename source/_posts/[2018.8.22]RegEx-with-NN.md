title: RegEx with NN
categories:
  - research
tags:
  - neural-network
  - regular-expression
date: 2018-08-22 13:58:00
---

这两天在实习没有太多的时间写笔记orz，正好趁着公司内部分享的时候稍微写几笔。（然而一直没发出来）

# Marrying Up Regexs with Neural Networks

## 概述

- 正则表达式 

  - 简明、扼要、可调，不依赖大规模标注数据

  - 泛化性能差，所以变体、同义词都需要人为编写

- 神经网络 
  - 拟合能力强、泛化性能强  
  - 需要大量标注数据，解释性差

因此工程上常常结合两个，正则解决部分cases，剩下交给统计模型，一般来说就是神经网络了。

那么有没有可能正则和神经网络结合起来？Bingfeng et al. 2018[^1]给出了一些思路。

## Problem def. and the baselines 

文章主要解决两个问题，intent detection和slot filling，也可以认为是classification和seq2seq的任务。这里的baselines主要有两个，正则表达式的尝试和Liu, Bing, and Ian Lane. 2016[^2]提出的attention-based rnn。思路可以见下图。

![image-20180916143407395](http://oi4yiqiop.bkt.clouddn.com/2018-09-16-063410.png)

## Approaches

文章主要在三个方面进行尝试：input level、network level、output level。

### Input level

- For intent detection, 

  two possible approach:

  - Append the embedding to all words (deprecated <= 从结果上看会导致网络过于依赖正则)
  - Append the embedding to the input of softmax layer(① in Fig(a) )

- For slot filling, 

  Embed and average the REtags into a vector fi for each word and append it to the corresponding word embedding wi (① in Fig(b) )

![image-20180916144059096](http://oi4yiqiop.bkt.clouddn.com/2018-09-16-064101.png)

### Network level

- For intent detection, 

  For each intent label k, use different attention ak , which is used to generate the sentence embedding sk   (② in Fig(a) )

  Note that a RE can also indicate that a sentence does not express intent k (negative REs), it is also necessary to set another group of attention. 


$$
  s_{k} = \sum_i {\alpha_{ki} h_i}, \quad \alpha_{ki} = \frac{\exp(h_i^T W_{a} c_k)}{\sum_i {\exp(h_i^T W_{a} c_k)}}
$$

- For slot filling, 

  The mechanism introduced for intent detection is unsuitable for slot filling.

  A simple version of the two-side attention, where all the slot labels share the same set of positive and negative attention. (② in Fig(b) )

  $$
  s_{pi} = \sum_j {\alpha_{pij} h_j}, \quad \alpha_{pij} = \frac{\exp(h_j^T W_{sp} h_i)}{\sum_j {\exp(h_j^T W_{sp} h_i)}}
  $$


![image-20180916144733081](http://oi4yiqiop.bkt.clouddn.com/2018-09-16-064735.png)

### Output level

Let $z_k$ be a 0-1 indicator of whether there is at least one matched RE that leads to target label $k$ (intent or slot label), the final logits of label k for a sentence (or a spefic word for slot filling) is:
$$
logit_k = logit_k' + w_k z_k
$$
where $logit′_k$ is the logit produced by the original NN, and $w_k$ is a trainable weight indicating the overall confidence for REs that lead to target label $k$.

## Experimental Results 

![image-20180916145004296](http://oi4yiqiop.bkt.clouddn.com/2018-09-16-065006.png) 

![image-20180916145033498](http://oi4yiqiop.bkt.clouddn.com/2018-09-16-065035.png)

## Bibliography

[^1]: Luo, Bingfeng, et al. Marrying up Regular Expressions with Neural Networks: A Case Study for Spoken Language Understanding. arXiv preprint arXiv:1805.05588 (2018). 
[^2]: Liu, Bing, and Ian Lane. Attention-based recurrent neural network models for joint intent detection and slot filling. arXiv preprint arXiv:1609.01454 (2016). 


