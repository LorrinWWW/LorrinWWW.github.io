---
title: ML CNN
date: 2016-12-14 10:34:43
categories: [programming, unfinished]
tags: [machine-learning, programming, algo, CNN]
---

笔记向

目前来说，深度学习中比较火的两大类是卷积神经网络(CNN)和递归神经网络(RNN)。

# 全连接层

全连接层一般由两个部分— 线性部分和非线性部分组成。

## 线性部分

输入：
$$
x = [x_0,x_1,...,x_n]^T
$$
输出：
$$
z = [z_0,z_1,...,z_m]^T
$$
参数为一个矩阵
$$
W_{m \times n}
$$
偏执项：
$$
b = [b_0,b_1,...,b_m]^T
$$
于是：
$$
W * x + b = z
$$

## 非线性部分

根据我们线性代数的知识，如果一系列变换都是线形变换，那么我们可以使用一次线形变换来代替之前的所有变换。也就是说，若没有非线性部分，那么**多元**神经元在这里也就没有意义了(因为只需要一层神经元就可以代替其他所有的)。

当然，非线性部分也不是为了存在而存在的，用途我们会在后面有更深的理解。

非线性部分的模型也不是唯一的，但是我们常常使用以下几种。

1. sigmoid

   记得就在前几天，在数理统计的书上也看到了这个函数— sigmoid。

   先写出它的形式：
   $$
   f(x) = \frac{1}{1+e^{-x}}
   $$
   这个函数把一个取值在R上的变量，变成了一个取值在(0, 1)上的变量。

2. 双曲正切
   $$
   f(x) = \frac{e^x - e^{-x}}{e^x+e^{-x}}
   $$
   它的取值范围是(-1,1)。可以看出，它是有正有负的，而sigmoid是全为正的。

