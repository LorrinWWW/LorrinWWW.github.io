---
title: Several models for knowledge graph representing and completing 几个知识图谱模型
date: 2018-03-10 08:00:00
categories: [research]
tags: [KGC, knowledge-graph]
---

# 2018.3.10

## Several models for knowledge graph representing and completing

**摘要**：上次看到的ConMask在开放领域knowledge graph completion有着不错的表现，这次我们不考虑开放领域，介绍几个经典的模型。

### 1. Series of Trans

#### 1.1 TransE \[1\] 

TransE \[1\] 可能是最为常用也最为基础的方法是一种基于强化学习(RL)的模型. 它有一个简单实用的假设：
$$
h+r = t
$$
其中h是head entity的向量，t是tail entity的向量，r是关系向量。

TransE定义了loss function：
$$
\mathcal{L(T)} = \sum_{<h,r,t>\in T} [\gamma + E(<h,r,t>) - E(<h',r',t'>)]_+
$$
其中 $T$ 代表一个三元组的集合；$E(<h,r,t>) = ||h+r-t||_{L_n}$是energy function；$<h,r,t>$是G中的一个三元组；$<h',r',t'>$代表一个不存在于 $T$ 的三元组，通过随机替换一部分$<h,r,t>$来得到；$\gamma$ 表示边际距离

算法的核心是令正例的 h+r-t 趋近于 0，而负例的 h+r-t 趋近于无穷大。整个 TransE 模型的训练过程比较简单，首先对头尾节点以及关系进行初始化，然后每对一个正例取一个负例样本，然后利用 hinge loss function 尽可能使正例和负例分开，最后采用 SGD(Stochastic Gradient Descent) 方法更新参数。

由TransE又衍生出了许多模型。

#### 1.2 TransH \[2\]

虽然 TransE 模型训练速度快、易于实现，但是它不能够解决多对一和一对多关系的问题。为了解决这个问题并仍然维持较低的复杂度，TransH 不再严格要求 h+r-l=0，而是只需要保证头结点和尾节点在关系平面上的投影在一条直线上，因此能够得到图中头结点向量正确的表示。

<img src="http://oi4yiqiop.bkt.clouddn.com/2018-03-12-202332.jpg">

论文另一个亮点是设计了一种负类抽样的方法，即一对多的时候，给head更多的抽样概率， 同样的多对一的时候，给tail更多抽样概率。



#### 1.3 TransR/CTrans \[3\] 

TransE和TransH都假设了实体和关系都在同一个空间内，但由于实体和关系本身的不同，简单地认为它们embedding在同一个空间内是不充分的。因此，对于**每一个r**，我们设置mapping矩阵$M_r$，使得
$$
h_{\perp} + r \simeq t_{\perp} \quad \text{where } h_{\perp} = M_{r}*h, t_{\perp} = M_{r}*t
$$
后面的方法就和TransE较为类似了。

#### 1.4 TransD \[4\]

实际上TransD是对TransR/CTrans的改进。TransR有以下缺点：

- 对于每个r，所有的实体都使用同一个mapping矩阵$M_{r}$，但是实际上对应于一个r的实体有不同的种类、特征，这么做会有一些问题；
- mapping是entity和relation之间的交互过程，mapping矩阵仅由关系决定是不合理的；
- 矩阵与向量的运算的计算比较复杂，如果在一个Knowledge graph里有较多的relation，那么就会有大量的参数，以及较高的复杂度，因此导致计算量过大，无法应用到大规模knowledge graph。

对于任意一个entity或relation，TransD定义两个向量，第一个表示entity或relation的含义，另一个用于把entity投影到relation空间（或者说用于构造mapping矩阵）。因此对于每个entity-relation pair，都有一个动态生成的唯一的mapping矩阵。此外，上述过程没有用到矩阵向量运算，而用向量的运算代替了。

<img src="http://oi4yiqiop.bkt.clouddn.com/2018-03-12-202326.jpg">

TransD大大降低计算复杂度，但仍然保持不错的效果。

#### 1.5 TransA \[5\]

前面的模型都是基于欧式距离计算，也就是认为每一维的重要性是相同的。但实际上，有一些维度能较好的区分不同的entity和relation，但也有许多不包含什么有效信息，因此甚至可以被认为是噪音，因此不同维度的重要性显然是不同的。

TransA 模型通过引入加权矩阵，赋给每一维度权重。

结果如下图，欧式距离每一维都是同等重要的，体现为圆形；而TransA体现为椭圆形，显然更符合数据的分布。

<img src="http://oi4yiqiop.bkt.clouddn.com/2018-03-12-202330.jpg">



### 2. ProjE \[6\]

\[6\]的作者就是Open-World Knowledge Graph Completion的作者，两篇文章分别被AAAI2017和AAAI2018收录，可以认为是KGC任务的 state-of-the-art。这里提一下ProjE的思路。

#### 2.1 Model Architecture

给出\<h, r, ?\>，已知h、r，要求预测 ? 。通常的做法就是把所有的候选entity都拿来打分，得分最高的就是预测的结果。为了得到这一系列打分，首先我们通过一个运算符来合并h和r，得到一个向量，然后我们把所有候选entity投影到这个向量上，随后运算得到分数。

现有的模型（包括上面提到的一系列trans）往往通过mapping矩阵来合并entity和relation，这里作者也是这么做，但是他认为目前还不需要考虑各个维度之间的相互作用，因此这个mapping矩阵应该是一个对角矩阵。所以这个合并操作可以被定义为：
$$
e \oplus r = D_e e + D_r r + b_c
$$
其中$D_e$和$D_r$就是两个对角矩阵。

由此，我们可以定义embedding映射函数：
$$
h(e, r) = g(W^c f(e \oplus r) + b_r )
$$
其中f和g是激活函数(activation function)，我们在后面定义；$W^c$是候选entity构成的矩阵。因此h就是对所有候选entity的打分矩阵。

<img src="http://oi4yiqiop.bkt.clouddn.com/2018-03-12-202328.jpg">

上图是ProjE的结构，包括两部分神经网络层，其中上半部分是合并操作，即$e \oplus r$；下半部分是映射函数，或者说打分函数，即$h(e,r)$。

#### 2.2 Loss function

我们这里分析方便只看pointwise loss function：
$$
\mathcal{L}(e, r, y) = - \sum_{i\in\{i|y_i=1\}} {log(h(e,r)_i)} - \sum_{m} {\mathbb{E}_{j \sim P_y} log(h(e,r)_j)}
$$
其中$y$是一个布尔向量，1为正例，0为反例；m个反例。pointwise ProjE 可以被看作是多类分类问题，所以我们选取 f 和 g 分别为 sigmoid 和 tanh 函数。

代码实现 https://github.com/bxshi/ProjE

## Bibliographies

笔记参考：

http://www.infosec-wiki.com/?p=175755

https://www.jiqizhixin.com/articles/2017-11-03-5

\[1\] Bordes, A., Usunier, N., Garcia-Duran, A., Weston, J., & Yakhnenko, O. (2013). Translating embeddings for modeling multi-relational data. In *Advances in neural information processing systems* (pp. 2787-2795).

\[2\] Wang, Z., Zhang, J., Feng, J., & Chen, Z. (2014, July). Knowledge Graph Embedding by Translating on Hyperplanes. In *AAAI* (Vol. 14, pp. 1112-1119).

\[3\] Lin, Y., Liu, Z., Sun, M., Liu, Y., & Zhu, X. (2015, January). Learning entity and relation embeddings for knowledge graph completion. In *AAAI* (Vol. 15, pp. 2181-2187).

\[4\] Ji, G., He, S., Xu, L., Liu, K., & Zhao, J. (2015). Knowledge graph embedding via dynamic mapping matrix. In *Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)* (Vol. 1, pp. 687-696).

\[5\] Xiao, H., Huang, M., Hao, Y., & Zhu, X. (2015). TransA: An adaptive approach for knowledge graph embedding. *arXiv preprint arXiv:1509.05490*.

\[6\] Shi, B., & Weninger, T. (2017, February). ProjE: Embedding Projection for Knowledge Graph Completion. In *AAAI* (Vol. 17, pp. 1236-1242).