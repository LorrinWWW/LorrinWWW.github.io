---
title: 数据挖掘-分类与预测
date: 2016-12-26 14:56:57
categories: [programming]
tags: [datamining, programming, classification, prediction]
hidden: true
---

# 基本知识

主要分为两个步骤：

1. 建立一个描述已知数据集类别或概念的模型。

   分类规则形式、决策树形式，或数学公式形式。

2. 利用所获得的模型进行分类操作。

   首先要对模型分类准确率进行估计。

   若其准确率可以接受，则可以使用该模型进行分类。   	

可以根据以下几条标准对各种分类方法进行比较:

1. 预测准确率，它描述(学习所获)模型能够正确预测未知对象类别或(类别)数值的能力。
2. 速度，它描述在构造和使用模型时的计算效率。 
3. 鲁棒性，它描述在数据带有噪声和有数据遗失情况下，(学习所获)模型仍能进行正确预测的能力。 
4. 可扩展性，它描述对处理大量数据并构造相应学习模型所需要的能力。
5. 易理解性，它描述学习所获模型表示的可理解程度。 在本章的后面各节，将要陆续介绍上述有关问题的实现方法。

# 基于决策树的分类

1. 决策树的生成算法

   - 基本决策树算法

     本质是一个贪心算法，自上而下分而治之。

2. 属性选择方法

   信息量
   $$
   I(s_1,…,s_m) = - \sum_{i=1}^{m}{p_i \log (p_i)}
   $$
   利用属性A划分当前样本集合所需要的信息可以计算：
   $$
   E(A) = \sum_{j=1}^{v}{\frac{s_{1j}+s_{2j}+...+s_{mj}}{s}I(s_1,…,s_m) }
   $$
   E(A)的值越小，说明其子集划分结果越纯，即越好。而对于一个子集Sj，它的信息为
   $$
   I(s_{1j},…,s_{mj}) = - \sum_{i=1}^{m}{p_{ij} \log (p_{ij})}
   $$
   这样利用属性A对当前分支结点进行相应样本集合划分所获得的信息增益是:
   $$
   Gain(A) =I(s_1,…,s_m)- E(A)
   $$
   也就是说Gain(A)被认为是利用属性A进行划分后，所获的的信息熵的减少量。

   决策树归纳算法计算每个属性的Gain，选择信息增益最大的属性，作为测试属性并由此产生相应的分支节点。

3. 树枝的修剪

   1. 事前修剪

      基本原理是设置一个阀值，当某一节点的的样本数少于阀值，则停止细分。难点在于设置一个合理的阀值。

   2. 事后修剪

      从一个充分生长的树中修建。

      可以使用基于代价成本的方法，也可以使用最短描述长度(MDL)。

      前者计算其分类错误率，若修剪枝导致分类错误率变高，则保留，否则剪枝；后者选择最简单的，无需独立的数据测试

4. 规则获取

   在已经获得了一个决策树的基础上，可以使用"if...else..."语句描述该决策树。

5. 基本决策树方法改进及其扩展性

#  贝叶斯分类方法

贝叶斯分类器是一个统计分类器，基于贝叶斯定理。

简单贝叶斯分类器的分类性能可以与决策树和神经网络相比。

简单贝叶斯分类器假设一个指定类别中各属性的取值是互相独立的，它可以帮助减少计算量。

1. 贝叶斯定理

   设X为一个类别未知的数据样本，H为某个假设，则：
   $$
   P(H|X) = \frac{P(X|H)P(H)}{P(X)}
   $$

2. 简单贝叶斯分类方法

   步骤说明如下：

   1. 每一个数据都是有一个n维特征向量X={x1,…,xn}构成，分别描述其n个属性(A1,…,An)。

   2. 若有m个不同的类别(C1,…,Cm)，分类器将未知X归属到类别Ci，当仅当
      $$
      P(C_i|X) = \max(P(C_j|X)|1\le j\le m)
      $$
      所以我们要找到最大的
      $$
      P(C_i|X) = \frac{P(X|C_i)P(C_i)}{P(X)}
      $$

   3. 要找到它，只需要P(X|Ci)P(Ci)取最大即可。由于各类别的事前概率是未知的，我们假设P(Ci)是互相相等的，这样，第二步中的式子取最大就转化为了寻找最大的P(X|Ci)

   4. 由于所含的属性比较多，直接计算P(X|Ci)的计算量还是很大的。由于简单贝叶斯分类器假设各属性独立，则：
      $$
      P(X|C_i) = \prod{P(x_k|C_i)}
      $$
      可以根据训练数据样本估算P(xk|Ci)的值。具体如下：

      - 若Ak是符号量
        $$
        P(x_k|Ci)=\frac{s_{ik}}{s_i}
        $$
        这里sik为训练样本中类别为Ci且属性Ak取vk的样本数。si为训练样本中类别为Ci的样本数。

      - 若Ak是连续量
        $$
        P(x_k|Ci)=g(x_k,\mu_{C_i},\sigma_{C_i}) = \frac{1}{\sqrt{2\pi}\sigma_{C_i}}e^{-\frac{(x-\mu_{C_i})^2}{2\sigma^2_{C_i}}}
        $$
        其中$g(x_k,\mu_{C_i},\sigma_{C_i})$ï为属性为Ak的高斯规范密度函数。

   5. 为了预测X的分类，我们通过上述方法估计各个类别的正确率，将X归属到可能性最高的类别。

3. 贝叶斯信念网络

   简单贝叶斯假设属性相互独立，从而简化计算，而实际上属性相互依赖的情况比较常见，所以又出现了贝叶斯信念网络，用于描述这种相互关联的概率分布。

   贝叶斯信念网络提供了一个图形模型来描述其中的因果关系。信念网络包括两个部分。

   - 第一部分是有向无环图

     每一个节点代表随机变量。

     每一条弧代表一个概率依赖。

     给定父节点，每个变量有条件的独立于图中非子节点。

   - 第二部分是包含所有变量的条件概率表(CPT)

     对于一个变量Z，CPT定义了P(Z|parent(Z))，由此可以得到一个表。

     例如，LunCancer的父节点是FamilyHistory和Smoker，

|             | FH, S | FH, ~S | ~FH, S | ~FH, ~S |
| ----------- | ----- | ------ | ------ | ------- |
| LungCancer  | 0.8   | 0.5    | 0.7    | 0.1     |
| ~LungCancer | 0.2   | 0.5    | 0.3    | 0.9     |


数据对象的联合概率通过如下公式计算。
$$
P(z_1,...,z_n) = \prod{P(z_i|parent(z_i))}
$$

4. 贝叶斯信念网络的学习

   学习算法步骤如下：

   1. 计算下降梯度
      $$
      \frac{\partial\ln{P_w(S)}}{\partial w_{ijk}} = \sum_{d=1}^{s}{\frac{P(Y_i=y_{ij}, U_i=u_{ik} |X_d)}{w_{ijk}}}
      $$
      ​
      左边是计算训练集合S中每个样本Xd的概率。设这一概率为p。

      由Yi和Ui表示隐含变量，p可通过样本中可观察到的变量以及标准贝叶斯网络推理计算得到。

   2. 沿梯度方向前进一小步，权重更新计算如下
      $$
      w_{ijk} \leftarrow w_{ijk} + (l)\frac{\partial\ln{P_w(S)}}{\partial w_{ijk}}
      $$
      l为学习速率代表学习步长。通常学习速率为较小的常数。

   3. 重新规格化权重

      保证wijk的取值在0～1，其和等于1.

# 神经网络分类方法

1. 多层反馈神经网络

   输入同时赋给第一层(输入层)单元，这些单元输出赋予权重，输出给第二层(隐藏层)。

   隐藏层的带权输出又作为输入再馈给另一隐层。

   最后的隐层结点带权输出馈给输出层单元，最终给出相应样本的预测输出。

   该网络是前馈的，即每一个反馈只能发送到前面的输出层或隐含层。

   它也是全连接的，即每一个层中单元均与前面一层的各单元相连接。

   只要中间隐层足够多的话，多层前馈网络中的线性阈值函数，可以充分逼近任何函数。

2. 神经网络结构

   确定结构，即：

   - 输入层的单元数
   - 隐含层的个数(和层数)
   - 每个隐含层的单元数目
   - 输出层单元数目

   对输入值规格化，一般取值在0～1.

   离散数据可以为每一个取值增加一个节点进行编码。例如A={a0,a1,a2}则我们设立三个输入单元I0, I1, I2，每个单元默认为0，若A=a0，则I1置为1.

   此外没有什么特定的规律或规则来指导隐含层的最佳单元数量，它的确定是一个不断试错的过程。

3. 后传方法

   后传方法不断地处理一个训练样本集。将处理结果与训练样本已知类别比较所获误差，修改权重，使网络输出与实际类别之间的均方差最小。权重的修改是后传的，即从前向后的。

   其收敛性不能保证。

   流程(伪代码)：

   ```python
   # 定义sum函数，以变量x对f(x)求和
   def sum(f(x), x):
       pass

   # 初始化
   init();

   while !conditions:           # 条件不满足时
       for X in samples:
           for each layer:      # 每个隐含层和输出层
               O[j] = 1 / (1 + exp( - I[j]))
               I[j] = sum(w[i][j]O[i], i) + theta[j]
           for each unit of output layer as j:    # 每个输出层单元j，计算向后传播误差
               Err[j] = O[j] * (1 - O[j]) * (T[j] - O[j])
           for each unit of hidden layer as j:    # 每个隐含层单元j，从最后一层到第一层隐含层
               Err[j] = O[j] * (1 - O[j]) * sum(Err[k] * w[i][j][k], k)
           for each w[i][j] in the network:       # network中的权重wij
               delta_w[i][j] = (l) * Err[j] * O[i]     # (l)是学习速率，取值在0～1
               w[i][j] += delta_w[i][j]
           for each theta[j] in the network:      # network中的偏差thetaij
               delta_theta[j] = (l) * Err[j]
               v[i] = theta[j] + delta_theta[j]
           
   ```

   看伪代码基本就能明白它的过程了。虽然伪代码可能写的比较迷。。。

# 基于关联的分类方法

略，后面会详细说。

# 其他分类

其他分类我也没有细看，纪录一下名字，以后有时间再看。

1. k-最邻近方法

   这个很简单，就是比较两个点的欧式距离。比较基本的分类方法，略。

2. 基于示例推理

3. 遗传算法

4. 粗糙集方法

5. 模糊集合方法

# 预测方法

1. 线形与多变量回归

   线性代数、概率统计、数值计算方法里都学过。核心是利用最小二乘法。

2. 非线性回归

   例如多项式回归
   $$
   Y = \alpha + \beta_1 X+\beta_2 X^2+\beta_3 X^3
   $$
   为了将其转化为线性形式，令：
   $$
   X_1 = X;X_2 = X^2 ; X_3 = X^3
   $$
   接下来就用最小二乘法即可。

3. 其他回归模型

   对数回归、泊松回归等

# 分类器准确性

分层k次交叉验证方法普遍应用于对分类器预测准确性的评估方面。而bagging方法和boosting方法则通过学习和组合多个(单)分类器来帮助提高整个(数据训练样本所获)分类器的预测准确性。