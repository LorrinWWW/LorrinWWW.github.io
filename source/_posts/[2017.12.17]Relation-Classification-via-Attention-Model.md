---
title: Relation Classification via Attention Model 笔记
date: 2017-12-17 08:00:00
categories: [research]
tags: [relation-classification, attention, relation-extraction]
---

## Relation Classification via Attention Model

这个笔记主要是阅读论文[1]，它的工作重点是在神经网络构成的端到端学习的关系抽取任务中加入Attention机制。作者主要通过自动学习关系句中注意力较高的部分，而引入attention机制，对反映实体关系更加重要的词语给予更大的attention，较好地提高了关系抽取的效果。

<img src="https://github.com/lawlietAi/relation-classification-via-attention-model/raw/master/acnn_structure.png" width="50%">

### 1. Attention

#### 1.1 概述

Attention机制最早是在视觉图像领域被提出来的。在NLP任务上，Bahdanau[2]等人使用类似attention的机制在机器翻译任务上将翻译和对齐同时进行。接着类似的基于attention机制的深度学习模型开始广泛应用到各种NLP任务中。

####1.2 Recurrent Models of Visual Attention 

人们在进行观察图像的时候，其实并不是一次就把整幅图像的每个位置像素都看过，大多是根据需求将注意力集中到图像的特定部分。由此，在传统的RNN上加入了attention机制，每次当前状态，都会根据前一个状态学习得到的要关注的位置和当前输入的图像，去处理注意力部分像素。可以看到应用Attention机制后，任务的复杂度被降低了很多。

#### 1.3 Attention-based RNN in NLP

[1]的成果是在机器翻译任务，一般机器翻译工作由一个Encoder和一个Decoder构成，一个典型的Seq2seq任务。Encoder将源句子进行编码，再利用Decoder将编码后的向量解码成目标语言。

我们在求注意力分配概率分布的时候，对于输入句子中任意一个单词都给出个概率，从而得到一个概率分布，再对输入句子所有单词的概率进行加权求和，得到Decoder的注意力分配。如下图。

<img src="http://oi4yiqiop.bkt.clouddn.com/2018-03-12-201155.jpg" width="30%">

另一个扩展性更好的论文是[3]，他们的工作告诉了大家attention在RNN中可以如何进行扩展。

#### 1.4 Attention-based CNN in NLP

[4]这篇论文研究的是两个CNN网络，分别处理两个句子，最后输入到分类器中处理。但是这样的模型在输入分类器前句对间是没有相互联系的，作者就想通过设计attention机制将不同cnn通道的句对联系起来。于是提出了3中在CNN中使用attention的方法。

- ABCNN-1: 在卷积前进行attention，通过attention矩阵计算出相应句对的attention feature map，然后连同原来的feature map一起输入到卷积层。
- ABCNN-2: 在池化时进行attention，通过attention对卷积后的表达重新加权，然后再进行池化.
- ABCNN-3: ABCNN-1 + ABCNN-2

### 2. Relation Classification 

<img src="https://github.com/lawlietAi/relation-classification-via-attention-model/raw/master/acnn_structure.png" width="50%">

#### 	2.1 Classification Objective

作者提出了一种距离函数，即正则化向量差的L2范数：
$$
\delta_{\theta}(S,y) = ||\frac{w^O}{|w^O|} - W_y^L||_{L^2} \\
S:\text{Sentence}, y:\text{Output relation}, w^O: \text{Network output}, W^L:\text{Relation embedding}
$$
基于此，作者定义了目标函数：
$$
\mathcal{L} = [\delta_\theta(S,y) + (1-\delta_\theta(S, \hat{y}^-))] + \beta||\theta||^2 \\
\hat{y}^- : \text{A selected incorrect relation label chosen as the one with the highest score among all i.e.} \\
\hat{y}^- = argmax_{y'\in \mathcal{Y},y'\ne y}(\delta(S, y'))
$$
目标中的两个距离分别为网络输出向量与正例和与某负例的距离，该负例是所有错误类别中与该输出最接近的。最后加上一个正则项，通过使该目标函数最小化来训练网络中的各参数，$\beta$用于控制其比重。

#### 2.2 Input Representation 

现有句子，以及两个已知的实体e1,e2：
$$
S = (w_1,w_2,...,w_n) \\
e_1 := w_p, e_2 := w_t . p,t\in [1,n], p\ne t
$$
为了得到它们的关系，我们把所有词转为词向量；并且根据每个词与实体的相对位置，也转为word position embeddings，每个词与两个实体有两个相对位置，所以得到第i个词的Embedding：
$$
w_i^M = [(w_i^d)^T, (w_{i,2}^p)^T,(w_{i,2}^p)^T]^T
$$
为了充分得到上下文的信息，再考虑大小为k的滑窗，得到最终的input representation
$$
z_i = [(w_{i - (k-1)/2}^M)^T,...,(w_{i + (k-1)/2}^M)^T]^T
$$

#### 2.3 Input Attention Mechanism

<img src="https://pic3.zhimg.com/50/v2-2399a406ad0960c422702728b6418fa3_hd.jpg" width="70%">

输入级的attention机制是设计两个关于实体对上下文相关的对角矩阵，该矩阵中各元素反映该词语与给定实体间联系的强弱，如$A_{i,i}^j=f(e_j,w_i)$反映了wi和ej之间的联系强弱，这里作者给的 f 就是内积。我们定义：
$$
\alpha_i^j = \frac{exp(A_{i,i}^j)}{\sum_{i'=1}^{n}{exp(A_{i',i}^j)}}
$$
对于j=1,2 两个相关因子，作者提出了三种处理方式:

- 平均
  $$
  r_i = z_i \frac{\alpha_i^1 + \alpha_i^2}{2}
  $$

- 串联
  $$
  r_i = [(z_i \alpha_i^1)^T, (z_i \alpha_i^2)^T]^T
  $$

- 距离
  $$
  r_i = z_i \frac{\alpha_i^1 - \alpha_i^2}{2}
  $$






最终得到$R = [r_1, r_2,…,r_n]$

#### 2.4 Convolutional Max-Pooling with Secondary Attention

将前面得到的矩阵R送入卷积核大小为dc的卷积层，卷积操作可形式化表示为:
$$
R^* = tanh(W_fR+B_f), \text{where the siaze of Wf is } d^c \times k(d^w+2d^p)
$$
然后构建一个相关性矩阵来捕获卷积层输出R*与实体关系WL之间的联系
$$
G = R^{*T}UW^L, \\U :\text{weighting matrix learnt by the network}
$$

再用softmax函数来处理相关性矩阵G，获得attention pooling matrix Ap:
$$
A_{i,j}^p = \frac{exp(G_{i,j})}{\sum_{i'=1}^n{exp(G_{i',j})}}
$$
最后用Ap与卷积层输出R*相乘，也就是加入混合中的attention，然后取出每一维度的最大值，得到网络的输出
$$
w_i^O = max_j(R^*A^p)_{i,j}
$$

### 3. 总结

从[1]中提到的结果上看，attention的表现确实是在重要的词上有更好的权重，在Sem-Eval-2010 Task 8数据集上取得了显著的效果提升。对于关系抽取来说无疑是非常大的一个进步。

但是还是有一些不足：

- 它要求实体已知，因此需要其他工作来完成实体的识别，使得一些信息的丢失以及错误累加。此时并行模型或端到端模型，同时完成实体识别可能效果会更好；
- 关系是事先定义的集合，因此更多的是对关系的分类，若能启发式地抽取关系可能会有更广的应用空间；
- 对于一些上下文没有明显帮助的隐式关系或是使用了比喻之类的修辞，较为容易出错。

这次选择读这篇文章也是想更具体地了解Attention机制，同时了解一些关系抽取的方案，它也有一个pytorch版本的[实现](https://github.com/lawlietAi/relation-classification-via-attention-model)，可以辅以参考。

## Reference

\*笔记部分参考https://zhuanlan.zhihu.com/p/22867750

[1] Wang, L., Cao, Z., Melo, G. D., & Liu, Z. (2016). Relation Classification via Multi-Level Attention CNNs. *Meeting of the Association for Computational Linguistics* (pp.1298-1307).

[2] Bahdanau, D., Cho, K., & Bengio, Y. (2014). Neural machine translation by jointly learning to align and translate. *Computer Science*.

[3] Luong, M. T., Pham, H., & Manning, C. D. (2015). Effective approaches to attention-based neural machine translation. *Computer Science*.

[4] Yin, W., Schütze, H., Xiang, B., & Zhou, B. (2015). Abcnn: attention-based convolutional neural network for modeling sentence pairs. *Computer Science*.