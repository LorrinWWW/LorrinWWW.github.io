---
title: nlp short reviews - week 1
date: 2018-09-18 11:24:29
categories: [research]
tags: [review, nlp]
---

## 9.18

### Learning to Accept New Classes without Training. (arXiv:1809.06004v1 \[cs.CL\])]

***Abstract:*** Classic supervised learning makes the closed-world assumption, meaning that classes seen in testing must have been seen in training. However, in the dynamic world, new or unseen class examples may appear constantly. A model working in such an environment must be able to reject unseen classes (not seen or used in training). If enough data is collected for the unseen classes, the system should incrementally learn to accept/classify them. This learning paradigm is called open-world learning (OWL). Existing OWL methods all need some form of re-training to accept or include the new classes in the overall model. In this paper, we propose a meta-learning approach to the problem. Its key novelty is that it only needs to train a meta-classifier, which can then continually accept new classes when they have enough labeled data for the meta-classifier to use, and also detect/reject future unseen classes. No re-training of the meta-classifier or a new overall classifier covering all old and new classes is needed. In testing, the method only uses the examples of the seen classes (including the newly added classes) on-the-fly for classification and rejection. Experimental results demonstrate the effectiveness of the new approach.

***Comment:*** 想法还是值得借鉴的，用一个meta-classifier来避免新类需要重新训练、缺少数据集等问题。但是最后的实现有点像knn，这样纯粹的非监督学习总觉得效果可能还不能达到现有监督学习。可以考虑怎么把meta-classifier加到常见的有监督学习模型中，不降低准确率的情况下，提高泛化效果。

### Events Beyond ACE: Curated Training for Events. (arXiv:1809.05576v1 \[cs.CL\])

***Abstract:*** We explore a human-driven approach to annotation, curated training (CT), in which annotation is framed as teaching the system by using interactive search to identify informative snippets of text to annotate, unlike traditional approaches which either annotate preselected text or use active learning. A trained annotator performed 80 hours of CT for the thirty event types of the NIST TAC KBP Event Argument Extraction evaluation. Combining this annotation with ACE results in a 6% reduction in error and the learning curve of CT plateaus more slowly than for full-document annotation. 3 NLP researchers performed CT for one event type and showed much sharper learning curves with all three exceeding ACE performance in less than ninety minutes, suggesting that CT can provide further benefits when the annotator deeply understands the system.

***Comment:*** 模型驱动标注，使得标注效率更高，mark。

### Extending Neural Generative Conversational Model using External Knowledge Sources. (arXiv:1809.05524v1 [cs.CL])

***Abstract:*** The use of connectionist approaches in conversational agents has been progressing rapidly due to the availability of large corpora. However current generative dialogue models often lack coherence and are content poor. This work proposes an architecture to incorporate unstructured knowledge sources to enhance the next utterance prediction in chit-chat type of generative dialogue models. We focus on Sequence-to-Sequence (Seq2Seq) conversational agents trained with the Reddit News dataset, and consider incorporating external knowledge from Wikipedia summaries as well as from the NELL knowledge base. Our experiments show faster training time and improved perplexity when leveraging external knowledge.

***Comment:*** 近期这一类利用外部知识库的paper挺多的，到时候需要用到知识图谱的时候都可以参考一下。

